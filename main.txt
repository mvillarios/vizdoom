/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 500      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 48       |
|    time_elapsed     | 17       |
|    total_timesteps  | 840      |
----------------------------------
Eval num_timesteps=1000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
Eval num_timesteps=1500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 1500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 37       |
|    time_elapsed     | 45       |
|    total_timesteps  | 1680     |
----------------------------------
Eval num_timesteps=2000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 2000     |
----------------------------------
Eval num_timesteps=2500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 2500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 34       |
|    time_elapsed     | 72       |
|    total_timesteps  | 2520     |
----------------------------------
Eval num_timesteps=3000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 3000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 38       |
|    time_elapsed     | 86       |
|    total_timesteps  | 3360     |
----------------------------------
Eval num_timesteps=3500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 3500     |
----------------------------------
Eval num_timesteps=4000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.981    |
| time/               |          |
|    total_timesteps  | 4000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 35       |
|    time_elapsed     | 118      |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=4500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 4500     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 34       |
|    time_elapsed     | 146      |
|    total_timesteps  | 5040     |
----------------------------------
Eval num_timesteps=5500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.974    |
| time/               |          |
|    total_timesteps  | 5500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 37       |
|    time_elapsed     | 157      |
|    total_timesteps  | 5880     |
----------------------------------
Eval num_timesteps=6000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.972    |
| time/               |          |
|    total_timesteps  | 6000     |
----------------------------------
Eval num_timesteps=6500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.969    |
| time/               |          |
|    total_timesteps  | 6500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 36       |
|    time_elapsed     | 184      |
|    total_timesteps  | 6720     |
----------------------------------
Eval num_timesteps=7000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 7000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 36       |
|    time_elapsed     | 200      |
|    total_timesteps  | 7408     |
----------------------------------
Eval num_timesteps=7500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.964    |
| time/               |          |
|    total_timesteps  | 7500     |
----------------------------------
Eval num_timesteps=8000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.962    |
| time/               |          |
|    total_timesteps  | 8000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 36       |
|    time_elapsed     | 227      |
|    total_timesteps  | 8248     |
----------------------------------
Eval num_timesteps=8500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.96     |
| time/               |          |
|    total_timesteps  | 8500     |
----------------------------------
Eval num_timesteps=9000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 9000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 35       |
|    time_elapsed     | 255      |
|    total_timesteps  | 9081     |
----------------------------------
Eval num_timesteps=9500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.955    |
| time/               |          |
|    total_timesteps  | 9500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 36       |
|    time_elapsed     | 268      |
|    total_timesteps  | 9921     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.953    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
Eval num_timesteps=10500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.95     |
| time/               |          |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.22e-06 |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 52       |
|    fps              | 35       |
|    time_elapsed     | 295      |
|    total_timesteps  | 10618    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.41e-06 |
|    n_updates        | 154      |
----------------------------------
Eval num_timesteps=11000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.948    |
| time/               |          |
|    total_timesteps  | 11000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.6e-05  |
|    n_updates        | 249      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 36       |
|    time_elapsed     | 310      |
|    total_timesteps  | 11392    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000178 |
|    n_updates        | 347      |
----------------------------------
Eval num_timesteps=11500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.945    |
| time/               |          |
|    total_timesteps  | 11500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.78e-06 |
|    n_updates        | 374      |
----------------------------------
Eval num_timesteps=12000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.943    |
| time/               |          |
|    total_timesteps  | 12000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-06 |
|    n_updates        | 499      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 60       |
|    fps              | 36       |
|    time_elapsed     | 338      |
|    total_timesteps  | 12232    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9e-07    |
|    n_updates        | 557      |
----------------------------------
Eval num_timesteps=12500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.941    |
| time/               |          |
|    total_timesteps  | 12500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.64e-06 |
|    n_updates        | 624      |
----------------------------------
Eval num_timesteps=13000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.938    |
| time/               |          |
|    total_timesteps  | 13000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.76e-07 |
|    n_updates        | 749      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 64       |
|    fps              | 35       |
|    time_elapsed     | 366      |
|    total_timesteps  | 13072    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34e-07 |
|    n_updates        | 767      |
----------------------------------
Eval num_timesteps=13500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.936    |
| time/               |          |
|    total_timesteps  | 13500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.2e-07  |
|    n_updates        | 874      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 68       |
|    fps              | 36       |
|    time_elapsed     | 382      |
|    total_timesteps  | 13912    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.9e-07  |
|    n_updates        | 977      |
----------------------------------
Eval num_timesteps=14000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.934    |
| time/               |          |
|    total_timesteps  | 14000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.7e-06  |
|    n_updates        | 999      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.931    |
| time/               |          |
|    episodes         | 72       |
|    fps              | 36       |
|    time_elapsed     | 394      |
|    total_timesteps  | 14472    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.08e-08 |
|    n_updates        | 1117     |
----------------------------------
Eval num_timesteps=14500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.931    |
| time/               |          |
|    total_timesteps  | 14500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25e-07 |
|    n_updates        | 1124     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.929    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.17e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 76       |
|    fps              | 36       |
|    time_elapsed     | 422      |
|    total_timesteps  | 15312    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.64e-07 |
|    n_updates        | 1327     |
----------------------------------
Eval num_timesteps=15500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.926    |
| time/               |          |
|    total_timesteps  | 15500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.34e-06 |
|    n_updates        | 1374     |
----------------------------------
Eval num_timesteps=16000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.924    |
| time/               |          |
|    total_timesteps  | 16000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.01e-06 |
|    n_updates        | 1499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 80       |
|    fps              | 35       |
|    time_elapsed     | 455      |
|    total_timesteps  | 16152    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.83e-07 |
|    n_updates        | 1537     |
----------------------------------
Eval num_timesteps=16500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.922    |
| time/               |          |
|    total_timesteps  | 16500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.65e-06 |
|    n_updates        | 1624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 84       |
|    fps              | 36       |
|    time_elapsed     | 471      |
|    total_timesteps  | 16992    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.9e-07  |
|    n_updates        | 1747     |
----------------------------------
Eval num_timesteps=17000, episode_reward=-0.16 +/- 0.23
Episode length: 203.58 +/- 32.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | -0.164   |
| rollout/            |          |
|    exploration_rate | 0.919    |
| time/               |          |
|    total_timesteps  | 17000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.6e-07  |
|    n_updates        | 1749     |
----------------------------------
New best mean reward!
Eval num_timesteps=17500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.917    |
| time/               |          |
|    total_timesteps  | 17500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-06 |
|    n_updates        | 1874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 88       |
|    fps              | 35       |
|    time_elapsed     | 499      |
|    total_timesteps  | 17635    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.93e-05 |
|    n_updates        | 1908     |
----------------------------------
Eval num_timesteps=18000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.915    |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.94e-07 |
|    n_updates        | 1999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.913    |
| time/               |          |
|    episodes         | 92       |
|    fps              | 35       |
|    time_elapsed     | 513      |
|    total_timesteps  | 18291    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1e-07  |
|    n_updates        | 2072     |
----------------------------------
Eval num_timesteps=18500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.912    |
| time/               |          |
|    total_timesteps  | 18500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.14e-08 |
|    n_updates        | 2124     |
----------------------------------
Eval num_timesteps=19000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.91     |
| time/               |          |
|    total_timesteps  | 19000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.36e-05 |
|    n_updates        | 2249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 96       |
|    fps              | 35       |
|    time_elapsed     | 541      |
|    total_timesteps  | 19131    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.29e-06 |
|    n_updates        | 2282     |
----------------------------------
Eval num_timesteps=19500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.907    |
| time/               |          |
|    total_timesteps  | 19500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.03e-06 |
|    n_updates        | 2374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0866  |
|    exploration_rate | 0.907    |
| time/               |          |
|    episodes         | 100      |
|    fps              | 35       |
|    time_elapsed     | 557      |
|    total_timesteps  | 19671    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.62e-06 |
|    n_updates        | 2417     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.905    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.58e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0748  |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 104      |
|    fps              | 35       |
|    time_elapsed     | 575      |
|    total_timesteps  | 20328    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.01e-06 |
|    n_updates        | 2581     |
----------------------------------
Eval num_timesteps=20500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.903    |
| time/               |          |
|    total_timesteps  | 20500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.67e-07 |
|    n_updates        | 2624     |
----------------------------------
Eval num_timesteps=21000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.9      |
| time/               |          |
|    total_timesteps  | 21000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.76e-07 |
|    n_updates        | 2749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0633  |
|    exploration_rate | 0.9      |
| time/               |          |
|    episodes         | 108      |
|    fps              | 34       |
|    time_elapsed     | 603      |
|    total_timesteps  | 21015    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.13e-07 |
|    n_updates        | 2753     |
----------------------------------
Eval num_timesteps=21500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.898    |
| time/               |          |
|    total_timesteps  | 21500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.09e-06 |
|    n_updates        | 2874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0633  |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 112      |
|    fps              | 35       |
|    time_elapsed     | 615      |
|    total_timesteps  | 21855    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.25e-06 |
|    n_updates        | 2963     |
----------------------------------
Eval num_timesteps=22000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 22000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.72e-06 |
|    n_updates        | 2999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | -0.0405  |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 116      |
|    fps              | 35       |
|    time_elapsed     | 631      |
|    total_timesteps  | 22422    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.87e-07 |
|    n_updates        | 3105     |
----------------------------------
Eval num_timesteps=22500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.893    |
| time/               |          |
|    total_timesteps  | 22500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.32e-07 |
|    n_updates        | 3124     |
----------------------------------
Eval num_timesteps=23000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.891    |
| time/               |          |
|    total_timesteps  | 23000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000174 |
|    n_updates        | 3249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | -0.0405  |
|    exploration_rate | 0.89     |
| time/               |          |
|    episodes         | 120      |
|    fps              | 35       |
|    time_elapsed     | 659      |
|    total_timesteps  | 23262    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.83e-06 |
|    n_updates        | 3315     |
----------------------------------
Eval num_timesteps=23500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.888    |
| time/               |          |
|    total_timesteps  | 23500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.83e-08 |
|    n_updates        | 3374     |
----------------------------------
Eval num_timesteps=24000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.886    |
| time/               |          |
|    total_timesteps  | 24000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21e-06 |
|    n_updates        | 3499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | -0.0405  |
|    exploration_rate | 0.886    |
| time/               |          |
|    episodes         | 124      |
|    fps              | 35       |
|    time_elapsed     | 686      |
|    total_timesteps  | 24102    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28e-05 |
|    n_updates        | 3525     |
----------------------------------
Eval num_timesteps=24500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 24500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55e-05 |
|    n_updates        | 3624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | -0.0405  |
|    exploration_rate | 0.882    |
| time/               |          |
|    episodes         | 128      |
|    fps              | 35       |
|    time_elapsed     | 701      |
|    total_timesteps  | 24942    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.23e-07 |
|    n_updates        | 3735     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.881    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.63e-07 |
|    n_updates        | 3749     |
----------------------------------
Eval num_timesteps=25500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.879    |
| time/               |          |
|    total_timesteps  | 25500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01e-06 |
|    n_updates        | 3874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | -0.0405  |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 132      |
|    fps              | 35       |
|    time_elapsed     | 729      |
|    total_timesteps  | 25782    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-05 |
|    n_updates        | 3945     |
----------------------------------
Eval num_timesteps=26000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.877    |
| time/               |          |
|    total_timesteps  | 26000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23e-05 |
|    n_updates        | 3999     |
----------------------------------
New best mean reward!
Eval num_timesteps=26500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.874    |
| time/               |          |
|    total_timesteps  | 26500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.09e-07 |
|    n_updates        | 4124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.0621  |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 136      |
|    fps              | 35       |
|    time_elapsed     | 758      |
|    total_timesteps  | 26622    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1e-06    |
|    n_updates        | 4155     |
----------------------------------
Eval num_timesteps=27000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.872    |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.01e-06 |
|    n_updates        | 4249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.0621  |
|    exploration_rate | 0.87     |
| time/               |          |
|    episodes         | 140      |
|    fps              | 35       |
|    time_elapsed     | 772      |
|    total_timesteps  | 27462    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88e-05 |
|    n_updates        | 4365     |
----------------------------------
Eval num_timesteps=27500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.869    |
| time/               |          |
|    total_timesteps  | 27500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.19e-06 |
|    n_updates        | 4374     |
----------------------------------
Eval num_timesteps=28000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.867    |
| time/               |          |
|    total_timesteps  | 28000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.48e-07 |
|    n_updates        | 4499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.0722  |
|    exploration_rate | 0.866    |
| time/               |          |
|    episodes         | 144      |
|    fps              | 35       |
|    time_elapsed     | 799      |
|    total_timesteps  | 28302    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.36e-06 |
|    n_updates        | 4575     |
----------------------------------
Eval num_timesteps=28500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.865    |
| time/               |          |
|    total_timesteps  | 28500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.91e-07 |
|    n_updates        | 4624     |
----------------------------------
Eval num_timesteps=29000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.862    |
| time/               |          |
|    total_timesteps  | 29000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000147 |
|    n_updates        | 4749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.0722  |
|    exploration_rate | 0.862    |
| time/               |          |
|    episodes         | 148      |
|    fps              | 35       |
|    time_elapsed     | 829      |
|    total_timesteps  | 29142    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.79e-07 |
|    n_updates        | 4785     |
----------------------------------
Eval num_timesteps=29500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.86     |
| time/               |          |
|    total_timesteps  | 29500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000157 |
|    n_updates        | 4874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0836  |
|    exploration_rate | 0.858    |
| time/               |          |
|    episodes         | 152      |
|    fps              | 35       |
|    time_elapsed     | 848      |
|    total_timesteps  | 29982    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.69e-07 |
|    n_updates        | 4995     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.858    |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000165 |
|    n_updates        | 4999     |
----------------------------------
Eval num_timesteps=30500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.855    |
| time/               |          |
|    total_timesteps  | 30500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.96e-07 |
|    n_updates        | 5124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0943  |
|    exploration_rate | 0.854    |
| time/               |          |
|    episodes         | 156      |
|    fps              | 35       |
|    time_elapsed     | 876      |
|    total_timesteps  | 30822    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000159 |
|    n_updates        | 5205     |
----------------------------------
Eval num_timesteps=31000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.853    |
| time/               |          |
|    total_timesteps  | 31000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.28e-07 |
|    n_updates        | 5249     |
----------------------------------
Eval num_timesteps=31500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.85     |
| time/               |          |
|    total_timesteps  | 31500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.92e-07 |
|    n_updates        | 5374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0831  |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 160      |
|    fps              | 34       |
|    time_elapsed     | 903      |
|    total_timesteps  | 31542    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87e-05 |
|    n_updates        | 5385     |
----------------------------------
Eval num_timesteps=32000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.848    |
| time/               |          |
|    total_timesteps  | 32000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.71e-06 |
|    n_updates        | 5499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.0724  |
|    exploration_rate | 0.846    |
| time/               |          |
|    episodes         | 164      |
|    fps              | 35       |
|    time_elapsed     | 921      |
|    total_timesteps  | 32316    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000161 |
|    n_updates        | 5578     |
----------------------------------
Eval num_timesteps=32500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.846    |
| time/               |          |
|    total_timesteps  | 32500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.63e-06 |
|    n_updates        | 5624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | -0.0508  |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 168      |
|    fps              | 35       |
|    time_elapsed     | 933      |
|    total_timesteps  | 32996    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000162 |
|    n_updates        | 5748     |
----------------------------------
Eval num_timesteps=33000, episode_reward=-0.14 +/- 0.29
Episode length: 197.72 +/- 48.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.843    |
| time/               |          |
|    total_timesteps  | 33000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00016  |
|    n_updates        | 5749     |
----------------------------------
New best mean reward!
Eval num_timesteps=33500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.841    |
| time/               |          |
|    total_timesteps  | 33500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.61e-08 |
|    n_updates        | 5874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0736  |
|    exploration_rate | 0.839    |
| time/               |          |
|    episodes         | 172      |
|    fps              | 35       |
|    time_elapsed     | 962      |
|    total_timesteps  | 33836    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.78e-07 |
|    n_updates        | 5958     |
----------------------------------
Eval num_timesteps=34000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.839    |
| time/               |          |
|    total_timesteps  | 34000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000152 |
|    n_updates        | 5999     |
----------------------------------
Eval num_timesteps=34500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.836    |
| time/               |          |
|    total_timesteps  | 34500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.3e-06  |
|    n_updates        | 6124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0736  |
|    exploration_rate | 0.835    |
| time/               |          |
|    episodes         | 176      |
|    fps              | 34       |
|    time_elapsed     | 992      |
|    total_timesteps  | 34676    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.93e-06 |
|    n_updates        | 6168     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.834    |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.92e-06 |
|    n_updates        | 6249     |
----------------------------------
Eval num_timesteps=35500, episode_reward=-0.14 +/- 0.28
Episode length: 198.04 +/- 47.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.831    |
| time/               |          |
|    total_timesteps  | 35500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.1e-07  |
|    n_updates        | 6374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0634  |
|    exploration_rate | 0.831    |
| time/               |          |
|    episodes         | 180      |
|    fps              | 34       |
|    time_elapsed     | 1020     |
|    total_timesteps  | 35501    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.05e-07 |
|    n_updates        | 6375     |
----------------------------------
Eval num_timesteps=36000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.829    |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.98e-06 |
|    n_updates        | 6499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0634  |
|    exploration_rate | 0.827    |
| time/               |          |
|    episodes         | 184      |
|    fps              | 35       |
|    time_elapsed     | 1036     |
|    total_timesteps  | 36341    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.13e-07 |
|    n_updates        | 6585     |
----------------------------------
Eval num_timesteps=36500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.827    |
| time/               |          |
|    total_timesteps  | 36500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.68e-07 |
|    n_updates        | 6624     |
----------------------------------
Eval num_timesteps=37000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.824    |
| time/               |          |
|    total_timesteps  | 37000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.92e-07 |
|    n_updates        | 6749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0754  |
|    exploration_rate | 0.823    |
| time/               |          |
|    episodes         | 188      |
|    fps              | 34       |
|    time_elapsed     | 1065     |
|    total_timesteps  | 37181    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.61e-07 |
|    n_updates        | 6795     |
----------------------------------
Eval num_timesteps=37500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.822    |
| time/               |          |
|    total_timesteps  | 37500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0129   |
|    n_updates        | 6874     |
----------------------------------
Eval num_timesteps=38000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.82     |
| time/               |          |
|    total_timesteps  | 38000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.94e-07 |
|    n_updates        | 6999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0772  |
|    exploration_rate | 0.819    |
| time/               |          |
|    episodes         | 192      |
|    fps              | 34       |
|    time_elapsed     | 1094     |
|    total_timesteps  | 38017    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1e-07  |
|    n_updates        | 7004     |
----------------------------------
Eval num_timesteps=38500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.817    |
| time/               |          |
|    total_timesteps  | 38500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-07 |
|    n_updates        | 7124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0772  |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 196      |
|    fps              | 34       |
|    time_elapsed     | 1111     |
|    total_timesteps  | 38857    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.07e-05 |
|    n_updates        | 7214     |
----------------------------------
Eval num_timesteps=39000, episode_reward=-0.16 +/- 0.24
Episode length: 201.86 +/- 39.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.815    |
| time/               |          |
|    total_timesteps  | 39000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6e-06    |
|    n_updates        | 7249     |
----------------------------------
Eval num_timesteps=39500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.812    |
| time/               |          |
|    total_timesteps  | 39500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000128 |
|    n_updates        | 7374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.811    |
| time/               |          |
|    episodes         | 200      |
|    fps              | 34       |
|    time_elapsed     | 1139     |
|    total_timesteps  | 39697    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.99e-07 |
|    n_updates        | 7424     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.81     |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.2e-06  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.808    |
| time/               |          |
|    episodes         | 204      |
|    fps              | 35       |
|    time_elapsed     | 1154     |
|    total_timesteps  | 40448    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.74e-07 |
|    n_updates        | 7611     |
----------------------------------
Eval num_timesteps=40500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.808    |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1e-06    |
|    n_updates        | 7624     |
----------------------------------
Eval num_timesteps=41000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.805    |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2e-06  |
|    n_updates        | 7749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.804    |
| time/               |          |
|    episodes         | 208      |
|    fps              | 34       |
|    time_elapsed     | 1182     |
|    total_timesteps  | 41288    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000146 |
|    n_updates        | 7821     |
----------------------------------
Eval num_timesteps=41500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.803    |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000146 |
|    n_updates        | 7874     |
----------------------------------
Eval num_timesteps=42000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.801    |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000143 |
|    n_updates        | 7999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.8      |
| time/               |          |
|    episodes         | 212      |
|    fps              | 34       |
|    time_elapsed     | 1210     |
|    total_timesteps  | 42128    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.71e-05 |
|    n_updates        | 8031     |
----------------------------------
Eval num_timesteps=42500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.798    |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.49e-07 |
|    n_updates        | 8124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.796    |
| time/               |          |
|    episodes         | 216      |
|    fps              | 35       |
|    time_elapsed     | 1223     |
|    total_timesteps  | 42968    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.79e-06 |
|    n_updates        | 8241     |
----------------------------------
Eval num_timesteps=43000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.796    |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.46e-05 |
|    n_updates        | 8249     |
----------------------------------
Eval num_timesteps=43500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.793    |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.66e-06 |
|    n_updates        | 8374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.792    |
| time/               |          |
|    episodes         | 220      |
|    fps              | 34       |
|    time_elapsed     | 1255     |
|    total_timesteps  | 43808    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.7e-07  |
|    n_updates        | 8451     |
----------------------------------
Eval num_timesteps=44000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.791    |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.4e-07  |
|    n_updates        | 8499     |
----------------------------------
Eval num_timesteps=44500, episode_reward=-0.14 +/- 0.29
Episode length: 197.68 +/- 48.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.789    |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.17e-06 |
|    n_updates        | 8624     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.788    |
| time/               |          |
|    episodes         | 224      |
|    fps              | 34       |
|    time_elapsed     | 1283     |
|    total_timesteps  | 44643    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.43e-06 |
|    n_updates        | 8660     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.786    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.75e-07 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.785    |
| time/               |          |
|    episodes         | 228      |
|    fps              | 34       |
|    time_elapsed     | 1299     |
|    total_timesteps  | 45330    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.14e-06 |
|    n_updates        | 8832     |
----------------------------------
Eval num_timesteps=45500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.784    |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.17e-06 |
|    n_updates        | 8874     |
----------------------------------
Eval num_timesteps=46000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.782    |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.58e-05 |
|    n_updates        | 8999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.781    |
| time/               |          |
|    episodes         | 232      |
|    fps              | 34       |
|    time_elapsed     | 1327     |
|    total_timesteps  | 46128    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59e-06 |
|    n_updates        | 9031     |
----------------------------------
Eval num_timesteps=46500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.779    |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.73e-07 |
|    n_updates        | 9124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.0815  |
|    exploration_rate | 0.778    |
| time/               |          |
|    episodes         | 236      |
|    fps              | 34       |
|    time_elapsed     | 1342     |
|    total_timesteps  | 46782    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.13e-06 |
|    n_updates        | 9195     |
----------------------------------
Eval num_timesteps=47000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.777    |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-06 |
|    n_updates        | 9249     |
----------------------------------
Eval num_timesteps=47500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.774    |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96e-06 |
|    n_updates        | 9374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.0815  |
|    exploration_rate | 0.774    |
| time/               |          |
|    episodes         | 240      |
|    fps              | 34       |
|    time_elapsed     | 1371     |
|    total_timesteps  | 47622    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74e-06 |
|    n_updates        | 9405     |
----------------------------------
Eval num_timesteps=48000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.772    |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.83e-06 |
|    n_updates        | 9499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.0815  |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 244      |
|    fps              | 34       |
|    time_elapsed     | 1384     |
|    total_timesteps  | 48462    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.62e-07 |
|    n_updates        | 9615     |
----------------------------------
Eval num_timesteps=48500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.77     |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-06 |
|    n_updates        | 9624     |
----------------------------------
Eval num_timesteps=49000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.767    |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.12e-06 |
|    n_updates        | 9749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0698  |
|    exploration_rate | 0.767    |
| time/               |          |
|    episodes         | 248      |
|    fps              | 34       |
|    time_elapsed     | 1417     |
|    total_timesteps  | 49126    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-06 |
|    n_updates        | 9781     |
----------------------------------
Eval num_timesteps=49500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.765    |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.57e-07 |
|    n_updates        | 9874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0698  |
|    exploration_rate | 0.763    |
| time/               |          |
|    episodes         | 252      |
|    fps              | 34       |
|    time_elapsed     | 1432     |
|    total_timesteps  | 49966    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.87e-07 |
|    n_updates        | 9991     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.763    |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.97e-07 |
|    n_updates        | 9999     |
----------------------------------
Eval num_timesteps=50500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.76     |
| time/               |          |
|    total_timesteps  | 50500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.04e-06 |
|    n_updates        | 10124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0488  |
|    exploration_rate | 0.759    |
| time/               |          |
|    episodes         | 256      |
|    fps              | 34       |
|    time_elapsed     | 1460     |
|    total_timesteps  | 50705    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000236 |
|    n_updates        | 10176    |
----------------------------------
Eval num_timesteps=51000, episode_reward=-0.16 +/- 0.23
Episode length: 203.66 +/- 32.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | -0.164   |
| rollout/            |          |
|    exploration_rate | 0.758    |
| time/               |          |
|    total_timesteps  | 51000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.28e-06 |
|    n_updates        | 10249    |
----------------------------------
Eval num_timesteps=51500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.755    |
| time/               |          |
|    total_timesteps  | 51500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000488 |
|    n_updates        | 10374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.06    |
|    exploration_rate | 0.755    |
| time/               |          |
|    episodes         | 260      |
|    fps              | 34       |
|    time_elapsed     | 1489     |
|    total_timesteps  | 51545    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.64e-06 |
|    n_updates        | 10386    |
----------------------------------
Eval num_timesteps=52000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.753    |
| time/               |          |
|    total_timesteps  | 52000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.93e-07 |
|    n_updates        | 10499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.0706  |
|    exploration_rate | 0.751    |
| time/               |          |
|    episodes         | 264      |
|    fps              | 34       |
|    time_elapsed     | 1504     |
|    total_timesteps  | 52385    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.93e-07 |
|    n_updates        | 10596    |
----------------------------------
Eval num_timesteps=52500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.751    |
| time/               |          |
|    total_timesteps  | 52500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.73e-07 |
|    n_updates        | 10624    |
----------------------------------
Eval num_timesteps=53000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.748    |
| time/               |          |
|    total_timesteps  | 53000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.9e-06  |
|    n_updates        | 10749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.0806  |
|    exploration_rate | 0.748    |
| time/               |          |
|    episodes         | 268      |
|    fps              | 34       |
|    time_elapsed     | 1531     |
|    total_timesteps  | 53066    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.61e-06 |
|    n_updates        | 10766    |
----------------------------------
Eval num_timesteps=53500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.746    |
| time/               |          |
|    total_timesteps  | 53500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000267 |
|    n_updates        | 10874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.0806  |
|    exploration_rate | 0.744    |
| time/               |          |
|    episodes         | 272      |
|    fps              | 34       |
|    time_elapsed     | 1544     |
|    total_timesteps  | 53906    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.82e-07 |
|    n_updates        | 10976    |
----------------------------------
Eval num_timesteps=54000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.744    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0115   |
|    n_updates        | 10999    |
----------------------------------
Eval num_timesteps=54500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.741    |
| time/               |          |
|    total_timesteps  | 54500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5e-07  |
|    n_updates        | 11124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.0806  |
|    exploration_rate | 0.74     |
| time/               |          |
|    episodes         | 276      |
|    fps              | 34       |
|    time_elapsed     | 1576     |
|    total_timesteps  | 54746    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.48e-07 |
|    n_updates        | 11186    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.739    |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34e-08 |
|    n_updates        | 11249    |
----------------------------------
Eval num_timesteps=55500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.736    |
| time/               |          |
|    total_timesteps  | 55500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.43e-06 |
|    n_updates        | 11374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.0908  |
|    exploration_rate | 0.736    |
| time/               |          |
|    episodes         | 280      |
|    fps              | 34       |
|    time_elapsed     | 1604     |
|    total_timesteps  | 55586    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9e-05  |
|    n_updates        | 11396    |
----------------------------------
Eval num_timesteps=56000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.734    |
| time/               |          |
|    total_timesteps  | 56000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58e-06 |
|    n_updates        | 11499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.0908  |
|    exploration_rate | 0.732    |
| time/               |          |
|    episodes         | 284      |
|    fps              | 34       |
|    time_elapsed     | 1621     |
|    total_timesteps  | 56426    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86e-06 |
|    n_updates        | 11606    |
----------------------------------
Eval num_timesteps=56500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.732    |
| time/               |          |
|    total_timesteps  | 56500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-05 |
|    n_updates        | 11624    |
----------------------------------
Eval num_timesteps=57000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.729    |
| time/               |          |
|    total_timesteps  | 57000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.42e-06 |
|    n_updates        | 11749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.0908  |
|    exploration_rate | 0.728    |
| time/               |          |
|    episodes         | 288      |
|    fps              | 34       |
|    time_elapsed     | 1649     |
|    total_timesteps  | 57266    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000337 |
|    n_updates        | 11816    |
----------------------------------
Eval num_timesteps=57500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.727    |
| time/               |          |
|    total_timesteps  | 57500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.29e-07 |
|    n_updates        | 11874    |
----------------------------------
Eval num_timesteps=58000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.725    |
| time/               |          |
|    total_timesteps  | 58000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.52e-06 |
|    n_updates        | 11999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.724    |
| time/               |          |
|    episodes         | 292      |
|    fps              | 34       |
|    time_elapsed     | 1679     |
|    total_timesteps  | 58106    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.43e-06 |
|    n_updates        | 12026    |
----------------------------------
Eval num_timesteps=58500, episode_reward=-0.14 +/- 0.29
Episode length: 197.68 +/- 48.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.722    |
| time/               |          |
|    total_timesteps  | 58500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.61e-06 |
|    n_updates        | 12124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0897  |
|    exploration_rate | 0.721    |
| time/               |          |
|    episodes         | 296      |
|    fps              | 34       |
|    time_elapsed     | 1697     |
|    total_timesteps  | 58828    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.48e-06 |
|    n_updates        | 12206    |
----------------------------------
Eval num_timesteps=59000, episode_reward=-0.16 +/- 0.23
Episode length: 202.42 +/- 37.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.72     |
| time/               |          |
|    total_timesteps  | 59000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.72e-05 |
|    n_updates        | 12249    |
----------------------------------
Eval num_timesteps=59500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.717    |
| time/               |          |
|    total_timesteps  | 59500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.46e-07 |
|    n_updates        | 12374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0788  |
|    exploration_rate | 0.717    |
| time/               |          |
|    episodes         | 300      |
|    fps              | 34       |
|    time_elapsed     | 1725     |
|    total_timesteps  | 59585    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-05 |
|    n_updates        | 12396    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.715    |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33e-05 |
|    n_updates        | 12499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0897  |
|    exploration_rate | 0.713    |
| time/               |          |
|    episodes         | 304      |
|    fps              | 34       |
|    time_elapsed     | 1737     |
|    total_timesteps  | 60425    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.96e-06 |
|    n_updates        | 12606    |
----------------------------------
Eval num_timesteps=60500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.713    |
| time/               |          |
|    total_timesteps  | 60500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.72e-06 |
|    n_updates        | 12624    |
----------------------------------
Eval num_timesteps=61000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.71     |
| time/               |          |
|    total_timesteps  | 61000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-06 |
|    n_updates        | 12749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0794  |
|    exploration_rate | 0.709    |
| time/               |          |
|    episodes         | 308      |
|    fps              | 34       |
|    time_elapsed     | 1765     |
|    total_timesteps  | 61231    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.52e-07 |
|    n_updates        | 12807    |
----------------------------------
Eval num_timesteps=61500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.708    |
| time/               |          |
|    total_timesteps  | 61500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.96e-06 |
|    n_updates        | 12874    |
----------------------------------
Eval num_timesteps=62000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.706    |
| time/               |          |
|    total_timesteps  | 62000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.67e-06 |
|    n_updates        | 12999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0794  |
|    exploration_rate | 0.705    |
| time/               |          |
|    episodes         | 312      |
|    fps              | 34       |
|    time_elapsed     | 1798     |
|    total_timesteps  | 62071    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.93e-07 |
|    n_updates        | 13017    |
----------------------------------
Eval num_timesteps=62500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.703    |
| time/               |          |
|    total_timesteps  | 62500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.1e-06  |
|    n_updates        | 13124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0677  |
|    exploration_rate | 0.702    |
| time/               |          |
|    episodes         | 316      |
|    fps              | 34       |
|    time_elapsed     | 1810     |
|    total_timesteps  | 62745    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.05e-06 |
|    n_updates        | 13186    |
----------------------------------
Eval num_timesteps=63000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.701    |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.97e-06 |
|    n_updates        | 13249    |
----------------------------------
Eval num_timesteps=63500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.698    |
| time/               |          |
|    total_timesteps  | 63500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.1e-07  |
|    n_updates        | 13374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0573  |
|    exploration_rate | 0.698    |
| time/               |          |
|    episodes         | 320      |
|    fps              | 34       |
|    time_elapsed     | 1839     |
|    total_timesteps  | 63549    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.85e-06 |
|    n_updates        | 13387    |
----------------------------------
Eval num_timesteps=64000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.696    |
| time/               |          |
|    total_timesteps  | 64000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.14e-06 |
|    n_updates        | 13499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0561  |
|    exploration_rate | 0.695    |
| time/               |          |
|    episodes         | 324      |
|    fps              | 34       |
|    time_elapsed     | 1857     |
|    total_timesteps  | 64255    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.83e-06 |
|    n_updates        | 13563    |
----------------------------------
Eval num_timesteps=64500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.694    |
| time/               |          |
|    total_timesteps  | 64500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.19e-06 |
|    n_updates        | 13624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0557  |
|    exploration_rate | 0.692    |
| time/               |          |
|    episodes         | 328      |
|    fps              | 34       |
|    time_elapsed     | 1869     |
|    total_timesteps  | 64909    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.83e-08 |
|    n_updates        | 13727    |
----------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.691    |
| time/               |          |
|    total_timesteps  | 65000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.13e-07 |
|    n_updates        | 13749    |
----------------------------------
Eval num_timesteps=65500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.689    |
| time/               |          |
|    total_timesteps  | 65500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.22e-06 |
|    n_updates        | 13874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0662  |
|    exploration_rate | 0.688    |
| time/               |          |
|    episodes         | 332      |
|    fps              | 34       |
|    time_elapsed     | 1898     |
|    total_timesteps  | 65749    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000385 |
|    n_updates        | 13937    |
----------------------------------
Eval num_timesteps=66000, episode_reward=-0.19 +/- 0.16
Episode length: 207.30 +/- 18.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.687    |
| time/               |          |
|    total_timesteps  | 66000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.71e-05 |
|    n_updates        | 13999    |
----------------------------------
Eval num_timesteps=66500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.684    |
| time/               |          |
|    total_timesteps  | 66500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.21e-06 |
|    n_updates        | 14124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.088   |
|    exploration_rate | 0.684    |
| time/               |          |
|    episodes         | 336      |
|    fps              | 34       |
|    time_elapsed     | 1926     |
|    total_timesteps  | 66589    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21e-06 |
|    n_updates        | 14147    |
----------------------------------
Eval num_timesteps=67000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.682    |
| time/               |          |
|    total_timesteps  | 67000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.09e-06 |
|    n_updates        | 14249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.088   |
|    exploration_rate | 0.68     |
| time/               |          |
|    episodes         | 340      |
|    fps              | 34       |
|    time_elapsed     | 1941     |
|    total_timesteps  | 67429    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.78e-07 |
|    n_updates        | 14357    |
----------------------------------
Eval num_timesteps=67500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.679    |
| time/               |          |
|    total_timesteps  | 67500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.53e-05 |
|    n_updates        | 14374    |
----------------------------------
Eval num_timesteps=68000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.677    |
| time/               |          |
|    total_timesteps  | 68000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13e-06 |
|    n_updates        | 14499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.088   |
|    exploration_rate | 0.676    |
| time/               |          |
|    episodes         | 344      |
|    fps              | 34       |
|    time_elapsed     | 1969     |
|    total_timesteps  | 68269    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000386 |
|    n_updates        | 14567    |
----------------------------------
Eval num_timesteps=68500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.675    |
| time/               |          |
|    total_timesteps  | 68500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.56e-07 |
|    n_updates        | 14624    |
----------------------------------
Eval num_timesteps=69000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.672    |
| time/               |          |
|    total_timesteps  | 69000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.39e-06 |
|    n_updates        | 14749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0998  |
|    exploration_rate | 0.672    |
| time/               |          |
|    episodes         | 348      |
|    fps              | 34       |
|    time_elapsed     | 1997     |
|    total_timesteps  | 69109    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000426 |
|    n_updates        | 14777    |
----------------------------------
Eval num_timesteps=69500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.67     |
| time/               |          |
|    total_timesteps  | 69500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.81e-06 |
|    n_updates        | 14874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.088   |
|    exploration_rate | 0.669    |
| time/               |          |
|    episodes         | 352      |
|    fps              | 34       |
|    time_elapsed     | 2014     |
|    total_timesteps  | 69768    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.41e-07 |
|    n_updates        | 14941    |
----------------------------------
Eval num_timesteps=70000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.668    |
| time/               |          |
|    total_timesteps  | 70000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16e-06 |
|    n_updates        | 14999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0976  |
|    exploration_rate | 0.665    |
| time/               |          |
|    episodes         | 356      |
|    fps              | 34       |
|    time_elapsed     | 2026     |
|    total_timesteps  | 70465    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.17e-05 |
|    n_updates        | 15116    |
----------------------------------
Eval num_timesteps=70500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.665    |
| time/               |          |
|    total_timesteps  | 70500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000409 |
|    n_updates        | 15124    |
----------------------------------
Eval num_timesteps=71000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.663    |
| time/               |          |
|    total_timesteps  | 71000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000419 |
|    n_updates        | 15249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0976  |
|    exploration_rate | 0.661    |
| time/               |          |
|    episodes         | 360      |
|    fps              | 34       |
|    time_elapsed     | 2059     |
|    total_timesteps  | 71305    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.24e-06 |
|    n_updates        | 15326    |
----------------------------------
Eval num_timesteps=71500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.66     |
| time/               |          |
|    total_timesteps  | 71500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.7e-06  |
|    n_updates        | 15374    |
----------------------------------
Eval num_timesteps=72000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.658    |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82e-06 |
|    n_updates        | 15499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0976  |
|    exploration_rate | 0.657    |
| time/               |          |
|    episodes         | 364      |
|    fps              | 34       |
|    time_elapsed     | 2087     |
|    total_timesteps  | 72145    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.52e-07 |
|    n_updates        | 15536    |
----------------------------------
Eval num_timesteps=72500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.656    |
| time/               |          |
|    total_timesteps  | 72500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000384 |
|    n_updates        | 15624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.653    |
| time/               |          |
|    episodes         | 368      |
|    fps              | 34       |
|    time_elapsed     | 2102     |
|    total_timesteps  | 72985    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.28e-06 |
|    n_updates        | 15746    |
----------------------------------
Eval num_timesteps=73000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.653    |
| time/               |          |
|    total_timesteps  | 73000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000367 |
|    n_updates        | 15749    |
----------------------------------
Eval num_timesteps=73500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.651    |
| time/               |          |
|    total_timesteps  | 73500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-06 |
|    n_updates        | 15874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0988  |
|    exploration_rate | 0.649    |
| time/               |          |
|    episodes         | 372      |
|    fps              | 34       |
|    time_elapsed     | 2134     |
|    total_timesteps  | 73790    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.15e-06 |
|    n_updates        | 15947    |
----------------------------------
Eval num_timesteps=74000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.649    |
| time/               |          |
|    total_timesteps  | 74000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4e-06  |
|    n_updates        | 15999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.646    |
| time/               |          |
|    episodes         | 376      |
|    fps              | 34       |
|    time_elapsed     | 2151     |
|    total_timesteps  | 74475    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66e-06 |
|    n_updates        | 16118    |
----------------------------------
Eval num_timesteps=74500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.646    |
| time/               |          |
|    total_timesteps  | 74500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.06e-07 |
|    n_updates        | 16124    |
----------------------------------
Eval num_timesteps=75000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.644    |
| time/               |          |
|    total_timesteps  | 75000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.19e-06 |
|    n_updates        | 16249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.642    |
| time/               |          |
|    episodes         | 380      |
|    fps              | 34       |
|    time_elapsed     | 2179     |
|    total_timesteps  | 75315    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000363 |
|    n_updates        | 16328    |
----------------------------------
Eval num_timesteps=75500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.641    |
| time/               |          |
|    total_timesteps  | 75500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.88e-07 |
|    n_updates        | 16374    |
----------------------------------
Eval num_timesteps=76000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.639    |
| time/               |          |
|    total_timesteps  | 76000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.37e-07 |
|    n_updates        | 16499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.638    |
| time/               |          |
|    episodes         | 384      |
|    fps              | 34       |
|    time_elapsed     | 2207     |
|    total_timesteps  | 76155    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.65e-07 |
|    n_updates        | 16538    |
----------------------------------
Eval num_timesteps=76500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.637    |
| time/               |          |
|    total_timesteps  | 76500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000418 |
|    n_updates        | 16624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.634    |
| time/               |          |
|    episodes         | 388      |
|    fps              | 34       |
|    time_elapsed     | 2222     |
|    total_timesteps  | 76995    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.82e-07 |
|    n_updates        | 16748    |
----------------------------------
Eval num_timesteps=77000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.634    |
| time/               |          |
|    total_timesteps  | 77000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.44e-07 |
|    n_updates        | 16749    |
----------------------------------
Eval num_timesteps=77500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.632    |
| time/               |          |
|    total_timesteps  | 77500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.85e-06 |
|    n_updates        | 16874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0772  |
|    exploration_rate | 0.63     |
| time/               |          |
|    episodes         | 392      |
|    fps              | 34       |
|    time_elapsed     | 2256     |
|    total_timesteps  | 77831    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.38e-07 |
|    n_updates        | 16957    |
----------------------------------
Eval num_timesteps=78000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.63     |
| time/               |          |
|    total_timesteps  | 78000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-06 |
|    n_updates        | 16999    |
----------------------------------
Eval num_timesteps=78500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.627    |
| time/               |          |
|    total_timesteps  | 78500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000341 |
|    n_updates        | 17124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0884  |
|    exploration_rate | 0.626    |
| time/               |          |
|    episodes         | 396      |
|    fps              | 34       |
|    time_elapsed     | 2284     |
|    total_timesteps  | 78671    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.03e-07 |
|    n_updates        | 17167    |
----------------------------------
Eval num_timesteps=79000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.625    |
| time/               |          |
|    total_timesteps  | 79000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74e-06 |
|    n_updates        | 17249    |
----------------------------------
Eval num_timesteps=79500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.622    |
| time/               |          |
|    total_timesteps  | 79500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.39e-06 |
|    n_updates        | 17374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0992  |
|    exploration_rate | 0.622    |
| time/               |          |
|    episodes         | 400      |
|    fps              | 34       |
|    time_elapsed     | 2313     |
|    total_timesteps  | 79511    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.89e-06 |
|    n_updates        | 17377    |
----------------------------------
Eval num_timesteps=80000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.62     |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74e-06 |
|    n_updates        | 17499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0992  |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 404      |
|    fps              | 34       |
|    time_elapsed     | 2326     |
|    total_timesteps  | 80351    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.35e-06 |
|    n_updates        | 17587    |
----------------------------------
Eval num_timesteps=80500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.618    |
| time/               |          |
|    total_timesteps  | 80500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.98e-06 |
|    n_updates        | 17624    |
----------------------------------
Eval num_timesteps=81000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.615    |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.46e-06 |
|    n_updates        | 17749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.614    |
| time/               |          |
|    episodes         | 408      |
|    fps              | 34       |
|    time_elapsed     | 2355     |
|    total_timesteps  | 81191    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.28e-06 |
|    n_updates        | 17797    |
----------------------------------
Eval num_timesteps=81500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.613    |
| time/               |          |
|    total_timesteps  | 81500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.5e-06  |
|    n_updates        | 17874    |
----------------------------------
Eval num_timesteps=82000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.611    |
| time/               |          |
|    total_timesteps  | 82000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88e-06 |
|    n_updates        | 17999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.61     |
| time/               |          |
|    episodes         | 412      |
|    fps              | 34       |
|    time_elapsed     | 2382     |
|    total_timesteps  | 82031    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.65e-07 |
|    n_updates        | 18007    |
----------------------------------
Eval num_timesteps=82500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.608    |
| time/               |          |
|    total_timesteps  | 82500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000328 |
|    n_updates        | 18124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.606    |
| time/               |          |
|    episodes         | 416      |
|    fps              | 34       |
|    time_elapsed     | 2396     |
|    total_timesteps  | 82871    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.91e-07 |
|    n_updates        | 18217    |
----------------------------------
Eval num_timesteps=83000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.606    |
| time/               |          |
|    total_timesteps  | 83000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.64e-07 |
|    n_updates        | 18249    |
----------------------------------
Eval num_timesteps=83500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.603    |
| time/               |          |
|    total_timesteps  | 83500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.76e-07 |
|    n_updates        | 18374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.602    |
| time/               |          |
|    episodes         | 420      |
|    fps              | 34       |
|    time_elapsed     | 2425     |
|    total_timesteps  | 83711    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.13e-06 |
|    n_updates        | 18427    |
----------------------------------
Eval num_timesteps=84000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.601    |
| time/               |          |
|    total_timesteps  | 84000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.6e-07  |
|    n_updates        | 18499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.599    |
| time/               |          |
|    episodes         | 424      |
|    fps              | 34       |
|    time_elapsed     | 2441     |
|    total_timesteps  | 84422    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.96e-07 |
|    n_updates        | 18605    |
----------------------------------
Eval num_timesteps=84500, episode_reward=-0.14 +/- 0.29
Episode length: 197.72 +/- 48.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.599    |
| time/               |          |
|    total_timesteps  | 84500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.55e-07 |
|    n_updates        | 18624    |
----------------------------------
Eval num_timesteps=85000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.596    |
| time/               |          |
|    total_timesteps  | 85000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0271   |
|    n_updates        | 18749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.595    |
| time/               |          |
|    episodes         | 428      |
|    fps              | 34       |
|    time_elapsed     | 2469     |
|    total_timesteps  | 85262    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.47e-06 |
|    n_updates        | 18815    |
----------------------------------
Eval num_timesteps=85500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.594    |
| time/               |          |
|    total_timesteps  | 85500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59e-06 |
|    n_updates        | 18874    |
----------------------------------
Eval num_timesteps=86000, episode_reward=-0.19 +/- 0.16
Episode length: 207.42 +/- 18.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.592    |
| time/               |          |
|    total_timesteps  | 86000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0275   |
|    n_updates        | 18999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.591    |
| time/               |          |
|    episodes         | 432      |
|    fps              | 34       |
|    time_elapsed     | 2497     |
|    total_timesteps  | 86102    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.92e-06 |
|    n_updates        | 19025    |
----------------------------------
Eval num_timesteps=86500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.589    |
| time/               |          |
|    total_timesteps  | 86500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.94e-07 |
|    n_updates        | 19124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.587    |
| time/               |          |
|    episodes         | 436      |
|    fps              | 34       |
|    time_elapsed     | 2510     |
|    total_timesteps  | 86927    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.97e-06 |
|    n_updates        | 19231    |
----------------------------------
Eval num_timesteps=87000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.587    |
| time/               |          |
|    total_timesteps  | 87000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000311 |
|    n_updates        | 19249    |
----------------------------------
Eval num_timesteps=87500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.584    |
| time/               |          |
|    total_timesteps  | 87500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.19e-06 |
|    n_updates        | 19374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.583    |
| time/               |          |
|    episodes         | 440      |
|    fps              | 34       |
|    time_elapsed     | 2538     |
|    total_timesteps  | 87767    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.4e-07  |
|    n_updates        | 19441    |
----------------------------------
Eval num_timesteps=88000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.582    |
| time/               |          |
|    total_timesteps  | 88000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.6e-06  |
|    n_updates        | 19499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.58     |
| time/               |          |
|    episodes         | 444      |
|    fps              | 34       |
|    time_elapsed     | 2556     |
|    total_timesteps  | 88406    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92e-07 |
|    n_updates        | 19601    |
----------------------------------
Eval num_timesteps=88500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.58     |
| time/               |          |
|    total_timesteps  | 88500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87e-07 |
|    n_updates        | 19624    |
----------------------------------
Eval num_timesteps=89000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.577    |
| time/               |          |
|    total_timesteps  | 89000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000327 |
|    n_updates        | 19749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.576    |
| time/               |          |
|    episodes         | 448      |
|    fps              | 34       |
|    time_elapsed     | 2585     |
|    total_timesteps  | 89246    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000354 |
|    n_updates        | 19811    |
----------------------------------
Eval num_timesteps=89500, episode_reward=-0.16 +/- 0.23
Episode length: 202.28 +/- 37.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.575    |
| time/               |          |
|    total_timesteps  | 89500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.04e-06 |
|    n_updates        | 19874    |
----------------------------------
Eval num_timesteps=90000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.573    |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.49e-06 |
|    n_updates        | 19999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.572    |
| time/               |          |
|    episodes         | 452      |
|    fps              | 34       |
|    time_elapsed     | 2613     |
|    total_timesteps  | 90086    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.43e-06 |
|    n_updates        | 20021    |
----------------------------------
Eval num_timesteps=90500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.57     |
| time/               |          |
|    total_timesteps  | 90500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.63e-06 |
|    n_updates        | 20124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.569    |
| time/               |          |
|    episodes         | 456      |
|    fps              | 34       |
|    time_elapsed     | 2626     |
|    total_timesteps  | 90774    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.45e-06 |
|    n_updates        | 20193    |
----------------------------------
Eval num_timesteps=91000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.568    |
| time/               |          |
|    total_timesteps  | 91000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000343 |
|    n_updates        | 20249    |
----------------------------------
Eval num_timesteps=91500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.565    |
| time/               |          |
|    total_timesteps  | 91500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.3e-06  |
|    n_updates        | 20374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.565    |
| time/               |          |
|    episodes         | 460      |
|    fps              | 34       |
|    time_elapsed     | 2654     |
|    total_timesteps  | 91614    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.84e-06 |
|    n_updates        | 20403    |
----------------------------------
Eval num_timesteps=92000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.563    |
| time/               |          |
|    total_timesteps  | 92000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04e-05 |
|    n_updates        | 20499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.562    |
| time/               |          |
|    episodes         | 464      |
|    fps              | 34       |
|    time_elapsed     | 2670     |
|    total_timesteps  | 92272    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94e-05 |
|    n_updates        | 20567    |
----------------------------------
Eval num_timesteps=92500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.561    |
| time/               |          |
|    total_timesteps  | 92500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.86e-07 |
|    n_updates        | 20624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.559    |
| time/               |          |
|    episodes         | 468      |
|    fps              | 34       |
|    time_elapsed     | 2683     |
|    total_timesteps  | 92947    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0111   |
|    n_updates        | 20736    |
----------------------------------
Eval num_timesteps=93000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.558    |
| time/               |          |
|    total_timesteps  | 93000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.47e-06 |
|    n_updates        | 20749    |
----------------------------------
Eval num_timesteps=93500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.556    |
| time/               |          |
|    total_timesteps  | 93500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21e-06 |
|    n_updates        | 20874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.555    |
| time/               |          |
|    episodes         | 472      |
|    fps              | 34       |
|    time_elapsed     | 2712     |
|    total_timesteps  | 93787    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.07e-06 |
|    n_updates        | 20946    |
----------------------------------
Eval num_timesteps=94000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.554    |
| time/               |          |
|    total_timesteps  | 94000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-06 |
|    n_updates        | 20999    |
----------------------------------
Eval num_timesteps=94500, episode_reward=-0.19 +/- 0.17
Episode length: 206.30 +/- 25.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.551    |
| time/               |          |
|    total_timesteps  | 94500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-05 |
|    n_updates        | 21124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.551    |
| time/               |          |
|    episodes         | 476      |
|    fps              | 34       |
|    time_elapsed     | 2741     |
|    total_timesteps  | 94627    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.79e-06 |
|    n_updates        | 21156    |
----------------------------------
Eval num_timesteps=95000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.549    |
| time/               |          |
|    total_timesteps  | 95000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.98e-07 |
|    n_updates        | 21249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.547    |
| time/               |          |
|    episodes         | 480      |
|    fps              | 34       |
|    time_elapsed     | 2755     |
|    total_timesteps  | 95467    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.03e-06 |
|    n_updates        | 21366    |
----------------------------------
Eval num_timesteps=95500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.546    |
| time/               |          |
|    total_timesteps  | 95500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.15e-06 |
|    n_updates        | 21374    |
----------------------------------
Eval num_timesteps=96000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.544    |
| time/               |          |
|    total_timesteps  | 96000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.42e-07 |
|    n_updates        | 21499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.543    |
| time/               |          |
|    episodes         | 484      |
|    fps              | 34       |
|    time_elapsed     | 2782     |
|    total_timesteps  | 96307    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.81e-06 |
|    n_updates        | 21576    |
----------------------------------
Eval num_timesteps=96500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.542    |
| time/               |          |
|    total_timesteps  | 96500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.41e-06 |
|    n_updates        | 21624    |
----------------------------------
Eval num_timesteps=97000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.539    |
| time/               |          |
|    total_timesteps  | 97000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000396 |
|    n_updates        | 21749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.539    |
| time/               |          |
|    episodes         | 488      |
|    fps              | 34       |
|    time_elapsed     | 2810     |
|    total_timesteps  | 97147    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.93e-07 |
|    n_updates        | 21786    |
----------------------------------
Eval num_timesteps=97500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.537    |
| time/               |          |
|    total_timesteps  | 97500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000408 |
|    n_updates        | 21874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.535    |
| time/               |          |
|    episodes         | 492      |
|    fps              | 34       |
|    time_elapsed     | 2827     |
|    total_timesteps  | 97987    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000377 |
|    n_updates        | 21996    |
----------------------------------
Eval num_timesteps=98000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.535    |
| time/               |          |
|    total_timesteps  | 98000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1e-06  |
|    n_updates        | 21999    |
----------------------------------
Eval num_timesteps=98500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.532    |
| time/               |          |
|    total_timesteps  | 98500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28e-05 |
|    n_updates        | 22124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.531    |
| time/               |          |
|    episodes         | 496      |
|    fps              | 34       |
|    time_elapsed     | 2855     |
|    total_timesteps  | 98667    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01e-06 |
|    n_updates        | 22166    |
----------------------------------
Eval num_timesteps=99000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.53     |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.16e-07 |
|    n_updates        | 22249    |
----------------------------------
Eval num_timesteps=99500, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.527    |
| time/               |          |
|    total_timesteps  | 99500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.43e-06 |
|    n_updates        | 22374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.527    |
| time/               |          |
|    episodes         | 500      |
|    fps              | 34       |
|    time_elapsed     | 2884     |
|    total_timesteps  | 99507    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000353 |
|    n_updates        | 22376    |
----------------------------------
Eval num_timesteps=100000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.525    |
| time/               |          |
|    total_timesteps  | 100000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.55e-06 |
|    n_updates        | 22499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.524    |
| time/               |          |
|    episodes         | 504      |
|    fps              | 34       |
|    time_elapsed     | 2895     |
|    total_timesteps  | 100155   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.46e-06 |
|    n_updates        | 22538    |
----------------------------------
Eval num_timesteps=100500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.523    |
| time/               |          |
|    total_timesteps  | 100500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.05e-07 |
|    n_updates        | 22624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.52     |
| time/               |          |
|    episodes         | 508      |
|    fps              | 34       |
|    time_elapsed     | 2914     |
|    total_timesteps  | 100995   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.41e-06 |
|    n_updates        | 22748    |
----------------------------------
Eval num_timesteps=101000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.52     |
| time/               |          |
|    total_timesteps  | 101000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.84e-06 |
|    n_updates        | 22749    |
----------------------------------
Eval num_timesteps=101500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.518    |
| time/               |          |
|    total_timesteps  | 101500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01e-06 |
|    n_updates        | 22874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.516    |
| time/               |          |
|    episodes         | 512      |
|    fps              | 34       |
|    time_elapsed     | 2943     |
|    total_timesteps  | 101835   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.91e-07 |
|    n_updates        | 22958    |
----------------------------------
Eval num_timesteps=102000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.516    |
| time/               |          |
|    total_timesteps  | 102000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.62e-06 |
|    n_updates        | 22999    |
----------------------------------
Eval num_timesteps=102500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.513    |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.58e-07 |
|    n_updates        | 23124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.512    |
| time/               |          |
|    episodes         | 516      |
|    fps              | 34       |
|    time_elapsed     | 2971     |
|    total_timesteps  | 102675   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000383 |
|    n_updates        | 23168    |
----------------------------------
Eval num_timesteps=103000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.511    |
| time/               |          |
|    total_timesteps  | 103000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.61e-07 |
|    n_updates        | 23249    |
----------------------------------
Eval num_timesteps=103500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.508    |
| time/               |          |
|    total_timesteps  | 103500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6e-07    |
|    n_updates        | 23374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.508    |
| time/               |          |
|    episodes         | 520      |
|    fps              | 34       |
|    time_elapsed     | 2999     |
|    total_timesteps  | 103515   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.64e-07 |
|    n_updates        | 23378    |
----------------------------------
Eval num_timesteps=104000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.506    |
| time/               |          |
|    total_timesteps  | 104000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.7e-06  |
|    n_updates        | 23499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.505    |
| time/               |          |
|    episodes         | 524      |
|    fps              | 34       |
|    time_elapsed     | 3013     |
|    total_timesteps  | 104235   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6e-07    |
|    n_updates        | 23558    |
----------------------------------
Eval num_timesteps=104500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.504    |
| time/               |          |
|    total_timesteps  | 104500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73e-05 |
|    n_updates        | 23624    |
----------------------------------
Eval num_timesteps=105000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.501    |
| time/               |          |
|    total_timesteps  | 105000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.94e-06 |
|    n_updates        | 23749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.501    |
| time/               |          |
|    episodes         | 528      |
|    fps              | 34       |
|    time_elapsed     | 3042     |
|    total_timesteps  | 105075   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.79e-06 |
|    n_updates        | 23768    |
----------------------------------
Eval num_timesteps=105500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.499    |
| time/               |          |
|    total_timesteps  | 105500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73e-06 |
|    n_updates        | 23874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.497    |
| time/               |          |
|    episodes         | 532      |
|    fps              | 34       |
|    time_elapsed     | 3056     |
|    total_timesteps  | 105915   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.71e-07 |
|    n_updates        | 23978    |
----------------------------------
Eval num_timesteps=106000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.497    |
| time/               |          |
|    total_timesteps  | 106000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.39e-07 |
|    n_updates        | 23999    |
----------------------------------
Eval num_timesteps=106500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.494    |
| time/               |          |
|    total_timesteps  | 106500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.33e-07 |
|    n_updates        | 24124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.493    |
| time/               |          |
|    episodes         | 536      |
|    fps              | 34       |
|    time_elapsed     | 3084     |
|    total_timesteps  | 106755   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000397 |
|    n_updates        | 24188    |
----------------------------------
Eval num_timesteps=107000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.492    |
| time/               |          |
|    total_timesteps  | 107000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.27e-05 |
|    n_updates        | 24249    |
----------------------------------
Eval num_timesteps=107500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.489    |
| time/               |          |
|    total_timesteps  | 107500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.61e-06 |
|    n_updates        | 24374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.489    |
| time/               |          |
|    episodes         | 540      |
|    fps              | 34       |
|    time_elapsed     | 3117     |
|    total_timesteps  | 107595   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.37e-07 |
|    n_updates        | 24398    |
----------------------------------
Eval num_timesteps=108000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.487    |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-06 |
|    n_updates        | 24499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.486    |
| time/               |          |
|    episodes         | 544      |
|    fps              | 34       |
|    time_elapsed     | 3133     |
|    total_timesteps  | 108248   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2e-06  |
|    n_updates        | 24561    |
----------------------------------
Eval num_timesteps=108500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.485    |
| time/               |          |
|    total_timesteps  | 108500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.16e-06 |
|    n_updates        | 24624    |
----------------------------------
Eval num_timesteps=109000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.482    |
| time/               |          |
|    total_timesteps  | 109000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96e-05 |
|    n_updates        | 24749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.482    |
| time/               |          |
|    episodes         | 548      |
|    fps              | 34       |
|    time_elapsed     | 3161     |
|    total_timesteps  | 109088   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.05e-06 |
|    n_updates        | 24771    |
----------------------------------
Eval num_timesteps=109500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.48     |
| time/               |          |
|    total_timesteps  | 109500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000508 |
|    n_updates        | 24874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.479    |
| time/               |          |
|    episodes         | 552      |
|    fps              | 34       |
|    time_elapsed     | 3176     |
|    total_timesteps  | 109679   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54e-06 |
|    n_updates        | 24919    |
----------------------------------
Eval num_timesteps=110000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.478    |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.67e-06 |
|    n_updates        | 24999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.476    |
| time/               |          |
|    episodes         | 556      |
|    fps              | 34       |
|    time_elapsed     | 3190     |
|    total_timesteps  | 110318   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.91e-07 |
|    n_updates        | 25079    |
----------------------------------
Eval num_timesteps=110500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.475    |
| time/               |          |
|    total_timesteps  | 110500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.93e-06 |
|    n_updates        | 25124    |
----------------------------------
Eval num_timesteps=111000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.473    |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-05 |
|    n_updates        | 25249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.472    |
| time/               |          |
|    episodes         | 560      |
|    fps              | 34       |
|    time_elapsed     | 3217     |
|    total_timesteps  | 111158   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2e-07    |
|    n_updates        | 25289    |
----------------------------------
Eval num_timesteps=111500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.47     |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59e-06 |
|    n_updates        | 25374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.468    |
| time/               |          |
|    episodes         | 564      |
|    fps              | 34       |
|    time_elapsed     | 3234     |
|    total_timesteps  | 111998   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-06 |
|    n_updates        | 25499    |
----------------------------------
Eval num_timesteps=112000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.468    |
| time/               |          |
|    total_timesteps  | 112000   |
----------------------------------
Eval num_timesteps=112500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.466    |
| time/               |          |
|    total_timesteps  | 112500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-06 |
|    n_updates        | 25624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.464    |
| time/               |          |
|    episodes         | 568      |
|    fps              | 34       |
|    time_elapsed     | 3263     |
|    total_timesteps  | 112838   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.03e-06 |
|    n_updates        | 25709    |
----------------------------------
Eval num_timesteps=113000, episode_reward=-0.14 +/- 0.29
Episode length: 197.70 +/- 48.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.463    |
| time/               |          |
|    total_timesteps  | 113000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.08e-07 |
|    n_updates        | 25749    |
----------------------------------
Eval num_timesteps=113500, episode_reward=-0.14 +/- 0.28
Episode length: 198.60 +/- 45.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | -0.139   |
| rollout/            |          |
|    exploration_rate | 0.461    |
| time/               |          |
|    total_timesteps  | 113500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.97e-06 |
|    n_updates        | 25874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.46     |
| time/               |          |
|    episodes         | 572      |
|    fps              | 34       |
|    time_elapsed     | 3291     |
|    total_timesteps  | 113678   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.22e-08 |
|    n_updates        | 25919    |
----------------------------------
Eval num_timesteps=114000, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.459    |
| time/               |          |
|    total_timesteps  | 114000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.88e-07 |
|    n_updates        | 25999    |
----------------------------------
Eval num_timesteps=114500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.456    |
| time/               |          |
|    total_timesteps  | 114500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.41e-07 |
|    n_updates        | 26124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.456    |
| time/               |          |
|    episodes         | 576      |
|    fps              | 34       |
|    time_elapsed     | 3319     |
|    total_timesteps  | 114518   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59e-07 |
|    n_updates        | 26129    |
----------------------------------
Eval num_timesteps=115000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.454    |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25e-06 |
|    n_updates        | 26249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.452    |
| time/               |          |
|    episodes         | 580      |
|    fps              | 34       |
|    time_elapsed     | 3331     |
|    total_timesteps  | 115265   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000419 |
|    n_updates        | 26316    |
----------------------------------
Eval num_timesteps=115500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.451    |
| time/               |          |
|    total_timesteps  | 115500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.06e-07 |
|    n_updates        | 26374    |
----------------------------------
Eval num_timesteps=116000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.449    |
| time/               |          |
|    total_timesteps  | 116000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35e-06 |
|    n_updates        | 26499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.449    |
| time/               |          |
|    episodes         | 584      |
|    fps              | 34       |
|    time_elapsed     | 3364     |
|    total_timesteps  | 116105   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-06  |
|    n_updates        | 26526    |
----------------------------------
Eval num_timesteps=116500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.447    |
| time/               |          |
|    total_timesteps  | 116500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.55e-07 |
|    n_updates        | 26624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.445    |
| time/               |          |
|    episodes         | 588      |
|    fps              | 34       |
|    time_elapsed     | 3378     |
|    total_timesteps  | 116808   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.26e-07 |
|    n_updates        | 26701    |
----------------------------------
Eval num_timesteps=117000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.444    |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.21e-06 |
|    n_updates        | 26749    |
----------------------------------
Eval num_timesteps=117500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.442    |
| time/               |          |
|    total_timesteps  | 117500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.24e-06 |
|    n_updates        | 26874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0965  |
|    exploration_rate | 0.441    |
| time/               |          |
|    episodes         | 592      |
|    fps              | 34       |
|    time_elapsed     | 3407     |
|    total_timesteps  | 117646   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.86e-07 |
|    n_updates        | 26911    |
----------------------------------
Eval num_timesteps=118000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.44     |
| time/               |          |
|    total_timesteps  | 118000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.64e-06 |
|    n_updates        | 26999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.437    |
| time/               |          |
|    episodes         | 596      |
|    fps              | 34       |
|    time_elapsed     | 3424     |
|    total_timesteps  | 118486   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-06 |
|    n_updates        | 27121    |
----------------------------------
Eval num_timesteps=118500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.437    |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.31e-07 |
|    n_updates        | 27124    |
----------------------------------
Eval num_timesteps=119000, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.435    |
| time/               |          |
|    total_timesteps  | 119000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000508 |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.433    |
| time/               |          |
|    episodes         | 600      |
|    fps              | 34       |
|    time_elapsed     | 3452     |
|    total_timesteps  | 119326   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.01e-07 |
|    n_updates        | 27331    |
----------------------------------
Eval num_timesteps=119500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.432    |
| time/               |          |
|    total_timesteps  | 119500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.77e-06 |
|    n_updates        | 27374    |
----------------------------------
Eval num_timesteps=120000, episode_reward=-0.11 +/- 0.33
Episode length: 193.64 +/- 55.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | -0.114   |
| rollout/            |          |
|    exploration_rate | 0.43     |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.29e-06 |
|    n_updates        | 27499    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.429    |
| time/               |          |
|    episodes         | 604      |
|    fps              | 34       |
|    time_elapsed     | 3481     |
|    total_timesteps  | 120166   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.21e-07 |
|    n_updates        | 27541    |
----------------------------------
Eval num_timesteps=120500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.428    |
| time/               |          |
|    total_timesteps  | 120500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.85e-07 |
|    n_updates        | 27624    |
----------------------------------
Eval num_timesteps=121000, episode_reward=-0.16 +/- 0.23
Episode length: 202.22 +/- 38.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.425    |
| time/               |          |
|    total_timesteps  | 121000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94e-07 |
|    n_updates        | 27749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.425    |
| time/               |          |
|    episodes         | 608      |
|    fps              | 34       |
|    time_elapsed     | 3509     |
|    total_timesteps  | 121006   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.48e-06 |
|    n_updates        | 27751    |
----------------------------------
Eval num_timesteps=121500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.423    |
| time/               |          |
|    total_timesteps  | 121500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-06 |
|    n_updates        | 27874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.421    |
| time/               |          |
|    episodes         | 612      |
|    fps              | 34       |
|    time_elapsed     | 3521     |
|    total_timesteps  | 121846   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.44e-07 |
|    n_updates        | 27961    |
----------------------------------
Eval num_timesteps=122000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.421    |
| time/               |          |
|    total_timesteps  | 122000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.19e-07 |
|    n_updates        | 27999    |
----------------------------------
Eval num_timesteps=122500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.418    |
| time/               |          |
|    total_timesteps  | 122500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56e-05 |
|    n_updates        | 28124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.418    |
| time/               |          |
|    episodes         | 616      |
|    fps              | 34       |
|    time_elapsed     | 3550     |
|    total_timesteps  | 122552   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.18e-06 |
|    n_updates        | 28137    |
----------------------------------
Eval num_timesteps=123000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.416    |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.92e-07 |
|    n_updates        | 28249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.414    |
| time/               |          |
|    episodes         | 620      |
|    fps              | 34       |
|    time_elapsed     | 3569     |
|    total_timesteps  | 123392   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-05 |
|    n_updates        | 28347    |
----------------------------------
Eval num_timesteps=123500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.413    |
| time/               |          |
|    total_timesteps  | 123500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.86e-06 |
|    n_updates        | 28374    |
----------------------------------
Eval num_timesteps=124000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.411    |
| time/               |          |
|    total_timesteps  | 124000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-06 |
|    n_updates        | 28499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.41     |
| time/               |          |
|    episodes         | 624      |
|    fps              | 34       |
|    time_elapsed     | 3598     |
|    total_timesteps  | 124232   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.74e-08 |
|    n_updates        | 28557    |
----------------------------------
Eval num_timesteps=124500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.409    |
| time/               |          |
|    total_timesteps  | 124500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.88e-06 |
|    n_updates        | 28624    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.406    |
| time/               |          |
|    total_timesteps  | 125000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44e-06 |
|    n_updates        | 28749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.406    |
| time/               |          |
|    episodes         | 628      |
|    fps              | 34       |
|    time_elapsed     | 3626     |
|    total_timesteps  | 125072   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.63e-07 |
|    n_updates        | 28767    |
----------------------------------
Eval num_timesteps=125500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.404    |
| time/               |          |
|    total_timesteps  | 125500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.38e-07 |
|    n_updates        | 28874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.402    |
| time/               |          |
|    episodes         | 632      |
|    fps              | 34       |
|    time_elapsed     | 3644     |
|    total_timesteps  | 125805   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-06 |
|    n_updates        | 28951    |
----------------------------------
Eval num_timesteps=126000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.402    |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92e-06 |
|    n_updates        | 28999    |
----------------------------------
Eval num_timesteps=126500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.399    |
| time/               |          |
|    total_timesteps  | 126500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.76e-08 |
|    n_updates        | 29124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.398    |
| time/               |          |
|    episodes         | 636      |
|    fps              | 34       |
|    time_elapsed     | 3672     |
|    total_timesteps  | 126645   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.27e-06 |
|    n_updates        | 29161    |
----------------------------------
Eval num_timesteps=127000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.397    |
| time/               |          |
|    total_timesteps  | 127000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.46e-06 |
|    n_updates        | 29249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0978  |
|    exploration_rate | 0.395    |
| time/               |          |
|    episodes         | 640      |
|    fps              | 34       |
|    time_elapsed     | 3685     |
|    total_timesteps  | 127377   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21e-06 |
|    n_updates        | 29344    |
----------------------------------
Eval num_timesteps=127500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.394    |
| time/               |          |
|    total_timesteps  | 127500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.78e-07 |
|    n_updates        | 29374    |
----------------------------------
Eval num_timesteps=128000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.392    |
| time/               |          |
|    total_timesteps  | 128000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.65e-07 |
|    n_updates        | 29499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.391    |
| time/               |          |
|    episodes         | 644      |
|    fps              | 34       |
|    time_elapsed     | 3714     |
|    total_timesteps  | 128217   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.04e-06 |
|    n_updates        | 29554    |
----------------------------------
Eval num_timesteps=128500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.39     |
| time/               |          |
|    total_timesteps  | 128500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2e-06    |
|    n_updates        | 29624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.099   |
|    exploration_rate | 0.387    |
| time/               |          |
|    episodes         | 648      |
|    fps              | 34       |
|    time_elapsed     | 3731     |
|    total_timesteps  | 128991   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.39e-06 |
|    n_updates        | 29747    |
----------------------------------
Eval num_timesteps=129000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.387    |
| time/               |          |
|    total_timesteps  | 129000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.08e-06 |
|    n_updates        | 29749    |
----------------------------------
Eval num_timesteps=129500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.385    |
| time/               |          |
|    total_timesteps  | 129500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000401 |
|    n_updates        | 29874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.384    |
| time/               |          |
|    episodes         | 652      |
|    fps              | 34       |
|    time_elapsed     | 3759     |
|    total_timesteps  | 129771   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.33e-06 |
|    n_updates        | 29942    |
----------------------------------
Eval num_timesteps=130000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.383    |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.42e-07 |
|    n_updates        | 29999    |
----------------------------------
Eval num_timesteps=130500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.38     |
| time/               |          |
|    total_timesteps  | 130500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34e-06 |
|    n_updates        | 30124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.38     |
| time/               |          |
|    episodes         | 656      |
|    fps              | 34       |
|    time_elapsed     | 3789     |
|    total_timesteps  | 130611   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.58e-07 |
|    n_updates        | 30152    |
----------------------------------
Eval num_timesteps=131000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.378    |
| time/               |          |
|    total_timesteps  | 131000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73e-06 |
|    n_updates        | 30249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.376    |
| time/               |          |
|    episodes         | 660      |
|    fps              | 34       |
|    time_elapsed     | 3808     |
|    total_timesteps  | 131420   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.29e-08 |
|    n_updates        | 30354    |
----------------------------------
Eval num_timesteps=131500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.375    |
| time/               |          |
|    total_timesteps  | 131500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.16e-06 |
|    n_updates        | 30374    |
----------------------------------
Eval num_timesteps=132000, episode_reward=-0.19 +/- 0.17
Episode length: 206.34 +/- 25.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.373    |
| time/               |          |
|    total_timesteps  | 132000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.82e-08 |
|    n_updates        | 30499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.372    |
| time/               |          |
|    episodes         | 664      |
|    fps              | 34       |
|    time_elapsed     | 3836     |
|    total_timesteps  | 132260   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000367 |
|    n_updates        | 30564    |
----------------------------------
Eval num_timesteps=132500, episode_reward=-0.19 +/- 0.17
Episode length: 206.26 +/- 26.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.371    |
| time/               |          |
|    total_timesteps  | 132500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.32e-06 |
|    n_updates        | 30624    |
----------------------------------
Eval num_timesteps=133000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.368    |
| time/               |          |
|    total_timesteps  | 133000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.76e-07 |
|    n_updates        | 30749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.368    |
| time/               |          |
|    episodes         | 668      |
|    fps              | 34       |
|    time_elapsed     | 3865     |
|    total_timesteps  | 133100   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0004   |
|    n_updates        | 30774    |
----------------------------------
Eval num_timesteps=133500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.366    |
| time/               |          |
|    total_timesteps  | 133500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1e-06  |
|    n_updates        | 30874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.364    |
| time/               |          |
|    episodes         | 672      |
|    fps              | 34       |
|    time_elapsed     | 3883     |
|    total_timesteps  | 133940   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.22e-06 |
|    n_updates        | 30984    |
----------------------------------
Eval num_timesteps=134000, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.364    |
| time/               |          |
|    total_timesteps  | 134000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.06e-06 |
|    n_updates        | 30999    |
----------------------------------
Eval num_timesteps=134500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.361    |
| time/               |          |
|    total_timesteps  | 134500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-07  |
|    n_updates        | 31124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.36     |
| time/               |          |
|    episodes         | 676      |
|    fps              | 34       |
|    time_elapsed     | 3912     |
|    total_timesteps  | 134780   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-06 |
|    n_updates        | 31194    |
----------------------------------
Eval num_timesteps=135000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.359    |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.02e-07 |
|    n_updates        | 31249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.357    |
| time/               |          |
|    episodes         | 680      |
|    fps              | 34       |
|    time_elapsed     | 3923     |
|    total_timesteps  | 135441   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.48e-07 |
|    n_updates        | 31360    |
----------------------------------
Eval num_timesteps=135500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.356    |
| time/               |          |
|    total_timesteps  | 135500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-06 |
|    n_updates        | 31374    |
----------------------------------
Eval num_timesteps=136000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.354    |
| time/               |          |
|    total_timesteps  | 136000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000363 |
|    n_updates        | 31499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.353    |
| time/               |          |
|    episodes         | 684      |
|    fps              | 34       |
|    time_elapsed     | 3953     |
|    total_timesteps  | 136281   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.22e-06 |
|    n_updates        | 31570    |
----------------------------------
Eval num_timesteps=136500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.352    |
| time/               |          |
|    total_timesteps  | 136500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.27e-06 |
|    n_updates        | 31624    |
----------------------------------
Eval num_timesteps=137000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.349    |
| time/               |          |
|    total_timesteps  | 137000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.12e-07 |
|    n_updates        | 31749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.349    |
| time/               |          |
|    episodes         | 688      |
|    fps              | 34       |
|    time_elapsed     | 3982     |
|    total_timesteps  | 137121   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.66e-06 |
|    n_updates        | 31780    |
----------------------------------
Eval num_timesteps=137500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.347    |
| time/               |          |
|    total_timesteps  | 137500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.74e-06 |
|    n_updates        | 31874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.345    |
| time/               |          |
|    episodes         | 692      |
|    fps              | 34       |
|    time_elapsed     | 3999     |
|    total_timesteps  | 137864   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000365 |
|    n_updates        | 31965    |
----------------------------------
Eval num_timesteps=138000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.345    |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.01e-06 |
|    n_updates        | 31999    |
----------------------------------
Eval num_timesteps=138500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.342    |
| time/               |          |
|    total_timesteps  | 138500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.66e-06 |
|    n_updates        | 32124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.342    |
| time/               |          |
|    episodes         | 696      |
|    fps              | 34       |
|    time_elapsed     | 4027     |
|    total_timesteps  | 138581   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.4e-06  |
|    n_updates        | 32145    |
----------------------------------
Eval num_timesteps=139000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.34     |
| time/               |          |
|    total_timesteps  | 139000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.37e-05 |
|    n_updates        | 32249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0995  |
|    exploration_rate | 0.338    |
| time/               |          |
|    episodes         | 700      |
|    fps              | 34       |
|    time_elapsed     | 4041     |
|    total_timesteps  | 139282   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.52e-07 |
|    n_updates        | 32320    |
----------------------------------
Eval num_timesteps=139500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.337    |
| time/               |          |
|    total_timesteps  | 139500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000443 |
|    n_updates        | 32374    |
----------------------------------
Eval num_timesteps=140000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.335    |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.37e-06 |
|    n_updates        | 32499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0995  |
|    exploration_rate | 0.334    |
| time/               |          |
|    episodes         | 704      |
|    fps              | 34       |
|    time_elapsed     | 4069     |
|    total_timesteps  | 140122   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000467 |
|    n_updates        | 32530    |
----------------------------------
Eval num_timesteps=140500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.333    |
| time/               |          |
|    total_timesteps  | 140500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-06 |
|    n_updates        | 32624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.078   |
|    exploration_rate | 0.331    |
| time/               |          |
|    episodes         | 708      |
|    fps              | 34       |
|    time_elapsed     | 4083     |
|    total_timesteps  | 140815   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-05 |
|    n_updates        | 32703    |
----------------------------------
Eval num_timesteps=141000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.33     |
| time/               |          |
|    total_timesteps  | 141000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.49e-06 |
|    n_updates        | 32749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.066   |
|    exploration_rate | 0.328    |
| time/               |          |
|    episodes         | 712      |
|    fps              | 34       |
|    time_elapsed     | 4099     |
|    total_timesteps  | 141452   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-06 |
|    n_updates        | 32862    |
----------------------------------
Eval num_timesteps=141500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.328    |
| time/               |          |
|    total_timesteps  | 141500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000415 |
|    n_updates        | 32874    |
----------------------------------
Eval num_timesteps=142000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.326    |
| time/               |          |
|    total_timesteps  | 142000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.98e-07 |
|    n_updates        | 32999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0569  |
|    exploration_rate | 0.324    |
| time/               |          |
|    episodes         | 716      |
|    fps              | 34       |
|    time_elapsed     | 4128     |
|    total_timesteps  | 142252   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.85e-07 |
|    n_updates        | 33062    |
----------------------------------
Eval num_timesteps=142500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.323    |
| time/               |          |
|    total_timesteps  | 142500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.36e-06 |
|    n_updates        | 33124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0449  |
|    exploration_rate | 0.321    |
| time/               |          |
|    episodes         | 720      |
|    fps              | 34       |
|    time_elapsed     | 4140     |
|    total_timesteps  | 142888   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.32e-07 |
|    n_updates        | 33221    |
----------------------------------
Eval num_timesteps=143000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.321    |
| time/               |          |
|    total_timesteps  | 143000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-06  |
|    n_updates        | 33249    |
----------------------------------
Eval num_timesteps=143500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.318    |
| time/               |          |
|    total_timesteps  | 143500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.4e-06  |
|    n_updates        | 33374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0449  |
|    exploration_rate | 0.317    |
| time/               |          |
|    episodes         | 724      |
|    fps              | 34       |
|    time_elapsed     | 4168     |
|    total_timesteps  | 143728   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.69e-08 |
|    n_updates        | 33431    |
----------------------------------
Eval num_timesteps=144000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.316    |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.03e-07 |
|    n_updates        | 33499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0341  |
|    exploration_rate | 0.314    |
| time/               |          |
|    episodes         | 728      |
|    fps              | 34       |
|    time_elapsed     | 4187     |
|    total_timesteps  | 144490   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.53e-06 |
|    n_updates        | 33622    |
----------------------------------
Eval num_timesteps=144500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.314    |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.01e-06 |
|    n_updates        | 33624    |
----------------------------------
Eval num_timesteps=145000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.311    |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000391 |
|    n_updates        | 33749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0452  |
|    exploration_rate | 0.31     |
| time/               |          |
|    episodes         | 732      |
|    fps              | 34       |
|    time_elapsed     | 4215     |
|    total_timesteps  | 145330   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.43e-06 |
|    n_updates        | 33832    |
----------------------------------
Eval num_timesteps=145500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.309    |
| time/               |          |
|    total_timesteps  | 145500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-06 |
|    n_updates        | 33874    |
----------------------------------
Eval num_timesteps=146000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.307    |
| time/               |          |
|    total_timesteps  | 146000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.72e-06 |
|    n_updates        | 33999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0452  |
|    exploration_rate | 0.306    |
| time/               |          |
|    episodes         | 736      |
|    fps              | 34       |
|    time_elapsed     | 4245     |
|    total_timesteps  | 146170   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.93e-06 |
|    n_updates        | 34042    |
----------------------------------
Eval num_timesteps=146500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.304    |
| time/               |          |
|    total_timesteps  | 146500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.79e-06 |
|    n_updates        | 34124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0444  |
|    exploration_rate | 0.303    |
| time/               |          |
|    episodes         | 740      |
|    fps              | 34       |
|    time_elapsed     | 4257     |
|    total_timesteps  | 146826   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.95e-07 |
|    n_updates        | 34206    |
----------------------------------
Eval num_timesteps=147000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.302    |
| time/               |          |
|    total_timesteps  | 147000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.44e-06 |
|    n_updates        | 34249    |
----------------------------------
Eval num_timesteps=147500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.299    |
| time/               |          |
|    total_timesteps  | 147500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.72e-07 |
|    n_updates        | 34374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0444  |
|    exploration_rate | 0.299    |
| time/               |          |
|    episodes         | 744      |
|    fps              | 34       |
|    time_elapsed     | 4285     |
|    total_timesteps  | 147666   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.64e-07 |
|    n_updates        | 34416    |
----------------------------------
Eval num_timesteps=148000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.297    |
| time/               |          |
|    total_timesteps  | 148000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000355 |
|    n_updates        | 34499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.0318  |
|    exploration_rate | 0.296    |
| time/               |          |
|    episodes         | 748      |
|    fps              | 34       |
|    time_elapsed     | 4301     |
|    total_timesteps  | 148182   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.19e-06 |
|    n_updates        | 34545    |
----------------------------------
Eval num_timesteps=148500, episode_reward=-0.14 +/- 0.29
Episode length: 197.74 +/- 48.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.295    |
| time/               |          |
|    total_timesteps  | 148500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.15e-06 |
|    n_updates        | 34624    |
----------------------------------
Eval num_timesteps=149000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.292    |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.81e-07 |
|    n_updates        | 34749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0424  |
|    exploration_rate | 0.292    |
| time/               |          |
|    episodes         | 752      |
|    fps              | 34       |
|    time_elapsed     | 4329     |
|    total_timesteps  | 149022   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.98e-06 |
|    n_updates        | 34755    |
----------------------------------
Eval num_timesteps=149500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.29     |
| time/               |          |
|    total_timesteps  | 149500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.08e-07 |
|    n_updates        | 34874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.0324  |
|    exploration_rate | 0.288    |
| time/               |          |
|    episodes         | 756      |
|    fps              | 34       |
|    time_elapsed     | 4344     |
|    total_timesteps  | 149854   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.05e-07 |
|    n_updates        | 34963    |
----------------------------------
Eval num_timesteps=150000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.288    |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.69e-07 |
|    n_updates        | 34999    |
----------------------------------
Eval num_timesteps=150500, episode_reward=-0.19 +/- 0.15
Episode length: 209.14 +/- 6.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | -0.189   |
| rollout/            |          |
|    exploration_rate | 0.285    |
| time/               |          |
|    total_timesteps  | 150500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.6e-06  |
|    n_updates        | 35124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0427  |
|    exploration_rate | 0.284    |
| time/               |          |
|    episodes         | 760      |
|    fps              | 34       |
|    time_elapsed     | 4374     |
|    total_timesteps  | 150694   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1e-06    |
|    n_updates        | 35173    |
----------------------------------
Eval num_timesteps=151000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.283    |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.64e-06 |
|    n_updates        | 35249    |
----------------------------------
Eval num_timesteps=151500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.28     |
| time/               |          |
|    total_timesteps  | 151500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.31e-07 |
|    n_updates        | 35374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0326  |
|    exploration_rate | 0.28     |
| time/               |          |
|    episodes         | 764      |
|    fps              | 34       |
|    time_elapsed     | 4407     |
|    total_timesteps  | 151529   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8e-07    |
|    n_updates        | 35382    |
----------------------------------
Eval num_timesteps=152000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.278    |
| time/               |          |
|    total_timesteps  | 152000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.97e-07 |
|    n_updates        | 35499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0326  |
|    exploration_rate | 0.276    |
| time/               |          |
|    episodes         | 768      |
|    fps              | 34       |
|    time_elapsed     | 4421     |
|    total_timesteps  | 152369   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.94e-06 |
|    n_updates        | 35592    |
----------------------------------
Eval num_timesteps=152500, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.276    |
| time/               |          |
|    total_timesteps  | 152500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.27e-06 |
|    n_updates        | 35624    |
----------------------------------
Eval num_timesteps=153000, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.273    |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.88e-07 |
|    n_updates        | 35749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0326  |
|    exploration_rate | 0.272    |
| time/               |          |
|    episodes         | 772      |
|    fps              | 34       |
|    time_elapsed     | 4449     |
|    total_timesteps  | 153209   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.11e-07 |
|    n_updates        | 35802    |
----------------------------------
Eval num_timesteps=153500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.271    |
| time/               |          |
|    total_timesteps  | 153500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.61e-07 |
|    n_updates        | 35874    |
----------------------------------
Eval num_timesteps=154000, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.269    |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.63e-07 |
|    n_updates        | 35999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0326  |
|    exploration_rate | 0.268    |
| time/               |          |
|    episodes         | 776      |
|    fps              | 34       |
|    time_elapsed     | 4481     |
|    total_timesteps  | 154049   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.22e-07 |
|    n_updates        | 36012    |
----------------------------------
Eval num_timesteps=154500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.266    |
| time/               |          |
|    total_timesteps  | 154500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04e-06 |
|    n_updates        | 36124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0326  |
|    exploration_rate | 0.265    |
| time/               |          |
|    episodes         | 780      |
|    fps              | 34       |
|    time_elapsed     | 4494     |
|    total_timesteps  | 154705   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.68e-06 |
|    n_updates        | 36176    |
----------------------------------
Eval num_timesteps=155000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.264    |
| time/               |          |
|    total_timesteps  | 155000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000285 |
|    n_updates        | 36249    |
----------------------------------
Eval num_timesteps=155500, episode_reward=-0.16 +/- 0.23
Episode length: 202.32 +/- 37.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.261    |
| time/               |          |
|    total_timesteps  | 155500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.74e-06 |
|    n_updates        | 36374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0326  |
|    exploration_rate | 0.261    |
| time/               |          |
|    episodes         | 784      |
|    fps              | 34       |
|    time_elapsed     | 4522     |
|    total_timesteps  | 155545   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.33e-06 |
|    n_updates        | 36386    |
----------------------------------
Eval num_timesteps=156000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.259    |
| time/               |          |
|    total_timesteps  | 156000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.48e-06 |
|    n_updates        | 36499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0326  |
|    exploration_rate | 0.257    |
| time/               |          |
|    episodes         | 788      |
|    fps              | 34       |
|    time_elapsed     | 4539     |
|    total_timesteps  | 156385   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.72e-07 |
|    n_updates        | 36596    |
----------------------------------
Eval num_timesteps=156500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.257    |
| time/               |          |
|    total_timesteps  | 156500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.05e-06 |
|    n_updates        | 36624    |
----------------------------------
Eval num_timesteps=157000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.254    |
| time/               |          |
|    total_timesteps  | 157000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.2e-06  |
|    n_updates        | 36749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.0315  |
|    exploration_rate | 0.254    |
| time/               |          |
|    episodes         | 792      |
|    fps              | 34       |
|    time_elapsed     | 4567     |
|    total_timesteps  | 157018   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.33e-06 |
|    n_updates        | 36754    |
----------------------------------
Eval num_timesteps=157500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.252    |
| time/               |          |
|    total_timesteps  | 157500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000327 |
|    n_updates        | 36874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.0317  |
|    exploration_rate | 0.251    |
| time/               |          |
|    episodes         | 796      |
|    fps              | 34       |
|    time_elapsed     | 4581     |
|    total_timesteps  | 157764   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44e-08 |
|    n_updates        | 36940    |
----------------------------------
Eval num_timesteps=158000, episode_reward=-0.19 +/- 0.17
Episode length: 206.24 +/- 26.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.25     |
| time/               |          |
|    total_timesteps  | 158000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.69e-06 |
|    n_updates        | 36999    |
----------------------------------
Eval num_timesteps=158500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.247    |
| time/               |          |
|    total_timesteps  | 158500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.45e-08 |
|    n_updates        | 37124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0431  |
|    exploration_rate | 0.247    |
| time/               |          |
|    episodes         | 800      |
|    fps              | 34       |
|    time_elapsed     | 4610     |
|    total_timesteps  | 158604   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.85e-07 |
|    n_updates        | 37150    |
----------------------------------
Eval num_timesteps=159000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.245    |
| time/               |          |
|    total_timesteps  | 159000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.09e-06 |
|    n_updates        | 37249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.0323  |
|    exploration_rate | 0.243    |
| time/               |          |
|    episodes         | 804      |
|    fps              | 34       |
|    time_elapsed     | 4627     |
|    total_timesteps  | 159355   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.48e-07 |
|    n_updates        | 37338    |
----------------------------------
Eval num_timesteps=159500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.242    |
| time/               |          |
|    total_timesteps  | 159500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000288 |
|    n_updates        | 37374    |
----------------------------------
Eval num_timesteps=160000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.24     |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04e-08 |
|    n_updates        | 37499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0537  |
|    exploration_rate | 0.239    |
| time/               |          |
|    episodes         | 808      |
|    fps              | 34       |
|    time_elapsed     | 4656     |
|    total_timesteps  | 160195   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.19e-05 |
|    n_updates        | 37548    |
----------------------------------
Eval num_timesteps=160500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.238    |
| time/               |          |
|    total_timesteps  | 160500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.55e-07 |
|    n_updates        | 37624    |
----------------------------------
Eval num_timesteps=161000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.235    |
| time/               |          |
|    total_timesteps  | 161000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-06 |
|    n_updates        | 37749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0658  |
|    exploration_rate | 0.235    |
| time/               |          |
|    episodes         | 812      |
|    fps              | 34       |
|    time_elapsed     | 4690     |
|    total_timesteps  | 161035   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.94e-06 |
|    n_updates        | 37758    |
----------------------------------
Eval num_timesteps=161500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.233    |
| time/               |          |
|    total_timesteps  | 161500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1e-07  |
|    n_updates        | 37874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0742  |
|    exploration_rate | 0.232    |
| time/               |          |
|    episodes         | 816      |
|    fps              | 34       |
|    time_elapsed     | 4703     |
|    total_timesteps  | 161673   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.36e-07 |
|    n_updates        | 37918    |
----------------------------------
Eval num_timesteps=162000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.231    |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.89e-07 |
|    n_updates        | 37999    |
----------------------------------
Eval num_timesteps=162500, episode_reward=-0.16 +/- 0.23
Episode length: 202.38 +/- 37.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.228    |
| time/               |          |
|    total_timesteps  | 162500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.34e-08 |
|    n_updates        | 38124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0862  |
|    exploration_rate | 0.228    |
| time/               |          |
|    episodes         | 820      |
|    fps              | 34       |
|    time_elapsed     | 4731     |
|    total_timesteps  | 162513   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.18e-09 |
|    n_updates        | 38128    |
----------------------------------
Eval num_timesteps=163000, episode_reward=-0.19 +/- 0.16
Episode length: 206.48 +/- 24.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.226    |
| time/               |          |
|    total_timesteps  | 163000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13e-05 |
|    n_updates        | 38249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0756  |
|    exploration_rate | 0.224    |
| time/               |          |
|    episodes         | 824      |
|    fps              | 34       |
|    time_elapsed     | 4749     |
|    total_timesteps  | 163298   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55e-07 |
|    n_updates        | 38324    |
----------------------------------
Eval num_timesteps=163500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.223    |
| time/               |          |
|    total_timesteps  | 163500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00032  |
|    n_updates        | 38374    |
----------------------------------
Eval num_timesteps=164000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.221    |
| time/               |          |
|    total_timesteps  | 164000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.26e-06 |
|    n_updates        | 38499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0864  |
|    exploration_rate | 0.22     |
| time/               |          |
|    episodes         | 828      |
|    fps              | 34       |
|    time_elapsed     | 4777     |
|    total_timesteps  | 164138   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.74e-06 |
|    n_updates        | 38534    |
----------------------------------
Eval num_timesteps=164500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.219    |
| time/               |          |
|    total_timesteps  | 164500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.49e-06 |
|    n_updates        | 38624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0864  |
|    exploration_rate | 0.216    |
| time/               |          |
|    episodes         | 832      |
|    fps              | 34       |
|    time_elapsed     | 4796     |
|    total_timesteps  | 164978   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.25e-06 |
|    n_updates        | 38744    |
----------------------------------
Eval num_timesteps=165000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.216    |
| time/               |          |
|    total_timesteps  | 165000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.32e-06 |
|    n_updates        | 38749    |
----------------------------------
Eval num_timesteps=165500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.214    |
| time/               |          |
|    total_timesteps  | 165500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.79e-06 |
|    n_updates        | 38874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0755  |
|    exploration_rate | 0.213    |
| time/               |          |
|    episodes         | 836      |
|    fps              | 34       |
|    time_elapsed     | 4825     |
|    total_timesteps  | 165727   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8e-06  |
|    n_updates        | 38931    |
----------------------------------
Eval num_timesteps=166000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.212    |
| time/               |          |
|    total_timesteps  | 166000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.56e-06 |
|    n_updates        | 38999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0765  |
|    exploration_rate | 0.209    |
| time/               |          |
|    episodes         | 840      |
|    fps              | 34       |
|    time_elapsed     | 4837     |
|    total_timesteps  | 166479   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.03e-07 |
|    n_updates        | 39119    |
----------------------------------
Eval num_timesteps=166500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.209    |
| time/               |          |
|    total_timesteps  | 166500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.19e-06 |
|    n_updates        | 39124    |
----------------------------------
Eval num_timesteps=167000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.207    |
| time/               |          |
|    total_timesteps  | 167000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0119   |
|    n_updates        | 39249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0649  |
|    exploration_rate | 0.206    |
| time/               |          |
|    episodes         | 844      |
|    fps              | 34       |
|    time_elapsed     | 4866     |
|    total_timesteps  | 167167   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.89e-06 |
|    n_updates        | 39291    |
----------------------------------
Eval num_timesteps=167500, episode_reward=-0.14 +/- 0.29
Episode length: 197.64 +/- 48.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.204    |
| time/               |          |
|    total_timesteps  | 167500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.33e-06 |
|    n_updates        | 39374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0766  |
|    exploration_rate | 0.203    |
| time/               |          |
|    episodes         | 848      |
|    fps              | 34       |
|    time_elapsed     | 4882     |
|    total_timesteps  | 167846   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.65e-07 |
|    n_updates        | 39461    |
----------------------------------
Eval num_timesteps=168000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.202    |
| time/               |          |
|    total_timesteps  | 168000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00037  |
|    n_updates        | 39499    |
----------------------------------
Eval num_timesteps=168500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 168500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-06 |
|    n_updates        | 39624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0766  |
|    exploration_rate | 0.199    |
| time/               |          |
|    episodes         | 852      |
|    fps              | 34       |
|    time_elapsed     | 4910     |
|    total_timesteps  | 168686   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000385 |
|    n_updates        | 39671    |
----------------------------------
Eval num_timesteps=169000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.197    |
| time/               |          |
|    total_timesteps  | 169000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000398 |
|    n_updates        | 39749    |
----------------------------------
Eval num_timesteps=169500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.195    |
| time/               |          |
|    total_timesteps  | 169500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.62e-07 |
|    n_updates        | 39874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0867  |
|    exploration_rate | 0.195    |
| time/               |          |
|    episodes         | 856      |
|    fps              | 34       |
|    time_elapsed     | 4938     |
|    total_timesteps  | 169526   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.46e-07 |
|    n_updates        | 39881    |
----------------------------------
Eval num_timesteps=170000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.193    |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.04e-06 |
|    n_updates        | 39999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0747  |
|    exploration_rate | 0.192    |
| time/               |          |
|    episodes         | 860      |
|    fps              | 34       |
|    time_elapsed     | 4957     |
|    total_timesteps  | 170172   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.96e-07 |
|    n_updates        | 40042    |
----------------------------------
Eval num_timesteps=170500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.19     |
| time/               |          |
|    total_timesteps  | 170500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.09e-06 |
|    n_updates        | 40124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0742  |
|    exploration_rate | 0.188    |
| time/               |          |
|    episodes         | 864      |
|    fps              | 34       |
|    time_elapsed     | 4969     |
|    total_timesteps  | 170953   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.48e-07 |
|    n_updates        | 40238    |
----------------------------------
Eval num_timesteps=171000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.188    |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.94e-07 |
|    n_updates        | 40249    |
----------------------------------
Eval num_timesteps=171500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.185    |
| time/               |          |
|    total_timesteps  | 171500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.3e-06  |
|    n_updates        | 40374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0628  |
|    exploration_rate | 0.185    |
| time/               |          |
|    episodes         | 868      |
|    fps              | 34       |
|    time_elapsed     | 4998     |
|    total_timesteps  | 171652   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000389 |
|    n_updates        | 40412    |
----------------------------------
Eval num_timesteps=172000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.183    |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21e-06 |
|    n_updates        | 40499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | -0.0409  |
|    exploration_rate | 0.182    |
| time/               |          |
|    episodes         | 872      |
|    fps              | 34       |
|    time_elapsed     | 5014     |
|    total_timesteps  | 172305   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.97e-06 |
|    n_updates        | 40576    |
----------------------------------
Eval num_timesteps=172500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.181    |
| time/               |          |
|    total_timesteps  | 172500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.6e-07  |
|    n_updates        | 40624    |
----------------------------------
Eval num_timesteps=173000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.178    |
| time/               |          |
|    total_timesteps  | 173000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000396 |
|    n_updates        | 40749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | -0.0409  |
|    exploration_rate | 0.178    |
| time/               |          |
|    episodes         | 876      |
|    fps              | 34       |
|    time_elapsed     | 5043     |
|    total_timesteps  | 173145   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.96e-06 |
|    n_updates        | 40786    |
----------------------------------
Eval num_timesteps=173500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.176    |
| time/               |          |
|    total_timesteps  | 173500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.85e-07 |
|    n_updates        | 40874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0527  |
|    exploration_rate | 0.174    |
| time/               |          |
|    episodes         | 880      |
|    fps              | 34       |
|    time_elapsed     | 5057     |
|    total_timesteps  | 173985   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0115   |
|    n_updates        | 40996    |
----------------------------------
Eval num_timesteps=174000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.174    |
| time/               |          |
|    total_timesteps  | 174000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.35e-06 |
|    n_updates        | 40999    |
----------------------------------
Eval num_timesteps=174500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.171    |
| time/               |          |
|    total_timesteps  | 174500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000399 |
|    n_updates        | 41124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.042   |
|    exploration_rate | 0.17     |
| time/               |          |
|    episodes         | 884      |
|    fps              | 34       |
|    time_elapsed     | 5087     |
|    total_timesteps  | 174753   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.66e-06 |
|    n_updates        | 41188    |
----------------------------------
Eval num_timesteps=175000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.169    |
| time/               |          |
|    total_timesteps  | 175000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.13e-07 |
|    n_updates        | 41249    |
----------------------------------
Eval num_timesteps=175500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.166    |
| time/               |          |
|    total_timesteps  | 175500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-06 |
|    n_updates        | 41374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | -0.042   |
|    exploration_rate | 0.166    |
| time/               |          |
|    episodes         | 888      |
|    fps              | 34       |
|    time_elapsed     | 5115     |
|    total_timesteps  | 175593   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.95e-06 |
|    n_updates        | 41398    |
----------------------------------
Eval num_timesteps=176000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.164    |
| time/               |          |
|    total_timesteps  | 176000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000372 |
|    n_updates        | 41499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0541  |
|    exploration_rate | 0.162    |
| time/               |          |
|    episodes         | 892      |
|    fps              | 34       |
|    time_elapsed     | 5128     |
|    total_timesteps  | 176433   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.75e-06 |
|    n_updates        | 41608    |
----------------------------------
Eval num_timesteps=176500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.162    |
| time/               |          |
|    total_timesteps  | 176500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-07 |
|    n_updates        | 41624    |
----------------------------------
Eval num_timesteps=177000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.159    |
| time/               |          |
|    total_timesteps  | 177000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000371 |
|    n_updates        | 41749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0534  |
|    exploration_rate | 0.159    |
| time/               |          |
|    episodes         | 896      |
|    fps              | 34       |
|    time_elapsed     | 5156     |
|    total_timesteps  | 177115   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-06 |
|    n_updates        | 41778    |
----------------------------------
Eval num_timesteps=177500, episode_reward=-0.14 +/- 0.29
Episode length: 197.72 +/- 48.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.157    |
| time/               |          |
|    total_timesteps  | 177500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.82e-06 |
|    n_updates        | 41874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0534  |
|    exploration_rate | 0.155    |
| time/               |          |
|    episodes         | 900      |
|    fps              | 34       |
|    time_elapsed     | 5174     |
|    total_timesteps  | 177955   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.78e-08 |
|    n_updates        | 41988    |
----------------------------------
Eval num_timesteps=178000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.155    |
| time/               |          |
|    total_timesteps  | 178000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.07e-08 |
|    n_updates        | 41999    |
----------------------------------
Eval num_timesteps=178500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.152    |
| time/               |          |
|    total_timesteps  | 178500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.61e-07 |
|    n_updates        | 42124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0643  |
|    exploration_rate | 0.151    |
| time/               |          |
|    episodes         | 904      |
|    fps              | 34       |
|    time_elapsed     | 5203     |
|    total_timesteps  | 178795   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.77e-06 |
|    n_updates        | 42198    |
----------------------------------
Eval num_timesteps=179000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.15     |
| time/               |          |
|    total_timesteps  | 179000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.23e-07 |
|    n_updates        | 42249    |
----------------------------------
Eval num_timesteps=179500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.147    |
| time/               |          |
|    total_timesteps  | 179500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-07 |
|    n_updates        | 42374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.0643  |
|    exploration_rate | 0.147    |
| time/               |          |
|    episodes         | 908      |
|    fps              | 34       |
|    time_elapsed     | 5233     |
|    total_timesteps  | 179635   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29e-06 |
|    n_updates        | 42408    |
----------------------------------
Eval num_timesteps=180000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.145    |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.95e-07 |
|    n_updates        | 42499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.0525  |
|    exploration_rate | 0.144    |
| time/               |          |
|    episodes         | 912      |
|    fps              | 34       |
|    time_elapsed     | 5244     |
|    total_timesteps  | 180291   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.77e-06 |
|    n_updates        | 42572    |
----------------------------------
Eval num_timesteps=180500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.143    |
| time/               |          |
|    total_timesteps  | 180500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.7e-06  |
|    n_updates        | 42624    |
----------------------------------
Eval num_timesteps=181000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.14     |
| time/               |          |
|    total_timesteps  | 181000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.38e-07 |
|    n_updates        | 42749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0645  |
|    exploration_rate | 0.14     |
| time/               |          |
|    episodes         | 916      |
|    fps              | 34       |
|    time_elapsed     | 5279     |
|    total_timesteps  | 181131   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58e-06 |
|    n_updates        | 42782    |
----------------------------------
Eval num_timesteps=181500, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.138    |
| time/               |          |
|    total_timesteps  | 181500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.12e-06 |
|    n_updates        | 42874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0645  |
|    exploration_rate | 0.136    |
| time/               |          |
|    episodes         | 920      |
|    fps              | 34       |
|    time_elapsed     | 5291     |
|    total_timesteps  | 181971   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92e-08 |
|    n_updates        | 42992    |
----------------------------------
Eval num_timesteps=182000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.136    |
| time/               |          |
|    total_timesteps  | 182000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0115   |
|    n_updates        | 42999    |
----------------------------------
Eval num_timesteps=182500, episode_reward=-0.16 +/- 0.23
Episode length: 202.26 +/- 37.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.133    |
| time/               |          |
|    total_timesteps  | 182500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.87e-06 |
|    n_updates        | 43124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0751  |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 924      |
|    fps              | 34       |
|    time_elapsed     | 5318     |
|    total_timesteps  | 182811   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000311 |
|    n_updates        | 43202    |
----------------------------------
Eval num_timesteps=183000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.131    |
| time/               |          |
|    total_timesteps  | 183000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91e-06 |
|    n_updates        | 43249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.063   |
|    exploration_rate | 0.129    |
| time/               |          |
|    episodes         | 928      |
|    fps              | 34       |
|    time_elapsed     | 5336     |
|    total_timesteps  | 183448   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000308 |
|    n_updates        | 43361    |
----------------------------------
Eval num_timesteps=183500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.128    |
| time/               |          |
|    total_timesteps  | 183500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88e-06 |
|    n_updates        | 43374    |
----------------------------------
Eval num_timesteps=184000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.126    |
| time/               |          |
|    total_timesteps  | 184000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.48e-05 |
|    n_updates        | 43499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | -0.063   |
|    exploration_rate | 0.125    |
| time/               |          |
|    episodes         | 932      |
|    fps              | 34       |
|    time_elapsed     | 5365     |
|    total_timesteps  | 184288   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.98e-05 |
|    n_updates        | 43571    |
----------------------------------
Eval num_timesteps=184500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.124    |
| time/               |          |
|    total_timesteps  | 184500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.52e-06 |
|    n_updates        | 43624    |
----------------------------------
Eval num_timesteps=185000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.121    |
| time/               |          |
|    total_timesteps  | 185000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21e-06 |
|    n_updates        | 43749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | -0.074   |
|    exploration_rate | 0.121    |
| time/               |          |
|    episodes         | 936      |
|    fps              | 34       |
|    time_elapsed     | 5394     |
|    total_timesteps  | 185128   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000349 |
|    n_updates        | 43781    |
----------------------------------
Eval num_timesteps=185500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.119    |
| time/               |          |
|    total_timesteps  | 185500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.29e-05 |
|    n_updates        | 43874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | -0.0848  |
|    exploration_rate | 0.117    |
| time/               |          |
|    episodes         | 940      |
|    fps              | 34       |
|    time_elapsed     | 5409     |
|    total_timesteps  | 185968   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.22e-06 |
|    n_updates        | 43991    |
----------------------------------
Eval num_timesteps=186000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.117    |
| time/               |          |
|    total_timesteps  | 186000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.56e-06 |
|    n_updates        | 43999    |
----------------------------------
Eval num_timesteps=186500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.114    |
| time/               |          |
|    total_timesteps  | 186500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.85e-06 |
|    n_updates        | 44124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0862  |
|    exploration_rate | 0.113    |
| time/               |          |
|    episodes         | 944      |
|    fps              | 34       |
|    time_elapsed     | 5438     |
|    total_timesteps  | 186790   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.68e-06 |
|    n_updates        | 44197    |
----------------------------------
Eval num_timesteps=187000, episode_reward=-0.16 +/- 0.23
Episode length: 202.56 +/- 36.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | -0.163   |
| rollout/            |          |
|    exploration_rate | 0.112    |
| time/               |          |
|    total_timesteps  | 187000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.29e-07 |
|    n_updates        | 44249    |
----------------------------------
Eval num_timesteps=187500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.109    |
| time/               |          |
|    total_timesteps  | 187500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-05 |
|    n_updates        | 44374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0978  |
|    exploration_rate | 0.109    |
| time/               |          |
|    episodes         | 948      |
|    fps              | 34       |
|    time_elapsed     | 5468     |
|    total_timesteps  | 187630   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5e-06  |
|    n_updates        | 44407    |
----------------------------------
Eval num_timesteps=188000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.107    |
| time/               |          |
|    total_timesteps  | 188000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.37e-06 |
|    n_updates        | 44499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.105    |
| time/               |          |
|    episodes         | 952      |
|    fps              | 34       |
|    time_elapsed     | 5480     |
|    total_timesteps  | 188423   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.93e-07 |
|    n_updates        | 44605    |
----------------------------------
Eval num_timesteps=188500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.105    |
| time/               |          |
|    total_timesteps  | 188500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.39e-07 |
|    n_updates        | 44624    |
----------------------------------
Eval num_timesteps=189000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.102    |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.43e-06 |
|    n_updates        | 44749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.101    |
| time/               |          |
|    episodes         | 956      |
|    fps              | 34       |
|    time_elapsed     | 5509     |
|    total_timesteps  | 189263   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.68e-07 |
|    n_updates        | 44815    |
----------------------------------
Eval num_timesteps=189500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0999   |
| time/               |          |
|    total_timesteps  | 189500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.29e-07 |
|    n_updates        | 44874    |
----------------------------------
Eval num_timesteps=190000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0975   |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000377 |
|    n_updates        | 44999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0993  |
|    exploration_rate | 0.097    |
| time/               |          |
|    episodes         | 960      |
|    fps              | 34       |
|    time_elapsed     | 5538     |
|    total_timesteps  | 190103   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.61e-06 |
|    n_updates        | 45025    |
----------------------------------
Eval num_timesteps=190500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0951   |
| time/               |          |
|    total_timesteps  | 190500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.69e-07 |
|    n_updates        | 45124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.093    |
| time/               |          |
|    episodes         | 964      |
|    fps              | 34       |
|    time_elapsed     | 5557     |
|    total_timesteps  | 190943   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00039  |
|    n_updates        | 45235    |
----------------------------------
Eval num_timesteps=191000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0928   |
| time/               |          |
|    total_timesteps  | 191000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000394 |
|    n_updates        | 45249    |
----------------------------------
Eval num_timesteps=191500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.0904   |
| time/               |          |
|    total_timesteps  | 191500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.57e-06 |
|    n_updates        | 45374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 968      |
|    fps              | 34       |
|    time_elapsed     | 5586     |
|    total_timesteps  | 191783   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.14e-06 |
|    n_updates        | 45445    |
----------------------------------
Eval num_timesteps=192000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.088    |
| time/               |          |
|    total_timesteps  | 192000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.69e-07 |
|    n_updates        | 45499    |
----------------------------------
Eval num_timesteps=192500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0856   |
| time/               |          |
|    total_timesteps  | 192500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-06 |
|    n_updates        | 45624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.085    |
| time/               |          |
|    episodes         | 972      |
|    fps              | 34       |
|    time_elapsed     | 5615     |
|    total_timesteps  | 192623   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-07 |
|    n_updates        | 45655    |
----------------------------------
Eval num_timesteps=193000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0833   |
| time/               |          |
|    total_timesteps  | 193000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000373 |
|    n_updates        | 45749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.0811   |
| time/               |          |
|    episodes         | 976      |
|    fps              | 34       |
|    time_elapsed     | 5629     |
|    total_timesteps  | 193463   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-05 |
|    n_updates        | 45865    |
----------------------------------
Eval num_timesteps=193500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0809   |
| time/               |          |
|    total_timesteps  | 193500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.08e-06 |
|    n_updates        | 45874    |
----------------------------------
Eval num_timesteps=194000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 194000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-06 |
|    n_updates        | 45999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0773   |
| time/               |          |
|    episodes         | 980      |
|    fps              | 34       |
|    time_elapsed     | 5662     |
|    total_timesteps  | 194263   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.01e-08 |
|    n_updates        | 46065    |
----------------------------------
Eval num_timesteps=194500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.0761   |
| time/               |          |
|    total_timesteps  | 194500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.81e-06 |
|    n_updates        | 46124    |
----------------------------------
Eval num_timesteps=195000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0738   |
| time/               |          |
|    total_timesteps  | 195000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.37e-07 |
|    n_updates        | 46249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.0733   |
| time/               |          |
|    episodes         | 984      |
|    fps              | 34       |
|    time_elapsed     | 5691     |
|    total_timesteps  | 195103   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000345 |
|    n_updates        | 46275    |
----------------------------------
Eval num_timesteps=195500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.0714   |
| time/               |          |
|    total_timesteps  | 195500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.63e-07 |
|    n_updates        | 46374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.0693   |
| time/               |          |
|    episodes         | 988      |
|    fps              | 34       |
|    time_elapsed     | 5703     |
|    total_timesteps  | 195943   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43e-07 |
|    n_updates        | 46485    |
----------------------------------
Eval num_timesteps=196000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.069    |
| time/               |          |
|    total_timesteps  | 196000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.12e-07 |
|    n_updates        | 46499    |
----------------------------------
Eval num_timesteps=196500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0666   |
| time/               |          |
|    total_timesteps  | 196500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94e-06 |
|    n_updates        | 46624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.0653   |
| time/               |          |
|    episodes         | 992      |
|    fps              | 34       |
|    time_elapsed     | 5736     |
|    total_timesteps  | 196783   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.13e-06 |
|    n_updates        | 46695    |
----------------------------------
Eval num_timesteps=197000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0643   |
| time/               |          |
|    total_timesteps  | 197000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.21e-06 |
|    n_updates        | 46749    |
----------------------------------
Eval num_timesteps=197500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.0619   |
| time/               |          |
|    total_timesteps  | 197500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000365 |
|    n_updates        | 46874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.0613   |
| time/               |          |
|    episodes         | 996      |
|    fps              | 34       |
|    time_elapsed     | 5765     |
|    total_timesteps  | 197623   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.2e-06  |
|    n_updates        | 46905    |
----------------------------------
Eval num_timesteps=198000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0595   |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.93e-06 |
|    n_updates        | 46999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.0573   |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 34       |
|    time_elapsed     | 5777     |
|    total_timesteps  | 198463   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.02e-06 |
|    n_updates        | 47115    |
----------------------------------
Eval num_timesteps=198500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.0571   |
| time/               |          |
|    total_timesteps  | 198500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.08e-06 |
|    n_updates        | 47124    |
----------------------------------
Eval num_timesteps=199000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0548   |
| time/               |          |
|    total_timesteps  | 199000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.92e-07 |
|    n_updates        | 47249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0545   |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 34       |
|    time_elapsed     | 5811     |
|    total_timesteps  | 199052   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.72e-06 |
|    n_updates        | 47262    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0525   |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 34       |
|    time_elapsed     | 5812     |
|    total_timesteps  | 199483   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8e-06  |
|    n_updates        | 47370    |
----------------------------------
Eval num_timesteps=199500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0524   |
| time/               |          |
|    total_timesteps  | 199500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.26e-07 |
|    n_updates        | 47374    |
----------------------------------
Eval num_timesteps=200000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000377 |
|    n_updates        | 47499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 34       |
|    time_elapsed     | 5841     |
|    total_timesteps  | 200323   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.28e-07 |
|    n_updates        | 47580    |
----------------------------------
Eval num_timesteps=200500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 200500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.92e-07 |
|    n_updates        | 47624    |
----------------------------------
Eval num_timesteps=201000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 201000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.72e-06 |
|    n_updates        | 47749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 34       |
|    time_elapsed     | 5870     |
|    total_timesteps  | 201163   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.88e-07 |
|    n_updates        | 47790    |
----------------------------------
Eval num_timesteps=201500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 201500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.79e-06 |
|    n_updates        | 47874    |
----------------------------------
Eval num_timesteps=202000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 202000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.35e-07 |
|    n_updates        | 47999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 34       |
|    time_elapsed     | 5900     |
|    total_timesteps  | 202003   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.14e-06 |
|    n_updates        | 48000    |
----------------------------------
Eval num_timesteps=202500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 202500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.5e-06  |
|    n_updates        | 48124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 34       |
|    time_elapsed     | 5918     |
|    total_timesteps  | 202759   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1e-07  |
|    n_updates        | 48189    |
----------------------------------
Eval num_timesteps=203000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 203000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-06 |
|    n_updates        | 48249    |
----------------------------------
Eval num_timesteps=203500, episode_reward=-0.16 +/- 0.24
Episode length: 201.86 +/- 39.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 203500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5e-07    |
|    n_updates        | 48374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 34       |
|    time_elapsed     | 5946     |
|    total_timesteps  | 203599   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000398 |
|    n_updates        | 48399    |
----------------------------------
Eval num_timesteps=204000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 204000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000393 |
|    n_updates        | 48499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 34       |
|    time_elapsed     | 5959     |
|    total_timesteps  | 204439   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.87e-07 |
|    n_updates        | 48609    |
----------------------------------
Eval num_timesteps=204500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.12e-06 |
|    n_updates        | 48624    |
----------------------------------
Eval num_timesteps=205000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.66e-07 |
|    n_updates        | 48749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 34       |
|    time_elapsed     | 5988     |
|    total_timesteps  | 205279   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000406 |
|    n_updates        | 48819    |
----------------------------------
Eval num_timesteps=205500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 205500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.48e-06 |
|    n_updates        | 48874    |
----------------------------------
Eval num_timesteps=206000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000364 |
|    n_updates        | 48999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 34       |
|    time_elapsed     | 6017     |
|    total_timesteps  | 206119   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4e-06  |
|    n_updates        | 49029    |
----------------------------------
Eval num_timesteps=206500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 206500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.67e-06 |
|    n_updates        | 49124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 34       |
|    time_elapsed     | 6037     |
|    total_timesteps  | 206945   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.35e-07 |
|    n_updates        | 49236    |
----------------------------------
Eval num_timesteps=207000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-07 |
|    n_updates        | 49249    |
----------------------------------
Eval num_timesteps=207500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.63e-07 |
|    n_updates        | 49374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 34       |
|    time_elapsed     | 6065     |
|    total_timesteps  | 207785   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13e-06 |
|    n_updates        | 49446    |
----------------------------------
Eval num_timesteps=208000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 208000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.6e-06  |
|    n_updates        | 49499    |
----------------------------------
Eval num_timesteps=208500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 208500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.11e-07 |
|    n_updates        | 49624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 34       |
|    time_elapsed     | 6093     |
|    total_timesteps  | 208625   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000332 |
|    n_updates        | 49656    |
----------------------------------
Eval num_timesteps=209000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 209000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.52e-07 |
|    n_updates        | 49749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 34       |
|    time_elapsed     | 6108     |
|    total_timesteps  | 209465   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000334 |
|    n_updates        | 49866    |
----------------------------------
Eval num_timesteps=209500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 209500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01e-06 |
|    n_updates        | 49874    |
----------------------------------
Eval num_timesteps=210000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-05 |
|    n_updates        | 49999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 34       |
|    time_elapsed     | 6137     |
|    total_timesteps  | 210182   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.78e-07 |
|    n_updates        | 50045    |
----------------------------------
Eval num_timesteps=210500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 210500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.78e-06 |
|    n_updates        | 50124    |
----------------------------------
Eval num_timesteps=211000, episode_reward=-0.19 +/- 0.15
Episode length: 208.72 +/- 8.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | -0.189   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 211000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.18e-06 |
|    n_updates        | 50249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 34       |
|    time_elapsed     | 6166     |
|    total_timesteps  | 211022   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28e-06 |
|    n_updates        | 50255    |
----------------------------------
Eval num_timesteps=211500, episode_reward=-0.16 +/- 0.23
Episode length: 202.16 +/- 38.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.54e-06 |
|    n_updates        | 50374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 34       |
|    time_elapsed     | 6183     |
|    total_timesteps  | 211862   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.07e-07 |
|    n_updates        | 50465    |
----------------------------------
Eval num_timesteps=212000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 212000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.84e-08 |
|    n_updates        | 50499    |
----------------------------------
Eval num_timesteps=212500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 212500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.4e-06  |
|    n_updates        | 50624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 34       |
|    time_elapsed     | 6212     |
|    total_timesteps  | 212702   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.16e-07 |
|    n_updates        | 50675    |
----------------------------------
Eval num_timesteps=213000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 213000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-06 |
|    n_updates        | 50749    |
----------------------------------
Eval num_timesteps=213500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.49e-06 |
|    n_updates        | 50874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 34       |
|    time_elapsed     | 6241     |
|    total_timesteps  | 213542   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0113   |
|    n_updates        | 50885    |
----------------------------------
Eval num_timesteps=214000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 214000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-07 |
|    n_updates        | 50999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 34       |
|    time_elapsed     | 6256     |
|    total_timesteps  | 214382   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.37e-07 |
|    n_updates        | 51095    |
----------------------------------
Eval num_timesteps=214500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 214500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.29e-07 |
|    n_updates        | 51124    |
----------------------------------
Eval num_timesteps=215000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 215000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.79e-07 |
|    n_updates        | 51249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 34       |
|    time_elapsed     | 6284     |
|    total_timesteps  | 215111   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.87e-06 |
|    n_updates        | 51277    |
----------------------------------
Eval num_timesteps=215500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 215500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.13e-07 |
|    n_updates        | 51374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0994  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 34       |
|    time_elapsed     | 6302     |
|    total_timesteps  | 215888   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.52e-07 |
|    n_updates        | 51471    |
----------------------------------
Eval num_timesteps=216000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.6e-07  |
|    n_updates        | 51499    |
----------------------------------
Eval num_timesteps=216500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 216500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-06 |
|    n_updates        | 51624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0994  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 34       |
|    time_elapsed     | 6331     |
|    total_timesteps  | 216728   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.61e-07 |
|    n_updates        | 51681    |
----------------------------------
Eval num_timesteps=217000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 217000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54e-06 |
|    n_updates        | 51749    |
----------------------------------
Eval num_timesteps=217500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.84e-06 |
|    n_updates        | 51874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0994  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 34       |
|    time_elapsed     | 6361     |
|    total_timesteps  | 217568   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13e-06 |
|    n_updates        | 51891    |
----------------------------------
Eval num_timesteps=218000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 218000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000322 |
|    n_updates        | 51999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0994  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 34       |
|    time_elapsed     | 6377     |
|    total_timesteps  | 218408   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.27e-07 |
|    n_updates        | 52101    |
----------------------------------
Eval num_timesteps=218500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 218500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.3e-06  |
|    n_updates        | 52124    |
----------------------------------
Eval num_timesteps=219000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 219000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.08e-07 |
|    n_updates        | 52249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 34       |
|    time_elapsed     | 6410     |
|    total_timesteps  | 219047   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-06 |
|    n_updates        | 52261    |
----------------------------------
Eval num_timesteps=219500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 219500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.2e-06  |
|    n_updates        | 52374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 34       |
|    time_elapsed     | 6426     |
|    total_timesteps  | 219887   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000304 |
|    n_updates        | 52471    |
----------------------------------
Eval num_timesteps=220000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.43e-07 |
|    n_updates        | 52499    |
----------------------------------
Eval num_timesteps=220500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 220500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-06 |
|    n_updates        | 52624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 34       |
|    time_elapsed     | 6455     |
|    total_timesteps  | 220727   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.31e-07 |
|    n_updates        | 52681    |
----------------------------------
Eval num_timesteps=221000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 221000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0119   |
|    n_updates        | 52749    |
----------------------------------
Eval num_timesteps=221500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 221500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-06 |
|    n_updates        | 52874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 34       |
|    time_elapsed     | 6485     |
|    total_timesteps  | 221567   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58e-07 |
|    n_updates        | 52891    |
----------------------------------
Eval num_timesteps=222000, episode_reward=-0.16 +/- 0.23
Episode length: 202.82 +/- 35.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | -0.163   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 222000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.02e-06 |
|    n_updates        | 52999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 34       |
|    time_elapsed     | 6497     |
|    total_timesteps  | 222407   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.77e-07 |
|    n_updates        | 53101    |
----------------------------------
Eval num_timesteps=222500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 222500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.03e-07 |
|    n_updates        | 53124    |
----------------------------------
Eval num_timesteps=223000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 223000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.44e-07 |
|    n_updates        | 53249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 34       |
|    time_elapsed     | 6526     |
|    total_timesteps  | 223247   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.46e-07 |
|    n_updates        | 53311    |
----------------------------------
Eval num_timesteps=223500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 223500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.68e-07 |
|    n_updates        | 53374    |
----------------------------------
Eval num_timesteps=224000, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 224000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.44e-06 |
|    n_updates        | 53499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 34       |
|    time_elapsed     | 6554     |
|    total_timesteps  | 224087   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.62e-06 |
|    n_updates        | 53521    |
----------------------------------
Eval num_timesteps=224500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 224500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.27e-06 |
|    n_updates        | 53624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 34       |
|    time_elapsed     | 6572     |
|    total_timesteps  | 224927   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.78e-06 |
|    n_updates        | 53731    |
----------------------------------
Eval num_timesteps=225000, episode_reward=-0.19 +/- 0.16
Episode length: 207.42 +/- 18.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00032  |
|    n_updates        | 53749    |
----------------------------------
Eval num_timesteps=225500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 225500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000308 |
|    n_updates        | 53874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 34       |
|    time_elapsed     | 6600     |
|    total_timesteps  | 225606   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.68e-06 |
|    n_updates        | 53901    |
----------------------------------
Eval num_timesteps=226000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 226000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35e-06 |
|    n_updates        | 53999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 34       |
|    time_elapsed     | 6614     |
|    total_timesteps  | 226446   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000335 |
|    n_updates        | 54111    |
----------------------------------
Eval num_timesteps=226500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 226500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.45e-07 |
|    n_updates        | 54124    |
----------------------------------
Eval num_timesteps=227000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 227000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.09e-07 |
|    n_updates        | 54249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 34       |
|    time_elapsed     | 6645     |
|    total_timesteps  | 227286   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.38e-07 |
|    n_updates        | 54321    |
----------------------------------
Eval num_timesteps=227500, episode_reward=-0.19 +/- 0.17
Episode length: 206.28 +/- 26.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 227500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18e-07 |
|    n_updates        | 54374    |
----------------------------------
Eval num_timesteps=228000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 228000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.11e-07 |
|    n_updates        | 54499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 34       |
|    time_elapsed     | 6674     |
|    total_timesteps  | 228126   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.43e-06 |
|    n_updates        | 54531    |
----------------------------------
Eval num_timesteps=228500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 228500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.28e-07 |
|    n_updates        | 54624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 34       |
|    time_elapsed     | 6691     |
|    total_timesteps  | 228966   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.55e-06 |
|    n_updates        | 54741    |
----------------------------------
Eval num_timesteps=229000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 229000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.07e-07 |
|    n_updates        | 54749    |
----------------------------------
Eval num_timesteps=229500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 229500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000283 |
|    n_updates        | 54874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 34       |
|    time_elapsed     | 6721     |
|    total_timesteps  | 229806   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.78e-06 |
|    n_updates        | 54951    |
----------------------------------
Eval num_timesteps=230000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000285 |
|    n_updates        | 54999    |
----------------------------------
Eval num_timesteps=230500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 230500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.43e-07 |
|    n_updates        | 55124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 34       |
|    time_elapsed     | 6750     |
|    total_timesteps  | 230646   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000554 |
|    n_updates        | 55161    |
----------------------------------
Eval num_timesteps=231000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 231000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-07 |
|    n_updates        | 55249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 34       |
|    time_elapsed     | 6764     |
|    total_timesteps  | 231486   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.76e-06 |
|    n_updates        | 55371    |
----------------------------------
Eval num_timesteps=231500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 231500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.15e-06 |
|    n_updates        | 55374    |
----------------------------------
Eval num_timesteps=232000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-06 |
|    n_updates        | 55499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 34       |
|    time_elapsed     | 6792     |
|    total_timesteps  | 232326   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.85e-06 |
|    n_updates        | 55581    |
----------------------------------
Eval num_timesteps=232500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 232500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.13e-07 |
|    n_updates        | 55624    |
----------------------------------
Eval num_timesteps=233000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 233000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000303 |
|    n_updates        | 55749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 34       |
|    time_elapsed     | 6821     |
|    total_timesteps  | 233166   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.1e-07  |
|    n_updates        | 55791    |
----------------------------------
Eval num_timesteps=233500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 233500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.25e-07 |
|    n_updates        | 55874    |
----------------------------------
Eval num_timesteps=234000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.58e-07 |
|    n_updates        | 55999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 34       |
|    time_elapsed     | 6850     |
|    total_timesteps  | 234006   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00031  |
|    n_updates        | 56001    |
----------------------------------
Eval num_timesteps=234500, episode_reward=-0.19 +/- 0.16
Episode length: 206.48 +/- 24.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 234500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67e-07 |
|    n_updates        | 56124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 34       |
|    time_elapsed     | 6869     |
|    total_timesteps  | 234846   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.34e-07 |
|    n_updates        | 56211    |
----------------------------------
Eval num_timesteps=235000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 235000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.81e-07 |
|    n_updates        | 56249    |
----------------------------------
Eval num_timesteps=235500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 235500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.38e-06 |
|    n_updates        | 56374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 34       |
|    time_elapsed     | 6898     |
|    total_timesteps  | 235686   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.34e-06 |
|    n_updates        | 56421    |
----------------------------------
Eval num_timesteps=236000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.52e-06 |
|    n_updates        | 56499    |
----------------------------------
Eval num_timesteps=236500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 236500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1e-05    |
|    n_updates        | 56624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.186   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 34       |
|    time_elapsed     | 6926     |
|    total_timesteps  | 236526   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.45e-06 |
|    n_updates        | 56631    |
----------------------------------
Eval num_timesteps=237000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 237000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.97e-06 |
|    n_updates        | 56749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.186   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 34       |
|    time_elapsed     | 6944     |
|    total_timesteps  | 237366   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67e-06 |
|    n_updates        | 56841    |
----------------------------------
Eval num_timesteps=237500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 237500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.1e-07  |
|    n_updates        | 56874    |
----------------------------------
Eval num_timesteps=238000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 238000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-07 |
|    n_updates        | 56999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.186   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 34       |
|    time_elapsed     | 6973     |
|    total_timesteps  | 238206   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-06 |
|    n_updates        | 57051    |
----------------------------------
Eval num_timesteps=238500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 238500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.6e-07  |
|    n_updates        | 57124    |
----------------------------------
Eval num_timesteps=239000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 239000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000323 |
|    n_updates        | 57249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.186   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 34       |
|    time_elapsed     | 7001     |
|    total_timesteps  | 239046   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000316 |
|    n_updates        | 57261    |
----------------------------------
Eval num_timesteps=239500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 239500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-05 |
|    n_updates        | 57374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 34       |
|    time_elapsed     | 7014     |
|    total_timesteps  | 239886   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87e-07 |
|    n_updates        | 57471    |
----------------------------------
Eval num_timesteps=240000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 240000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000317 |
|    n_updates        | 57499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 34       |
|    time_elapsed     | 7031     |
|    total_timesteps  | 240453   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.92e-08 |
|    n_updates        | 57613    |
----------------------------------
Eval num_timesteps=240500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 240500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.48e-07 |
|    n_updates        | 57624    |
----------------------------------
Eval num_timesteps=241000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 241000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000311 |
|    n_updates        | 57749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 34       |
|    time_elapsed     | 7058     |
|    total_timesteps  | 241293   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.78e-07 |
|    n_updates        | 57823    |
----------------------------------
Eval num_timesteps=241500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 241500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.16e-07 |
|    n_updates        | 57874    |
----------------------------------
Eval num_timesteps=242000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000303 |
|    n_updates        | 57999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 34       |
|    time_elapsed     | 7086     |
|    total_timesteps  | 242133   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.36e-07 |
|    n_updates        | 58033    |
----------------------------------
Eval num_timesteps=242500, episode_reward=-0.14 +/- 0.29
Episode length: 197.66 +/- 48.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 242500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.68e-06 |
|    n_updates        | 58124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 34       |
|    time_elapsed     | 7101     |
|    total_timesteps  | 242973   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.48e-07 |
|    n_updates        | 58243    |
----------------------------------
Eval num_timesteps=243000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.6e-06  |
|    n_updates        | 58249    |
----------------------------------
Eval num_timesteps=243500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 243500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.6e-07  |
|    n_updates        | 58374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 34       |
|    time_elapsed     | 7130     |
|    total_timesteps  | 243813   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2e-06  |
|    n_updates        | 58453    |
----------------------------------
Eval num_timesteps=244000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 244000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.39e-07 |
|    n_updates        | 58499    |
----------------------------------
Eval num_timesteps=244500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 244500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.62e-07 |
|    n_updates        | 58624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 34       |
|    time_elapsed     | 7159     |
|    total_timesteps  | 244653   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.83e-06 |
|    n_updates        | 58663    |
----------------------------------
Eval num_timesteps=245000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 245000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.21e-08 |
|    n_updates        | 58749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 34       |
|    time_elapsed     | 7172     |
|    total_timesteps  | 245493   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000302 |
|    n_updates        | 58873    |
----------------------------------
Eval num_timesteps=245500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 245500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33e-06 |
|    n_updates        | 58874    |
----------------------------------
Eval num_timesteps=246000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 246000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000306 |
|    n_updates        | 58999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 34       |
|    time_elapsed     | 7205     |
|    total_timesteps  | 246333   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.05e-08 |
|    n_updates        | 59083    |
----------------------------------
Eval num_timesteps=246500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 246500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-06 |
|    n_updates        | 59124    |
----------------------------------
Eval num_timesteps=247000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 247000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.05e-08 |
|    n_updates        | 59249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 34       |
|    time_elapsed     | 7234     |
|    total_timesteps  | 247173   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.15e-07 |
|    n_updates        | 59293    |
----------------------------------
Eval num_timesteps=247500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 247500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.56e-07 |
|    n_updates        | 59374    |
----------------------------------
Eval num_timesteps=248000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 248000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.85e-07 |
|    n_updates        | 59499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 34       |
|    time_elapsed     | 7263     |
|    total_timesteps  | 248013   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74e-06 |
|    n_updates        | 59503    |
----------------------------------
Eval num_timesteps=248500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 248500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.55e-06 |
|    n_updates        | 59624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 34       |
|    time_elapsed     | 7279     |
|    total_timesteps  | 248648   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000282 |
|    n_updates        | 59661    |
----------------------------------
Eval num_timesteps=249000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 249000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5e-07  |
|    n_updates        | 59749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 34       |
|    time_elapsed     | 7298     |
|    total_timesteps  | 249488   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-06 |
|    n_updates        | 59871    |
----------------------------------
Eval num_timesteps=249500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 249500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96e-06 |
|    n_updates        | 59874    |
----------------------------------
Eval num_timesteps=250000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 250000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.59e-07 |
|    n_updates        | 59999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 34       |
|    time_elapsed     | 7327     |
|    total_timesteps  | 250181   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-06 |
|    n_updates        | 60045    |
----------------------------------
Eval num_timesteps=250500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 250500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.4e-07  |
|    n_updates        | 60124    |
----------------------------------
Eval num_timesteps=251000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 251000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.42e-07 |
|    n_updates        | 60249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 34       |
|    time_elapsed     | 7355     |
|    total_timesteps  | 251021   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.39e-06 |
|    n_updates        | 60255    |
----------------------------------
Eval num_timesteps=251500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 251500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.59e-07 |
|    n_updates        | 60374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 34       |
|    time_elapsed     | 7373     |
|    total_timesteps  | 251861   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.06e-07 |
|    n_updates        | 60465    |
----------------------------------
Eval num_timesteps=252000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.76e-07 |
|    n_updates        | 60499    |
----------------------------------
Eval num_timesteps=252500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 252500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.47e-07 |
|    n_updates        | 60624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 34       |
|    time_elapsed     | 7402     |
|    total_timesteps  | 252510   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000257 |
|    n_updates        | 60627    |
----------------------------------
Eval num_timesteps=253000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 253000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28e-06 |
|    n_updates        | 60749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 34       |
|    time_elapsed     | 7414     |
|    total_timesteps  | 253350   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000242 |
|    n_updates        | 60837    |
----------------------------------
Eval num_timesteps=253500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 253500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.94e-07 |
|    n_updates        | 60874    |
----------------------------------
Eval num_timesteps=254000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 254000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.19e-07 |
|    n_updates        | 60999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 34       |
|    time_elapsed     | 7443     |
|    total_timesteps  | 254190   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000243 |
|    n_updates        | 61047    |
----------------------------------
Eval num_timesteps=254500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 254500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.05e-07 |
|    n_updates        | 61124    |
----------------------------------
Eval num_timesteps=255000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 255000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.6e-06  |
|    n_updates        | 61249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 34       |
|    time_elapsed     | 7471     |
|    total_timesteps  | 255030   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.1e-07  |
|    n_updates        | 61257    |
----------------------------------
Eval num_timesteps=255500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 255500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.63e-07 |
|    n_updates        | 61374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1284     |
|    fps              | 34       |
|    time_elapsed     | 7488     |
|    total_timesteps  | 255844   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.53e-07 |
|    n_updates        | 61460    |
----------------------------------
Eval num_timesteps=256000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 256000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.96e-07 |
|    n_updates        | 61499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1288     |
|    fps              | 34       |
|    time_elapsed     | 7502     |
|    total_timesteps  | 256491   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.35e-06 |
|    n_updates        | 61622    |
----------------------------------
Eval num_timesteps=256500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 256500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.48e-06 |
|    n_updates        | 61624    |
----------------------------------
Eval num_timesteps=257000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 257000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.52e-07 |
|    n_updates        | 61749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1292     |
|    fps              | 34       |
|    time_elapsed     | 7531     |
|    total_timesteps  | 257331   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.06e-07 |
|    n_updates        | 61832    |
----------------------------------
Eval num_timesteps=257500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 257500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000219 |
|    n_updates        | 61874    |
----------------------------------
Eval num_timesteps=258000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.37e-07 |
|    n_updates        | 61999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1296     |
|    fps              | 34       |
|    time_elapsed     | 7560     |
|    total_timesteps  | 258171   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.09e-06 |
|    n_updates        | 62042    |
----------------------------------
Eval num_timesteps=258500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 258500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.44e-07 |
|    n_updates        | 62124    |
----------------------------------
Eval num_timesteps=259000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 259000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.94e-07 |
|    n_updates        | 62249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1300     |
|    fps              | 34       |
|    time_elapsed     | 7590     |
|    total_timesteps  | 259011   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0122   |
|    n_updates        | 62252    |
----------------------------------
Eval num_timesteps=259500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.6e-06  |
|    n_updates        | 62374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1304     |
|    fps              | 34       |
|    time_elapsed     | 7603     |
|    total_timesteps  | 259851   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-06 |
|    n_updates        | 62462    |
----------------------------------
Eval num_timesteps=260000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 260000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.24e-06 |
|    n_updates        | 62499    |
----------------------------------
Eval num_timesteps=260500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 260500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.97e-07 |
|    n_updates        | 62624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 34       |
|    time_elapsed     | 7631     |
|    total_timesteps  | 260691   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.98e-08 |
|    n_updates        | 62672    |
----------------------------------
Eval num_timesteps=261000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.53e-05 |
|    n_updates        | 62749    |
----------------------------------
Eval num_timesteps=261500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 261500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.53e-07 |
|    n_updates        | 62874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 34       |
|    time_elapsed     | 7660     |
|    total_timesteps  | 261531   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74e-06 |
|    n_updates        | 62882    |
----------------------------------
Eval num_timesteps=262000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 262000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.99e-07 |
|    n_updates        | 62999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1316     |
|    fps              | 34       |
|    time_elapsed     | 7679     |
|    total_timesteps  | 262371   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.52e-07 |
|    n_updates        | 63092    |
----------------------------------
Eval num_timesteps=262500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 262500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.76e-06 |
|    n_updates        | 63124    |
----------------------------------
Eval num_timesteps=263000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 263000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.41e-07 |
|    n_updates        | 63249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1320     |
|    fps              | 34       |
|    time_elapsed     | 7708     |
|    total_timesteps  | 263211   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.75e-06 |
|    n_updates        | 63302    |
----------------------------------
Eval num_timesteps=263500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 263500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.55e-06 |
|    n_updates        | 63374    |
----------------------------------
Eval num_timesteps=264000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 264000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.14e-07 |
|    n_updates        | 63499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1324     |
|    fps              | 34       |
|    time_elapsed     | 7736     |
|    total_timesteps  | 264051   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.76e-07 |
|    n_updates        | 63512    |
----------------------------------
Eval num_timesteps=264500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 264500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43e-06 |
|    n_updates        | 63624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1328     |
|    fps              | 34       |
|    time_elapsed     | 7755     |
|    total_timesteps  | 264891   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.29e-06 |
|    n_updates        | 63722    |
----------------------------------
Eval num_timesteps=265000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 265000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.29e-07 |
|    n_updates        | 63749    |
----------------------------------
Eval num_timesteps=265500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 265500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.51e-07 |
|    n_updates        | 63874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1332     |
|    fps              | 34       |
|    time_elapsed     | 7783     |
|    total_timesteps  | 265525   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.03e-07 |
|    n_updates        | 63881    |
----------------------------------
Eval num_timesteps=266000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 266000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.05e-06 |
|    n_updates        | 63999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1336     |
|    fps              | 34       |
|    time_elapsed     | 7796     |
|    total_timesteps  | 266365   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.46e-06 |
|    n_updates        | 64091    |
----------------------------------
Eval num_timesteps=266500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 266500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000258 |
|    n_updates        | 64124    |
----------------------------------
Eval num_timesteps=267000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 267000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.56e-06 |
|    n_updates        | 64249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1340     |
|    fps              | 34       |
|    time_elapsed     | 7825     |
|    total_timesteps  | 267205   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.08e-06 |
|    n_updates        | 64301    |
----------------------------------
Eval num_timesteps=267500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 267500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55e-07 |
|    n_updates        | 64374    |
----------------------------------
Eval num_timesteps=268000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 268000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.13e-06 |
|    n_updates        | 64499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1344     |
|    fps              | 34       |
|    time_elapsed     | 7854     |
|    total_timesteps  | 268045   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.38e-06 |
|    n_updates        | 64511    |
----------------------------------
Eval num_timesteps=268500, episode_reward=-0.16 +/- 0.24
Episode length: 201.86 +/- 39.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.97e-07 |
|    n_updates        | 64624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 34       |
|    time_elapsed     | 7871     |
|    total_timesteps  | 268694   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.8e-06  |
|    n_updates        | 64673    |
----------------------------------
Eval num_timesteps=269000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 269000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33e-06 |
|    n_updates        | 64749    |
----------------------------------
Eval num_timesteps=269500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 269500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.36e-07 |
|    n_updates        | 64874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1352     |
|    fps              | 34       |
|    time_elapsed     | 7897     |
|    total_timesteps  | 269534   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.52e-08 |
|    n_updates        | 64883    |
----------------------------------
Eval num_timesteps=270000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.54e-07 |
|    n_updates        | 64999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1356     |
|    fps              | 34       |
|    time_elapsed     | 7913     |
|    total_timesteps  | 270374   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41e-06 |
|    n_updates        | 65093    |
----------------------------------
Eval num_timesteps=270500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 270500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.42e-07 |
|    n_updates        | 65124    |
----------------------------------
Eval num_timesteps=271000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 271000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000251 |
|    n_updates        | 65249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 34       |
|    time_elapsed     | 7943     |
|    total_timesteps  | 271214   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2e-07  |
|    n_updates        | 65303    |
----------------------------------
Eval num_timesteps=271500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 271500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.96e-07 |
|    n_updates        | 65374    |
----------------------------------
Eval num_timesteps=272000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 272000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.21e-07 |
|    n_updates        | 65499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1364     |
|    fps              | 34       |
|    time_elapsed     | 7974     |
|    total_timesteps  | 272054   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.71e-07 |
|    n_updates        | 65513    |
----------------------------------
Eval num_timesteps=272500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.95e-08 |
|    n_updates        | 65624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1368     |
|    fps              | 34       |
|    time_elapsed     | 7989     |
|    total_timesteps  | 272894   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.36e-06 |
|    n_updates        | 65723    |
----------------------------------
Eval num_timesteps=273000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 273000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.26e-06 |
|    n_updates        | 65749    |
----------------------------------
Eval num_timesteps=273500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 273500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-05 |
|    n_updates        | 65874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1372     |
|    fps              | 34       |
|    time_elapsed     | 8018     |
|    total_timesteps  | 273734   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000287 |
|    n_updates        | 65933    |
----------------------------------
Eval num_timesteps=274000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 274000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.9e-07  |
|    n_updates        | 65999    |
----------------------------------
Eval num_timesteps=274500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 274500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.05e-07 |
|    n_updates        | 66124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1376     |
|    fps              | 34       |
|    time_elapsed     | 8048     |
|    total_timesteps  | 274574   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.2e-07  |
|    n_updates        | 66143    |
----------------------------------
Eval num_timesteps=275000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 275000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.41e-06 |
|    n_updates        | 66249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1380     |
|    fps              | 34       |
|    time_elapsed     | 8061     |
|    total_timesteps  | 275414   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.45e-07 |
|    n_updates        | 66353    |
----------------------------------
Eval num_timesteps=275500, episode_reward=-0.19 +/- 0.15
Episode length: 208.60 +/- 9.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | -0.189   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 275500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.69e-07 |
|    n_updates        | 66374    |
----------------------------------
Eval num_timesteps=276000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 276000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.11e-07 |
|    n_updates        | 66499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1384     |
|    fps              | 34       |
|    time_elapsed     | 8094     |
|    total_timesteps  | 276254   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.64e-06 |
|    n_updates        | 66563    |
----------------------------------
Eval num_timesteps=276500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 276500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.23e-07 |
|    n_updates        | 66624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1388     |
|    fps              | 34       |
|    time_elapsed     | 8110     |
|    total_timesteps  | 276934   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.63e-07 |
|    n_updates        | 66733    |
----------------------------------
Eval num_timesteps=277000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.69e-06 |
|    n_updates        | 66749    |
----------------------------------
Eval num_timesteps=277500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 277500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.56e-07 |
|    n_updates        | 66874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1392     |
|    fps              | 34       |
|    time_elapsed     | 8137     |
|    total_timesteps  | 277568   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58e-06 |
|    n_updates        | 66891    |
----------------------------------
Eval num_timesteps=278000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 278000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.15e-07 |
|    n_updates        | 66999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1396     |
|    fps              | 34       |
|    time_elapsed     | 8153     |
|    total_timesteps  | 278408   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.49e-06 |
|    n_updates        | 67101    |
----------------------------------
Eval num_timesteps=278500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 278500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.19e-06 |
|    n_updates        | 67124    |
----------------------------------
Eval num_timesteps=279000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.72e-07 |
|    n_updates        | 67249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1400     |
|    fps              | 34       |
|    time_elapsed     | 8183     |
|    total_timesteps  | 279248   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000246 |
|    n_updates        | 67311    |
----------------------------------
Eval num_timesteps=279500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 279500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-06 |
|    n_updates        | 67374    |
----------------------------------
Eval num_timesteps=280000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 280000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-06 |
|    n_updates        | 67499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.162   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1404     |
|    fps              | 34       |
|    time_elapsed     | 8215     |
|    total_timesteps  | 280088   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.37e-06 |
|    n_updates        | 67521    |
----------------------------------
Eval num_timesteps=280500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 280500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28e-06 |
|    n_updates        | 67624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1408     |
|    fps              | 34       |
|    time_elapsed     | 8228     |
|    total_timesteps  | 280906   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.26e-07 |
|    n_updates        | 67726    |
----------------------------------
Eval num_timesteps=281000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 281000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.08e-06 |
|    n_updates        | 67749    |
----------------------------------
Eval num_timesteps=281500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 281500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.72e-07 |
|    n_updates        | 67874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1412     |
|    fps              | 34       |
|    time_elapsed     | 8262     |
|    total_timesteps  | 281717   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1e-06  |
|    n_updates        | 67929    |
----------------------------------
Eval num_timesteps=282000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 282000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.65e-07 |
|    n_updates        | 67999    |
----------------------------------
Eval num_timesteps=282500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 282500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.48e-06 |
|    n_updates        | 68124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1416     |
|    fps              | 34       |
|    time_elapsed     | 8291     |
|    total_timesteps  | 282557   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.53e-07 |
|    n_updates        | 68139    |
----------------------------------
Eval num_timesteps=283000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 283000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000482 |
|    n_updates        | 68249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1420     |
|    fps              | 34       |
|    time_elapsed     | 8304     |
|    total_timesteps  | 283397   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.78e-07 |
|    n_updates        | 68349    |
----------------------------------
Eval num_timesteps=283500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 283500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.43e-06 |
|    n_updates        | 68374    |
----------------------------------
Eval num_timesteps=284000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 284000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.78e-08 |
|    n_updates        | 68499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1424     |
|    fps              | 34       |
|    time_elapsed     | 8332     |
|    total_timesteps  | 284237   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-06 |
|    n_updates        | 68559    |
----------------------------------
Eval num_timesteps=284500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 284500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.22e-07 |
|    n_updates        | 68624    |
----------------------------------
Eval num_timesteps=285000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 285000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-06  |
|    n_updates        | 68749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1428     |
|    fps              | 34       |
|    time_elapsed     | 8362     |
|    total_timesteps  | 285077   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.71e-06 |
|    n_updates        | 68769    |
----------------------------------
Eval num_timesteps=285500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 285500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.88e-07 |
|    n_updates        | 68874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1432     |
|    fps              | 34       |
|    time_elapsed     | 8377     |
|    total_timesteps  | 285917   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.75e-07 |
|    n_updates        | 68979    |
----------------------------------
Eval num_timesteps=286000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 286000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.49e-06 |
|    n_updates        | 68999    |
----------------------------------
Eval num_timesteps=286500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 286500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.04e-06 |
|    n_updates        | 69124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1436     |
|    fps              | 34       |
|    time_elapsed     | 8405     |
|    total_timesteps  | 286757   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-07 |
|    n_updates        | 69189    |
----------------------------------
Eval num_timesteps=287000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 287000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.78e-07 |
|    n_updates        | 69249    |
----------------------------------
Eval num_timesteps=287500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 287500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44e-06 |
|    n_updates        | 69374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1440     |
|    fps              | 34       |
|    time_elapsed     | 8433     |
|    total_timesteps  | 287597   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.3e-07  |
|    n_updates        | 69399    |
----------------------------------
Eval num_timesteps=288000, episode_reward=-0.14 +/- 0.29
Episode length: 197.68 +/- 48.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.44e-07 |
|    n_updates        | 69499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1444     |
|    fps              | 34       |
|    time_elapsed     | 8452     |
|    total_timesteps  | 288437   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.95e-06 |
|    n_updates        | 69609    |
----------------------------------
Eval num_timesteps=288500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 288500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.93e-06 |
|    n_updates        | 69624    |
----------------------------------
Eval num_timesteps=289000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 289000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.87e-07 |
|    n_updates        | 69749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1448     |
|    fps              | 34       |
|    time_elapsed     | 8481     |
|    total_timesteps  | 289277   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000235 |
|    n_updates        | 69819    |
----------------------------------
Eval num_timesteps=289500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 289500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.68e-07 |
|    n_updates        | 69874    |
----------------------------------
Eval num_timesteps=290000, episode_reward=-0.14 +/- 0.29
Episode length: 197.74 +/- 48.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.45e-07 |
|    n_updates        | 69999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1452     |
|    fps              | 34       |
|    time_elapsed     | 8510     |
|    total_timesteps  | 290117   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000222 |
|    n_updates        | 70029    |
----------------------------------
Eval num_timesteps=290500, episode_reward=-0.14 +/- 0.29
Episode length: 197.62 +/- 49.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 290500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.61e-07 |
|    n_updates        | 70124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1456     |
|    fps              | 34       |
|    time_elapsed     | 8526     |
|    total_timesteps  | 290957   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-06 |
|    n_updates        | 70239    |
----------------------------------
Eval num_timesteps=291000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 291000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95e-07 |
|    n_updates        | 70249    |
----------------------------------
Eval num_timesteps=291500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 291500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.52e-07 |
|    n_updates        | 70374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 34       |
|    time_elapsed     | 8555     |
|    total_timesteps  | 291716   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-06 |
|    n_updates        | 70428    |
----------------------------------
Eval num_timesteps=292000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 292000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.07e-06 |
|    n_updates        | 70499    |
----------------------------------
Eval num_timesteps=292500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 292500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.49e-07 |
|    n_updates        | 70624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1464     |
|    fps              | 34       |
|    time_elapsed     | 8584     |
|    total_timesteps  | 292523   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.16e-07 |
|    n_updates        | 70630    |
----------------------------------
Eval num_timesteps=293000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.17e-06 |
|    n_updates        | 70749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1468     |
|    fps              | 34       |
|    time_elapsed     | 8597     |
|    total_timesteps  | 293363   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.99e-07 |
|    n_updates        | 70840    |
----------------------------------
Eval num_timesteps=293500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.74e-08 |
|    n_updates        | 70874    |
----------------------------------
Eval num_timesteps=294000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 294000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.29e-06 |
|    n_updates        | 70999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 34       |
|    time_elapsed     | 8627     |
|    total_timesteps  | 294203   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.5e-07  |
|    n_updates        | 71050    |
----------------------------------
Eval num_timesteps=294500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 294500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.58e-06 |
|    n_updates        | 71124    |
----------------------------------
Eval num_timesteps=295000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 295000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47e-06 |
|    n_updates        | 71249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1476     |
|    fps              | 34       |
|    time_elapsed     | 8657     |
|    total_timesteps  | 295043   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.67e-07 |
|    n_updates        | 71260    |
----------------------------------
Eval num_timesteps=295500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 295500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000261 |
|    n_updates        | 71374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1480     |
|    fps              | 34       |
|    time_elapsed     | 8673     |
|    total_timesteps  | 295701   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00026  |
|    n_updates        | 71425    |
----------------------------------
Eval num_timesteps=296000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 296000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-07 |
|    n_updates        | 71499    |
----------------------------------
Eval num_timesteps=296500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 296500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.8e-08  |
|    n_updates        | 71624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1484     |
|    fps              | 34       |
|    time_elapsed     | 8701     |
|    total_timesteps  | 296541   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.63e-07 |
|    n_updates        | 71635    |
----------------------------------
Eval num_timesteps=297000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.18e-07 |
|    n_updates        | 71749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1488     |
|    fps              | 34       |
|    time_elapsed     | 8716     |
|    total_timesteps  | 297381   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.77e-07 |
|    n_updates        | 71845    |
----------------------------------
Eval num_timesteps=297500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 297500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4e-07  |
|    n_updates        | 71874    |
----------------------------------
Eval num_timesteps=298000, episode_reward=-0.14 +/- 0.29
Episode length: 197.68 +/- 48.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 298000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000245 |
|    n_updates        | 71999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1492     |
|    fps              | 34       |
|    time_elapsed     | 8744     |
|    total_timesteps  | 298221   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.22e-07 |
|    n_updates        | 72055    |
----------------------------------
Eval num_timesteps=298500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 298500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.84e-06 |
|    n_updates        | 72124    |
----------------------------------
Eval num_timesteps=299000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 299000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.76e-07 |
|    n_updates        | 72249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1496     |
|    fps              | 34       |
|    time_elapsed     | 8777     |
|    total_timesteps  | 299061   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.46e-06 |
|    n_updates        | 72265    |
----------------------------------
Eval num_timesteps=299500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 299500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.76e-06 |
|    n_updates        | 72374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1500     |
|    fps              | 34       |
|    time_elapsed     | 8790     |
|    total_timesteps  | 299901   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.55e-07 |
|    n_updates        | 72475    |
----------------------------------
Eval num_timesteps=300000, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 300000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01e-06 |
|    n_updates        | 72499    |
----------------------------------
Eval num_timesteps=300500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 300500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.91e-07 |
|    n_updates        | 72624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1504     |
|    fps              | 34       |
|    time_elapsed     | 8819     |
|    total_timesteps  | 300741   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.5e-07  |
|    n_updates        | 72685    |
----------------------------------
Eval num_timesteps=301000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 301000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43e-06 |
|    n_updates        | 72749    |
----------------------------------
Eval num_timesteps=301500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 301500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000235 |
|    n_updates        | 72874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1508     |
|    fps              | 34       |
|    time_elapsed     | 8847     |
|    total_timesteps  | 301581   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000238 |
|    n_updates        | 72895    |
----------------------------------
Eval num_timesteps=302000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 302000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.75e-05 |
|    n_updates        | 72999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1512     |
|    fps              | 34       |
|    time_elapsed     | 8862     |
|    total_timesteps  | 302421   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.09e-06 |
|    n_updates        | 73105    |
----------------------------------
Eval num_timesteps=302500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 302500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63e-06 |
|    n_updates        | 73124    |
----------------------------------
Eval num_timesteps=303000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 303000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.61e-06 |
|    n_updates        | 73249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1516     |
|    fps              | 34       |
|    time_elapsed     | 8892     |
|    total_timesteps  | 303261   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.07e-06 |
|    n_updates        | 73315    |
----------------------------------
Eval num_timesteps=303500, episode_reward=-0.16 +/- 0.23
Episode length: 202.96 +/- 34.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | -0.163   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 303500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28e-06 |
|    n_updates        | 73374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1520     |
|    fps              | 34       |
|    time_elapsed     | 8910     |
|    total_timesteps  | 303895   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-06 |
|    n_updates        | 73473    |
----------------------------------
Eval num_timesteps=304000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 304000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-06 |
|    n_updates        | 73499    |
----------------------------------
Eval num_timesteps=304500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 304500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.64e-06 |
|    n_updates        | 73624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 34       |
|    time_elapsed     | 8938     |
|    total_timesteps  | 304735   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.89e-07 |
|    n_updates        | 73683    |
----------------------------------
Eval num_timesteps=305000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 305000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.14e-06 |
|    n_updates        | 73749    |
----------------------------------
Eval num_timesteps=305500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 305500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1e-07  |
|    n_updates        | 73874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1528     |
|    fps              | 34       |
|    time_elapsed     | 8966     |
|    total_timesteps  | 305575   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000323 |
|    n_updates        | 73893    |
----------------------------------
Eval num_timesteps=306000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 306000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.87e-08 |
|    n_updates        | 73999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1532     |
|    fps              | 34       |
|    time_elapsed     | 8979     |
|    total_timesteps  | 306415   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.82e-06 |
|    n_updates        | 74103    |
----------------------------------
Eval num_timesteps=306500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 306500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-06 |
|    n_updates        | 74124    |
----------------------------------
Eval num_timesteps=307000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 307000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94e-06 |
|    n_updates        | 74249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1536     |
|    fps              | 34       |
|    time_elapsed     | 9009     |
|    total_timesteps  | 307255   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-06 |
|    n_updates        | 74313    |
----------------------------------
Eval num_timesteps=307500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 307500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.36e-06 |
|    n_updates        | 74374    |
----------------------------------
Eval num_timesteps=308000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 308000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-05 |
|    n_updates        | 74499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1540     |
|    fps              | 34       |
|    time_elapsed     | 9038     |
|    total_timesteps  | 308095   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74e-06 |
|    n_updates        | 74523    |
----------------------------------
Eval num_timesteps=308500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 308500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.48e-07 |
|    n_updates        | 74624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1544     |
|    fps              | 34       |
|    time_elapsed     | 9053     |
|    total_timesteps  | 308935   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41e-06 |
|    n_updates        | 74733    |
----------------------------------
Eval num_timesteps=309000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 309000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.97e-07 |
|    n_updates        | 74749    |
----------------------------------
Eval num_timesteps=309500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 309500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.31e-07 |
|    n_updates        | 74874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1548     |
|    fps              | 34       |
|    time_elapsed     | 9081     |
|    total_timesteps  | 309724   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-07 |
|    n_updates        | 74930    |
----------------------------------
Eval num_timesteps=310000, episode_reward=-0.16 +/- 0.23
Episode length: 202.12 +/- 38.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 310000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2e-06  |
|    n_updates        | 74999    |
----------------------------------
Eval num_timesteps=310500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 310500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.38e-06 |
|    n_updates        | 75124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1552     |
|    fps              | 34       |
|    time_elapsed     | 9109     |
|    total_timesteps  | 310564   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9e-06  |
|    n_updates        | 75140    |
----------------------------------
Eval num_timesteps=311000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 311000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.11e-07 |
|    n_updates        | 75249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 34       |
|    time_elapsed     | 9126     |
|    total_timesteps  | 311404   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-06 |
|    n_updates        | 75350    |
----------------------------------
Eval num_timesteps=311500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 311500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000327 |
|    n_updates        | 75374    |
----------------------------------
Eval num_timesteps=312000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 312000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.57e-06 |
|    n_updates        | 75499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1560     |
|    fps              | 34       |
|    time_elapsed     | 9156     |
|    total_timesteps  | 312244   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.3e-06  |
|    n_updates        | 75560    |
----------------------------------
Eval num_timesteps=312500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 312500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.15e-07 |
|    n_updates        | 75624    |
----------------------------------
Eval num_timesteps=313000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 313000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-07 |
|    n_updates        | 75749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1564     |
|    fps              | 34       |
|    time_elapsed     | 9184     |
|    total_timesteps  | 313084   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.12e-06 |
|    n_updates        | 75770    |
----------------------------------
Eval num_timesteps=313500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 313500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94e-05 |
|    n_updates        | 75874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1568     |
|    fps              | 34       |
|    time_elapsed     | 9200     |
|    total_timesteps  | 313924   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.84e-06 |
|    n_updates        | 75980    |
----------------------------------
Eval num_timesteps=314000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 314000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4e-06  |
|    n_updates        | 75999    |
----------------------------------
Eval num_timesteps=314500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 314500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.54e-07 |
|    n_updates        | 76124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1572     |
|    fps              | 34       |
|    time_elapsed     | 9229     |
|    total_timesteps  | 314624   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.28e-06 |
|    n_updates        | 76155    |
----------------------------------
Eval num_timesteps=315000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 315000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.59e-06 |
|    n_updates        | 76249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1576     |
|    fps              | 34       |
|    time_elapsed     | 9242     |
|    total_timesteps  | 315464   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.02e-07 |
|    n_updates        | 76365    |
----------------------------------
Eval num_timesteps=315500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 315500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.14e-07 |
|    n_updates        | 76374    |
----------------------------------
Eval num_timesteps=316000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 316000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-06 |
|    n_updates        | 76499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1580     |
|    fps              | 34       |
|    time_elapsed     | 9270     |
|    total_timesteps  | 316304   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.03e-07 |
|    n_updates        | 76575    |
----------------------------------
Eval num_timesteps=316500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 316500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-05 |
|    n_updates        | 76624    |
----------------------------------
Eval num_timesteps=317000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 317000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.38e-07 |
|    n_updates        | 76749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1584     |
|    fps              | 34       |
|    time_elapsed     | 9299     |
|    total_timesteps  | 317144   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23e-06 |
|    n_updates        | 76785    |
----------------------------------
Eval num_timesteps=317500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 317500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.21e-07 |
|    n_updates        | 76874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1588     |
|    fps              | 34       |
|    time_elapsed     | 9318     |
|    total_timesteps  | 317984   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.03e-07 |
|    n_updates        | 76995    |
----------------------------------
Eval num_timesteps=318000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 318000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.5e-07  |
|    n_updates        | 76999    |
----------------------------------
Eval num_timesteps=318500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 318500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.59e-08 |
|    n_updates        | 77124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1592     |
|    fps              | 34       |
|    time_elapsed     | 9346     |
|    total_timesteps  | 318824   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.87e-06 |
|    n_updates        | 77205    |
----------------------------------
Eval num_timesteps=319000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 319000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13e-06 |
|    n_updates        | 77249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1596     |
|    fps              | 34       |
|    time_elapsed     | 9358     |
|    total_timesteps  | 319493   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.45e-07 |
|    n_updates        | 77373    |
----------------------------------
Eval num_timesteps=319500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 319500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.14e-07 |
|    n_updates        | 77374    |
----------------------------------
Eval num_timesteps=320000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 320000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.87e-08 |
|    n_updates        | 77499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1600     |
|    fps              | 34       |
|    time_elapsed     | 9388     |
|    total_timesteps  | 320333   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56e-07 |
|    n_updates        | 77583    |
----------------------------------
Eval num_timesteps=320500, episode_reward=-0.16 +/- 0.24
Episode length: 201.86 +/- 39.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 320500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.29e-06 |
|    n_updates        | 77624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1604     |
|    fps              | 34       |
|    time_elapsed     | 9403     |
|    total_timesteps  | 320927   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.09e-07 |
|    n_updates        | 77731    |
----------------------------------
Eval num_timesteps=321000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 321000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.09e-06 |
|    n_updates        | 77749    |
----------------------------------
Eval num_timesteps=321500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 321500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.78e-07 |
|    n_updates        | 77874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1608     |
|    fps              | 34       |
|    time_elapsed     | 9431     |
|    total_timesteps  | 321767   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0003   |
|    n_updates        | 77941    |
----------------------------------
Eval num_timesteps=322000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 322000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.96e-06 |
|    n_updates        | 77999    |
----------------------------------
Eval num_timesteps=322500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 322500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.54e-06 |
|    n_updates        | 78124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1612     |
|    fps              | 34       |
|    time_elapsed     | 9462     |
|    total_timesteps  | 322607   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.45e-06 |
|    n_updates        | 78151    |
----------------------------------
Eval num_timesteps=323000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 323000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.99e-08 |
|    n_updates        | 78249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1616     |
|    fps              | 34       |
|    time_elapsed     | 9481     |
|    total_timesteps  | 323447   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.45e-06 |
|    n_updates        | 78361    |
----------------------------------
Eval num_timesteps=323500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 323500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.56e-06 |
|    n_updates        | 78374    |
----------------------------------
Eval num_timesteps=324000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 324000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.93e-07 |
|    n_updates        | 78499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1620     |
|    fps              | 34       |
|    time_elapsed     | 9509     |
|    total_timesteps  | 324287   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0117   |
|    n_updates        | 78571    |
----------------------------------
Eval num_timesteps=324500, episode_reward=-0.19 +/- 0.17
Episode length: 206.22 +/- 26.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 324500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.53e-06 |
|    n_updates        | 78624    |
----------------------------------
Eval num_timesteps=325000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 325000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.93e-06 |
|    n_updates        | 78749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1624     |
|    fps              | 34       |
|    time_elapsed     | 9537     |
|    total_timesteps  | 325127   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.65e-07 |
|    n_updates        | 78781    |
----------------------------------
Eval num_timesteps=325500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 325500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8e-06  |
|    n_updates        | 78874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1628     |
|    fps              | 34       |
|    time_elapsed     | 9552     |
|    total_timesteps  | 325967   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.89e-07 |
|    n_updates        | 78991    |
----------------------------------
Eval num_timesteps=326000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 326000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.25e-07 |
|    n_updates        | 78999    |
----------------------------------
Eval num_timesteps=326500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 326500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-06  |
|    n_updates        | 79124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1632     |
|    fps              | 34       |
|    time_elapsed     | 9579     |
|    total_timesteps  | 326807   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-07 |
|    n_updates        | 79201    |
----------------------------------
Eval num_timesteps=327000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 327000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000275 |
|    n_updates        | 79249    |
----------------------------------
Eval num_timesteps=327500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 327500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.3e-06  |
|    n_updates        | 79374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1636     |
|    fps              | 34       |
|    time_elapsed     | 9609     |
|    total_timesteps  | 327647   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1e-07  |
|    n_updates        | 79411    |
----------------------------------
Eval num_timesteps=328000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 328000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51e-05 |
|    n_updates        | 79499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1640     |
|    fps              | 34       |
|    time_elapsed     | 9626     |
|    total_timesteps  | 328487   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-07  |
|    n_updates        | 79621    |
----------------------------------
Eval num_timesteps=328500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 328500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.99e-07 |
|    n_updates        | 79624    |
----------------------------------
Eval num_timesteps=329000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 329000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.65e-06 |
|    n_updates        | 79749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1644     |
|    fps              | 34       |
|    time_elapsed     | 9655     |
|    total_timesteps  | 329327   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.88e-07 |
|    n_updates        | 79831    |
----------------------------------
Eval num_timesteps=329500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 329500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.45e-06 |
|    n_updates        | 79874    |
----------------------------------
Eval num_timesteps=330000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 330000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59e-06 |
|    n_updates        | 79999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1648     |
|    fps              | 34       |
|    time_elapsed     | 9687     |
|    total_timesteps  | 330059   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25e-06 |
|    n_updates        | 80014    |
----------------------------------
Eval num_timesteps=330500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 330500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16e-06 |
|    n_updates        | 80124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1652     |
|    fps              | 34       |
|    time_elapsed     | 9700     |
|    total_timesteps  | 330899   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.2e-07  |
|    n_updates        | 80224    |
----------------------------------
Eval num_timesteps=331000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 331000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000311 |
|    n_updates        | 80249    |
----------------------------------
Eval num_timesteps=331500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 331500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0003   |
|    n_updates        | 80374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1656     |
|    fps              | 34       |
|    time_elapsed     | 9729     |
|    total_timesteps  | 331739   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3e-07  |
|    n_updates        | 80434    |
----------------------------------
Eval num_timesteps=332000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 332000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.87e-06 |
|    n_updates        | 80499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1660     |
|    fps              | 34       |
|    time_elapsed     | 9748     |
|    total_timesteps  | 332474   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000323 |
|    n_updates        | 80618    |
----------------------------------
Eval num_timesteps=332500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 332500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.67e-06 |
|    n_updates        | 80624    |
----------------------------------
Eval num_timesteps=333000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 333000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.77e-08 |
|    n_updates        | 80749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1664     |
|    fps              | 34       |
|    time_elapsed     | 9777     |
|    total_timesteps  | 333314   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23e-06 |
|    n_updates        | 80828    |
----------------------------------
Eval num_timesteps=333500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 333500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.19e-07 |
|    n_updates        | 80874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1668     |
|    fps              | 34       |
|    time_elapsed     | 9790     |
|    total_timesteps  | 333966   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.72e-07 |
|    n_updates        | 80991    |
----------------------------------
Eval num_timesteps=334000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 334000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47e-07 |
|    n_updates        | 80999    |
----------------------------------
Eval num_timesteps=334500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 334500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.85e-07 |
|    n_updates        | 81124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1672     |
|    fps              | 34       |
|    time_elapsed     | 9818     |
|    total_timesteps  | 334806   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000313 |
|    n_updates        | 81201    |
----------------------------------
Eval num_timesteps=335000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 335000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000308 |
|    n_updates        | 81249    |
----------------------------------
Eval num_timesteps=335500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 335500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.17e-07 |
|    n_updates        | 81374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1676     |
|    fps              | 34       |
|    time_elapsed     | 9847     |
|    total_timesteps  | 335646   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.99e-06 |
|    n_updates        | 81411    |
----------------------------------
Eval num_timesteps=336000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 336000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.61e-07 |
|    n_updates        | 81499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1680     |
|    fps              | 34       |
|    time_elapsed     | 9861     |
|    total_timesteps  | 336486   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.65e-08 |
|    n_updates        | 81621    |
----------------------------------
Eval num_timesteps=336500, episode_reward=-0.19 +/- 0.17
Episode length: 206.36 +/- 25.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 336500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18e-08 |
|    n_updates        | 81624    |
----------------------------------
Eval num_timesteps=337000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 337000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.6e-06  |
|    n_updates        | 81749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1684     |
|    fps              | 34       |
|    time_elapsed     | 9889     |
|    total_timesteps  | 337135   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.99e-07 |
|    n_updates        | 81783    |
----------------------------------
Eval num_timesteps=337500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 337500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.53e-06 |
|    n_updates        | 81874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1688     |
|    fps              | 34       |
|    time_elapsed     | 9908     |
|    total_timesteps  | 337975   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000318 |
|    n_updates        | 81993    |
----------------------------------
Eval num_timesteps=338000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 338000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91e-06 |
|    n_updates        | 81999    |
----------------------------------
Eval num_timesteps=338500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 338500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0119   |
|    n_updates        | 82124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1692     |
|    fps              | 34       |
|    time_elapsed     | 9937     |
|    total_timesteps  | 338815   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.55e-07 |
|    n_updates        | 82203    |
----------------------------------
Eval num_timesteps=339000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 339000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.97e-07 |
|    n_updates        | 82249    |
----------------------------------
Eval num_timesteps=339500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 339500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.58e-06 |
|    n_updates        | 82374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1696     |
|    fps              | 34       |
|    time_elapsed     | 9966     |
|    total_timesteps  | 339655   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.27e-06 |
|    n_updates        | 82413    |
----------------------------------
Eval num_timesteps=340000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 340000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.14e-06 |
|    n_updates        | 82499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1700     |
|    fps              | 34       |
|    time_elapsed     | 9978     |
|    total_timesteps  | 340495   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.08e-07 |
|    n_updates        | 82623    |
----------------------------------
Eval num_timesteps=340500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 340500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.77e-07 |
|    n_updates        | 82624    |
----------------------------------
Eval num_timesteps=341000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 341000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000288 |
|    n_updates        | 82749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1704     |
|    fps              | 34       |
|    time_elapsed     | 10007    |
|    total_timesteps  | 341335   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.13e-07 |
|    n_updates        | 82833    |
----------------------------------
Eval num_timesteps=341500, episode_reward=-0.16 +/- 0.23
Episode length: 202.64 +/- 36.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | -0.163   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 341500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36e-06 |
|    n_updates        | 82874    |
----------------------------------
Eval num_timesteps=342000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 342000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.29e-06 |
|    n_updates        | 82999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1708     |
|    fps              | 34       |
|    time_elapsed     | 10036    |
|    total_timesteps  | 342175   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000276 |
|    n_updates        | 83043    |
----------------------------------
Eval num_timesteps=342500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 342500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-06 |
|    n_updates        | 83124    |
----------------------------------
Eval num_timesteps=343000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 343000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000274 |
|    n_updates        | 83249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1712     |
|    fps              | 34       |
|    time_elapsed     | 10064    |
|    total_timesteps  | 343015   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.64e-07 |
|    n_updates        | 83253    |
----------------------------------
Eval num_timesteps=343500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 343500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.43e-05 |
|    n_updates        | 83374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1716     |
|    fps              | 34       |
|    time_elapsed     | 10079    |
|    total_timesteps  | 343855   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.87e-06 |
|    n_updates        | 83463    |
----------------------------------
Eval num_timesteps=344000, episode_reward=-0.14 +/- 0.29
Episode length: 197.62 +/- 49.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 344000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2e-06  |
|    n_updates        | 83499    |
----------------------------------
Eval num_timesteps=344500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 344500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.37e-08 |
|    n_updates        | 83624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1720     |
|    fps              | 34       |
|    time_elapsed     | 10106    |
|    total_timesteps  | 344695   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.61e-07 |
|    n_updates        | 83673    |
----------------------------------
Eval num_timesteps=345000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 345000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.5e-07  |
|    n_updates        | 83749    |
----------------------------------
Eval num_timesteps=345500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 345500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.22e-07 |
|    n_updates        | 83874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1724     |
|    fps              | 34       |
|    time_elapsed     | 10134    |
|    total_timesteps  | 345535   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.87e-08 |
|    n_updates        | 83883    |
----------------------------------
Eval num_timesteps=346000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 346000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.26e-07 |
|    n_updates        | 83999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1728     |
|    fps              | 34       |
|    time_elapsed     | 10152    |
|    total_timesteps  | 346375   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.44e-07 |
|    n_updates        | 84093    |
----------------------------------
Eval num_timesteps=346500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 346500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.62e-06 |
|    n_updates        | 84124    |
----------------------------------
Eval num_timesteps=347000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 347000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000264 |
|    n_updates        | 84249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1732     |
|    fps              | 34       |
|    time_elapsed     | 10183    |
|    total_timesteps  | 347166   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73e-06 |
|    n_updates        | 84291    |
----------------------------------
Eval num_timesteps=347500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 347500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.03e-07 |
|    n_updates        | 84374    |
----------------------------------
Eval num_timesteps=348000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 348000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.15e-06 |
|    n_updates        | 84499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1736     |
|    fps              | 34       |
|    time_elapsed     | 10211    |
|    total_timesteps  | 348006   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.38e-07 |
|    n_updates        | 84501    |
----------------------------------
Eval num_timesteps=348500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 348500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.31e-06 |
|    n_updates        | 84624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1740     |
|    fps              | 34       |
|    time_elapsed     | 10226    |
|    total_timesteps  | 348846   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.47e-06 |
|    n_updates        | 84711    |
----------------------------------
Eval num_timesteps=349000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 349000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.76e-07 |
|    n_updates        | 84749    |
----------------------------------
Eval num_timesteps=349500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 349500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000219 |
|    n_updates        | 84874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1744     |
|    fps              | 34       |
|    time_elapsed     | 10255    |
|    total_timesteps  | 349686   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34e-05 |
|    n_updates        | 84921    |
----------------------------------
Eval num_timesteps=350000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 350000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55e-05 |
|    n_updates        | 84999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1748     |
|    fps              | 34       |
|    time_elapsed     | 10273    |
|    total_timesteps  | 350346   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000271 |
|    n_updates        | 85086    |
----------------------------------
Eval num_timesteps=350500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 350500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36e-06 |
|    n_updates        | 85124    |
----------------------------------
Eval num_timesteps=351000, episode_reward=-0.14 +/- 0.29
Episode length: 197.64 +/- 48.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 351000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.81e-06 |
|    n_updates        | 85249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1752     |
|    fps              | 34       |
|    time_elapsed     | 10302    |
|    total_timesteps  | 351186   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86e-08 |
|    n_updates        | 85296    |
----------------------------------
Eval num_timesteps=351500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 351500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55e-06 |
|    n_updates        | 85374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1756     |
|    fps              | 34       |
|    time_elapsed     | 10317    |
|    total_timesteps  | 351893   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.14e-07 |
|    n_updates        | 85473    |
----------------------------------
Eval num_timesteps=352000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 352000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.38e-07 |
|    n_updates        | 85499    |
----------------------------------
Eval num_timesteps=352500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 352500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.42e-07 |
|    n_updates        | 85624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1760     |
|    fps              | 34       |
|    time_elapsed     | 10345    |
|    total_timesteps  | 352660   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88e-07 |
|    n_updates        | 85664    |
----------------------------------
Eval num_timesteps=353000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 353000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.16e-08 |
|    n_updates        | 85749    |
----------------------------------
Eval num_timesteps=353500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 353500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.37e-08 |
|    n_updates        | 85874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1764     |
|    fps              | 34       |
|    time_elapsed     | 10374    |
|    total_timesteps  | 353500   |
----------------------------------
Eval num_timesteps=354000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 354000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.83e-08 |
|    n_updates        | 85999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1768     |
|    fps              | 34       |
|    time_elapsed     | 10390    |
|    total_timesteps  | 354340   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.05e-07 |
|    n_updates        | 86084    |
----------------------------------
Eval num_timesteps=354500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 354500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.27e-07 |
|    n_updates        | 86124    |
----------------------------------
Eval num_timesteps=355000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 355000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.99e-07 |
|    n_updates        | 86249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1772     |
|    fps              | 34       |
|    time_elapsed     | 10418    |
|    total_timesteps  | 355180   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.27e-08 |
|    n_updates        | 86294    |
----------------------------------
Eval num_timesteps=355500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 355500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.02e-07 |
|    n_updates        | 86374    |
----------------------------------
Eval num_timesteps=356000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 356000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000247 |
|    n_updates        | 86499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1776     |
|    fps              | 34       |
|    time_elapsed     | 10447    |
|    total_timesteps  | 356020   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.73e-07 |
|    n_updates        | 86504    |
----------------------------------
Eval num_timesteps=356500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 356500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43e-06 |
|    n_updates        | 86624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1780     |
|    fps              | 34       |
|    time_elapsed     | 10459    |
|    total_timesteps  | 356860   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.17e-06 |
|    n_updates        | 86714    |
----------------------------------
Eval num_timesteps=357000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 357000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000242 |
|    n_updates        | 86749    |
----------------------------------
Eval num_timesteps=357500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 357500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66e-06 |
|    n_updates        | 86874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1784     |
|    fps              | 34       |
|    time_elapsed     | 10488    |
|    total_timesteps  | 357700   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.24e-07 |
|    n_updates        | 86924    |
----------------------------------
Eval num_timesteps=358000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 358000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.8e-07  |
|    n_updates        | 86999    |
----------------------------------
Eval num_timesteps=358500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 358500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96e-05 |
|    n_updates        | 87124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1788     |
|    fps              | 34       |
|    time_elapsed     | 10518    |
|    total_timesteps  | 358540   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.91e-06 |
|    n_updates        | 87134    |
----------------------------------
Eval num_timesteps=359000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 359000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.27e-07 |
|    n_updates        | 87249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1792     |
|    fps              | 34       |
|    time_elapsed     | 10534    |
|    total_timesteps  | 359380   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.45e-07 |
|    n_updates        | 87344    |
----------------------------------
Eval num_timesteps=359500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 359500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67e-07 |
|    n_updates        | 87374    |
----------------------------------
Eval num_timesteps=360000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 360000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.66e-07 |
|    n_updates        | 87499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1796     |
|    fps              | 34       |
|    time_elapsed     | 10563    |
|    total_timesteps  | 360220   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.87e-07 |
|    n_updates        | 87554    |
----------------------------------
Eval num_timesteps=360500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 360500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.58e-06 |
|    n_updates        | 87624    |
----------------------------------
Eval num_timesteps=361000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 361000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.79e-07 |
|    n_updates        | 87749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1800     |
|    fps              | 34       |
|    time_elapsed     | 10591    |
|    total_timesteps  | 361060   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.71e-06 |
|    n_updates        | 87764    |
----------------------------------
Eval num_timesteps=361500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 361500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000203 |
|    n_updates        | 87874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1804     |
|    fps              | 34       |
|    time_elapsed     | 10608    |
|    total_timesteps  | 361900   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.01e-07 |
|    n_updates        | 87974    |
----------------------------------
Eval num_timesteps=362000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 362000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.55e-07 |
|    n_updates        | 87999    |
----------------------------------
Eval num_timesteps=362500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 362500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86e-06 |
|    n_updates        | 88124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1808     |
|    fps              | 34       |
|    time_elapsed     | 10638    |
|    total_timesteps  | 362740   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000199 |
|    n_updates        | 88184    |
----------------------------------
Eval num_timesteps=363000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 363000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.37e-06 |
|    n_updates        | 88249    |
----------------------------------
Eval num_timesteps=363500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 363500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.21e-07 |
|    n_updates        | 88374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1812     |
|    fps              | 34       |
|    time_elapsed     | 10667    |
|    total_timesteps  | 363580   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.91e-06 |
|    n_updates        | 88394    |
----------------------------------
Eval num_timesteps=364000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 364000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.12e-07 |
|    n_updates        | 88499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1816     |
|    fps              | 34       |
|    time_elapsed     | 10681    |
|    total_timesteps  | 364420   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000185 |
|    n_updates        | 88604    |
----------------------------------
Eval num_timesteps=364500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 364500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.41e-07 |
|    n_updates        | 88624    |
----------------------------------
Eval num_timesteps=365000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 365000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000185 |
|    n_updates        | 88749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1820     |
|    fps              | 34       |
|    time_elapsed     | 10710    |
|    total_timesteps  | 365260   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.94e-07 |
|    n_updates        | 88814    |
----------------------------------
Eval num_timesteps=365500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 365500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-07 |
|    n_updates        | 88874    |
----------------------------------
Eval num_timesteps=366000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 366000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13e-06 |
|    n_updates        | 88999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1824     |
|    fps              | 34       |
|    time_elapsed     | 10738    |
|    total_timesteps  | 366100   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21e-06 |
|    n_updates        | 89024    |
----------------------------------
Eval num_timesteps=366500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 366500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.46e-06 |
|    n_updates        | 89124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1828     |
|    fps              | 34       |
|    time_elapsed     | 10754    |
|    total_timesteps  | 366940   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-08  |
|    n_updates        | 89234    |
----------------------------------
Eval num_timesteps=367000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 367000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.58e-07 |
|    n_updates        | 89249    |
----------------------------------
Eval num_timesteps=367500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 367500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.31e-07 |
|    n_updates        | 89374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1832     |
|    fps              | 34       |
|    time_elapsed     | 10785    |
|    total_timesteps  | 367780   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.91e-07 |
|    n_updates        | 89444    |
----------------------------------
Eval num_timesteps=368000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 368000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18e-06 |
|    n_updates        | 89499    |
----------------------------------
Eval num_timesteps=368500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 368500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.97e-07 |
|    n_updates        | 89624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1836     |
|    fps              | 34       |
|    time_elapsed     | 10813    |
|    total_timesteps  | 368620   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.14e-07 |
|    n_updates        | 89654    |
----------------------------------
Eval num_timesteps=369000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 369000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.43e-07 |
|    n_updates        | 89749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1840     |
|    fps              | 34       |
|    time_elapsed     | 10831    |
|    total_timesteps  | 369460   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.83e-07 |
|    n_updates        | 89864    |
----------------------------------
Eval num_timesteps=369500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 369500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-07 |
|    n_updates        | 89874    |
----------------------------------
Eval num_timesteps=370000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 370000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000161 |
|    n_updates        | 89999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1844     |
|    fps              | 34       |
|    time_elapsed     | 10859    |
|    total_timesteps  | 370300   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.71e-07 |
|    n_updates        | 90074    |
----------------------------------
Eval num_timesteps=370500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 370500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.88e-07 |
|    n_updates        | 90124    |
----------------------------------
Eval num_timesteps=371000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 371000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.15e-07 |
|    n_updates        | 90249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1848     |
|    fps              | 34       |
|    time_elapsed     | 10887    |
|    total_timesteps  | 371033   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.66e-08 |
|    n_updates        | 90258    |
----------------------------------
Eval num_timesteps=371500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 371500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.72e-07 |
|    n_updates        | 90374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1852     |
|    fps              | 34       |
|    time_elapsed     | 10902    |
|    total_timesteps  | 371873   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.58e-08 |
|    n_updates        | 90468    |
----------------------------------
Eval num_timesteps=372000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 372000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.56e-07 |
|    n_updates        | 90499    |
----------------------------------
Eval num_timesteps=372500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 372500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.11e-06 |
|    n_updates        | 90624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1856     |
|    fps              | 34       |
|    time_elapsed     | 10931    |
|    total_timesteps  | 372687   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.6e-06  |
|    n_updates        | 90671    |
----------------------------------
Eval num_timesteps=373000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 373000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.85e-07 |
|    n_updates        | 90749    |
----------------------------------
Eval num_timesteps=373500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 373500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.64e-07 |
|    n_updates        | 90874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.189   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1860     |
|    fps              | 34       |
|    time_elapsed     | 10961    |
|    total_timesteps  | 373527   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.74e-08 |
|    n_updates        | 90881    |
----------------------------------
Eval num_timesteps=374000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 374000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.16e-08 |
|    n_updates        | 90999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.189   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1864     |
|    fps              | 34       |
|    time_elapsed     | 10973    |
|    total_timesteps  | 374367   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000158 |
|    n_updates        | 91091    |
----------------------------------
Eval num_timesteps=374500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 374500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.65e-07 |
|    n_updates        | 91124    |
----------------------------------
Eval num_timesteps=375000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 375000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000172 |
|    n_updates        | 91249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.189   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1868     |
|    fps              | 34       |
|    time_elapsed     | 11002    |
|    total_timesteps  | 375207   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.11e-06 |
|    n_updates        | 91301    |
----------------------------------
Eval num_timesteps=375500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 375500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25e-07 |
|    n_updates        | 91374    |
----------------------------------
Eval num_timesteps=376000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 376000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73e-06 |
|    n_updates        | 91499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.189   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1872     |
|    fps              | 34       |
|    time_elapsed     | 11037    |
|    total_timesteps  | 376047   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.56e-06 |
|    n_updates        | 91511    |
----------------------------------
Eval num_timesteps=376500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 376500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.31e-06 |
|    n_updates        | 91624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1876     |
|    fps              | 34       |
|    time_elapsed     | 11051    |
|    total_timesteps  | 376514   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.81e-06 |
|    n_updates        | 91628    |
----------------------------------
Eval num_timesteps=377000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 377000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.29e-07 |
|    n_updates        | 91749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1880     |
|    fps              | 34       |
|    time_elapsed     | 11067    |
|    total_timesteps  | 377155   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56e-08 |
|    n_updates        | 91788    |
----------------------------------
Eval num_timesteps=377500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 377500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.77e-07 |
|    n_updates        | 91874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1884     |
|    fps              | 34       |
|    time_elapsed     | 11080    |
|    total_timesteps  | 377995   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.74e-07 |
|    n_updates        | 91998    |
----------------------------------
Eval num_timesteps=378000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 378000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.82e-07 |
|    n_updates        | 91999    |
----------------------------------
Eval num_timesteps=378500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 378500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.86e-07 |
|    n_updates        | 92124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1888     |
|    fps              | 34       |
|    time_elapsed     | 11109    |
|    total_timesteps  | 378835   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.53e-07 |
|    n_updates        | 92208    |
----------------------------------
Eval num_timesteps=379000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 379000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.48e-08 |
|    n_updates        | 92249    |
----------------------------------
Eval num_timesteps=379500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 379500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.71e-08 |
|    n_updates        | 92374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1892     |
|    fps              | 34       |
|    time_elapsed     | 11139    |
|    total_timesteps  | 379675   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.56e-08 |
|    n_updates        | 92418    |
----------------------------------
Eval num_timesteps=380000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 380000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.15e-07 |
|    n_updates        | 92499    |
----------------------------------
Eval num_timesteps=380500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 380500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.89e-06 |
|    n_updates        | 92624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1896     |
|    fps              | 34       |
|    time_elapsed     | 11172    |
|    total_timesteps  | 380515   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.95e-07 |
|    n_updates        | 92628    |
----------------------------------
Eval num_timesteps=381000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 381000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.51e-07 |
|    n_updates        | 92749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1900     |
|    fps              | 34       |
|    time_elapsed     | 11185    |
|    total_timesteps  | 381355   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.14e-07 |
|    n_updates        | 92838    |
----------------------------------
Eval num_timesteps=381500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 381500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.1e-08  |
|    n_updates        | 92874    |
----------------------------------
Eval num_timesteps=382000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 382000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.36e-06 |
|    n_updates        | 92999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1904     |
|    fps              | 34       |
|    time_elapsed     | 11213    |
|    total_timesteps  | 382195   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.03e-07 |
|    n_updates        | 93048    |
----------------------------------
Eval num_timesteps=382500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 382500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18e-07 |
|    n_updates        | 93124    |
----------------------------------
Eval num_timesteps=383000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 383000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.86e-07 |
|    n_updates        | 93249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1908     |
|    fps              | 34       |
|    time_elapsed     | 11242    |
|    total_timesteps  | 383035   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.3e-07  |
|    n_updates        | 93258    |
----------------------------------
Eval num_timesteps=383500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 383500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.67e-07 |
|    n_updates        | 93374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1912     |
|    fps              | 34       |
|    time_elapsed     | 11260    |
|    total_timesteps  | 383875   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.84e-07 |
|    n_updates        | 93468    |
----------------------------------
Eval num_timesteps=384000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 384000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.54e-07 |
|    n_updates        | 93499    |
----------------------------------
Eval num_timesteps=384500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 384500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-07 |
|    n_updates        | 93624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1916     |
|    fps              | 34       |
|    time_elapsed     | 11291    |
|    total_timesteps  | 384715   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28e-06 |
|    n_updates        | 93678    |
----------------------------------
Eval num_timesteps=385000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 385000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.85e-06 |
|    n_updates        | 93749    |
----------------------------------
Eval num_timesteps=385500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 385500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.38e-07 |
|    n_updates        | 93874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1920     |
|    fps              | 34       |
|    time_elapsed     | 11320    |
|    total_timesteps  | 385555   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2e-07  |
|    n_updates        | 93888    |
----------------------------------
Eval num_timesteps=386000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 386000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.17e-07 |
|    n_updates        | 93999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1924     |
|    fps              | 34       |
|    time_elapsed     | 11338    |
|    total_timesteps  | 386395   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-07 |
|    n_updates        | 94098    |
----------------------------------
Eval num_timesteps=386500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 386500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.42e-07 |
|    n_updates        | 94124    |
----------------------------------
Eval num_timesteps=387000, episode_reward=-0.14 +/- 0.29
Episode length: 197.66 +/- 48.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 387000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0157   |
|    n_updates        | 94249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1928     |
|    fps              | 34       |
|    time_elapsed     | 11365    |
|    total_timesteps  | 387205   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.38e-06 |
|    n_updates        | 94301    |
----------------------------------
Eval num_timesteps=387500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 387500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-06 |
|    n_updates        | 94374    |
----------------------------------
Eval num_timesteps=388000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 388000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000181 |
|    n_updates        | 94499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1932     |
|    fps              | 34       |
|    time_elapsed     | 11393    |
|    total_timesteps  | 388045   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.1e-07  |
|    n_updates        | 94511    |
----------------------------------
Eval num_timesteps=388500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 388500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.16e-07 |
|    n_updates        | 94624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1936     |
|    fps              | 34       |
|    time_elapsed     | 11407    |
|    total_timesteps  | 388696   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28e-07 |
|    n_updates        | 94673    |
----------------------------------
Eval num_timesteps=389000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 389000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000181 |
|    n_updates        | 94749    |
----------------------------------
Eval num_timesteps=389500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 389500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000194 |
|    n_updates        | 94874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1940     |
|    fps              | 34       |
|    time_elapsed     | 11437    |
|    total_timesteps  | 389536   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.26e-07 |
|    n_updates        | 94883    |
----------------------------------
Eval num_timesteps=390000, episode_reward=-0.14 +/- 0.29
Episode length: 197.68 +/- 48.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 390000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.02e-06 |
|    n_updates        | 94999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1944     |
|    fps              | 34       |
|    time_elapsed     | 11450    |
|    total_timesteps  | 390376   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13e-06 |
|    n_updates        | 95093    |
----------------------------------
Eval num_timesteps=390500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 390500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28e-07 |
|    n_updates        | 95124    |
----------------------------------
Eval num_timesteps=391000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 391000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.29e-07 |
|    n_updates        | 95249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1948     |
|    fps              | 34       |
|    time_elapsed     | 11478    |
|    total_timesteps  | 391216   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000188 |
|    n_updates        | 95303    |
----------------------------------
Eval num_timesteps=391500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 391500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.53e-08 |
|    n_updates        | 95374    |
----------------------------------
Eval num_timesteps=392000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 392000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.04e-07 |
|    n_updates        | 95499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1952     |
|    fps              | 34       |
|    time_elapsed     | 11507    |
|    total_timesteps  | 392056   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.13e-07 |
|    n_updates        | 95513    |
----------------------------------
Eval num_timesteps=392500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 392500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18e-06 |
|    n_updates        | 95624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1956     |
|    fps              | 34       |
|    time_elapsed     | 11526    |
|    total_timesteps  | 392896   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.12e-06 |
|    n_updates        | 95723    |
----------------------------------
Eval num_timesteps=393000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 393000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.97e-07 |
|    n_updates        | 95749    |
----------------------------------
Eval num_timesteps=393500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 393500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.66e-07 |
|    n_updates        | 95874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1960     |
|    fps              | 34       |
|    time_elapsed     | 11554    |
|    total_timesteps  | 393736   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000175 |
|    n_updates        | 95933    |
----------------------------------
Eval num_timesteps=394000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 394000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.25e-07 |
|    n_updates        | 95999    |
----------------------------------
Eval num_timesteps=394500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 394500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66e-07 |
|    n_updates        | 96124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1964     |
|    fps              | 34       |
|    time_elapsed     | 11584    |
|    total_timesteps  | 394576   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.63e-07 |
|    n_updates        | 96143    |
----------------------------------
Eval num_timesteps=395000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 395000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000174 |
|    n_updates        | 96249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1968     |
|    fps              | 34       |
|    time_elapsed     | 11602    |
|    total_timesteps  | 395416   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.7e-07  |
|    n_updates        | 96353    |
----------------------------------
Eval num_timesteps=395500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 395500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66e-07 |
|    n_updates        | 96374    |
----------------------------------
Eval num_timesteps=396000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 396000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.51e-07 |
|    n_updates        | 96499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1972     |
|    fps              | 34       |
|    time_elapsed     | 11631    |
|    total_timesteps  | 396256   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-06 |
|    n_updates        | 96563    |
----------------------------------
Eval num_timesteps=396500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 396500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.75e-06 |
|    n_updates        | 96624    |
----------------------------------
Eval num_timesteps=397000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 397000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54e-06 |
|    n_updates        | 96749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1976     |
|    fps              | 34       |
|    time_elapsed     | 11660    |
|    total_timesteps  | 397096   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.9e-07  |
|    n_updates        | 96773    |
----------------------------------
Eval num_timesteps=397500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 397500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.35e-05 |
|    n_updates        | 96874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1980     |
|    fps              | 34       |
|    time_elapsed     | 11672    |
|    total_timesteps  | 397936   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.42e-07 |
|    n_updates        | 96983    |
----------------------------------
Eval num_timesteps=398000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 398000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.72e-07 |
|    n_updates        | 96999    |
----------------------------------
Eval num_timesteps=398500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 398500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.13e-05 |
|    n_updates        | 97124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1984     |
|    fps              | 34       |
|    time_elapsed     | 11705    |
|    total_timesteps  | 398776   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.65e-06 |
|    n_updates        | 97193    |
----------------------------------
Eval num_timesteps=399000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 399000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.83e-06 |
|    n_updates        | 97249    |
----------------------------------
Eval num_timesteps=399500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 399500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.84e-07 |
|    n_updates        | 97374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1988     |
|    fps              | 34       |
|    time_elapsed     | 11734    |
|    total_timesteps  | 399616   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43e-06 |
|    n_updates        | 97403    |
----------------------------------
Eval num_timesteps=400000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 400000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.08e-07 |
|    n_updates        | 97499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1992     |
|    fps              | 34       |
|    time_elapsed     | 11747    |
|    total_timesteps  | 400456   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.85e-05 |
|    n_updates        | 97613    |
----------------------------------
Eval num_timesteps=400500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 400500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.59e-06 |
|    n_updates        | 97624    |
----------------------------------
Eval num_timesteps=401000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 401000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01e-05 |
|    n_updates        | 97749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1996     |
|    fps              | 34       |
|    time_elapsed     | 11776    |
|    total_timesteps  | 401296   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.57e-07 |
|    n_updates        | 97823    |
----------------------------------
Eval num_timesteps=401500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 401500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.25e-07 |
|    n_updates        | 97874    |
----------------------------------
Eval num_timesteps=402000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 402000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000245 |
|    n_updates        | 97999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2000     |
|    fps              | 34       |
|    time_elapsed     | 11805    |
|    total_timesteps  | 402136   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.33e-07 |
|    n_updates        | 98033    |
----------------------------------
Eval num_timesteps=402500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 402500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000257 |
|    n_updates        | 98124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2004     |
|    fps              | 34       |
|    time_elapsed     | 11821    |
|    total_timesteps  | 402976   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8e-07    |
|    n_updates        | 98243    |
----------------------------------
Eval num_timesteps=403000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 403000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.98e-07 |
|    n_updates        | 98249    |
----------------------------------
Eval num_timesteps=403500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 403500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000249 |
|    n_updates        | 98374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2008     |
|    fps              | 34       |
|    time_elapsed     | 11848    |
|    total_timesteps  | 403695   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.47e-07 |
|    n_updates        | 98423    |
----------------------------------
Eval num_timesteps=404000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 404000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.56e-06 |
|    n_updates        | 98499    |
----------------------------------
Eval num_timesteps=404500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 404500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-06 |
|    n_updates        | 98624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2012     |
|    fps              | 34       |
|    time_elapsed     | 11876    |
|    total_timesteps  | 404535   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.81e-06 |
|    n_updates        | 98633    |
----------------------------------
Eval num_timesteps=405000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 405000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.33e-06 |
|    n_updates        | 98749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2016     |
|    fps              | 34       |
|    time_elapsed     | 11894    |
|    total_timesteps  | 405375   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.65e-06 |
|    n_updates        | 98843    |
----------------------------------
Eval num_timesteps=405500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 405500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41e-06 |
|    n_updates        | 98874    |
----------------------------------
Eval num_timesteps=406000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 406000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.03e-06 |
|    n_updates        | 98999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2020     |
|    fps              | 34       |
|    time_elapsed     | 11925    |
|    total_timesteps  | 406215   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.75e-07 |
|    n_updates        | 99053    |
----------------------------------
Eval num_timesteps=406500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 406500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000289 |
|    n_updates        | 99124    |
----------------------------------
Eval num_timesteps=407000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 407000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.59e-06 |
|    n_updates        | 99249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2024     |
|    fps              | 34       |
|    time_elapsed     | 11954    |
|    total_timesteps  | 407055   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.5e-06  |
|    n_updates        | 99263    |
----------------------------------
Eval num_timesteps=407500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 407500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.43e-07 |
|    n_updates        | 99374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2028     |
|    fps              | 34       |
|    time_elapsed     | 11969    |
|    total_timesteps  | 407895   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.48e-07 |
|    n_updates        | 99473    |
----------------------------------
Eval num_timesteps=408000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 408000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.13e-08 |
|    n_updates        | 99499    |
----------------------------------
Eval num_timesteps=408500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 408500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000303 |
|    n_updates        | 99624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2032     |
|    fps              | 34       |
|    time_elapsed     | 11998    |
|    total_timesteps  | 408735   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55e-07 |
|    n_updates        | 99683    |
----------------------------------
Eval num_timesteps=409000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 409000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92e-07 |
|    n_updates        | 99749    |
----------------------------------
Eval num_timesteps=409500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 409500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.53e-06 |
|    n_updates        | 99874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2036     |
|    fps              | 34       |
|    time_elapsed     | 12027    |
|    total_timesteps  | 409575   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.28e-07 |
|    n_updates        | 99893    |
----------------------------------
Eval num_timesteps=410000, episode_reward=-0.14 +/- 0.29
Episode length: 197.66 +/- 48.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 410000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.38e-07 |
|    n_updates        | 99999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2040     |
|    fps              | 34       |
|    time_elapsed     | 12039    |
|    total_timesteps  | 410415   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.35e-07 |
|    n_updates        | 100103   |
----------------------------------
Eval num_timesteps=410500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 410500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.012    |
|    n_updates        | 100124   |
----------------------------------
Eval num_timesteps=411000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 411000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.45e-08 |
|    n_updates        | 100249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2044     |
|    fps              | 34       |
|    time_elapsed     | 12067    |
|    total_timesteps  | 411255   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3e-06  |
|    n_updates        | 100313   |
----------------------------------
Eval num_timesteps=411500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 411500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.34e-07 |
|    n_updates        | 100374   |
----------------------------------
Eval num_timesteps=412000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 412000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.15e-06 |
|    n_updates        | 100499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2048     |
|    fps              | 34       |
|    time_elapsed     | 12096    |
|    total_timesteps  | 412095   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.32e-07 |
|    n_updates        | 100523   |
----------------------------------
Eval num_timesteps=412500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 412500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.55e-07 |
|    n_updates        | 100624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2052     |
|    fps              | 34       |
|    time_elapsed     | 12114    |
|    total_timesteps  | 412935   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-06 |
|    n_updates        | 100733   |
----------------------------------
Eval num_timesteps=413000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 413000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51e-06 |
|    n_updates        | 100749   |
----------------------------------
Eval num_timesteps=413500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 413500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.93e-06 |
|    n_updates        | 100874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2056     |
|    fps              | 34       |
|    time_elapsed     | 12143    |
|    total_timesteps  | 413775   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.83e-07 |
|    n_updates        | 100943   |
----------------------------------
Eval num_timesteps=414000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 414000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.37e-07 |
|    n_updates        | 100999   |
----------------------------------
Eval num_timesteps=414500, episode_reward=-0.14 +/- 0.29
Episode length: 197.70 +/- 48.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 414500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.92e-07 |
|    n_updates        | 101124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2060     |
|    fps              | 34       |
|    time_elapsed     | 12170    |
|    total_timesteps  | 414615   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.56e-06 |
|    n_updates        | 101153   |
----------------------------------
Eval num_timesteps=415000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 415000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.27e-07 |
|    n_updates        | 101249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2064     |
|    fps              | 34       |
|    time_elapsed     | 12183    |
|    total_timesteps  | 415455   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.72e-06 |
|    n_updates        | 101363   |
----------------------------------
Eval num_timesteps=415500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 415500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01e-06 |
|    n_updates        | 101374   |
----------------------------------
Eval num_timesteps=416000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 416000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.77e-06 |
|    n_updates        | 101499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2068     |
|    fps              | 34       |
|    time_elapsed     | 12212    |
|    total_timesteps  | 416295   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.06e-07 |
|    n_updates        | 101573   |
----------------------------------
Eval num_timesteps=416500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 416500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.13e-08 |
|    n_updates        | 101624   |
----------------------------------
Eval num_timesteps=417000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 417000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.97e-06 |
|    n_updates        | 101749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2072     |
|    fps              | 34       |
|    time_elapsed     | 12242    |
|    total_timesteps  | 417135   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.65e-07 |
|    n_updates        | 101783   |
----------------------------------
Eval num_timesteps=417500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 417500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.85e-06 |
|    n_updates        | 101874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2076     |
|    fps              | 34       |
|    time_elapsed     | 12256    |
|    total_timesteps  | 417975   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.46e-07 |
|    n_updates        | 101993   |
----------------------------------
Eval num_timesteps=418000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 418000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.15e-06 |
|    n_updates        | 101999   |
----------------------------------
Eval num_timesteps=418500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 418500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.99e-07 |
|    n_updates        | 102124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2080     |
|    fps              | 34       |
|    time_elapsed     | 12284    |
|    total_timesteps  | 418815   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.33e-07 |
|    n_updates        | 102203   |
----------------------------------
Eval num_timesteps=419000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 419000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000329 |
|    n_updates        | 102249   |
----------------------------------
Eval num_timesteps=419500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 419500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.55e-07 |
|    n_updates        | 102374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2084     |
|    fps              | 34       |
|    time_elapsed     | 12313    |
|    total_timesteps  | 419655   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-06  |
|    n_updates        | 102413   |
----------------------------------
Eval num_timesteps=420000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 420000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.77e-07 |
|    n_updates        | 102499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2088     |
|    fps              | 34       |
|    time_elapsed     | 12330    |
|    total_timesteps  | 420291   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.6e-06  |
|    n_updates        | 102572   |
----------------------------------
Eval num_timesteps=420500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 420500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.95e-07 |
|    n_updates        | 102624   |
----------------------------------
Eval num_timesteps=421000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 421000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.21e-07 |
|    n_updates        | 102749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2092     |
|    fps              | 34       |
|    time_elapsed     | 12359    |
|    total_timesteps  | 421131   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.41e-06 |
|    n_updates        | 102782   |
----------------------------------
Eval num_timesteps=421500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 421500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.9e-06  |
|    n_updates        | 102874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2096     |
|    fps              | 34       |
|    time_elapsed     | 12373    |
|    total_timesteps  | 421971   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.47e-07 |
|    n_updates        | 102992   |
----------------------------------
Eval num_timesteps=422000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 422000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000298 |
|    n_updates        | 102999   |
----------------------------------
Eval num_timesteps=422500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 422500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36e-07 |
|    n_updates        | 103124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2100     |
|    fps              | 34       |
|    time_elapsed     | 12402    |
|    total_timesteps  | 422811   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.23e-06 |
|    n_updates        | 103202   |
----------------------------------
Eval num_timesteps=423000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 423000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.27e-07 |
|    n_updates        | 103249   |
----------------------------------
Eval num_timesteps=423500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 423500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000291 |
|    n_updates        | 103374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2104     |
|    fps              | 34       |
|    time_elapsed     | 12431    |
|    total_timesteps  | 423651   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.71e-06 |
|    n_updates        | 103412   |
----------------------------------
Eval num_timesteps=424000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 424000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00031  |
|    n_updates        | 103499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2108     |
|    fps              | 34       |
|    time_elapsed     | 12449    |
|    total_timesteps  | 424491   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.07e-06 |
|    n_updates        | 103622   |
----------------------------------
Eval num_timesteps=424500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 424500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-05 |
|    n_updates        | 103624   |
----------------------------------
Eval num_timesteps=425000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 425000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.32e-07 |
|    n_updates        | 103749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2112     |
|    fps              | 34       |
|    time_elapsed     | 12479    |
|    total_timesteps  | 425331   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.14e-06 |
|    n_updates        | 103832   |
----------------------------------
Eval num_timesteps=425500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 425500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-05 |
|    n_updates        | 103874   |
----------------------------------
Eval num_timesteps=426000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 426000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.49e-07 |
|    n_updates        | 103999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2116     |
|    fps              | 34       |
|    time_elapsed     | 12508    |
|    total_timesteps  | 426171   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000283 |
|    n_updates        | 104042   |
----------------------------------
Eval num_timesteps=426500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 426500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.11e-07 |
|    n_updates        | 104124   |
----------------------------------
Eval num_timesteps=427000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 427000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.21e-07 |
|    n_updates        | 104249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2120     |
|    fps              | 34       |
|    time_elapsed     | 12536    |
|    total_timesteps  | 427011   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000277 |
|    n_updates        | 104252   |
----------------------------------
Eval num_timesteps=427500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 427500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000278 |
|    n_updates        | 104374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2124     |
|    fps              | 34       |
|    time_elapsed     | 12552    |
|    total_timesteps  | 427851   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000275 |
|    n_updates        | 104462   |
----------------------------------
Eval num_timesteps=428000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 428000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.14e-07 |
|    n_updates        | 104499   |
----------------------------------
Eval num_timesteps=428500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 428500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.87e-07 |
|    n_updates        | 104624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2128     |
|    fps              | 34       |
|    time_elapsed     | 12580    |
|    total_timesteps  | 428691   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.79e-06 |
|    n_updates        | 104672   |
----------------------------------
Eval num_timesteps=429000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 429000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.4e-07  |
|    n_updates        | 104749   |
----------------------------------
Eval num_timesteps=429500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 429500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000259 |
|    n_updates        | 104874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2132     |
|    fps              | 34       |
|    time_elapsed     | 12608    |
|    total_timesteps  | 429531   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-06 |
|    n_updates        | 104882   |
----------------------------------
Eval num_timesteps=430000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 430000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.7e-06  |
|    n_updates        | 104999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2136     |
|    fps              | 34       |
|    time_elapsed     | 12625    |
|    total_timesteps  | 430371   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.48e-07 |
|    n_updates        | 105092   |
----------------------------------
Eval num_timesteps=430500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 430500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.41e-07 |
|    n_updates        | 105124   |
----------------------------------
Eval num_timesteps=431000, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 431000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63e-06 |
|    n_updates        | 105249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2140     |
|    fps              | 34       |
|    time_elapsed     | 12653    |
|    total_timesteps  | 431211   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86e-06 |
|    n_updates        | 105302   |
----------------------------------
Eval num_timesteps=431500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 431500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.19e-06 |
|    n_updates        | 105374   |
----------------------------------
Eval num_timesteps=432000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 432000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.26e-07 |
|    n_updates        | 105499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2144     |
|    fps              | 34       |
|    time_elapsed     | 12683    |
|    total_timesteps  | 432051   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.16e-06 |
|    n_updates        | 105512   |
----------------------------------
Eval num_timesteps=432500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 432500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.15e-07 |
|    n_updates        | 105624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2148     |
|    fps              | 34       |
|    time_elapsed     | 12695    |
|    total_timesteps  | 432891   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.31e-06 |
|    n_updates        | 105722   |
----------------------------------
Eval num_timesteps=433000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 433000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.74e-07 |
|    n_updates        | 105749   |
----------------------------------
Eval num_timesteps=433500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 433500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.84e-07 |
|    n_updates        | 105874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2152     |
|    fps              | 34       |
|    time_elapsed     | 12724    |
|    total_timesteps  | 433731   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.93e-07 |
|    n_updates        | 105932   |
----------------------------------
Eval num_timesteps=434000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 434000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.46e-07 |
|    n_updates        | 105999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2156     |
|    fps              | 34       |
|    time_elapsed     | 12742    |
|    total_timesteps  | 434487   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000252 |
|    n_updates        | 106121   |
----------------------------------
Eval num_timesteps=434500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 434500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-06 |
|    n_updates        | 106124   |
----------------------------------
Eval num_timesteps=435000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 435000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.93e-08 |
|    n_updates        | 106249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2160     |
|    fps              | 34       |
|    time_elapsed     | 12770    |
|    total_timesteps  | 435327   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000239 |
|    n_updates        | 106331   |
----------------------------------
Eval num_timesteps=435500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 435500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.78e-08 |
|    n_updates        | 106374   |
----------------------------------
Eval num_timesteps=436000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 436000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01e-06 |
|    n_updates        | 106499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2164     |
|    fps              | 34       |
|    time_elapsed     | 12799    |
|    total_timesteps  | 436167   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.75e-06 |
|    n_updates        | 106541   |
----------------------------------
Eval num_timesteps=436500, episode_reward=-0.14 +/- 0.29
Episode length: 197.72 +/- 48.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 436500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-06 |
|    n_updates        | 106624   |
----------------------------------
Eval num_timesteps=437000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 437000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.38e-07 |
|    n_updates        | 106749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2168     |
|    fps              | 34       |
|    time_elapsed     | 12827    |
|    total_timesteps  | 437007   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5e-06  |
|    n_updates        | 106751   |
----------------------------------
Eval num_timesteps=437500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 437500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000267 |
|    n_updates        | 106874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2172     |
|    fps              | 34       |
|    time_elapsed     | 12842    |
|    total_timesteps  | 437847   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.92e-07 |
|    n_updates        | 106961   |
----------------------------------
Eval num_timesteps=438000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 438000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.24e-07 |
|    n_updates        | 106999   |
----------------------------------
Eval num_timesteps=438500, episode_reward=-0.19 +/- 0.16
Episode length: 207.12 +/- 20.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 438500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.98e-06 |
|    n_updates        | 107124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2176     |
|    fps              | 34       |
|    time_elapsed     | 12876    |
|    total_timesteps  | 438687   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.95e-06 |
|    n_updates        | 107171   |
----------------------------------
Eval num_timesteps=439000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 439000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01e-07 |
|    n_updates        | 107249   |
----------------------------------
Eval num_timesteps=439500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 439500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-07 |
|    n_updates        | 107374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2180     |
|    fps              | 34       |
|    time_elapsed     | 12905    |
|    total_timesteps  | 439527   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-07 |
|    n_updates        | 107381   |
----------------------------------
Eval num_timesteps=440000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 440000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.72e-05 |
|    n_updates        | 107499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2184     |
|    fps              | 34       |
|    time_elapsed     | 12919    |
|    total_timesteps  | 440367   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.4e-07  |
|    n_updates        | 107591   |
----------------------------------
Eval num_timesteps=440500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 440500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000249 |
|    n_updates        | 107624   |
----------------------------------
Eval num_timesteps=441000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 441000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.38e-08 |
|    n_updates        | 107749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2188     |
|    fps              | 34       |
|    time_elapsed     | 12946    |
|    total_timesteps  | 441207   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000244 |
|    n_updates        | 107801   |
----------------------------------
Eval num_timesteps=441500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 441500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-06 |
|    n_updates        | 107874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2192     |
|    fps              | 34       |
|    time_elapsed     | 12961    |
|    total_timesteps  | 441911   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000243 |
|    n_updates        | 107977   |
----------------------------------
Eval num_timesteps=442000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 442000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.42e-07 |
|    n_updates        | 107999   |
----------------------------------
Eval num_timesteps=442500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 442500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.86e-08 |
|    n_updates        | 108124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2196     |
|    fps              | 34       |
|    time_elapsed     | 12989    |
|    total_timesteps  | 442751   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.65e-07 |
|    n_updates        | 108187   |
----------------------------------
Eval num_timesteps=443000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 443000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.81e-07 |
|    n_updates        | 108249   |
----------------------------------
Eval num_timesteps=443500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 443500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.67e-07 |
|    n_updates        | 108374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2200     |
|    fps              | 34       |
|    time_elapsed     | 13017    |
|    total_timesteps  | 443591   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4e-07  |
|    n_updates        | 108397   |
----------------------------------
Eval num_timesteps=444000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 444000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00025  |
|    n_updates        | 108499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2204     |
|    fps              | 34       |
|    time_elapsed     | 13034    |
|    total_timesteps  | 444269   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.57e-07 |
|    n_updates        | 108567   |
----------------------------------
Eval num_timesteps=444500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 444500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.17e-06 |
|    n_updates        | 108624   |
----------------------------------
Eval num_timesteps=445000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 445000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000234 |
|    n_updates        | 108749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2208     |
|    fps              | 34       |
|    time_elapsed     | 13063    |
|    total_timesteps  | 445109   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.7e-06  |
|    n_updates        | 108777   |
----------------------------------
Eval num_timesteps=445500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 445500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.74e-07 |
|    n_updates        | 108874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2212     |
|    fps              | 34       |
|    time_elapsed     | 13076    |
|    total_timesteps  | 445949   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.4e-07  |
|    n_updates        | 108987   |
----------------------------------
Eval num_timesteps=446000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 446000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-06 |
|    n_updates        | 108999   |
----------------------------------
Eval num_timesteps=446500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 446500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3e-06  |
|    n_updates        | 109124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2216     |
|    fps              | 34       |
|    time_elapsed     | 13105    |
|    total_timesteps  | 446789   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0121   |
|    n_updates        | 109197   |
----------------------------------
Eval num_timesteps=447000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 447000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-07 |
|    n_updates        | 109249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2220     |
|    fps              | 34       |
|    time_elapsed     | 13123    |
|    total_timesteps  | 447484   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.61e-06 |
|    n_updates        | 109370   |
----------------------------------
Eval num_timesteps=447500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 447500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.92e-06 |
|    n_updates        | 109374   |
----------------------------------
Eval num_timesteps=448000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 448000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.7e-06  |
|    n_updates        | 109499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2224     |
|    fps              | 34       |
|    time_elapsed     | 13151    |
|    total_timesteps  | 448324   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.76e-07 |
|    n_updates        | 109580   |
----------------------------------
Eval num_timesteps=448500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 448500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.6e-06  |
|    n_updates        | 109624   |
----------------------------------
Eval num_timesteps=449000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 449000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.61e-07 |
|    n_updates        | 109749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2228     |
|    fps              | 34       |
|    time_elapsed     | 13179    |
|    total_timesteps  | 449164   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.81e-07 |
|    n_updates        | 109790   |
----------------------------------
Eval num_timesteps=449500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 449500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01e-06 |
|    n_updates        | 109874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2232     |
|    fps              | 34       |
|    time_elapsed     | 13193    |
|    total_timesteps  | 449803   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.62e-07 |
|    n_updates        | 109950   |
----------------------------------
Eval num_timesteps=450000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 450000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.36e-07 |
|    n_updates        | 109999   |
----------------------------------
Eval num_timesteps=450500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 450500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.67e-08 |
|    n_updates        | 110124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2236     |
|    fps              | 34       |
|    time_elapsed     | 13222    |
|    total_timesteps  | 450643   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-06 |
|    n_updates        | 110160   |
----------------------------------
Eval num_timesteps=451000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 451000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.97e-07 |
|    n_updates        | 110249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2240     |
|    fps              | 34       |
|    time_elapsed     | 13236    |
|    total_timesteps  | 451483   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.81e-06 |
|    n_updates        | 110370   |
----------------------------------
Eval num_timesteps=451500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 451500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-05 |
|    n_updates        | 110374   |
----------------------------------
Eval num_timesteps=452000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 452000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.24e-07 |
|    n_updates        | 110499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2244     |
|    fps              | 34       |
|    time_elapsed     | 13264    |
|    total_timesteps  | 452323   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-06 |
|    n_updates        | 110580   |
----------------------------------
Eval num_timesteps=452500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 452500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000252 |
|    n_updates        | 110624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2248     |
|    fps              | 34       |
|    time_elapsed     | 13283    |
|    total_timesteps  | 452986   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87e-06 |
|    n_updates        | 110746   |
----------------------------------
Eval num_timesteps=453000, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 453000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.63e-07 |
|    n_updates        | 110749   |
----------------------------------
Eval num_timesteps=453500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 453500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.09e-07 |
|    n_updates        | 110874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2252     |
|    fps              | 34       |
|    time_elapsed     | 13312    |
|    total_timesteps  | 453773   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-06 |
|    n_updates        | 110943   |
----------------------------------
Eval num_timesteps=454000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 454000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-06 |
|    n_updates        | 110999   |
----------------------------------
Eval num_timesteps=454500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 454500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.19e-07 |
|    n_updates        | 111124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2256     |
|    fps              | 34       |
|    time_elapsed     | 13341    |
|    total_timesteps  | 454613   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.57e-06 |
|    n_updates        | 111153   |
----------------------------------
Eval num_timesteps=455000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 455000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.16e-07 |
|    n_updates        | 111249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2260     |
|    fps              | 34       |
|    time_elapsed     | 13353    |
|    total_timesteps  | 455316   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.88e-07 |
|    n_updates        | 111328   |
----------------------------------
Eval num_timesteps=455500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 455500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.32e-07 |
|    n_updates        | 111374   |
----------------------------------
Eval num_timesteps=456000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 456000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000194 |
|    n_updates        | 111499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2264     |
|    fps              | 34       |
|    time_elapsed     | 13381    |
|    total_timesteps  | 456156   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.57e-07 |
|    n_updates        | 111538   |
----------------------------------
Eval num_timesteps=456500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 456500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.32e-07 |
|    n_updates        | 111624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2268     |
|    fps              | 34       |
|    time_elapsed     | 13398    |
|    total_timesteps  | 456940   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000181 |
|    n_updates        | 111734   |
----------------------------------
Eval num_timesteps=457000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 457000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43e-06 |
|    n_updates        | 111749   |
----------------------------------
Eval num_timesteps=457500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 457500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.96e-06 |
|    n_updates        | 111874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2272     |
|    fps              | 34       |
|    time_elapsed     | 13426    |
|    total_timesteps  | 457780   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.46e-07 |
|    n_updates        | 111944   |
----------------------------------
Eval num_timesteps=458000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 458000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-06 |
|    n_updates        | 111999   |
----------------------------------
Eval num_timesteps=458500, episode_reward=-0.14 +/- 0.28
Episode length: 198.10 +/- 47.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 458500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.41e-07 |
|    n_updates        | 112124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2276     |
|    fps              | 34       |
|    time_elapsed     | 13454    |
|    total_timesteps  | 458620   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.14e-06 |
|    n_updates        | 112154   |
----------------------------------
Eval num_timesteps=459000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 459000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-07 |
|    n_updates        | 112249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2280     |
|    fps              | 34       |
|    time_elapsed     | 13470    |
|    total_timesteps  | 459460   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00018  |
|    n_updates        | 112364   |
----------------------------------
Eval num_timesteps=459500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 459500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.35e-07 |
|    n_updates        | 112374   |
----------------------------------
Eval num_timesteps=460000, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 460000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.91e-07 |
|    n_updates        | 112499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2284     |
|    fps              | 34       |
|    time_elapsed     | 13498    |
|    total_timesteps  | 460300   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28e-06 |
|    n_updates        | 112574   |
----------------------------------
Eval num_timesteps=460500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 460500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.5e-07  |
|    n_updates        | 112624   |
----------------------------------
Eval num_timesteps=461000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 461000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000167 |
|    n_updates        | 112749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2288     |
|    fps              | 34       |
|    time_elapsed     | 13527    |
|    total_timesteps  | 461140   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.99e-07 |
|    n_updates        | 112784   |
----------------------------------
Eval num_timesteps=461500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 461500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.24e-07 |
|    n_updates        | 112874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2292     |
|    fps              | 34       |
|    time_elapsed     | 13539    |
|    total_timesteps  | 461848   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.12e-06 |
|    n_updates        | 112961   |
----------------------------------
Eval num_timesteps=462000, episode_reward=-0.19 +/- 0.16
Episode length: 207.08 +/- 20.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 462000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.55e-07 |
|    n_updates        | 112999   |
----------------------------------
Eval num_timesteps=462500, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 462500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.9e-07  |
|    n_updates        | 113124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2296     |
|    fps              | 34       |
|    time_elapsed     | 13568    |
|    total_timesteps  | 462688   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-07 |
|    n_updates        | 113171   |
----------------------------------
Eval num_timesteps=463000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 463000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-05 |
|    n_updates        | 113249   |
----------------------------------
Eval num_timesteps=463500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 463500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.17e-06 |
|    n_updates        | 113374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2300     |
|    fps              | 34       |
|    time_elapsed     | 13597    |
|    total_timesteps  | 463528   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.85e-06 |
|    n_updates        | 113381   |
----------------------------------
Eval num_timesteps=464000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 464000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28e-07 |
|    n_updates        | 113499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2304     |
|    fps              | 34       |
|    time_elapsed     | 13616    |
|    total_timesteps  | 464368   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.36e-07 |
|    n_updates        | 113591   |
----------------------------------
Eval num_timesteps=464500, episode_reward=-0.14 +/- 0.29
Episode length: 197.64 +/- 48.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 464500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.95e-06 |
|    n_updates        | 113624   |
----------------------------------
Eval num_timesteps=465000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 465000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.47e-07 |
|    n_updates        | 113749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2308     |
|    fps              | 34       |
|    time_elapsed     | 13644    |
|    total_timesteps  | 465208   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4e-07    |
|    n_updates        | 113801   |
----------------------------------
Eval num_timesteps=465500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 465500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.03e-08 |
|    n_updates        | 113874   |
----------------------------------
Eval num_timesteps=466000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 466000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.43e-06 |
|    n_updates        | 113999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2312     |
|    fps              | 34       |
|    time_elapsed     | 13672    |
|    total_timesteps  | 466048   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.87e-06 |
|    n_updates        | 114011   |
----------------------------------
Eval num_timesteps=466500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 466500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000207 |
|    n_updates        | 114124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2316     |
|    fps              | 34       |
|    time_elapsed     | 13685    |
|    total_timesteps  | 466888   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.08e-07 |
|    n_updates        | 114221   |
----------------------------------
Eval num_timesteps=467000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 467000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.39e-06 |
|    n_updates        | 114249   |
----------------------------------
Eval num_timesteps=467500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 467500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59e-06 |
|    n_updates        | 114374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2320     |
|    fps              | 34       |
|    time_elapsed     | 13714    |
|    total_timesteps  | 467728   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1e-07  |
|    n_updates        | 114431   |
----------------------------------
Eval num_timesteps=468000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 468000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.37e-07 |
|    n_updates        | 114499   |
----------------------------------
Eval num_timesteps=468500, episode_reward=-0.19 +/- 0.16
Episode length: 206.48 +/- 24.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 468500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.55e-08 |
|    n_updates        | 114624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2324     |
|    fps              | 34       |
|    time_elapsed     | 13743    |
|    total_timesteps  | 468568   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000231 |
|    n_updates        | 114641   |
----------------------------------
Eval num_timesteps=469000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 469000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56e-06 |
|    n_updates        | 114749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2328     |
|    fps              | 34       |
|    time_elapsed     | 13757    |
|    total_timesteps  | 469408   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.14e-07 |
|    n_updates        | 114851   |
----------------------------------
Eval num_timesteps=469500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 469500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.93e-07 |
|    n_updates        | 114874   |
----------------------------------
Eval num_timesteps=470000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 470000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.46e-08 |
|    n_updates        | 114999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2332     |
|    fps              | 34       |
|    time_elapsed     | 13785    |
|    total_timesteps  | 470248   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.22e-06 |
|    n_updates        | 115061   |
----------------------------------
Eval num_timesteps=470500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 470500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.1e-07  |
|    n_updates        | 115124   |
----------------------------------
Eval num_timesteps=471000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 471000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25e-07 |
|    n_updates        | 115249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2336     |
|    fps              | 34       |
|    time_elapsed     | 13813    |
|    total_timesteps  | 471088   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.13e-06 |
|    n_updates        | 115271   |
----------------------------------
Eval num_timesteps=471500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 471500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.37e-07 |
|    n_updates        | 115374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2340     |
|    fps              | 34       |
|    time_elapsed     | 13832    |
|    total_timesteps  | 471928   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.39e-06 |
|    n_updates        | 115481   |
----------------------------------
Eval num_timesteps=472000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 472000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.64e-07 |
|    n_updates        | 115499   |
----------------------------------
Eval num_timesteps=472500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 472500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.85e-07 |
|    n_updates        | 115624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2344     |
|    fps              | 34       |
|    time_elapsed     | 13861    |
|    total_timesteps  | 472768   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.48e-06 |
|    n_updates        | 115691   |
----------------------------------
Eval num_timesteps=473000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 473000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.46e-07 |
|    n_updates        | 115749   |
----------------------------------
Eval num_timesteps=473500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 473500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2e-06  |
|    n_updates        | 115874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2348     |
|    fps              | 34       |
|    time_elapsed     | 13890    |
|    total_timesteps  | 473608   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.31e-06 |
|    n_updates        | 115901   |
----------------------------------
Eval num_timesteps=474000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 474000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.43e-07 |
|    n_updates        | 115999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2352     |
|    fps              | 34       |
|    time_elapsed     | 13903    |
|    total_timesteps  | 474301   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.25e-07 |
|    n_updates        | 116075   |
----------------------------------
Eval num_timesteps=474500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 474500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.74e-07 |
|    n_updates        | 116124   |
----------------------------------
Eval num_timesteps=475000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 475000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.08e-07 |
|    n_updates        | 116249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2356     |
|    fps              | 34       |
|    time_elapsed     | 13932    |
|    total_timesteps  | 475141   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23e-07 |
|    n_updates        | 116285   |
----------------------------------
Eval num_timesteps=475500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 475500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.14e-07 |
|    n_updates        | 116374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2360     |
|    fps              | 34       |
|    time_elapsed     | 13949    |
|    total_timesteps  | 475981   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.5e-06  |
|    n_updates        | 116495   |
----------------------------------
Eval num_timesteps=476000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 476000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.83e-06 |
|    n_updates        | 116499   |
----------------------------------
Eval num_timesteps=476500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 476500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.38e-07 |
|    n_updates        | 116624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2364     |
|    fps              | 34       |
|    time_elapsed     | 13977    |
|    total_timesteps  | 476821   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67e-07 |
|    n_updates        | 116705   |
----------------------------------
Eval num_timesteps=477000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 477000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.39e-07 |
|    n_updates        | 116749   |
----------------------------------
Eval num_timesteps=477500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 477500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.85e-07 |
|    n_updates        | 116874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2368     |
|    fps              | 34       |
|    time_elapsed     | 14005    |
|    total_timesteps  | 477537   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.89e-07 |
|    n_updates        | 116884   |
----------------------------------
Eval num_timesteps=478000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 478000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.92e-07 |
|    n_updates        | 116999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2372     |
|    fps              | 34       |
|    time_elapsed     | 14021    |
|    total_timesteps  | 478377   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.33e-07 |
|    n_updates        | 117094   |
----------------------------------
Eval num_timesteps=478500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 478500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.7e-08  |
|    n_updates        | 117124   |
----------------------------------
Eval num_timesteps=479000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 479000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.78e-07 |
|    n_updates        | 117249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2376     |
|    fps              | 34       |
|    time_elapsed     | 14050    |
|    total_timesteps  | 479217   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00017  |
|    n_updates        | 117304   |
----------------------------------
Eval num_timesteps=479500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 479500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.87e-07 |
|    n_updates        | 117374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2380     |
|    fps              | 34       |
|    time_elapsed     | 14062    |
|    total_timesteps  | 479856   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.42e-07 |
|    n_updates        | 117463   |
----------------------------------
Eval num_timesteps=480000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 480000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000167 |
|    n_updates        | 117499   |
----------------------------------
Eval num_timesteps=480500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 480500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.7e-08  |
|    n_updates        | 117624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2384     |
|    fps              | 34       |
|    time_elapsed     | 14091    |
|    total_timesteps  | 480696   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.86e-07 |
|    n_updates        | 117673   |
----------------------------------
Eval num_timesteps=481000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 481000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.85e-06 |
|    n_updates        | 117749   |
----------------------------------
Eval num_timesteps=481500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 481500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.38e-06 |
|    n_updates        | 117874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2388     |
|    fps              | 34       |
|    time_elapsed     | 14120    |
|    total_timesteps  | 481536   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000179 |
|    n_updates        | 117883   |
----------------------------------
Eval num_timesteps=482000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 482000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.81e-07 |
|    n_updates        | 117999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2392     |
|    fps              | 34       |
|    time_elapsed     | 14139    |
|    total_timesteps  | 482376   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.52e-06 |
|    n_updates        | 118093   |
----------------------------------
Eval num_timesteps=482500, episode_reward=-0.14 +/- 0.29
Episode length: 197.70 +/- 48.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 482500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.39e-06 |
|    n_updates        | 118124   |
----------------------------------
Eval num_timesteps=483000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 483000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.83e-07 |
|    n_updates        | 118249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2396     |
|    fps              | 34       |
|    time_elapsed     | 14167    |
|    total_timesteps  | 483078   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000317 |
|    n_updates        | 118269   |
----------------------------------
Eval num_timesteps=483500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 483500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.41e-07 |
|    n_updates        | 118374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2400     |
|    fps              | 34       |
|    time_elapsed     | 14183    |
|    total_timesteps  | 483918   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-07 |
|    n_updates        | 118479   |
----------------------------------
Eval num_timesteps=484000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 484000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.42e-07 |
|    n_updates        | 118499   |
----------------------------------
Eval num_timesteps=484500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 484500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.29e-07 |
|    n_updates        | 118624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2404     |
|    fps              | 34       |
|    time_elapsed     | 14211    |
|    total_timesteps  | 484758   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.6e-07  |
|    n_updates        | 118689   |
----------------------------------
Eval num_timesteps=485000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 485000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.78e-07 |
|    n_updates        | 118749   |
----------------------------------
Eval num_timesteps=485500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 485500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000135 |
|    n_updates        | 118874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2408     |
|    fps              | 34       |
|    time_elapsed     | 14241    |
|    total_timesteps  | 485598   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.77e-07 |
|    n_updates        | 118899   |
----------------------------------
Eval num_timesteps=486000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 486000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000149 |
|    n_updates        | 118999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2412     |
|    fps              | 34       |
|    time_elapsed     | 14253    |
|    total_timesteps  | 486417   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.35e-07 |
|    n_updates        | 119104   |
----------------------------------
Eval num_timesteps=486500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 486500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.69e-07 |
|    n_updates        | 119124   |
----------------------------------
Eval num_timesteps=487000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 487000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.21e-07 |
|    n_updates        | 119249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2416     |
|    fps              | 34       |
|    time_elapsed     | 14282    |
|    total_timesteps  | 487257   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.29e-07 |
|    n_updates        | 119314   |
----------------------------------
Eval num_timesteps=487500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 487500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.92e-07 |
|    n_updates        | 119374   |
----------------------------------
Eval num_timesteps=488000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 488000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55e-07 |
|    n_updates        | 119499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2420     |
|    fps              | 34       |
|    time_elapsed     | 14311    |
|    total_timesteps  | 488097   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.15e-07 |
|    n_updates        | 119524   |
----------------------------------
Eval num_timesteps=488500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 488500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.67e-06 |
|    n_updates        | 119624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2424     |
|    fps              | 34       |
|    time_elapsed     | 14328    |
|    total_timesteps  | 488937   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.54e-06 |
|    n_updates        | 119734   |
----------------------------------
Eval num_timesteps=489000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 489000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.34e-07 |
|    n_updates        | 119749   |
----------------------------------
Eval num_timesteps=489500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 489500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000128 |
|    n_updates        | 119874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2428     |
|    fps              | 34       |
|    time_elapsed     | 14356    |
|    total_timesteps  | 489603   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000127 |
|    n_updates        | 119900   |
----------------------------------
Eval num_timesteps=490000, episode_reward=-0.19 +/- 0.16
Episode length: 206.62 +/- 23.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 490000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1e-07  |
|    n_updates        | 119999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2432     |
|    fps              | 34       |
|    time_elapsed     | 14371    |
|    total_timesteps  | 490443   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.8e-06  |
|    n_updates        | 120110   |
----------------------------------
Eval num_timesteps=490500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 490500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000132 |
|    n_updates        | 120124   |
----------------------------------
Eval num_timesteps=491000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 491000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.05e-08 |
|    n_updates        | 120249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2436     |
|    fps              | 34       |
|    time_elapsed     | 14400    |
|    total_timesteps  | 491283   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000124 |
|    n_updates        | 120320   |
----------------------------------
Eval num_timesteps=491500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 491500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.33e-07 |
|    n_updates        | 120374   |
----------------------------------
Eval num_timesteps=492000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 492000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.38e-06 |
|    n_updates        | 120499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2440     |
|    fps              | 34       |
|    time_elapsed     | 14429    |
|    total_timesteps  | 492123   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0128   |
|    n_updates        | 120530   |
----------------------------------
Eval num_timesteps=492500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 492500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.23e-07 |
|    n_updates        | 120624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2444     |
|    fps              | 34       |
|    time_elapsed     | 14441    |
|    total_timesteps  | 492875   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000138 |
|    n_updates        | 120718   |
----------------------------------
Eval num_timesteps=493000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 493000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.96e-07 |
|    n_updates        | 120749   |
----------------------------------
Eval num_timesteps=493500, episode_reward=-0.19 +/- 0.16
Episode length: 206.58 +/- 23.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 493500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.83e-06 |
|    n_updates        | 120874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2448     |
|    fps              | 34       |
|    time_elapsed     | 14470    |
|    total_timesteps  | 493715   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.08e-07 |
|    n_updates        | 120928   |
----------------------------------
Eval num_timesteps=494000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 494000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16e-07 |
|    n_updates        | 120999   |
----------------------------------
Eval num_timesteps=494500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 494500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000131 |
|    n_updates        | 121124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2452     |
|    fps              | 34       |
|    time_elapsed     | 14499    |
|    total_timesteps  | 494555   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000134 |
|    n_updates        | 121138   |
----------------------------------
Eval num_timesteps=495000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 495000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000131 |
|    n_updates        | 121249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2456     |
|    fps              | 34       |
|    time_elapsed     | 14518    |
|    total_timesteps  | 495395   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.33e-06 |
|    n_updates        | 121348   |
----------------------------------
Eval num_timesteps=495500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 495500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.36e-08 |
|    n_updates        | 121374   |
----------------------------------
Eval num_timesteps=496000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 496000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.84e-08 |
|    n_updates        | 121499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2460     |
|    fps              | 34       |
|    time_elapsed     | 14547    |
|    total_timesteps  | 496235   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.66e-06 |
|    n_updates        | 121558   |
----------------------------------
Eval num_timesteps=496500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 496500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.68e-07 |
|    n_updates        | 121624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2464     |
|    fps              | 34       |
|    time_elapsed     | 14561    |
|    total_timesteps  | 496972   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2e-07    |
|    n_updates        | 121742   |
----------------------------------
Eval num_timesteps=497000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 497000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-07  |
|    n_updates        | 121749   |
----------------------------------
Eval num_timesteps=497500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 497500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.3e-05  |
|    n_updates        | 121874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2468     |
|    fps              | 34       |
|    time_elapsed     | 14589    |
|    total_timesteps  | 497812   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.82e-07 |
|    n_updates        | 121952   |
----------------------------------
Eval num_timesteps=498000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 498000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.32e-07 |
|    n_updates        | 121999   |
----------------------------------
Eval num_timesteps=498500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 498500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000364 |
|    n_updates        | 122124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2472     |
|    fps              | 34       |
|    time_elapsed     | 14619    |
|    total_timesteps  | 498652   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000124 |
|    n_updates        | 122162   |
----------------------------------
Eval num_timesteps=499000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 499000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.07e-07 |
|    n_updates        | 122249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2476     |
|    fps              | 34       |
|    time_elapsed     | 14631    |
|    total_timesteps  | 499492   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000123 |
|    n_updates        | 122372   |
----------------------------------
Eval num_timesteps=499500, episode_reward=-0.14 +/- 0.29
Episode length: 197.66 +/- 48.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 499500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.12e-06 |
|    n_updates        | 122374   |
----------------------------------
Eval num_timesteps=500000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 500000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.3e-07  |
|    n_updates        | 122499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2480     |
|    fps              | 34       |
|    time_elapsed     | 14659    |
|    total_timesteps  | 500332   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.18e-07 |
|    n_updates        | 122582   |
----------------------------------
Eval num_timesteps=500500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 500500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.46e-07 |
|    n_updates        | 122624   |
----------------------------------
Eval num_timesteps=501000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 501000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21e-07 |
|    n_updates        | 122749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2484     |
|    fps              | 34       |
|    time_elapsed     | 14687    |
|    total_timesteps  | 501050   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.19e-07 |
|    n_updates        | 122762   |
----------------------------------
Eval num_timesteps=501500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 501500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.14e-08 |
|    n_updates        | 122874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2488     |
|    fps              | 34       |
|    time_elapsed     | 14705    |
|    total_timesteps  | 501890   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16e-05 |
|    n_updates        | 122972   |
----------------------------------
Eval num_timesteps=502000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 502000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.4e-07  |
|    n_updates        | 122999   |
----------------------------------
Eval num_timesteps=502500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 502500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-05 |
|    n_updates        | 123124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2492     |
|    fps              | 34       |
|    time_elapsed     | 14734    |
|    total_timesteps  | 502730   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2e-07    |
|    n_updates        | 123182   |
----------------------------------
Eval num_timesteps=503000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 503000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.32e-07 |
|    n_updates        | 123249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2496     |
|    fps              | 34       |
|    time_elapsed     | 14746    |
|    total_timesteps  | 503421   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-07 |
|    n_updates        | 123355   |
----------------------------------
Eval num_timesteps=503500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 503500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.23e-08 |
|    n_updates        | 123374   |
----------------------------------
Eval num_timesteps=504000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 504000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.96e-07 |
|    n_updates        | 123499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2500     |
|    fps              | 34       |
|    time_elapsed     | 14775    |
|    total_timesteps  | 504261   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.97e-05 |
|    n_updates        | 123565   |
----------------------------------
Eval num_timesteps=504500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 504500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.9e-05  |
|    n_updates        | 123624   |
----------------------------------
Eval num_timesteps=505000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 505000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.89e-05 |
|    n_updates        | 123749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2504     |
|    fps              | 34       |
|    time_elapsed     | 14804    |
|    total_timesteps  | 505101   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.31e-05 |
|    n_updates        | 123775   |
----------------------------------
Eval num_timesteps=505500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 505500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0131   |
|    n_updates        | 123874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2508     |
|    fps              | 34       |
|    time_elapsed     | 14821    |
|    total_timesteps  | 505941   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.94e-07 |
|    n_updates        | 123985   |
----------------------------------
Eval num_timesteps=506000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 506000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.89e-08 |
|    n_updates        | 123999   |
----------------------------------
Eval num_timesteps=506500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 506500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.05e-06 |
|    n_updates        | 124124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2512     |
|    fps              | 34       |
|    time_elapsed     | 14849    |
|    total_timesteps  | 506781   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.75e-07 |
|    n_updates        | 124195   |
----------------------------------
Eval num_timesteps=507000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 507000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.32e-07 |
|    n_updates        | 124249   |
----------------------------------
Eval num_timesteps=507500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 507500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.77e-05 |
|    n_updates        | 124374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2516     |
|    fps              | 34       |
|    time_elapsed     | 14877    |
|    total_timesteps  | 507621   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.31e-07 |
|    n_updates        | 124405   |
----------------------------------
Eval num_timesteps=508000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 508000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.9e-05  |
|    n_updates        | 124499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2520     |
|    fps              | 34       |
|    time_elapsed     | 14894    |
|    total_timesteps  | 508461   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-07  |
|    n_updates        | 124615   |
----------------------------------
Eval num_timesteps=508500, episode_reward=-0.16 +/- 0.23
Episode length: 202.36 +/- 37.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 508500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5e-07    |
|    n_updates        | 124624   |
----------------------------------
Eval num_timesteps=509000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 509000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.13e-06 |
|    n_updates        | 124749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2524     |
|    fps              | 34       |
|    time_elapsed     | 14922    |
|    total_timesteps  | 509301   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0001   |
|    n_updates        | 124825   |
----------------------------------
Eval num_timesteps=509500, episode_reward=-0.19 +/- 0.16
Episode length: 207.16 +/- 19.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 509500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-06 |
|    n_updates        | 124874   |
----------------------------------
Eval num_timesteps=510000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 510000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.9e-06  |
|    n_updates        | 124999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2528     |
|    fps              | 34       |
|    time_elapsed     | 14951    |
|    total_timesteps  | 510071   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000121 |
|    n_updates        | 125017   |
----------------------------------
Eval num_timesteps=510500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 510500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58e-07 |
|    n_updates        | 125124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2532     |
|    fps              | 34       |
|    time_elapsed     | 14964    |
|    total_timesteps  | 510911   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000117 |
|    n_updates        | 125227   |
----------------------------------
Eval num_timesteps=511000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 511000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.46e-07 |
|    n_updates        | 125249   |
----------------------------------
Eval num_timesteps=511500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 511500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18e-07 |
|    n_updates        | 125374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2536     |
|    fps              | 34       |
|    time_elapsed     | 14994    |
|    total_timesteps  | 511751   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.17e-07 |
|    n_updates        | 125437   |
----------------------------------
Eval num_timesteps=512000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 512000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.46e-07 |
|    n_updates        | 125499   |
----------------------------------
Eval num_timesteps=512500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 512500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01e-06 |
|    n_updates        | 125624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2540     |
|    fps              | 34       |
|    time_elapsed     | 15022    |
|    total_timesteps  | 512591   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.65e-07 |
|    n_updates        | 125647   |
----------------------------------
Eval num_timesteps=513000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 513000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000112 |
|    n_updates        | 125749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2544     |
|    fps              | 34       |
|    time_elapsed     | 15040    |
|    total_timesteps  | 513343   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.23e-06 |
|    n_updates        | 125835   |
----------------------------------
Eval num_timesteps=513500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 513500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.74e-06 |
|    n_updates        | 125874   |
----------------------------------
Eval num_timesteps=514000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 514000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.81e-07 |
|    n_updates        | 125999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2548     |
|    fps              | 34       |
|    time_elapsed     | 15067    |
|    total_timesteps  | 514118   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-06 |
|    n_updates        | 126029   |
----------------------------------
Eval num_timesteps=514500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 514500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.12e-07 |
|    n_updates        | 126124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2552     |
|    fps              | 34       |
|    time_elapsed     | 15081    |
|    total_timesteps  | 514958   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.47e-08 |
|    n_updates        | 126239   |
----------------------------------
Eval num_timesteps=515000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 515000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.01e-08 |
|    n_updates        | 126249   |
----------------------------------
Eval num_timesteps=515500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 515500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.31e-06 |
|    n_updates        | 126374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2556     |
|    fps              | 34       |
|    time_elapsed     | 15110    |
|    total_timesteps  | 515798   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.17e-07 |
|    n_updates        | 126449   |
----------------------------------
Eval num_timesteps=516000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 516000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.97e-08 |
|    n_updates        | 126499   |
----------------------------------
Eval num_timesteps=516500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 516500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.15e-08 |
|    n_updates        | 126624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2560     |
|    fps              | 34       |
|    time_elapsed     | 15139    |
|    total_timesteps  | 516525   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.29e-07 |
|    n_updates        | 126631   |
----------------------------------
Eval num_timesteps=517000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 517000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91e-07 |
|    n_updates        | 126749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2564     |
|    fps              | 34       |
|    time_elapsed     | 15153    |
|    total_timesteps  | 517178   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.58e-07 |
|    n_updates        | 126794   |
----------------------------------
Eval num_timesteps=517500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 517500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.39e-07 |
|    n_updates        | 126874   |
----------------------------------
Eval num_timesteps=518000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 518000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.22e-07 |
|    n_updates        | 126999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2568     |
|    fps              | 34       |
|    time_elapsed     | 15181    |
|    total_timesteps  | 518018   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.05e-05 |
|    n_updates        | 127004   |
----------------------------------
Eval num_timesteps=518500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 518500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.16e-08 |
|    n_updates        | 127124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2572     |
|    fps              | 34       |
|    time_elapsed     | 15198    |
|    total_timesteps  | 518747   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.56e-07 |
|    n_updates        | 127186   |
----------------------------------
Eval num_timesteps=519000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 519000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87e-07 |
|    n_updates        | 127249   |
----------------------------------
Eval num_timesteps=519500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 519500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.59e-07 |
|    n_updates        | 127374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2576     |
|    fps              | 34       |
|    time_elapsed     | 15227    |
|    total_timesteps  | 519587   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.18e-06 |
|    n_updates        | 127396   |
----------------------------------
Eval num_timesteps=520000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 520000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25e-07 |
|    n_updates        | 127499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2580     |
|    fps              | 34       |
|    time_elapsed     | 15240    |
|    total_timesteps  | 520427   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.77e-07 |
|    n_updates        | 127606   |
----------------------------------
Eval num_timesteps=520500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 520500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.23e-08 |
|    n_updates        | 127624   |
----------------------------------
Eval num_timesteps=521000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 521000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.57e-08 |
|    n_updates        | 127749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2584     |
|    fps              | 34       |
|    time_elapsed     | 15268    |
|    total_timesteps  | 521267   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.26e-07 |
|    n_updates        | 127816   |
----------------------------------
Eval num_timesteps=521500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 521500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.6e-07  |
|    n_updates        | 127874   |
----------------------------------
Eval num_timesteps=522000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 522000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.84e-07 |
|    n_updates        | 127999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2588     |
|    fps              | 34       |
|    time_elapsed     | 15297    |
|    total_timesteps  | 522107   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.09e-07 |
|    n_updates        | 128026   |
----------------------------------
Eval num_timesteps=522500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 522500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.87e-06 |
|    n_updates        | 128124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2592     |
|    fps              | 34       |
|    time_elapsed     | 15314    |
|    total_timesteps  | 522947   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.54e-08 |
|    n_updates        | 128236   |
----------------------------------
Eval num_timesteps=523000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 523000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.9e-05  |
|    n_updates        | 128249   |
----------------------------------
Eval num_timesteps=523500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 523500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.08e-07 |
|    n_updates        | 128374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2596     |
|    fps              | 34       |
|    time_elapsed     | 15342    |
|    total_timesteps  | 523670   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.34e-07 |
|    n_updates        | 128417   |
----------------------------------
Eval num_timesteps=524000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 524000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.78e-06 |
|    n_updates        | 128499   |
----------------------------------
Eval num_timesteps=524500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 524500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.69e-07 |
|    n_updates        | 128624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2600     |
|    fps              | 34       |
|    time_elapsed     | 15370    |
|    total_timesteps  | 524510   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35e-06 |
|    n_updates        | 128627   |
----------------------------------
Eval num_timesteps=525000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 525000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.12e-05 |
|    n_updates        | 128749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2604     |
|    fps              | 34       |
|    time_elapsed     | 15385    |
|    total_timesteps  | 525252   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.88e-07 |
|    n_updates        | 128812   |
----------------------------------
Eval num_timesteps=525500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 525500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.52e-07 |
|    n_updates        | 128874   |
----------------------------------
Eval num_timesteps=526000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 526000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.76e-07 |
|    n_updates        | 128999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2608     |
|    fps              | 34       |
|    time_elapsed     | 15415    |
|    total_timesteps  | 526092   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16e-07 |
|    n_updates        | 129022   |
----------------------------------
Eval num_timesteps=526500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 526500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.22e-07 |
|    n_updates        | 129124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2612     |
|    fps              | 34       |
|    time_elapsed     | 15428    |
|    total_timesteps  | 526932   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.89e-07 |
|    n_updates        | 129232   |
----------------------------------
Eval num_timesteps=527000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 527000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.63e-08 |
|    n_updates        | 129249   |
----------------------------------
Eval num_timesteps=527500, episode_reward=-0.16 +/- 0.24
Episode length: 201.86 +/- 39.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 527500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4e-05  |
|    n_updates        | 129374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2616     |
|    fps              | 34       |
|    time_elapsed     | 15456    |
|    total_timesteps  | 527772   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.63e-06 |
|    n_updates        | 129442   |
----------------------------------
Eval num_timesteps=528000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 528000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.35e-08 |
|    n_updates        | 129499   |
----------------------------------
Eval num_timesteps=528500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 528500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.07e-08 |
|    n_updates        | 129624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2620     |
|    fps              | 34       |
|    time_elapsed     | 15484    |
|    total_timesteps  | 528612   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.68e-05 |
|    n_updates        | 129652   |
----------------------------------
Eval num_timesteps=529000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 529000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.6e-08  |
|    n_updates        | 129749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2624     |
|    fps              | 34       |
|    time_elapsed     | 15502    |
|    total_timesteps  | 529452   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.35e-05 |
|    n_updates        | 129862   |
----------------------------------
Eval num_timesteps=529500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 529500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.79e-07 |
|    n_updates        | 129874   |
----------------------------------
Eval num_timesteps=530000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 530000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.47e-07 |
|    n_updates        | 129999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2628     |
|    fps              | 34       |
|    time_elapsed     | 15532    |
|    total_timesteps  | 530292   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.92e-05 |
|    n_updates        | 130072   |
----------------------------------
Eval num_timesteps=530500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 530500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.48e-07 |
|    n_updates        | 130124   |
----------------------------------
Eval num_timesteps=531000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 531000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.47e-07 |
|    n_updates        | 130249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2632     |
|    fps              | 34       |
|    time_elapsed     | 15560    |
|    total_timesteps  | 531132   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33e-05 |
|    n_updates        | 130282   |
----------------------------------
Eval num_timesteps=531500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 531500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.84e-07 |
|    n_updates        | 130374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2636     |
|    fps              | 34       |
|    time_elapsed     | 15573    |
|    total_timesteps  | 531768   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.99e-05 |
|    n_updates        | 130441   |
----------------------------------
Eval num_timesteps=532000, episode_reward=-0.11 +/- 0.33
Episode length: 193.60 +/- 55.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | -0.114   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 532000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.67e-05 |
|    n_updates        | 130499   |
----------------------------------
New best mean reward!
Eval num_timesteps=532500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 532500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0138   |
|    n_updates        | 130624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2640     |
|    fps              | 34       |
|    time_elapsed     | 15601    |
|    total_timesteps  | 532608   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.85e-07 |
|    n_updates        | 130651   |
----------------------------------
Eval num_timesteps=533000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 533000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.47e-08 |
|    n_updates        | 130749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2644     |
|    fps              | 34       |
|    time_elapsed     | 15616    |
|    total_timesteps  | 533448   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-07  |
|    n_updates        | 130861   |
----------------------------------
Eval num_timesteps=533500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 533500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.32e-05 |
|    n_updates        | 130874   |
----------------------------------
Eval num_timesteps=534000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 534000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.32e-05 |
|    n_updates        | 130999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2648     |
|    fps              | 34       |
|    time_elapsed     | 15644    |
|    total_timesteps  | 534288   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.98e-05 |
|    n_updates        | 131071   |
----------------------------------
Eval num_timesteps=534500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 534500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.2e-06  |
|    n_updates        | 131124   |
----------------------------------
Eval num_timesteps=535000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 535000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000118 |
|    n_updates        | 131249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2652     |
|    fps              | 34       |
|    time_elapsed     | 15673    |
|    total_timesteps  | 535128   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9e-06  |
|    n_updates        | 131281   |
----------------------------------
Eval num_timesteps=535500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 535500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.87e-06 |
|    n_updates        | 131374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2656     |
|    fps              | 34       |
|    time_elapsed     | 15692    |
|    total_timesteps  | 535968   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.17e-08 |
|    n_updates        | 131491   |
----------------------------------
Eval num_timesteps=536000, episode_reward=-0.14 +/- 0.29
Episode length: 197.68 +/- 48.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 536000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.75e-08 |
|    n_updates        | 131499   |
----------------------------------
Eval num_timesteps=536500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 536500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.6e-06  |
|    n_updates        | 131624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2660     |
|    fps              | 34       |
|    time_elapsed     | 15720    |
|    total_timesteps  | 536808   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.94e-06 |
|    n_updates        | 131701   |
----------------------------------
Eval num_timesteps=537000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 537000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.66e-07 |
|    n_updates        | 131749   |
----------------------------------
Eval num_timesteps=537500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 537500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.29e-05 |
|    n_updates        | 131874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2664     |
|    fps              | 34       |
|    time_elapsed     | 15749    |
|    total_timesteps  | 537648   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.01e-08 |
|    n_updates        | 131911   |
----------------------------------
Eval num_timesteps=538000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 538000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.86e-07 |
|    n_updates        | 131999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2668     |
|    fps              | 34       |
|    time_elapsed     | 15762    |
|    total_timesteps  | 538472   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.29e-08 |
|    n_updates        | 132117   |
----------------------------------
Eval num_timesteps=538500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 538500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.94e-08 |
|    n_updates        | 132124   |
----------------------------------
Eval num_timesteps=539000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 539000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.92e-08 |
|    n_updates        | 132249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2672     |
|    fps              | 34       |
|    time_elapsed     | 15791    |
|    total_timesteps  | 539312   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04e-06 |
|    n_updates        | 132327   |
----------------------------------
Eval num_timesteps=539500, episode_reward=-0.19 +/- 0.16
Episode length: 207.52 +/- 17.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | -0.188   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 539500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-07 |
|    n_updates        | 132374   |
----------------------------------
Eval num_timesteps=540000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 540000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.14e-07 |
|    n_updates        | 132499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2676     |
|    fps              | 34       |
|    time_elapsed     | 15820    |
|    total_timesteps  | 540152   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59e-07 |
|    n_updates        | 132537   |
----------------------------------
Eval num_timesteps=540500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 540500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.34e-05 |
|    n_updates        | 132624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2680     |
|    fps              | 34       |
|    time_elapsed     | 15836    |
|    total_timesteps  | 540992   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88e-07 |
|    n_updates        | 132747   |
----------------------------------
Eval num_timesteps=541000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 541000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.47e-05 |
|    n_updates        | 132749   |
----------------------------------
Eval num_timesteps=541500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 541500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.66e-07 |
|    n_updates        | 132874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2684     |
|    fps              | 34       |
|    time_elapsed     | 15865    |
|    total_timesteps  | 541832   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.63e-08 |
|    n_updates        | 132957   |
----------------------------------
Eval num_timesteps=542000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 542000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.26e-05 |
|    n_updates        | 132999   |
----------------------------------
Eval num_timesteps=542500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 542500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-08 |
|    n_updates        | 133124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2688     |
|    fps              | 34       |
|    time_elapsed     | 15892    |
|    total_timesteps  | 542672   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.98e-07 |
|    n_updates        | 133167   |
----------------------------------
Eval num_timesteps=543000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 543000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-06 |
|    n_updates        | 133249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2692     |
|    fps              | 34       |
|    time_elapsed     | 15909    |
|    total_timesteps  | 543356   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.78e-07 |
|    n_updates        | 133338   |
----------------------------------
Eval num_timesteps=543500, episode_reward=-0.19 +/- 0.16
Episode length: 206.96 +/- 21.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 543500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25e-07 |
|    n_updates        | 133374   |
----------------------------------
Eval num_timesteps=544000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 544000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.65e-05 |
|    n_updates        | 133499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2696     |
|    fps              | 34       |
|    time_elapsed     | 15937    |
|    total_timesteps  | 544196   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.9e-07  |
|    n_updates        | 133548   |
----------------------------------
Eval num_timesteps=544500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 544500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.37e-06 |
|    n_updates        | 133624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2700     |
|    fps              | 34       |
|    time_elapsed     | 15950    |
|    total_timesteps  | 544997   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.86e-07 |
|    n_updates        | 133749   |
----------------------------------
Eval num_timesteps=545000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 545000   |
----------------------------------
Eval num_timesteps=545500, episode_reward=-0.19 +/- 0.15
Episode length: 208.86 +/- 7.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | -0.189   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 545500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.16e-06 |
|    n_updates        | 133874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2704     |
|    fps              | 34       |
|    time_elapsed     | 15978    |
|    total_timesteps  | 545837   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44e-07 |
|    n_updates        | 133959   |
----------------------------------
Eval num_timesteps=546000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 546000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.36e-08 |
|    n_updates        | 133999   |
----------------------------------
Eval num_timesteps=546500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 546500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.99e-06 |
|    n_updates        | 134124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2708     |
|    fps              | 34       |
|    time_elapsed     | 16007    |
|    total_timesteps  | 546677   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33e-05 |
|    n_updates        | 134169   |
----------------------------------
Eval num_timesteps=547000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 547000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.98e-07 |
|    n_updates        | 134249   |
----------------------------------
Eval num_timesteps=547500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 547500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.57e-06 |
|    n_updates        | 134374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2712     |
|    fps              | 34       |
|    time_elapsed     | 16036    |
|    total_timesteps  | 547517   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.59e-07 |
|    n_updates        | 134379   |
----------------------------------
Eval num_timesteps=548000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 548000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.57e-05 |
|    n_updates        | 134499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2716     |
|    fps              | 34       |
|    time_elapsed     | 16054    |
|    total_timesteps  | 548177   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.74e-08 |
|    n_updates        | 134544   |
----------------------------------
Eval num_timesteps=548500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 548500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.88e-07 |
|    n_updates        | 134624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2720     |
|    fps              | 34       |
|    time_elapsed     | 16067    |
|    total_timesteps  | 548853   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.37e-07 |
|    n_updates        | 134713   |
----------------------------------
Eval num_timesteps=549000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 549000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82e-07 |
|    n_updates        | 134749   |
----------------------------------
Eval num_timesteps=549500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 549500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1e-07  |
|    n_updates        | 134874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2724     |
|    fps              | 34       |
|    time_elapsed     | 16090    |
|    total_timesteps  | 549655   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.6e-08  |
|    n_updates        | 134913   |
----------------------------------
Eval num_timesteps=550000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 550000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.1e-06  |
|    n_updates        | 134999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2728     |
|    fps              | 34       |
|    time_elapsed     | 16097    |
|    total_timesteps  | 550441   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.74e-07 |
|    n_updates        | 135110   |
----------------------------------
Eval num_timesteps=550500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 550500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.47e-07 |
|    n_updates        | 135124   |
----------------------------------
Eval num_timesteps=551000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 551000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96e-07 |
|    n_updates        | 135249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2732     |
|    fps              | 34       |
|    time_elapsed     | 16110    |
|    total_timesteps  | 551281   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-07 |
|    n_updates        | 135320   |
----------------------------------
Eval num_timesteps=551500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 551500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.83e-07 |
|    n_updates        | 135374   |
----------------------------------
Eval num_timesteps=552000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 552000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.14e-07 |
|    n_updates        | 135499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2736     |
|    fps              | 34       |
|    time_elapsed     | 16123    |
|    total_timesteps  | 552121   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25e-08 |
|    n_updates        | 135530   |
----------------------------------
Eval num_timesteps=552500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 552500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.54e-05 |
|    n_updates        | 135624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2740     |
|    fps              | 34       |
|    time_elapsed     | 16130    |
|    total_timesteps  | 552961   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-07 |
|    n_updates        | 135740   |
----------------------------------
Eval num_timesteps=553000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 553000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.47e-05 |
|    n_updates        | 135749   |
----------------------------------
Eval num_timesteps=553500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 553500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.61e-07 |
|    n_updates        | 135874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2744     |
|    fps              | 34       |
|    time_elapsed     | 16143    |
|    total_timesteps  | 553801   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.48e-08 |
|    n_updates        | 135950   |
----------------------------------
Eval num_timesteps=554000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 554000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55e-08 |
|    n_updates        | 135999   |
----------------------------------
Eval num_timesteps=554500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 554500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.47e-06 |
|    n_updates        | 136124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2748     |
|    fps              | 34       |
|    time_elapsed     | 16155    |
|    total_timesteps  | 554641   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.32e-05 |
|    n_updates        | 136160   |
----------------------------------
Eval num_timesteps=555000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 555000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.75e-08 |
|    n_updates        | 136249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2752     |
|    fps              | 34       |
|    time_elapsed     | 16162    |
|    total_timesteps  | 555481   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.78e-07 |
|    n_updates        | 136370   |
----------------------------------
Eval num_timesteps=555500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 555500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.11e-05 |
|    n_updates        | 136374   |
----------------------------------
Eval num_timesteps=556000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 556000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.71e-07 |
|    n_updates        | 136499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2756     |
|    fps              | 34       |
|    time_elapsed     | 16175    |
|    total_timesteps  | 556223   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.65e-05 |
|    n_updates        | 136555   |
----------------------------------
Eval num_timesteps=556500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 556500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.09e-07 |
|    n_updates        | 136624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2760     |
|    fps              | 34       |
|    time_elapsed     | 16182    |
|    total_timesteps  | 556938   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.59e-05 |
|    n_updates        | 136734   |
----------------------------------
Eval num_timesteps=557000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 557000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.19e-07 |
|    n_updates        | 136749   |
----------------------------------
Eval num_timesteps=557500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 557500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.55e-06 |
|    n_updates        | 136874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0992  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2764     |
|    fps              | 34       |
|    time_elapsed     | 16194    |
|    total_timesteps  | 557572   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41e-06 |
|    n_updates        | 136892   |
----------------------------------
Eval num_timesteps=558000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 558000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.96e-08 |
|    n_updates        | 136999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2768     |
|    fps              | 34       |
|    time_elapsed     | 16201    |
|    total_timesteps  | 558412   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.49e-08 |
|    n_updates        | 137102   |
----------------------------------
Eval num_timesteps=558500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 558500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.92e-08 |
|    n_updates        | 137124   |
----------------------------------
Eval num_timesteps=559000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 559000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.47e-07 |
|    n_updates        | 137249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2772     |
|    fps              | 34       |
|    time_elapsed     | 16214    |
|    total_timesteps  | 559252   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.18e-08 |
|    n_updates        | 137312   |
----------------------------------
Eval num_timesteps=559500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 559500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.57e-08 |
|    n_updates        | 137374   |
----------------------------------
Eval num_timesteps=560000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 560000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.25e-05 |
|    n_updates        | 137499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2776     |
|    fps              | 34       |
|    time_elapsed     | 16226    |
|    total_timesteps  | 560092   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.94e-07 |
|    n_updates        | 137522   |
----------------------------------
Eval num_timesteps=560500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 560500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.32e-07 |
|    n_updates        | 137624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0868  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2780     |
|    fps              | 34       |
|    time_elapsed     | 16233    |
|    total_timesteps  | 560674   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.92e-08 |
|    n_updates        | 137668   |
----------------------------------
Eval num_timesteps=561000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 561000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.61e-07 |
|    n_updates        | 137749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0756  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2784     |
|    fps              | 34       |
|    time_elapsed     | 16240    |
|    total_timesteps  | 561398   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.13e-07 |
|    n_updates        | 137849   |
----------------------------------
Eval num_timesteps=561500, episode_reward=-0.19 +/- 0.16
Episode length: 207.64 +/- 16.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | -0.188   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 561500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-07 |
|    n_updates        | 137874   |
----------------------------------
Eval num_timesteps=562000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 562000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.31e-08 |
|    n_updates        | 137999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.0756  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2788     |
|    fps              | 34       |
|    time_elapsed     | 16253    |
|    total_timesteps  | 562238   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.78e-07 |
|    n_updates        | 138059   |
----------------------------------
Eval num_timesteps=562500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 562500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.15e-07 |
|    n_updates        | 138124   |
----------------------------------
Eval num_timesteps=563000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 563000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.86e-07 |
|    n_updates        | 138249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0872  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2792     |
|    fps              | 34       |
|    time_elapsed     | 16266    |
|    total_timesteps  | 563078   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.83e-06 |
|    n_updates        | 138269   |
----------------------------------
Eval num_timesteps=563500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 563500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.77e-05 |
|    n_updates        | 138374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0771  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2796     |
|    fps              | 34       |
|    time_elapsed     | 16273    |
|    total_timesteps  | 563912   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.06e-06 |
|    n_updates        | 138477   |
----------------------------------
Eval num_timesteps=564000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 564000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.59e-06 |
|    n_updates        | 138499   |
----------------------------------
Eval num_timesteps=564500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 564500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.42e-08 |
|    n_updates        | 138624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0875  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2800     |
|    fps              | 34       |
|    time_elapsed     | 16286    |
|    total_timesteps  | 564752   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.21e-08 |
|    n_updates        | 138687   |
----------------------------------
Eval num_timesteps=565000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 565000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.04e-06 |
|    n_updates        | 138749   |
----------------------------------
Eval num_timesteps=565500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 565500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.22e-05 |
|    n_updates        | 138874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0875  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2804     |
|    fps              | 34       |
|    time_elapsed     | 16299    |
|    total_timesteps  | 565592   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.15e-08 |
|    n_updates        | 138897   |
----------------------------------
Eval num_timesteps=566000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 566000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.22e-05 |
|    n_updates        | 138999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0875  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2808     |
|    fps              | 34       |
|    time_elapsed     | 16306    |
|    total_timesteps  | 566432   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41e-07 |
|    n_updates        | 139107   |
----------------------------------
Eval num_timesteps=566500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 566500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.89e-05 |
|    n_updates        | 139124   |
----------------------------------
Eval num_timesteps=567000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 567000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.85e-06 |
|    n_updates        | 139249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0875  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2812     |
|    fps              | 34       |
|    time_elapsed     | 16319    |
|    total_timesteps  | 567272   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-06 |
|    n_updates        | 139317   |
----------------------------------
Eval num_timesteps=567500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 567500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.7e-08  |
|    n_updates        | 139374   |
----------------------------------
Eval num_timesteps=568000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 568000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67e-07 |
|    n_updates        | 139499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0993  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2816     |
|    fps              | 34       |
|    time_elapsed     | 16332    |
|    total_timesteps  | 568112   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.9e-07  |
|    n_updates        | 139527   |
----------------------------------
Eval num_timesteps=568500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 568500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.85e-08 |
|    n_updates        | 139624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2820     |
|    fps              | 34       |
|    time_elapsed     | 16338    |
|    total_timesteps  | 568952   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.13e-06 |
|    n_updates        | 139737   |
----------------------------------
Eval num_timesteps=569000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 569000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.81e-07 |
|    n_updates        | 139749   |
----------------------------------
Eval num_timesteps=569500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 569500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000101 |
|    n_updates        | 139874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2824     |
|    fps              | 34       |
|    time_elapsed     | 16351    |
|    total_timesteps  | 569786   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.15e-07 |
|    n_updates        | 139946   |
----------------------------------
Eval num_timesteps=570000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 570000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.8e-06  |
|    n_updates        | 139999   |
----------------------------------
Eval num_timesteps=570500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 570500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94e-07 |
|    n_updates        | 140124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2828     |
|    fps              | 34       |
|    time_elapsed     | 16364    |
|    total_timesteps  | 570561   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.66e-08 |
|    n_updates        | 140140   |
----------------------------------
Eval num_timesteps=571000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 571000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-07 |
|    n_updates        | 140249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2832     |
|    fps              | 34       |
|    time_elapsed     | 16371    |
|    total_timesteps  | 571401   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.71e-08 |
|    n_updates        | 140350   |
----------------------------------
Eval num_timesteps=571500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 571500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.12e-08 |
|    n_updates        | 140374   |
----------------------------------
Eval num_timesteps=572000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 572000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86e-07 |
|    n_updates        | 140499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2836     |
|    fps              | 34       |
|    time_elapsed     | 16384    |
|    total_timesteps  | 572241   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.14e-06 |
|    n_updates        | 140560   |
----------------------------------
Eval num_timesteps=572500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 572500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.63e-05 |
|    n_updates        | 140624   |
----------------------------------
Eval num_timesteps=573000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 573000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.29e-08 |
|    n_updates        | 140749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2840     |
|    fps              | 34       |
|    time_elapsed     | 16396    |
|    total_timesteps  | 573081   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-07 |
|    n_updates        | 140770   |
----------------------------------
Eval num_timesteps=573500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 573500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.26e-06 |
|    n_updates        | 140874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2844     |
|    fps              | 34       |
|    time_elapsed     | 16403    |
|    total_timesteps  | 573921   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.9e-08  |
|    n_updates        | 140980   |
----------------------------------
Eval num_timesteps=574000, episode_reward=-0.14 +/- 0.29
Episode length: 197.66 +/- 48.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 574000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.86e-08 |
|    n_updates        | 140999   |
----------------------------------
Eval num_timesteps=574500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 574500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.19e-05 |
|    n_updates        | 141124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2848     |
|    fps              | 35       |
|    time_elapsed     | 16416    |
|    total_timesteps  | 574761   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.46e-08 |
|    n_updates        | 141190   |
----------------------------------
Eval num_timesteps=575000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 575000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.42e-06 |
|    n_updates        | 141249   |
----------------------------------
Eval num_timesteps=575500, episode_reward=-0.16 +/- 0.23
Episode length: 202.50 +/- 36.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 575500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.68e-08 |
|    n_updates        | 141374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2852     |
|    fps              | 35       |
|    time_elapsed     | 16428    |
|    total_timesteps  | 575601   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.44e-07 |
|    n_updates        | 141400   |
----------------------------------
Eval num_timesteps=576000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 576000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.37e-07 |
|    n_updates        | 141499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2856     |
|    fps              | 35       |
|    time_elapsed     | 16435    |
|    total_timesteps  | 576441   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47e-06 |
|    n_updates        | 141610   |
----------------------------------
Eval num_timesteps=576500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 576500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.75e-06 |
|    n_updates        | 141624   |
----------------------------------
Eval num_timesteps=577000, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 577000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.13e-06 |
|    n_updates        | 141749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2860     |
|    fps              | 35       |
|    time_elapsed     | 16448    |
|    total_timesteps  | 577281   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.95e-07 |
|    n_updates        | 141820   |
----------------------------------
Eval num_timesteps=577500, episode_reward=-0.16 +/- 0.23
Episode length: 202.50 +/- 36.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 577500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23e-06 |
|    n_updates        | 141874   |
----------------------------------
Eval num_timesteps=578000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 578000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01e-06 |
|    n_updates        | 141999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2864     |
|    fps              | 35       |
|    time_elapsed     | 16460    |
|    total_timesteps  | 578121   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.72e-07 |
|    n_updates        | 142030   |
----------------------------------
Eval num_timesteps=578500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 578500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.76e-07 |
|    n_updates        | 142124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2868     |
|    fps              | 35       |
|    time_elapsed     | 16467    |
|    total_timesteps  | 578961   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.78e-07 |
|    n_updates        | 142240   |
----------------------------------
Eval num_timesteps=579000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 579000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.4e-07  |
|    n_updates        | 142249   |
----------------------------------
Eval num_timesteps=579500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 579500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.55e-07 |
|    n_updates        | 142374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2872     |
|    fps              | 35       |
|    time_elapsed     | 16480    |
|    total_timesteps  | 579765   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36e-07 |
|    n_updates        | 142441   |
----------------------------------
Eval num_timesteps=580000, episode_reward=-0.19 +/- 0.16
Episode length: 206.80 +/- 22.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 580000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.46e-07 |
|    n_updates        | 142499   |
----------------------------------
Eval num_timesteps=580500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 580500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.35e-07 |
|    n_updates        | 142624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2876     |
|    fps              | 35       |
|    time_elapsed     | 16493    |
|    total_timesteps  | 580605   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.82e-05 |
|    n_updates        | 142651   |
----------------------------------
Eval num_timesteps=581000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 581000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.17e-07 |
|    n_updates        | 142749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.158   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2880     |
|    fps              | 35       |
|    time_elapsed     | 16500    |
|    total_timesteps  | 581445   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36e-06 |
|    n_updates        | 142861   |
----------------------------------
Eval num_timesteps=581500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 581500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.42e-07 |
|    n_updates        | 142874   |
----------------------------------
Eval num_timesteps=582000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 582000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.18e-07 |
|    n_updates        | 142999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.169   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2884     |
|    fps              | 35       |
|    time_elapsed     | 16513    |
|    total_timesteps  | 582285   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.66e-07 |
|    n_updates        | 143071   |
----------------------------------
Eval num_timesteps=582500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 582500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.59e-08 |
|    n_updates        | 143124   |
----------------------------------
Eval num_timesteps=583000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 583000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-07 |
|    n_updates        | 143249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.169   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2888     |
|    fps              | 35       |
|    time_elapsed     | 16526    |
|    total_timesteps  | 583125   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.42e-08 |
|    n_updates        | 143281   |
----------------------------------
Eval num_timesteps=583500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 583500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.01e-05 |
|    n_updates        | 143374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2892     |
|    fps              | 35       |
|    time_elapsed     | 16533    |
|    total_timesteps  | 583784   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-07 |
|    n_updates        | 143445   |
----------------------------------
Eval num_timesteps=584000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 584000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.13e-05 |
|    n_updates        | 143499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2896     |
|    fps              | 35       |
|    time_elapsed     | 16539    |
|    total_timesteps  | 584417   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.55e-08 |
|    n_updates        | 143604   |
----------------------------------
Eval num_timesteps=584500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 584500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.41e-07 |
|    n_updates        | 143624   |
----------------------------------
Eval num_timesteps=585000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 585000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1e-06  |
|    n_updates        | 143749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2900     |
|    fps              | 35       |
|    time_elapsed     | 16552    |
|    total_timesteps  | 585257   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.18e-07 |
|    n_updates        | 143814   |
----------------------------------
Eval num_timesteps=585500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 585500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.91e-07 |
|    n_updates        | 143874   |
----------------------------------
Eval num_timesteps=586000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 586000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.27e-05 |
|    n_updates        | 143999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2904     |
|    fps              | 35       |
|    time_elapsed     | 16565    |
|    total_timesteps  | 586097   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.34e-07 |
|    n_updates        | 144024   |
----------------------------------
Eval num_timesteps=586500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 586500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.33e-08 |
|    n_updates        | 144124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2908     |
|    fps              | 35       |
|    time_elapsed     | 16572    |
|    total_timesteps  | 586937   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.66e-05 |
|    n_updates        | 144234   |
----------------------------------
Eval num_timesteps=587000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 587000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.39e-08 |
|    n_updates        | 144249   |
----------------------------------
Eval num_timesteps=587500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 587500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95e-07 |
|    n_updates        | 144374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2912     |
|    fps              | 35       |
|    time_elapsed     | 16585    |
|    total_timesteps  | 587777   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.6e-05  |
|    n_updates        | 144444   |
----------------------------------
Eval num_timesteps=588000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 588000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.97e-09 |
|    n_updates        | 144499   |
----------------------------------
Eval num_timesteps=588500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 588500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-07 |
|    n_updates        | 144624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2916     |
|    fps              | 35       |
|    time_elapsed     | 16598    |
|    total_timesteps  | 588617   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.03e-07 |
|    n_updates        | 144654   |
----------------------------------
Eval num_timesteps=589000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 589000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1e-07  |
|    n_updates        | 144749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2920     |
|    fps              | 35       |
|    time_elapsed     | 16605    |
|    total_timesteps  | 589433   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.34e-07 |
|    n_updates        | 144858   |
----------------------------------
Eval num_timesteps=589500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 589500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28e-05 |
|    n_updates        | 144874   |
----------------------------------
Eval num_timesteps=590000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 590000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25e-06 |
|    n_updates        | 144999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2924     |
|    fps              | 35       |
|    time_elapsed     | 16618    |
|    total_timesteps  | 590273   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.58e-08 |
|    n_updates        | 145068   |
----------------------------------
Eval num_timesteps=590500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 590500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.08e-05 |
|    n_updates        | 145124   |
----------------------------------
Eval num_timesteps=591000, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 591000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.61e-05 |
|    n_updates        | 145249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2928     |
|    fps              | 35       |
|    time_elapsed     | 16630    |
|    total_timesteps  | 591113   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.12e-07 |
|    n_updates        | 145278   |
----------------------------------
Eval num_timesteps=591500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 591500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.12e-06 |
|    n_updates        | 145374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2932     |
|    fps              | 35       |
|    time_elapsed     | 16637    |
|    total_timesteps  | 591953   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.75e-08 |
|    n_updates        | 145488   |
----------------------------------
Eval num_timesteps=592000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 592000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.23e-05 |
|    n_updates        | 145499   |
----------------------------------
Eval num_timesteps=592500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 592500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.95e-07 |
|    n_updates        | 145624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2936     |
|    fps              | 35       |
|    time_elapsed     | 16650    |
|    total_timesteps  | 592609   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-07  |
|    n_updates        | 145652   |
----------------------------------
Eval num_timesteps=593000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 593000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.32e-07 |
|    n_updates        | 145749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2940     |
|    fps              | 35       |
|    time_elapsed     | 16656    |
|    total_timesteps  | 593449   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.19e-06 |
|    n_updates        | 145862   |
----------------------------------
Eval num_timesteps=593500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 593500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.73e-06 |
|    n_updates        | 145874   |
----------------------------------
Eval num_timesteps=594000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 594000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47e-07 |
|    n_updates        | 145999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2944     |
|    fps              | 35       |
|    time_elapsed     | 16669    |
|    total_timesteps  | 594163   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92e-08 |
|    n_updates        | 146040   |
----------------------------------
Eval num_timesteps=594500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 594500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.13e-07 |
|    n_updates        | 146124   |
----------------------------------
Eval num_timesteps=595000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 595000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.15e-07 |
|    n_updates        | 146249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2948     |
|    fps              | 35       |
|    time_elapsed     | 16682    |
|    total_timesteps  | 595003   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.77e-07 |
|    n_updates        | 146250   |
----------------------------------
Eval num_timesteps=595500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 595500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.57e-07 |
|    n_updates        | 146374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2952     |
|    fps              | 35       |
|    time_elapsed     | 16689    |
|    total_timesteps  | 595843   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.75e-07 |
|    n_updates        | 146460   |
----------------------------------
Eval num_timesteps=596000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 596000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.17e-08 |
|    n_updates        | 146499   |
----------------------------------
Eval num_timesteps=596500, episode_reward=-0.19 +/- 0.16
Episode length: 206.44 +/- 24.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 596500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.66e-06 |
|    n_updates        | 146624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2956     |
|    fps              | 35       |
|    time_elapsed     | 16702    |
|    total_timesteps  | 596683   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.78e-05 |
|    n_updates        | 146670   |
----------------------------------
Eval num_timesteps=597000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 597000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.41e-06 |
|    n_updates        | 146749   |
----------------------------------
Eval num_timesteps=597500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 597500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.64e-07 |
|    n_updates        | 146874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2960     |
|    fps              | 35       |
|    time_elapsed     | 16715    |
|    total_timesteps  | 597523   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-07 |
|    n_updates        | 146880   |
----------------------------------
Eval num_timesteps=598000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 598000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.48e-08 |
|    n_updates        | 146999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2964     |
|    fps              | 35       |
|    time_elapsed     | 16722    |
|    total_timesteps  | 598363   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.27e-07 |
|    n_updates        | 147090   |
----------------------------------
Eval num_timesteps=598500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 598500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.38e-07 |
|    n_updates        | 147124   |
----------------------------------
Eval num_timesteps=599000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 599000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.1e-07  |
|    n_updates        | 147249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2968     |
|    fps              | 35       |
|    time_elapsed     | 16735    |
|    total_timesteps  | 599203   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.54e-07 |
|    n_updates        | 147300   |
----------------------------------
Eval num_timesteps=599500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 599500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.33e-07 |
|    n_updates        | 147374   |
----------------------------------
Eval num_timesteps=600000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 600000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.38e-08 |
|    n_updates        | 147499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2972     |
|    fps              | 35       |
|    time_elapsed     | 16748    |
|    total_timesteps  | 600043   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.76e-07 |
|    n_updates        | 147510   |
----------------------------------
Eval num_timesteps=600500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 600500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58e-07 |
|    n_updates        | 147624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2976     |
|    fps              | 35       |
|    time_elapsed     | 16755    |
|    total_timesteps  | 600883   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.83e-07 |
|    n_updates        | 147720   |
----------------------------------
Eval num_timesteps=601000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 601000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.53e-07 |
|    n_updates        | 147749   |
----------------------------------
Eval num_timesteps=601500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 601500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.39e-07 |
|    n_updates        | 147874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2980     |
|    fps              | 35       |
|    time_elapsed     | 16767    |
|    total_timesteps  | 601723   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.44e-08 |
|    n_updates        | 147930   |
----------------------------------
Eval num_timesteps=602000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 602000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.49e-07 |
|    n_updates        | 147999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2984     |
|    fps              | 35       |
|    time_elapsed     | 16774    |
|    total_timesteps  | 602445   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.79e-05 |
|    n_updates        | 148111   |
----------------------------------
Eval num_timesteps=602500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 602500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.68e-05 |
|    n_updates        | 148124   |
----------------------------------
Eval num_timesteps=603000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 603000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.06e-07 |
|    n_updates        | 148249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2988     |
|    fps              | 35       |
|    time_elapsed     | 16787    |
|    total_timesteps  | 603285   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-07 |
|    n_updates        | 148321   |
----------------------------------
Eval num_timesteps=603500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 603500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.99e-06 |
|    n_updates        | 148374   |
----------------------------------
Eval num_timesteps=604000, episode_reward=-0.11 +/- 0.33
Episode length: 193.58 +/- 55.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | -0.114   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 604000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.41e-07 |
|    n_updates        | 148499   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2992     |
|    fps              | 35       |
|    time_elapsed     | 16799    |
|    total_timesteps  | 604125   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4e-07  |
|    n_updates        | 148531   |
----------------------------------
Eval num_timesteps=604500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 604500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.14e-07 |
|    n_updates        | 148624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 2996     |
|    fps              | 35       |
|    time_elapsed     | 16806    |
|    total_timesteps  | 604965   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.23e-05 |
|    n_updates        | 148741   |
----------------------------------
Eval num_timesteps=605000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 605000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.6e-08  |
|    n_updates        | 148749   |
----------------------------------
Eval num_timesteps=605500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 605500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.31e-07 |
|    n_updates        | 148874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3000     |
|    fps              | 36       |
|    time_elapsed     | 16819    |
|    total_timesteps  | 605805   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.39e-05 |
|    n_updates        | 148951   |
----------------------------------
Eval num_timesteps=606000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 606000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44e-06 |
|    n_updates        | 148999   |
----------------------------------
Eval num_timesteps=606500, episode_reward=-0.16 +/- 0.23
Episode length: 202.30 +/- 37.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 606500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58e-06 |
|    n_updates        | 149124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3004     |
|    fps              | 36       |
|    time_elapsed     | 16832    |
|    total_timesteps  | 606645   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.54e-07 |
|    n_updates        | 149161   |
----------------------------------
Eval num_timesteps=607000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 607000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91e-07 |
|    n_updates        | 149249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3008     |
|    fps              | 36       |
|    time_elapsed     | 16839    |
|    total_timesteps  | 607485   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-07 |
|    n_updates        | 149371   |
----------------------------------
Eval num_timesteps=607500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 607500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28e-07 |
|    n_updates        | 149374   |
----------------------------------
Eval num_timesteps=608000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 608000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25e-07 |
|    n_updates        | 149499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3012     |
|    fps              | 36       |
|    time_elapsed     | 16852    |
|    total_timesteps  | 608325   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.7e-07  |
|    n_updates        | 149581   |
----------------------------------
Eval num_timesteps=608500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 608500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.63e-07 |
|    n_updates        | 149624   |
----------------------------------
Eval num_timesteps=609000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 609000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.99e-07 |
|    n_updates        | 149749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3016     |
|    fps              | 36       |
|    time_elapsed     | 16865    |
|    total_timesteps  | 609165   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-07 |
|    n_updates        | 149791   |
----------------------------------
Eval num_timesteps=609500, episode_reward=-0.19 +/- 0.17
Episode length: 206.32 +/- 25.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 609500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.24e-05 |
|    n_updates        | 149874   |
----------------------------------
Eval num_timesteps=610000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 610000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.42e-07 |
|    n_updates        | 149999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3020     |
|    fps              | 36       |
|    time_elapsed     | 16878    |
|    total_timesteps  | 610005   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.83e-07 |
|    n_updates        | 150001   |
----------------------------------
Eval num_timesteps=610500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 610500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.17e-07 |
|    n_updates        | 150124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3024     |
|    fps              | 36       |
|    time_elapsed     | 16885    |
|    total_timesteps  | 610845   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.04e-08 |
|    n_updates        | 150211   |
----------------------------------
Eval num_timesteps=611000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 611000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94e-07 |
|    n_updates        | 150249   |
----------------------------------
Eval num_timesteps=611500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 611500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.42e-07 |
|    n_updates        | 150374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3028     |
|    fps              | 36       |
|    time_elapsed     | 16898    |
|    total_timesteps  | 611685   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.09e-07 |
|    n_updates        | 150421   |
----------------------------------
Eval num_timesteps=612000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 612000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.78e-06 |
|    n_updates        | 150499   |
----------------------------------
Eval num_timesteps=612500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 612500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.95e-06 |
|    n_updates        | 150624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3032     |
|    fps              | 36       |
|    time_elapsed     | 16911    |
|    total_timesteps  | 612525   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.78e-06 |
|    n_updates        | 150631   |
----------------------------------
Eval num_timesteps=613000, episode_reward=-0.14 +/- 0.29
Episode length: 197.64 +/- 48.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 613000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.12e-07 |
|    n_updates        | 150749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3036     |
|    fps              | 36       |
|    time_elapsed     | 16917    |
|    total_timesteps  | 613365   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.66e-07 |
|    n_updates        | 150841   |
----------------------------------
Eval num_timesteps=613500, episode_reward=-0.16 +/- 0.23
Episode length: 202.14 +/- 38.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 613500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.22e-08 |
|    n_updates        | 150874   |
----------------------------------
Eval num_timesteps=614000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 614000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.65e-07 |
|    n_updates        | 150999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3040     |
|    fps              | 36       |
|    time_elapsed     | 16930    |
|    total_timesteps  | 614205   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28e-07 |
|    n_updates        | 151051   |
----------------------------------
Eval num_timesteps=614500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 614500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.014    |
|    n_updates        | 151124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3044     |
|    fps              | 36       |
|    time_elapsed     | 16937    |
|    total_timesteps  | 614904   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.02e-07 |
|    n_updates        | 151225   |
----------------------------------
Eval num_timesteps=615000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 615000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.41e-08 |
|    n_updates        | 151249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3048     |
|    fps              | 36       |
|    time_elapsed     | 16943    |
|    total_timesteps  | 615497   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.56e-05 |
|    n_updates        | 151374   |
----------------------------------
Eval num_timesteps=615500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 615500   |
----------------------------------
Eval num_timesteps=616000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 616000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.32e-08 |
|    n_updates        | 151499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3052     |
|    fps              | 36       |
|    time_elapsed     | 16956    |
|    total_timesteps  | 616337   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91e-07 |
|    n_updates        | 151584   |
----------------------------------
Eval num_timesteps=616500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 616500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.25e-09 |
|    n_updates        | 151624   |
----------------------------------
Eval num_timesteps=617000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 617000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-07 |
|    n_updates        | 151749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3056     |
|    fps              | 36       |
|    time_elapsed     | 16969    |
|    total_timesteps  | 617177   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.24e-08 |
|    n_updates        | 151794   |
----------------------------------
Eval num_timesteps=617500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 617500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.5e-07  |
|    n_updates        | 151874   |
----------------------------------
Eval num_timesteps=618000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 618000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.08e-08 |
|    n_updates        | 151999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3060     |
|    fps              | 36       |
|    time_elapsed     | 16981    |
|    total_timesteps  | 618017   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.26e-07 |
|    n_updates        | 152004   |
----------------------------------
Eval num_timesteps=618500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 618500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.8e-06  |
|    n_updates        | 152124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3064     |
|    fps              | 36       |
|    time_elapsed     | 16988    |
|    total_timesteps  | 618857   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-07  |
|    n_updates        | 152214   |
----------------------------------
Eval num_timesteps=619000, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 619000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.63e-07 |
|    n_updates        | 152249   |
----------------------------------
Eval num_timesteps=619500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 619500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.06e-08 |
|    n_updates        | 152374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3068     |
|    fps              | 36       |
|    time_elapsed     | 17001    |
|    total_timesteps  | 619642   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.6e-07  |
|    n_updates        | 152410   |
----------------------------------
Eval num_timesteps=620000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 620000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.17e-07 |
|    n_updates        | 152499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3072     |
|    fps              | 36       |
|    time_elapsed     | 17007    |
|    total_timesteps  | 620482   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.42e-08 |
|    n_updates        | 152620   |
----------------------------------
Eval num_timesteps=620500, episode_reward=-0.14 +/- 0.28
Episode length: 198.28 +/- 46.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 620500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1e-08  |
|    n_updates        | 152624   |
----------------------------------
Eval num_timesteps=621000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 621000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.97e-07 |
|    n_updates        | 152749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3076     |
|    fps              | 36       |
|    time_elapsed     | 17020    |
|    total_timesteps  | 621322   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21e-07 |
|    n_updates        | 152830   |
----------------------------------
Eval num_timesteps=621500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 621500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.64e-05 |
|    n_updates        | 152874   |
----------------------------------
Eval num_timesteps=622000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 622000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.69e-05 |
|    n_updates        | 152999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3080     |
|    fps              | 36       |
|    time_elapsed     | 17033    |
|    total_timesteps  | 622162   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.4e-07  |
|    n_updates        | 153040   |
----------------------------------
Eval num_timesteps=622500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 622500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.83e-08 |
|    n_updates        | 153124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3084     |
|    fps              | 36       |
|    time_elapsed     | 17040    |
|    total_timesteps  | 622989   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.56e-07 |
|    n_updates        | 153247   |
----------------------------------
Eval num_timesteps=623000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 623000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.08e-07 |
|    n_updates        | 153249   |
----------------------------------
Eval num_timesteps=623500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 623500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.4e-08  |
|    n_updates        | 153374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3088     |
|    fps              | 36       |
|    time_elapsed     | 17052    |
|    total_timesteps  | 623829   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.38e-08 |
|    n_updates        | 153457   |
----------------------------------
Eval num_timesteps=624000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 624000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.89e-07 |
|    n_updates        | 153499   |
----------------------------------
Eval num_timesteps=624500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 624500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.32e-07 |
|    n_updates        | 153624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3092     |
|    fps              | 36       |
|    time_elapsed     | 17065    |
|    total_timesteps  | 624669   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.57e-06 |
|    n_updates        | 153667   |
----------------------------------
Eval num_timesteps=625000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 625000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6e-08    |
|    n_updates        | 153749   |
----------------------------------
Eval num_timesteps=625500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 625500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.51e-08 |
|    n_updates        | 153874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3096     |
|    fps              | 36       |
|    time_elapsed     | 17078    |
|    total_timesteps  | 625509   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55e-07 |
|    n_updates        | 153877   |
----------------------------------
Eval num_timesteps=626000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 626000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.96e-08 |
|    n_updates        | 153999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3100     |
|    fps              | 36       |
|    time_elapsed     | 17085    |
|    total_timesteps  | 626144   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13e-06 |
|    n_updates        | 154035   |
----------------------------------
Eval num_timesteps=626500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 626500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.53e-07 |
|    n_updates        | 154124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3104     |
|    fps              | 36       |
|    time_elapsed     | 17091    |
|    total_timesteps  | 626816   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.36e-06 |
|    n_updates        | 154203   |
----------------------------------
Eval num_timesteps=627000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 627000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.23e-08 |
|    n_updates        | 154249   |
----------------------------------
Eval num_timesteps=627500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 627500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.43e-07 |
|    n_updates        | 154374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3108     |
|    fps              | 36       |
|    time_elapsed     | 17104    |
|    total_timesteps  | 627656   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21e-08 |
|    n_updates        | 154413   |
----------------------------------
Eval num_timesteps=628000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 628000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.56e-07 |
|    n_updates        | 154499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3112     |
|    fps              | 36       |
|    time_elapsed     | 17111    |
|    total_timesteps  | 628496   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.17e-07 |
|    n_updates        | 154623   |
----------------------------------
Eval num_timesteps=628500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 628500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88e-07 |
|    n_updates        | 154624   |
----------------------------------
Eval num_timesteps=629000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 629000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55e-07 |
|    n_updates        | 154749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3116     |
|    fps              | 36       |
|    time_elapsed     | 17123    |
|    total_timesteps  | 629336   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.84e-07 |
|    n_updates        | 154833   |
----------------------------------
Eval num_timesteps=629500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 629500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.28e-07 |
|    n_updates        | 154874   |
----------------------------------
Eval num_timesteps=630000, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 630000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.53e-07 |
|    n_updates        | 154999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3120     |
|    fps              | 36       |
|    time_elapsed     | 17136    |
|    total_timesteps  | 630176   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.74e-06 |
|    n_updates        | 155043   |
----------------------------------
Eval num_timesteps=630500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 630500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.98e-07 |
|    n_updates        | 155124   |
----------------------------------
Eval num_timesteps=631000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 631000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.76e-06 |
|    n_updates        | 155249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3124     |
|    fps              | 36       |
|    time_elapsed     | 17149    |
|    total_timesteps  | 631016   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33e-05 |
|    n_updates        | 155253   |
----------------------------------
Eval num_timesteps=631500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 631500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.47e-05 |
|    n_updates        | 155374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3128     |
|    fps              | 36       |
|    time_elapsed     | 17156    |
|    total_timesteps  | 631856   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-06  |
|    n_updates        | 155463   |
----------------------------------
Eval num_timesteps=632000, episode_reward=-0.19 +/- 0.16
Episode length: 206.72 +/- 22.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 632000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.99e-05 |
|    n_updates        | 155499   |
----------------------------------
Eval num_timesteps=632500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 632500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3e-06  |
|    n_updates        | 155624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3132     |
|    fps              | 36       |
|    time_elapsed     | 17169    |
|    total_timesteps  | 632696   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.39e-07 |
|    n_updates        | 155673   |
----------------------------------
Eval num_timesteps=633000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 633000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.62e-08 |
|    n_updates        | 155749   |
----------------------------------
Eval num_timesteps=633500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 633500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.13e-06 |
|    n_updates        | 155874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3136     |
|    fps              | 36       |
|    time_elapsed     | 17181    |
|    total_timesteps  | 633536   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.42e-06 |
|    n_updates        | 155883   |
----------------------------------
Eval num_timesteps=634000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 634000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.08e-07 |
|    n_updates        | 155999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3140     |
|    fps              | 36       |
|    time_elapsed     | 17188    |
|    total_timesteps  | 634376   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-07 |
|    n_updates        | 156093   |
----------------------------------
Eval num_timesteps=634500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 634500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.28e-05 |
|    n_updates        | 156124   |
----------------------------------
Eval num_timesteps=635000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 635000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.18e-08 |
|    n_updates        | 156249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3144     |
|    fps              | 36       |
|    time_elapsed     | 17201    |
|    total_timesteps  | 635216   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.05e-05 |
|    n_updates        | 156303   |
----------------------------------
Eval num_timesteps=635500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 635500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47e-07 |
|    n_updates        | 156374   |
----------------------------------
Eval num_timesteps=636000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 636000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.06e-07 |
|    n_updates        | 156499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3148     |
|    fps              | 36       |
|    time_elapsed     | 17214    |
|    total_timesteps  | 636056   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.22e-07 |
|    n_updates        | 156513   |
----------------------------------
Eval num_timesteps=636500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 636500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.82e-07 |
|    n_updates        | 156624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3152     |
|    fps              | 36       |
|    time_elapsed     | 17221    |
|    total_timesteps  | 636896   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94e-07 |
|    n_updates        | 156723   |
----------------------------------
Eval num_timesteps=637000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 637000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.94e-05 |
|    n_updates        | 156749   |
----------------------------------
Eval num_timesteps=637500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 637500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.78e-06 |
|    n_updates        | 156874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3156     |
|    fps              | 36       |
|    time_elapsed     | 17234    |
|    total_timesteps  | 637630   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01e-08 |
|    n_updates        | 156907   |
----------------------------------
Eval num_timesteps=638000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 638000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.3e-07  |
|    n_updates        | 156999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3160     |
|    fps              | 37       |
|    time_elapsed     | 17240    |
|    total_timesteps  | 638470   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.78e-06 |
|    n_updates        | 157117   |
----------------------------------
Eval num_timesteps=638500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 638500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.7e-06  |
|    n_updates        | 157124   |
----------------------------------
Eval num_timesteps=639000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 639000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.72e-09 |
|    n_updates        | 157249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3164     |
|    fps              | 37       |
|    time_elapsed     | 17253    |
|    total_timesteps  | 639310   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.33e-05 |
|    n_updates        | 157327   |
----------------------------------
Eval num_timesteps=639500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 639500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.85e-08 |
|    n_updates        | 157374   |
----------------------------------
Eval num_timesteps=640000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 640000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.72e-08 |
|    n_updates        | 157499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3168     |
|    fps              | 37       |
|    time_elapsed     | 17266    |
|    total_timesteps  | 640150   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-06 |
|    n_updates        | 157537   |
----------------------------------
Eval num_timesteps=640500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 640500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.58e-08 |
|    n_updates        | 157624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3172     |
|    fps              | 37       |
|    time_elapsed     | 17273    |
|    total_timesteps  | 640990   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0143   |
|    n_updates        | 157747   |
----------------------------------
Eval num_timesteps=641000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 641000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-06 |
|    n_updates        | 157749   |
----------------------------------
Eval num_timesteps=641500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 641500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.69e-07 |
|    n_updates        | 157874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3176     |
|    fps              | 37       |
|    time_elapsed     | 17286    |
|    total_timesteps  | 641830   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.41e-06 |
|    n_updates        | 157957   |
----------------------------------
Eval num_timesteps=642000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 642000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-07 |
|    n_updates        | 157999   |
----------------------------------
Eval num_timesteps=642500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 642500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.53e-05 |
|    n_updates        | 158124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3180     |
|    fps              | 37       |
|    time_elapsed     | 17299    |
|    total_timesteps  | 642670   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.7e-08  |
|    n_updates        | 158167   |
----------------------------------
Eval num_timesteps=643000, episode_reward=-0.19 +/- 0.16
Episode length: 206.52 +/- 24.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 643000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.06e-05 |
|    n_updates        | 158249   |
----------------------------------
Eval num_timesteps=643500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 643500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.97e-07 |
|    n_updates        | 158374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3184     |
|    fps              | 37       |
|    time_elapsed     | 17312    |
|    total_timesteps  | 643510   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.64e-07 |
|    n_updates        | 158377   |
----------------------------------
Eval num_timesteps=644000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 644000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.49e-06 |
|    n_updates        | 158499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3188     |
|    fps              | 37       |
|    time_elapsed     | 17319    |
|    total_timesteps  | 644350   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.14e-07 |
|    n_updates        | 158587   |
----------------------------------
Eval num_timesteps=644500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 644500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.66e-05 |
|    n_updates        | 158624   |
----------------------------------
Eval num_timesteps=645000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 645000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.32e-06 |
|    n_updates        | 158749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3192     |
|    fps              | 37       |
|    time_elapsed     | 17332    |
|    total_timesteps  | 645190   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.62e-08 |
|    n_updates        | 158797   |
----------------------------------
Eval num_timesteps=645500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 645500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.77e-07 |
|    n_updates        | 158874   |
----------------------------------
Eval num_timesteps=646000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 646000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.95e-07 |
|    n_updates        | 158999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3196     |
|    fps              | 37       |
|    time_elapsed     | 17345    |
|    total_timesteps  | 646030   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.39e-08 |
|    n_updates        | 159007   |
----------------------------------
Eval num_timesteps=646500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 646500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.98e-07 |
|    n_updates        | 159124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3200     |
|    fps              | 37       |
|    time_elapsed     | 17351    |
|    total_timesteps  | 646777   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73e-07 |
|    n_updates        | 159194   |
----------------------------------
Eval num_timesteps=647000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 647000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.11e-09 |
|    n_updates        | 159249   |
----------------------------------
Eval num_timesteps=647500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 647500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.4e-08  |
|    n_updates        | 159374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3204     |
|    fps              | 37       |
|    time_elapsed     | 17364    |
|    total_timesteps  | 647617   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33e-07 |
|    n_updates        | 159404   |
----------------------------------
Eval num_timesteps=648000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 648000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.98e-06 |
|    n_updates        | 159499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3208     |
|    fps              | 37       |
|    time_elapsed     | 17371    |
|    total_timesteps  | 648457   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.66e-05 |
|    n_updates        | 159614   |
----------------------------------
Eval num_timesteps=648500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 648500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.79e-07 |
|    n_updates        | 159624   |
----------------------------------
Eval num_timesteps=649000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 649000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.9e-06  |
|    n_updates        | 159749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3212     |
|    fps              | 37       |
|    time_elapsed     | 17384    |
|    total_timesteps  | 649297   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.36e-05 |
|    n_updates        | 159824   |
----------------------------------
Eval num_timesteps=649500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 649500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5e-07  |
|    n_updates        | 159874   |
----------------------------------
Eval num_timesteps=650000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 650000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.88e-05 |
|    n_updates        | 159999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3216     |
|    fps              | 37       |
|    time_elapsed     | 17397    |
|    total_timesteps  | 650137   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.39e-07 |
|    n_updates        | 160034   |
----------------------------------
Eval num_timesteps=650500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 650500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41e-08 |
|    n_updates        | 160124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3220     |
|    fps              | 37       |
|    time_elapsed     | 17403    |
|    total_timesteps  | 650977   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.79e-06 |
|    n_updates        | 160244   |
----------------------------------
Eval num_timesteps=651000, episode_reward=-0.19 +/- 0.15
Episode length: 208.30 +/- 11.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | -0.188   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 651000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.19e-06 |
|    n_updates        | 160249   |
----------------------------------
Eval num_timesteps=651500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 651500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.6e-07  |
|    n_updates        | 160374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3224     |
|    fps              | 37       |
|    time_elapsed     | 17416    |
|    total_timesteps  | 651583   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.07e-07 |
|    n_updates        | 160395   |
----------------------------------
Eval num_timesteps=652000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 652000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.9e-09  |
|    n_updates        | 160499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3228     |
|    fps              | 37       |
|    time_elapsed     | 17423    |
|    total_timesteps  | 652423   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56e-07 |
|    n_updates        | 160605   |
----------------------------------
Eval num_timesteps=652500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 652500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56e-07 |
|    n_updates        | 160624   |
----------------------------------
Eval num_timesteps=653000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 653000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.71e-07 |
|    n_updates        | 160749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3232     |
|    fps              | 37       |
|    time_elapsed     | 17436    |
|    total_timesteps  | 653078   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.27e-08 |
|    n_updates        | 160769   |
----------------------------------
Eval num_timesteps=653500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 653500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.68e-07 |
|    n_updates        | 160874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3236     |
|    fps              | 37       |
|    time_elapsed     | 17443    |
|    total_timesteps  | 653918   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.27e-07 |
|    n_updates        | 160979   |
----------------------------------
Eval num_timesteps=654000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 654000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.2e-08  |
|    n_updates        | 160999   |
----------------------------------
Eval num_timesteps=654500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 654500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21e-07 |
|    n_updates        | 161124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3240     |
|    fps              | 37       |
|    time_elapsed     | 17456    |
|    total_timesteps  | 654753   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.41e-08 |
|    n_updates        | 161188   |
----------------------------------
Eval num_timesteps=655000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 655000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-07 |
|    n_updates        | 161249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3244     |
|    fps              | 37       |
|    time_elapsed     | 17463    |
|    total_timesteps  | 655473   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2e-07  |
|    n_updates        | 161368   |
----------------------------------
Eval num_timesteps=655500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 655500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.84e-07 |
|    n_updates        | 161374   |
----------------------------------
Eval num_timesteps=656000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 656000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0137   |
|    n_updates        | 161499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3248     |
|    fps              | 37       |
|    time_elapsed     | 17475    |
|    total_timesteps  | 656313   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.49e-07 |
|    n_updates        | 161578   |
----------------------------------
Eval num_timesteps=656500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 656500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.35e-07 |
|    n_updates        | 161624   |
----------------------------------
Eval num_timesteps=657000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 657000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63e-07 |
|    n_updates        | 161749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3252     |
|    fps              | 37       |
|    time_elapsed     | 17488    |
|    total_timesteps  | 657153   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000128 |
|    n_updates        | 161788   |
----------------------------------
Eval num_timesteps=657500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 657500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.47e-08 |
|    n_updates        | 161874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3256     |
|    fps              | 37       |
|    time_elapsed     | 17494    |
|    total_timesteps  | 657993   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.75e-08 |
|    n_updates        | 161998   |
----------------------------------
Eval num_timesteps=658000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 658000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.13e-08 |
|    n_updates        | 161999   |
----------------------------------
Eval num_timesteps=658500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 658500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.44e-07 |
|    n_updates        | 162124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3260     |
|    fps              | 37       |
|    time_elapsed     | 17507    |
|    total_timesteps  | 658833   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.74e-07 |
|    n_updates        | 162208   |
----------------------------------
Eval num_timesteps=659000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 659000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.1e-07  |
|    n_updates        | 162249   |
----------------------------------
Eval num_timesteps=659500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 659500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33e-05 |
|    n_updates        | 162374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3264     |
|    fps              | 37       |
|    time_elapsed     | 17520    |
|    total_timesteps  | 659673   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.49e-07 |
|    n_updates        | 162418   |
----------------------------------
Eval num_timesteps=660000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 660000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4e-07  |
|    n_updates        | 162499   |
----------------------------------
Eval num_timesteps=660500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 660500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.32e-07 |
|    n_updates        | 162624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3268     |
|    fps              | 37       |
|    time_elapsed     | 17533    |
|    total_timesteps  | 660513   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-07 |
|    n_updates        | 162628   |
----------------------------------
Eval num_timesteps=661000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 661000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3e-07  |
|    n_updates        | 162749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3272     |
|    fps              | 37       |
|    time_elapsed     | 17540    |
|    total_timesteps  | 661353   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000109 |
|    n_updates        | 162838   |
----------------------------------
Eval num_timesteps=661500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 661500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.48e-07 |
|    n_updates        | 162874   |
----------------------------------
Eval num_timesteps=662000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 662000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.14e-08 |
|    n_updates        | 162999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3276     |
|    fps              | 37       |
|    time_elapsed     | 17553    |
|    total_timesteps  | 662193   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.96e-07 |
|    n_updates        | 163048   |
----------------------------------
Eval num_timesteps=662500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 662500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92e-08 |
|    n_updates        | 163124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3280     |
|    fps              | 37       |
|    time_elapsed     | 17560    |
|    total_timesteps  | 662826   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.82e-08 |
|    n_updates        | 163206   |
----------------------------------
Eval num_timesteps=663000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 663000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96e-07 |
|    n_updates        | 163249   |
----------------------------------
Eval num_timesteps=663500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 663500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.15e-07 |
|    n_updates        | 163374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3284     |
|    fps              | 37       |
|    time_elapsed     | 17573    |
|    total_timesteps  | 663666   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.21e-05 |
|    n_updates        | 163416   |
----------------------------------
Eval num_timesteps=664000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 664000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01e-07 |
|    n_updates        | 163499   |
----------------------------------
Eval num_timesteps=664500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 664500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.33e-08 |
|    n_updates        | 163624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3288     |
|    fps              | 37       |
|    time_elapsed     | 17586    |
|    total_timesteps  | 664506   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.03e-05 |
|    n_updates        | 163626   |
----------------------------------
Eval num_timesteps=665000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 665000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33e-05 |
|    n_updates        | 163749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3292     |
|    fps              | 37       |
|    time_elapsed     | 17593    |
|    total_timesteps  | 665346   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.3e-06  |
|    n_updates        | 163836   |
----------------------------------
Eval num_timesteps=665500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 665500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.44e-08 |
|    n_updates        | 163874   |
----------------------------------
Eval num_timesteps=666000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 666000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.84e-07 |
|    n_updates        | 163999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3296     |
|    fps              | 37       |
|    time_elapsed     | 17606    |
|    total_timesteps  | 666186   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.38e-05 |
|    n_updates        | 164046   |
----------------------------------
Eval num_timesteps=666500, episode_reward=-0.19 +/- 0.16
Episode length: 206.72 +/- 22.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 666500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-07  |
|    n_updates        | 164124   |
----------------------------------
Eval num_timesteps=667000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 667000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.3e-05  |
|    n_updates        | 164249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3300     |
|    fps              | 37       |
|    time_elapsed     | 17618    |
|    total_timesteps  | 667026   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.4e-07  |
|    n_updates        | 164256   |
----------------------------------
Eval num_timesteps=667500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 667500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.31e-07 |
|    n_updates        | 164374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3304     |
|    fps              | 37       |
|    time_elapsed     | 17625    |
|    total_timesteps  | 667866   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.41e-08 |
|    n_updates        | 164466   |
----------------------------------
Eval num_timesteps=668000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 668000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.99e-08 |
|    n_updates        | 164499   |
----------------------------------
Eval num_timesteps=668500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 668500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73e-07 |
|    n_updates        | 164624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3308     |
|    fps              | 37       |
|    time_elapsed     | 17639    |
|    total_timesteps  | 668706   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.98e-07 |
|    n_updates        | 164676   |
----------------------------------
Eval num_timesteps=669000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 669000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.42e-07 |
|    n_updates        | 164749   |
----------------------------------
Eval num_timesteps=669500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 669500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.92e-07 |
|    n_updates        | 164874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3312     |
|    fps              | 37       |
|    time_elapsed     | 17651    |
|    total_timesteps  | 669546   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-07 |
|    n_updates        | 164886   |
----------------------------------
Eval num_timesteps=670000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 670000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.42e-07 |
|    n_updates        | 164999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3316     |
|    fps              | 37       |
|    time_elapsed     | 17658    |
|    total_timesteps  | 670386   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.87e-07 |
|    n_updates        | 165096   |
----------------------------------
Eval num_timesteps=670500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 670500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.67e-06 |
|    n_updates        | 165124   |
----------------------------------
Eval num_timesteps=671000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 671000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.15e-08 |
|    n_updates        | 165249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3320     |
|    fps              | 37       |
|    time_elapsed     | 17671    |
|    total_timesteps  | 671226   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4e-08    |
|    n_updates        | 165306   |
----------------------------------
Eval num_timesteps=671500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 671500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.05e-07 |
|    n_updates        | 165374   |
----------------------------------
Eval num_timesteps=672000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 672000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.57e-07 |
|    n_updates        | 165499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3324     |
|    fps              | 38       |
|    time_elapsed     | 17684    |
|    total_timesteps  | 672066   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16e-07 |
|    n_updates        | 165516   |
----------------------------------
Eval num_timesteps=672500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 672500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.87e-07 |
|    n_updates        | 165624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3328     |
|    fps              | 38       |
|    time_elapsed     | 17691    |
|    total_timesteps  | 672906   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.19e-08 |
|    n_updates        | 165726   |
----------------------------------
Eval num_timesteps=673000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 673000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74e-07 |
|    n_updates        | 165749   |
----------------------------------
Eval num_timesteps=673500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 673500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28e-07 |
|    n_updates        | 165874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3332     |
|    fps              | 38       |
|    time_elapsed     | 17704    |
|    total_timesteps  | 673746   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.2e-08  |
|    n_updates        | 165936   |
----------------------------------
Eval num_timesteps=674000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 674000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.49e-06 |
|    n_updates        | 165999   |
----------------------------------
Eval num_timesteps=674500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 674500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.53e-07 |
|    n_updates        | 166124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3336     |
|    fps              | 38       |
|    time_elapsed     | 17716    |
|    total_timesteps  | 674586   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.53e-07 |
|    n_updates        | 166146   |
----------------------------------
Eval num_timesteps=675000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 675000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.61e-07 |
|    n_updates        | 166249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3340     |
|    fps              | 38       |
|    time_elapsed     | 17723    |
|    total_timesteps  | 675426   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-05 |
|    n_updates        | 166356   |
----------------------------------
Eval num_timesteps=675500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 675500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4e-06  |
|    n_updates        | 166374   |
----------------------------------
Eval num_timesteps=676000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 676000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56e-06 |
|    n_updates        | 166499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3344     |
|    fps              | 38       |
|    time_elapsed     | 17736    |
|    total_timesteps  | 676266   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.21e-07 |
|    n_updates        | 166566   |
----------------------------------
Eval num_timesteps=676500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 676500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.04e-05 |
|    n_updates        | 166624   |
----------------------------------
Eval num_timesteps=677000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 677000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.93e-08 |
|    n_updates        | 166749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3348     |
|    fps              | 38       |
|    time_elapsed     | 17749    |
|    total_timesteps  | 677106   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.12e-07 |
|    n_updates        | 166776   |
----------------------------------
Eval num_timesteps=677500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 677500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.88e-07 |
|    n_updates        | 166874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3352     |
|    fps              | 38       |
|    time_elapsed     | 17756    |
|    total_timesteps  | 677946   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34e-07 |
|    n_updates        | 166986   |
----------------------------------
Eval num_timesteps=678000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 678000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23e-06 |
|    n_updates        | 166999   |
----------------------------------
Eval num_timesteps=678500, episode_reward=-0.14 +/- 0.29
Episode length: 197.58 +/- 49.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 678500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.01e-07 |
|    n_updates        | 167124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3356     |
|    fps              | 38       |
|    time_elapsed     | 17768    |
|    total_timesteps  | 678786   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.71e-07 |
|    n_updates        | 167196   |
----------------------------------
Eval num_timesteps=679000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 679000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.35e-07 |
|    n_updates        | 167249   |
----------------------------------
Eval num_timesteps=679500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 679500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91e-05 |
|    n_updates        | 167374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3360     |
|    fps              | 38       |
|    time_elapsed     | 17781    |
|    total_timesteps  | 679626   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51e-08 |
|    n_updates        | 167406   |
----------------------------------
Eval num_timesteps=680000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 680000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.76e-07 |
|    n_updates        | 167499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3364     |
|    fps              | 38       |
|    time_elapsed     | 17788    |
|    total_timesteps  | 680466   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.93e-07 |
|    n_updates        | 167616   |
----------------------------------
Eval num_timesteps=680500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 680500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86e-06 |
|    n_updates        | 167624   |
----------------------------------
Eval num_timesteps=681000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 681000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3e-07  |
|    n_updates        | 167749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3368     |
|    fps              | 38       |
|    time_elapsed     | 17801    |
|    total_timesteps  | 681306   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.83e-07 |
|    n_updates        | 167826   |
----------------------------------
Eval num_timesteps=681500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 681500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.63e-07 |
|    n_updates        | 167874   |
----------------------------------
Eval num_timesteps=682000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 682000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.96e-05 |
|    n_updates        | 167999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3372     |
|    fps              | 38       |
|    time_elapsed     | 17814    |
|    total_timesteps  | 682146   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.47e-08 |
|    n_updates        | 168036   |
----------------------------------
Eval num_timesteps=682500, episode_reward=-0.19 +/- 0.15
Episode length: 208.22 +/- 12.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | -0.188   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 682500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.92e-09 |
|    n_updates        | 168124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3376     |
|    fps              | 38       |
|    time_elapsed     | 17821    |
|    total_timesteps  | 682986   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.33e-08 |
|    n_updates        | 168246   |
----------------------------------
Eval num_timesteps=683000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 683000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.8e-05  |
|    n_updates        | 168249   |
----------------------------------
Eval num_timesteps=683500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 683500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.83e-07 |
|    n_updates        | 168374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3380     |
|    fps              | 38       |
|    time_elapsed     | 17834    |
|    total_timesteps  | 683826   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.91e-08 |
|    n_updates        | 168456   |
----------------------------------
Eval num_timesteps=684000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 684000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.04e-07 |
|    n_updates        | 168499   |
----------------------------------
Eval num_timesteps=684500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 684500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.67e-08 |
|    n_updates        | 168624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3384     |
|    fps              | 38       |
|    time_elapsed     | 17847    |
|    total_timesteps  | 684666   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.61e-06 |
|    n_updates        | 168666   |
----------------------------------
Eval num_timesteps=685000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 685000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.86e-08 |
|    n_updates        | 168749   |
----------------------------------
Eval num_timesteps=685500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 685500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.75e-07 |
|    n_updates        | 168874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3388     |
|    fps              | 38       |
|    time_elapsed     | 17860    |
|    total_timesteps  | 685506   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.16e-07 |
|    n_updates        | 168876   |
----------------------------------
Eval num_timesteps=686000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 686000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.26e-09 |
|    n_updates        | 168999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3392     |
|    fps              | 38       |
|    time_elapsed     | 17867    |
|    total_timesteps  | 686346   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.34e-05 |
|    n_updates        | 169086   |
----------------------------------
Eval num_timesteps=686500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 686500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.11e-07 |
|    n_updates        | 169124   |
----------------------------------
Eval num_timesteps=687000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 687000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55e-08 |
|    n_updates        | 169249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3396     |
|    fps              | 38       |
|    time_elapsed     | 17879    |
|    total_timesteps  | 687186   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.43e-08 |
|    n_updates        | 169296   |
----------------------------------
Eval num_timesteps=687500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 687500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-07  |
|    n_updates        | 169374   |
----------------------------------
Eval num_timesteps=688000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 688000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.14e-07 |
|    n_updates        | 169499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3400     |
|    fps              | 38       |
|    time_elapsed     | 17892    |
|    total_timesteps  | 688026   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.13e-07 |
|    n_updates        | 169506   |
----------------------------------
Eval num_timesteps=688500, episode_reward=-0.14 +/- 0.29
Episode length: 197.78 +/- 48.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 688500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.87e-05 |
|    n_updates        | 169624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3404     |
|    fps              | 38       |
|    time_elapsed     | 17899    |
|    total_timesteps  | 688866   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.97e-07 |
|    n_updates        | 169716   |
----------------------------------
Eval num_timesteps=689000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 689000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.62e-08 |
|    n_updates        | 169749   |
----------------------------------
Eval num_timesteps=689500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 689500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.36e-08 |
|    n_updates        | 169874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3408     |
|    fps              | 38       |
|    time_elapsed     | 17912    |
|    total_timesteps  | 689706   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.04e-08 |
|    n_updates        | 169926   |
----------------------------------
Eval num_timesteps=690000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 690000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.73e-08 |
|    n_updates        | 169999   |
----------------------------------
Eval num_timesteps=690500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 690500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.49e-05 |
|    n_updates        | 170124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3412     |
|    fps              | 38       |
|    time_elapsed     | 17925    |
|    total_timesteps  | 690546   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.36e-05 |
|    n_updates        | 170136   |
----------------------------------
Eval num_timesteps=691000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 691000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.35e-07 |
|    n_updates        | 170249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3416     |
|    fps              | 38       |
|    time_elapsed     | 17932    |
|    total_timesteps  | 691386   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.86e-08 |
|    n_updates        | 170346   |
----------------------------------
Eval num_timesteps=691500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 691500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.04e-08 |
|    n_updates        | 170374   |
----------------------------------
Eval num_timesteps=692000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 692000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2e-07  |
|    n_updates        | 170499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3420     |
|    fps              | 38       |
|    time_elapsed     | 17944    |
|    total_timesteps  | 692021   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.21e-07 |
|    n_updates        | 170505   |
----------------------------------
Eval num_timesteps=692500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 692500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.41e-07 |
|    n_updates        | 170624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.198   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3424     |
|    fps              | 38       |
|    time_elapsed     | 17951    |
|    total_timesteps  | 692861   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.46e-08 |
|    n_updates        | 170715   |
----------------------------------
Eval num_timesteps=693000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 693000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.91e-06 |
|    n_updates        | 170749   |
----------------------------------
Eval num_timesteps=693500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 693500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.82e-07 |
|    n_updates        | 170874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3428     |
|    fps              | 38       |
|    time_elapsed     | 17964    |
|    total_timesteps  | 693574   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.66e-08 |
|    n_updates        | 170893   |
----------------------------------
Eval num_timesteps=694000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 694000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.43e-07 |
|    n_updates        | 170999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3432     |
|    fps              | 38       |
|    time_elapsed     | 17971    |
|    total_timesteps  | 694414   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35e-07 |
|    n_updates        | 171103   |
----------------------------------
Eval num_timesteps=694500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 694500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.64e-07 |
|    n_updates        | 171124   |
----------------------------------
Eval num_timesteps=695000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 695000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.36e-07 |
|    n_updates        | 171249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3436     |
|    fps              | 38       |
|    time_elapsed     | 17984    |
|    total_timesteps  | 695254   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.52e-07 |
|    n_updates        | 171313   |
----------------------------------
Eval num_timesteps=695500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 695500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.03e-07 |
|    n_updates        | 171374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3440     |
|    fps              | 38       |
|    time_elapsed     | 17991    |
|    total_timesteps  | 695920   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.95e-07 |
|    n_updates        | 171479   |
----------------------------------
Eval num_timesteps=696000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 696000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.16e-08 |
|    n_updates        | 171499   |
----------------------------------
Eval num_timesteps=696500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 696500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92e-05 |
|    n_updates        | 171624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3444     |
|    fps              | 38       |
|    time_elapsed     | 18004    |
|    total_timesteps  | 696760   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.98e-09 |
|    n_updates        | 171689   |
----------------------------------
Eval num_timesteps=697000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 697000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9e-07  |
|    n_updates        | 171749   |
----------------------------------
Eval num_timesteps=697500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 697500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.68e-07 |
|    n_updates        | 171874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3448     |
|    fps              | 38       |
|    time_elapsed     | 18016    |
|    total_timesteps  | 697600   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-07 |
|    n_updates        | 171899   |
----------------------------------
Eval num_timesteps=698000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 698000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.75e-05 |
|    n_updates        | 171999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3452     |
|    fps              | 38       |
|    time_elapsed     | 18023    |
|    total_timesteps  | 698440   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.73e-08 |
|    n_updates        | 172109   |
----------------------------------
Eval num_timesteps=698500, episode_reward=-0.19 +/- 0.16
Episode length: 207.06 +/- 20.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 698500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-07 |
|    n_updates        | 172124   |
----------------------------------
Eval num_timesteps=699000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 699000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04e-07 |
|    n_updates        | 172249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3456     |
|    fps              | 38       |
|    time_elapsed     | 18036    |
|    total_timesteps  | 699280   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.37e-05 |
|    n_updates        | 172319   |
----------------------------------
Eval num_timesteps=699500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 699500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.93e-07 |
|    n_updates        | 172374   |
----------------------------------
Eval num_timesteps=700000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 700000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.39e-05 |
|    n_updates        | 172499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3460     |
|    fps              | 38       |
|    time_elapsed     | 18049    |
|    total_timesteps  | 700120   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.46e-07 |
|    n_updates        | 172529   |
----------------------------------
Eval num_timesteps=700500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 700500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9e-08  |
|    n_updates        | 172624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3464     |
|    fps              | 38       |
|    time_elapsed     | 18056    |
|    total_timesteps  | 700960   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.61e-08 |
|    n_updates        | 172739   |
----------------------------------
Eval num_timesteps=701000, episode_reward=-0.14 +/- 0.29
Episode length: 197.74 +/- 48.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 701000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.94e-08 |
|    n_updates        | 172749   |
----------------------------------
Eval num_timesteps=701500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 701500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1e-07  |
|    n_updates        | 172874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3468     |
|    fps              | 38       |
|    time_elapsed     | 18068    |
|    total_timesteps  | 701595   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.16e-08 |
|    n_updates        | 172898   |
----------------------------------
Eval num_timesteps=702000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 702000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.09e-05 |
|    n_updates        | 172999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3472     |
|    fps              | 38       |
|    time_elapsed     | 18075    |
|    total_timesteps  | 702297   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82e-06 |
|    n_updates        | 173074   |
----------------------------------
Eval num_timesteps=702500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 702500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.81e-07 |
|    n_updates        | 173124   |
----------------------------------
Eval num_timesteps=703000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 703000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25e-07 |
|    n_updates        | 173249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3476     |
|    fps              | 38       |
|    time_elapsed     | 18088    |
|    total_timesteps  | 703137   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-05 |
|    n_updates        | 173284   |
----------------------------------
Eval num_timesteps=703500, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 703500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.2e-05  |
|    n_updates        | 173374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3480     |
|    fps              | 38       |
|    time_elapsed     | 18094    |
|    total_timesteps  | 703977   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0141   |
|    n_updates        | 173494   |
----------------------------------
Eval num_timesteps=704000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 704000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.99e-06 |
|    n_updates        | 173499   |
----------------------------------
Eval num_timesteps=704500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 704500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.84e-08 |
|    n_updates        | 173624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3484     |
|    fps              | 38       |
|    time_elapsed     | 18107    |
|    total_timesteps  | 704815   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.84e-08 |
|    n_updates        | 173703   |
----------------------------------
Eval num_timesteps=705000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 705000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.19e-07 |
|    n_updates        | 173749   |
----------------------------------
Eval num_timesteps=705500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 705500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.31e-07 |
|    n_updates        | 173874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3488     |
|    fps              | 38       |
|    time_elapsed     | 18120    |
|    total_timesteps  | 705655   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.37e-05 |
|    n_updates        | 173913   |
----------------------------------
Eval num_timesteps=706000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 706000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.78e-07 |
|    n_updates        | 173999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3492     |
|    fps              | 38       |
|    time_elapsed     | 18127    |
|    total_timesteps  | 706495   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.014    |
|    n_updates        | 174123   |
----------------------------------
Eval num_timesteps=706500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 706500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.61e-06 |
|    n_updates        | 174124   |
----------------------------------
Eval num_timesteps=707000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 707000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-07 |
|    n_updates        | 174249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3496     |
|    fps              | 38       |
|    time_elapsed     | 18140    |
|    total_timesteps  | 707335   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.23e-06 |
|    n_updates        | 174333   |
----------------------------------
Eval num_timesteps=707500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 707500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25e-06 |
|    n_updates        | 174374   |
----------------------------------
Eval num_timesteps=708000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 708000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-06 |
|    n_updates        | 174499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3500     |
|    fps              | 39       |
|    time_elapsed     | 18153    |
|    total_timesteps  | 708092   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.04e-06 |
|    n_updates        | 174522   |
----------------------------------
Eval num_timesteps=708500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 708500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.89e-08 |
|    n_updates        | 174624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3504     |
|    fps              | 39       |
|    time_elapsed     | 18160    |
|    total_timesteps  | 708932   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.4e-07  |
|    n_updates        | 174732   |
----------------------------------
Eval num_timesteps=709000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 709000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.04e-07 |
|    n_updates        | 174749   |
----------------------------------
Eval num_timesteps=709500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 709500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.78e-07 |
|    n_updates        | 174874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3508     |
|    fps              | 39       |
|    time_elapsed     | 18173    |
|    total_timesteps  | 709772   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.59e-06 |
|    n_updates        | 174942   |
----------------------------------
Eval num_timesteps=710000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 710000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.84e-08 |
|    n_updates        | 174999   |
----------------------------------
Eval num_timesteps=710500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 710500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-07 |
|    n_updates        | 175124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3512     |
|    fps              | 39       |
|    time_elapsed     | 18186    |
|    total_timesteps  | 710612   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95e-08 |
|    n_updates        | 175152   |
----------------------------------
Eval num_timesteps=711000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 711000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.89e-07 |
|    n_updates        | 175249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3516     |
|    fps              | 39       |
|    time_elapsed     | 18193    |
|    total_timesteps  | 711285   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.39e-05 |
|    n_updates        | 175321   |
----------------------------------
Eval num_timesteps=711500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 711500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.15e-08 |
|    n_updates        | 175374   |
----------------------------------
Eval num_timesteps=712000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 712000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.06e-08 |
|    n_updates        | 175499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3520     |
|    fps              | 39       |
|    time_elapsed     | 18205    |
|    total_timesteps  | 712096   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.44e-07 |
|    n_updates        | 175523   |
----------------------------------
Eval num_timesteps=712500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 712500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.93e-07 |
|    n_updates        | 175624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3524     |
|    fps              | 39       |
|    time_elapsed     | 18212    |
|    total_timesteps  | 712936   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.13e-05 |
|    n_updates        | 175733   |
----------------------------------
Eval num_timesteps=713000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 713000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-07 |
|    n_updates        | 175749   |
----------------------------------
Eval num_timesteps=713500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 713500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2e-07  |
|    n_updates        | 175874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3528     |
|    fps              | 39       |
|    time_elapsed     | 18225    |
|    total_timesteps  | 713776   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8e-08  |
|    n_updates        | 175943   |
----------------------------------
Eval num_timesteps=714000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 714000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5e-07  |
|    n_updates        | 175999   |
----------------------------------
Eval num_timesteps=714500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 714500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0141   |
|    n_updates        | 176124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3532     |
|    fps              | 39       |
|    time_elapsed     | 18238    |
|    total_timesteps  | 714616   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.84e-07 |
|    n_updates        | 176153   |
----------------------------------
Eval num_timesteps=715000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 715000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.91e-07 |
|    n_updates        | 176249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3536     |
|    fps              | 39       |
|    time_elapsed     | 18245    |
|    total_timesteps  | 715456   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.63e-07 |
|    n_updates        | 176363   |
----------------------------------
Eval num_timesteps=715500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 715500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.38e-07 |
|    n_updates        | 176374   |
----------------------------------
Eval num_timesteps=716000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 716000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.39e-07 |
|    n_updates        | 176499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3540     |
|    fps              | 39       |
|    time_elapsed     | 18258    |
|    total_timesteps  | 716296   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.66e-07 |
|    n_updates        | 176573   |
----------------------------------
Eval num_timesteps=716500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 716500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.96e-07 |
|    n_updates        | 176624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3544     |
|    fps              | 39       |
|    time_elapsed     | 18265    |
|    total_timesteps  | 716959   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.78e-07 |
|    n_updates        | 176739   |
----------------------------------
Eval num_timesteps=717000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 717000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.14e-09 |
|    n_updates        | 176749   |
----------------------------------
Eval num_timesteps=717500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 717500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.08e-08 |
|    n_updates        | 176874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3548     |
|    fps              | 39       |
|    time_elapsed     | 18277    |
|    total_timesteps  | 717799   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-07 |
|    n_updates        | 176949   |
----------------------------------
Eval num_timesteps=718000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 718000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.03e-07 |
|    n_updates        | 176999   |
----------------------------------
Eval num_timesteps=718500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 718500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-07  |
|    n_updates        | 177124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3552     |
|    fps              | 39       |
|    time_elapsed     | 18290    |
|    total_timesteps  | 718639   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.89e-07 |
|    n_updates        | 177159   |
----------------------------------
Eval num_timesteps=719000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 719000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.59e-06 |
|    n_updates        | 177249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3556     |
|    fps              | 39       |
|    time_elapsed     | 18297    |
|    total_timesteps  | 719479   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34e-07 |
|    n_updates        | 177369   |
----------------------------------
Eval num_timesteps=719500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 719500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.4e-07  |
|    n_updates        | 177374   |
----------------------------------
Eval num_timesteps=720000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 720000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.7e-08  |
|    n_updates        | 177499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3560     |
|    fps              | 39       |
|    time_elapsed     | 18310    |
|    total_timesteps  | 720319   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.87e-07 |
|    n_updates        | 177579   |
----------------------------------
Eval num_timesteps=720500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 720500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.73e-08 |
|    n_updates        | 177624   |
----------------------------------
Eval num_timesteps=721000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 721000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.46e-08 |
|    n_updates        | 177749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3564     |
|    fps              | 39       |
|    time_elapsed     | 18322    |
|    total_timesteps  | 721159   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.17e-06 |
|    n_updates        | 177789   |
----------------------------------
Eval num_timesteps=721500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 721500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.7e-07  |
|    n_updates        | 177874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3568     |
|    fps              | 39       |
|    time_elapsed     | 18329    |
|    total_timesteps  | 721792   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.08e-07 |
|    n_updates        | 177947   |
----------------------------------
Eval num_timesteps=722000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 722000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-07 |
|    n_updates        | 177999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3572     |
|    fps              | 39       |
|    time_elapsed     | 18336    |
|    total_timesteps  | 722443   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.66e-07 |
|    n_updates        | 178110   |
----------------------------------
Eval num_timesteps=722500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 722500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01e-07 |
|    n_updates        | 178124   |
----------------------------------
Eval num_timesteps=723000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 723000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.99e-08 |
|    n_updates        | 178249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3576     |
|    fps              | 39       |
|    time_elapsed     | 18349    |
|    total_timesteps  | 723283   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.52e-07 |
|    n_updates        | 178320   |
----------------------------------
Eval num_timesteps=723500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 723500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.06e-08 |
|    n_updates        | 178374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3580     |
|    fps              | 39       |
|    time_elapsed     | 18356    |
|    total_timesteps  | 723951   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9e-08    |
|    n_updates        | 178487   |
----------------------------------
Eval num_timesteps=724000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 724000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.49e-08 |
|    n_updates        | 178499   |
----------------------------------
Eval num_timesteps=724500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 724500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.8e-09  |
|    n_updates        | 178624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3584     |
|    fps              | 39       |
|    time_elapsed     | 18369    |
|    total_timesteps  | 724791   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.93e-08 |
|    n_updates        | 178697   |
----------------------------------
Eval num_timesteps=725000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 725000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.38e-07 |
|    n_updates        | 178749   |
----------------------------------
Eval num_timesteps=725500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 725500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.44e-05 |
|    n_updates        | 178874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3588     |
|    fps              | 39       |
|    time_elapsed     | 18381    |
|    total_timesteps  | 725631   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.12e-07 |
|    n_updates        | 178907   |
----------------------------------
Eval num_timesteps=726000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 726000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.48e-08 |
|    n_updates        | 178999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3592     |
|    fps              | 39       |
|    time_elapsed     | 18388    |
|    total_timesteps  | 726471   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.31e-05 |
|    n_updates        | 179117   |
----------------------------------
Eval num_timesteps=726500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 726500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.74e-08 |
|    n_updates        | 179124   |
----------------------------------
Eval num_timesteps=727000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 727000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.32e-06 |
|    n_updates        | 179249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3596     |
|    fps              | 39       |
|    time_elapsed     | 18401    |
|    total_timesteps  | 727311   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.32e-06 |
|    n_updates        | 179327   |
----------------------------------
Eval num_timesteps=727500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 727500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.18e-08 |
|    n_updates        | 179374   |
----------------------------------
Eval num_timesteps=728000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 728000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.83e-07 |
|    n_updates        | 179499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3600     |
|    fps              | 39       |
|    time_elapsed     | 18414    |
|    total_timesteps  | 728023   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.18e-07 |
|    n_updates        | 179505   |
----------------------------------
Eval num_timesteps=728500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 728500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.17e-07 |
|    n_updates        | 179624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3604     |
|    fps              | 39       |
|    time_elapsed     | 18421    |
|    total_timesteps  | 728762   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.63e-06 |
|    n_updates        | 179690   |
----------------------------------
Eval num_timesteps=729000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 729000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.07e-07 |
|    n_updates        | 179749   |
----------------------------------
Eval num_timesteps=729500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 729500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.05e-06 |
|    n_updates        | 179874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3608     |
|    fps              | 39       |
|    time_elapsed     | 18434    |
|    total_timesteps  | 729602   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.82e-07 |
|    n_updates        | 179900   |
----------------------------------
Eval num_timesteps=730000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 730000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.67e-07 |
|    n_updates        | 179999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3612     |
|    fps              | 39       |
|    time_elapsed     | 18441    |
|    total_timesteps  | 730442   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.32e-08 |
|    n_updates        | 180110   |
----------------------------------
Eval num_timesteps=730500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 730500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43e-07 |
|    n_updates        | 180124   |
----------------------------------
Eval num_timesteps=731000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 731000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.96e-05 |
|    n_updates        | 180249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3616     |
|    fps              | 39       |
|    time_elapsed     | 18454    |
|    total_timesteps  | 731282   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.19e-05 |
|    n_updates        | 180320   |
----------------------------------
Eval num_timesteps=731500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 731500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.04e-08 |
|    n_updates        | 180374   |
----------------------------------
Eval num_timesteps=732000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 732000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.77e-05 |
|    n_updates        | 180499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3620     |
|    fps              | 39       |
|    time_elapsed     | 18467    |
|    total_timesteps  | 732122   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.98e-05 |
|    n_updates        | 180530   |
----------------------------------
Eval num_timesteps=732500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 732500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.65e-06 |
|    n_updates        | 180624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3624     |
|    fps              | 39       |
|    time_elapsed     | 18474    |
|    total_timesteps  | 732962   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-07 |
|    n_updates        | 180740   |
----------------------------------
Eval num_timesteps=733000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 733000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.97e-07 |
|    n_updates        | 180749   |
----------------------------------
Eval num_timesteps=733500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 733500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.32e-07 |
|    n_updates        | 180874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3628     |
|    fps              | 39       |
|    time_elapsed     | 18487    |
|    total_timesteps  | 733802   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.55e-08 |
|    n_updates        | 180950   |
----------------------------------
Eval num_timesteps=734000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 734000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.6e-06  |
|    n_updates        | 180999   |
----------------------------------
Eval num_timesteps=734500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 734500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.99e-08 |
|    n_updates        | 181124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3632     |
|    fps              | 39       |
|    time_elapsed     | 18500    |
|    total_timesteps  | 734574   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.09e-06 |
|    n_updates        | 181143   |
----------------------------------
Eval num_timesteps=735000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 735000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.36e-05 |
|    n_updates        | 181249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3636     |
|    fps              | 39       |
|    time_elapsed     | 18507    |
|    total_timesteps  | 735414   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86e-07 |
|    n_updates        | 181353   |
----------------------------------
Eval num_timesteps=735500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 735500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.57e-09 |
|    n_updates        | 181374   |
----------------------------------
Eval num_timesteps=736000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 736000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.55e-08 |
|    n_updates        | 181499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3640     |
|    fps              | 39       |
|    time_elapsed     | 18519    |
|    total_timesteps  | 736254   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.33e-08 |
|    n_updates        | 181563   |
----------------------------------
Eval num_timesteps=736500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 736500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.99e-07 |
|    n_updates        | 181624   |
----------------------------------
Eval num_timesteps=737000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 737000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.5e-06  |
|    n_updates        | 181749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3644     |
|    fps              | 39       |
|    time_elapsed     | 18532    |
|    total_timesteps  | 737094   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.31e-07 |
|    n_updates        | 181773   |
----------------------------------
Eval num_timesteps=737500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 737500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.35e-07 |
|    n_updates        | 181874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3648     |
|    fps              | 39       |
|    time_elapsed     | 18539    |
|    total_timesteps  | 737934   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.95e-06 |
|    n_updates        | 181983   |
----------------------------------
Eval num_timesteps=738000, episode_reward=-0.19 +/- 0.16
Episode length: 206.86 +/- 21.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 738000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.43e-06 |
|    n_updates        | 181999   |
----------------------------------
Eval num_timesteps=738500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 738500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.23e-05 |
|    n_updates        | 182124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3652     |
|    fps              | 39       |
|    time_elapsed     | 18552    |
|    total_timesteps  | 738774   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.03e-08 |
|    n_updates        | 182193   |
----------------------------------
Eval num_timesteps=739000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 739000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.05e-07 |
|    n_updates        | 182249   |
----------------------------------
Eval num_timesteps=739500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 739500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-07 |
|    n_updates        | 182374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3656     |
|    fps              | 39       |
|    time_elapsed     | 18564    |
|    total_timesteps  | 739614   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.57e-07 |
|    n_updates        | 182403   |
----------------------------------
Eval num_timesteps=740000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 740000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.5e-08  |
|    n_updates        | 182499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3660     |
|    fps              | 39       |
|    time_elapsed     | 18571    |
|    total_timesteps  | 740454   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35e-06 |
|    n_updates        | 182613   |
----------------------------------
Eval num_timesteps=740500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 740500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.05e-07 |
|    n_updates        | 182624   |
----------------------------------
Eval num_timesteps=741000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 741000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.19e-07 |
|    n_updates        | 182749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3664     |
|    fps              | 39       |
|    time_elapsed     | 18584    |
|    total_timesteps  | 741294   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.56e-05 |
|    n_updates        | 182823   |
----------------------------------
Eval num_timesteps=741500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 741500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-06 |
|    n_updates        | 182874   |
----------------------------------
Eval num_timesteps=742000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 742000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34e-07 |
|    n_updates        | 182999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3668     |
|    fps              | 39       |
|    time_elapsed     | 18597    |
|    total_timesteps  | 742134   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.15e-05 |
|    n_updates        | 183033   |
----------------------------------
Eval num_timesteps=742500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 742500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.3e-05  |
|    n_updates        | 183124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3672     |
|    fps              | 39       |
|    time_elapsed     | 18604    |
|    total_timesteps  | 742974   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.89e-07 |
|    n_updates        | 183243   |
----------------------------------
Eval num_timesteps=743000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 743000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.05e-05 |
|    n_updates        | 183249   |
----------------------------------
Eval num_timesteps=743500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 743500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.79e-07 |
|    n_updates        | 183374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3676     |
|    fps              | 39       |
|    time_elapsed     | 18616    |
|    total_timesteps  | 743814   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.8e-08  |
|    n_updates        | 183453   |
----------------------------------
Eval num_timesteps=744000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 744000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.62e-08 |
|    n_updates        | 183499   |
----------------------------------
Eval num_timesteps=744500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 744500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.9e-08  |
|    n_updates        | 183624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3680     |
|    fps              | 39       |
|    time_elapsed     | 18629    |
|    total_timesteps  | 744654   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.95e-07 |
|    n_updates        | 183663   |
----------------------------------
Eval num_timesteps=745000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 745000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.49e-05 |
|    n_updates        | 183749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3684     |
|    fps              | 40       |
|    time_elapsed     | 18636    |
|    total_timesteps  | 745494   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.32e-08 |
|    n_updates        | 183873   |
----------------------------------
Eval num_timesteps=745500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 745500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.59e-09 |
|    n_updates        | 183874   |
----------------------------------
Eval num_timesteps=746000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 746000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.45e-06 |
|    n_updates        | 183999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3688     |
|    fps              | 40       |
|    time_elapsed     | 18648    |
|    total_timesteps  | 746334   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.22e-05 |
|    n_updates        | 184083   |
----------------------------------
Eval num_timesteps=746500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 746500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.12e-08 |
|    n_updates        | 184124   |
----------------------------------
Eval num_timesteps=747000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 747000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.64e-07 |
|    n_updates        | 184249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3692     |
|    fps              | 40       |
|    time_elapsed     | 18661    |
|    total_timesteps  | 747174   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-07 |
|    n_updates        | 184293   |
----------------------------------
Eval num_timesteps=747500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 747500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.71e-07 |
|    n_updates        | 184374   |
----------------------------------
Eval num_timesteps=748000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 748000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.2e-08  |
|    n_updates        | 184499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3696     |
|    fps              | 40       |
|    time_elapsed     | 18674    |
|    total_timesteps  | 748014   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.51e-07 |
|    n_updates        | 184503   |
----------------------------------
Eval num_timesteps=748500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 748500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.86e-07 |
|    n_updates        | 184624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3700     |
|    fps              | 40       |
|    time_elapsed     | 18681    |
|    total_timesteps  | 748854   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.72e-06 |
|    n_updates        | 184713   |
----------------------------------
Eval num_timesteps=749000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 749000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0137   |
|    n_updates        | 184749   |
----------------------------------
Eval num_timesteps=749500, episode_reward=-0.19 +/- 0.17
Episode length: 206.26 +/- 26.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 749500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.79e-07 |
|    n_updates        | 184874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.189   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3704     |
|    fps              | 40       |
|    time_elapsed     | 18694    |
|    total_timesteps  | 749694   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-07  |
|    n_updates        | 184923   |
----------------------------------
Eval num_timesteps=750000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 750000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.52e-08 |
|    n_updates        | 184999   |
----------------------------------
Eval num_timesteps=750500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 750500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82e-07 |
|    n_updates        | 185124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.189   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3708     |
|    fps              | 40       |
|    time_elapsed     | 18707    |
|    total_timesteps  | 750534   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.65e-06 |
|    n_updates        | 185133   |
----------------------------------
Eval num_timesteps=751000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 751000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5e-07  |
|    n_updates        | 185249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.189   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3712     |
|    fps              | 40       |
|    time_elapsed     | 18714    |
|    total_timesteps  | 751374   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.3e-07  |
|    n_updates        | 185343   |
----------------------------------
Eval num_timesteps=751500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 751500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-07  |
|    n_updates        | 185374   |
----------------------------------
Eval num_timesteps=752000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 752000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58e-07 |
|    n_updates        | 185499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3716     |
|    fps              | 40       |
|    time_elapsed     | 18727    |
|    total_timesteps  | 752041   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54e-07 |
|    n_updates        | 185510   |
----------------------------------
Eval num_timesteps=752500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 752500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.36e-08 |
|    n_updates        | 185624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3720     |
|    fps              | 40       |
|    time_elapsed     | 18734    |
|    total_timesteps  | 752881   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.74e-07 |
|    n_updates        | 185720   |
----------------------------------
Eval num_timesteps=753000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 753000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.76e-06 |
|    n_updates        | 185749   |
----------------------------------
Eval num_timesteps=753500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 753500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.6e-08  |
|    n_updates        | 185874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3724     |
|    fps              | 40       |
|    time_elapsed     | 18747    |
|    total_timesteps  | 753721   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.37e-06 |
|    n_updates        | 185930   |
----------------------------------
Eval num_timesteps=754000, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 754000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.29e-06 |
|    n_updates        | 185999   |
----------------------------------
Eval num_timesteps=754500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 754500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.85e-07 |
|    n_updates        | 186124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3728     |
|    fps              | 40       |
|    time_elapsed     | 18759    |
|    total_timesteps  | 754561   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.38e-07 |
|    n_updates        | 186140   |
----------------------------------
Eval num_timesteps=755000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 755000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.91e-06 |
|    n_updates        | 186249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3732     |
|    fps              | 40       |
|    time_elapsed     | 18766    |
|    total_timesteps  | 755363   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.32e-07 |
|    n_updates        | 186340   |
----------------------------------
Eval num_timesteps=755500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 755500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.21e-06 |
|    n_updates        | 186374   |
----------------------------------
Eval num_timesteps=756000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 756000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.98e-08 |
|    n_updates        | 186499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3736     |
|    fps              | 40       |
|    time_elapsed     | 18779    |
|    total_timesteps  | 756203   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87e-06 |
|    n_updates        | 186550   |
----------------------------------
Eval num_timesteps=756500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 756500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.36e-07 |
|    n_updates        | 186624   |
----------------------------------
Eval num_timesteps=757000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 757000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.77e-07 |
|    n_updates        | 186749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3740     |
|    fps              | 40       |
|    time_elapsed     | 18792    |
|    total_timesteps  | 757043   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7e-07    |
|    n_updates        | 186760   |
----------------------------------
Eval num_timesteps=757500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 757500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.33e-07 |
|    n_updates        | 186874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3744     |
|    fps              | 40       |
|    time_elapsed     | 18799    |
|    total_timesteps  | 757883   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73e-07 |
|    n_updates        | 186970   |
----------------------------------
Eval num_timesteps=758000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 758000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11e-05 |
|    n_updates        | 186999   |
----------------------------------
Eval num_timesteps=758500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 758500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.76e-08 |
|    n_updates        | 187124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3748     |
|    fps              | 40       |
|    time_elapsed     | 18812    |
|    total_timesteps  | 758723   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000113 |
|    n_updates        | 187180   |
----------------------------------
Eval num_timesteps=759000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 759000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.48e-08 |
|    n_updates        | 187249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3752     |
|    fps              | 40       |
|    time_elapsed     | 18818    |
|    total_timesteps  | 759370   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.06e-06 |
|    n_updates        | 187342   |
----------------------------------
Eval num_timesteps=759500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 759500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.13e-07 |
|    n_updates        | 187374   |
----------------------------------
Eval num_timesteps=760000, episode_reward=-0.14 +/- 0.29
Episode length: 197.62 +/- 49.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 760000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000105 |
|    n_updates        | 187499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3756     |
|    fps              | 40       |
|    time_elapsed     | 18831    |
|    total_timesteps  | 760053   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.86e-07 |
|    n_updates        | 187513   |
----------------------------------
Eval num_timesteps=760500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 760500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04e-07 |
|    n_updates        | 187624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3760     |
|    fps              | 40       |
|    time_elapsed     | 18838    |
|    total_timesteps  | 760893   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.56e-07 |
|    n_updates        | 187723   |
----------------------------------
Eval num_timesteps=761000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 761000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000104 |
|    n_updates        | 187749   |
----------------------------------
Eval num_timesteps=761500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 761500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.19e-06 |
|    n_updates        | 187874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3764     |
|    fps              | 40       |
|    time_elapsed     | 18851    |
|    total_timesteps  | 761733   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.12e-07 |
|    n_updates        | 187933   |
----------------------------------
Eval num_timesteps=762000, episode_reward=-0.19 +/- 0.17
Episode length: 206.32 +/- 25.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 762000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.75e-07 |
|    n_updates        | 187999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3768     |
|    fps              | 40       |
|    time_elapsed     | 18858    |
|    total_timesteps  | 762404   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.24e-07 |
|    n_updates        | 188100   |
----------------------------------
Eval num_timesteps=762500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 762500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.23e-06 |
|    n_updates        | 188124   |
----------------------------------
Eval num_timesteps=763000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 763000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000201 |
|    n_updates        | 188249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3772     |
|    fps              | 40       |
|    time_elapsed     | 18870    |
|    total_timesteps  | 763244   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.7e-06  |
|    n_updates        | 188310   |
----------------------------------
Eval num_timesteps=763500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 763500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.33e-06 |
|    n_updates        | 188374   |
----------------------------------
Eval num_timesteps=764000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 764000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.42e-08 |
|    n_updates        | 188499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3776     |
|    fps              | 40       |
|    time_elapsed     | 18883    |
|    total_timesteps  | 764031   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-06 |
|    n_updates        | 188507   |
----------------------------------
Eval num_timesteps=764500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 764500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.6e-07  |
|    n_updates        | 188624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3780     |
|    fps              | 40       |
|    time_elapsed     | 18890    |
|    total_timesteps  | 764871   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.32e-07 |
|    n_updates        | 188717   |
----------------------------------
Eval num_timesteps=765000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 765000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.46e-06 |
|    n_updates        | 188749   |
----------------------------------
Eval num_timesteps=765500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 765500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.6e-07  |
|    n_updates        | 188874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3784     |
|    fps              | 40       |
|    time_elapsed     | 18903    |
|    total_timesteps  | 765711   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.8e-05  |
|    n_updates        | 188927   |
----------------------------------
Eval num_timesteps=766000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 766000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.51e-06 |
|    n_updates        | 188999   |
----------------------------------
Eval num_timesteps=766500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 766500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.36e-07 |
|    n_updates        | 189124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3788     |
|    fps              | 40       |
|    time_elapsed     | 18916    |
|    total_timesteps  | 766551   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.67e-06 |
|    n_updates        | 189137   |
----------------------------------
Eval num_timesteps=767000, episode_reward=-0.09 +/- 0.36
Episode length: 189.90 +/- 60.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | -0.0899  |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 767000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.04e-07 |
|    n_updates        | 189249   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3792     |
|    fps              | 40       |
|    time_elapsed     | 18922    |
|    total_timesteps  | 767391   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.71e-07 |
|    n_updates        | 189347   |
----------------------------------
Eval num_timesteps=767500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 767500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.55e-06 |
|    n_updates        | 189374   |
----------------------------------
Eval num_timesteps=768000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 768000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.29e-07 |
|    n_updates        | 189499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3796     |
|    fps              | 40       |
|    time_elapsed     | 18935    |
|    total_timesteps  | 768231   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.44e-07 |
|    n_updates        | 189557   |
----------------------------------
Eval num_timesteps=768500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 768500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0132   |
|    n_updates        | 189624   |
----------------------------------
Eval num_timesteps=769000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 769000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-07 |
|    n_updates        | 189749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3800     |
|    fps              | 40       |
|    time_elapsed     | 18949    |
|    total_timesteps  | 769071   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.96e-05 |
|    n_updates        | 189767   |
----------------------------------
Eval num_timesteps=769500, episode_reward=-0.19 +/- 0.17
Episode length: 206.34 +/- 25.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 769500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.3e-07  |
|    n_updates        | 189874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3804     |
|    fps              | 40       |
|    time_elapsed     | 18956    |
|    total_timesteps  | 769790   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.53e-07 |
|    n_updates        | 189947   |
----------------------------------
Eval num_timesteps=770000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 770000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.26e-07 |
|    n_updates        | 189999   |
----------------------------------
Eval num_timesteps=770500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 770500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0133   |
|    n_updates        | 190124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0899  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3808     |
|    fps              | 40       |
|    time_elapsed     | 18969    |
|    total_timesteps  | 770531   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.93e-06 |
|    n_updates        | 190132   |
----------------------------------
Eval num_timesteps=771000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 771000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.79e-05 |
|    n_updates        | 190249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0899  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3812     |
|    fps              | 40       |
|    time_elapsed     | 18976    |
|    total_timesteps  | 771371   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.75e-07 |
|    n_updates        | 190342   |
----------------------------------
Eval num_timesteps=771500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 771500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.93e-05 |
|    n_updates        | 190374   |
----------------------------------
Eval num_timesteps=772000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 772000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.2e-07  |
|    n_updates        | 190499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3816     |
|    fps              | 40       |
|    time_elapsed     | 18988    |
|    total_timesteps  | 772211   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.61e-07 |
|    n_updates        | 190552   |
----------------------------------
Eval num_timesteps=772500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 772500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-06 |
|    n_updates        | 190624   |
----------------------------------
Eval num_timesteps=773000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 773000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.52e-07 |
|    n_updates        | 190749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3820     |
|    fps              | 40       |
|    time_elapsed     | 19001    |
|    total_timesteps  | 773051   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.97e-07 |
|    n_updates        | 190762   |
----------------------------------
Eval num_timesteps=773500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 773500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.95e-07 |
|    n_updates        | 190874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3824     |
|    fps              | 40       |
|    time_elapsed     | 19008    |
|    total_timesteps  | 773891   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.77e-07 |
|    n_updates        | 190972   |
----------------------------------
Eval num_timesteps=774000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 774000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.49e-07 |
|    n_updates        | 190999   |
----------------------------------
Eval num_timesteps=774500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 774500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.06e-07 |
|    n_updates        | 191124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3828     |
|    fps              | 40       |
|    time_elapsed     | 19021    |
|    total_timesteps  | 774731   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-07 |
|    n_updates        | 191182   |
----------------------------------
Eval num_timesteps=775000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 775000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.66e-07 |
|    n_updates        | 191249   |
----------------------------------
Eval num_timesteps=775500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 775500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-07 |
|    n_updates        | 191374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3832     |
|    fps              | 40       |
|    time_elapsed     | 19034    |
|    total_timesteps  | 775571   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.17e-06 |
|    n_updates        | 191392   |
----------------------------------
Eval num_timesteps=776000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 776000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.16e-08 |
|    n_updates        | 191499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3836     |
|    fps              | 40       |
|    time_elapsed     | 19040    |
|    total_timesteps  | 776253   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000167 |
|    n_updates        | 191563   |
----------------------------------
Eval num_timesteps=776500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 776500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-07 |
|    n_updates        | 191624   |
----------------------------------
Eval num_timesteps=777000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 777000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1e-06    |
|    n_updates        | 191749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3840     |
|    fps              | 40       |
|    time_elapsed     | 19053    |
|    total_timesteps  | 777093   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.81e-07 |
|    n_updates        | 191773   |
----------------------------------
Eval num_timesteps=777500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 777500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.98e-07 |
|    n_updates        | 191874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0902  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3844     |
|    fps              | 40       |
|    time_elapsed     | 19060    |
|    total_timesteps  | 777912   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.21e-08 |
|    n_updates        | 191977   |
----------------------------------
Eval num_timesteps=778000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 778000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.07e-07 |
|    n_updates        | 191999   |
----------------------------------
Eval num_timesteps=778500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 778500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.49e-06 |
|    n_updates        | 192124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0902  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3848     |
|    fps              | 40       |
|    time_elapsed     | 19073    |
|    total_timesteps  | 778752   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.7e-05  |
|    n_updates        | 192187   |
----------------------------------
Eval num_timesteps=779000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 779000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.53e-06 |
|    n_updates        | 192249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3852     |
|    fps              | 40       |
|    time_elapsed     | 19080    |
|    total_timesteps  | 779385   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.88e-08 |
|    n_updates        | 192346   |
----------------------------------
Eval num_timesteps=779500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 779500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.83e-07 |
|    n_updates        | 192374   |
----------------------------------
Eval num_timesteps=780000, episode_reward=-0.14 +/- 0.29
Episode length: 198.02 +/- 47.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 780000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1e-05  |
|    n_updates        | 192499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3856     |
|    fps              | 40       |
|    time_elapsed     | 19092    |
|    total_timesteps  | 780189   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.17e-07 |
|    n_updates        | 192547   |
----------------------------------
Eval num_timesteps=780500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 780500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.34e-08 |
|    n_updates        | 192624   |
----------------------------------
Eval num_timesteps=781000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 781000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000107 |
|    n_updates        | 192749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3860     |
|    fps              | 40       |
|    time_elapsed     | 19105    |
|    total_timesteps  | 781029   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.31e-07 |
|    n_updates        | 192757   |
----------------------------------
Eval num_timesteps=781500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 781500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.21e-07 |
|    n_updates        | 192874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0905  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3864     |
|    fps              | 40       |
|    time_elapsed     | 19112    |
|    total_timesteps  | 781783   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.14e-06 |
|    n_updates        | 192945   |
----------------------------------
Eval num_timesteps=782000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 782000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.14e-07 |
|    n_updates        | 192999   |
----------------------------------
Eval num_timesteps=782500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 782500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.31e-07 |
|    n_updates        | 193124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3868     |
|    fps              | 40       |
|    time_elapsed     | 19125    |
|    total_timesteps  | 782623   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.12e-07 |
|    n_updates        | 193155   |
----------------------------------
Eval num_timesteps=783000, episode_reward=-0.14 +/- 0.28
Episode length: 198.28 +/- 46.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 783000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.76e-06 |
|    n_updates        | 193249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3872     |
|    fps              | 40       |
|    time_elapsed     | 19131    |
|    total_timesteps  | 783463   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56e-07 |
|    n_updates        | 193365   |
----------------------------------
Eval num_timesteps=783500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 783500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000123 |
|    n_updates        | 193374   |
----------------------------------
Eval num_timesteps=784000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 784000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.27e-07 |
|    n_updates        | 193499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3876     |
|    fps              | 40       |
|    time_elapsed     | 19144    |
|    total_timesteps  | 784303   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-07 |
|    n_updates        | 193575   |
----------------------------------
Eval num_timesteps=784500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 784500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.15e-07 |
|    n_updates        | 193624   |
----------------------------------
Eval num_timesteps=785000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 785000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.45e-08 |
|    n_updates        | 193749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3880     |
|    fps              | 40       |
|    time_elapsed     | 19157    |
|    total_timesteps  | 785143   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.51e-07 |
|    n_updates        | 193785   |
----------------------------------
Eval num_timesteps=785500, episode_reward=-0.14 +/- 0.28
Episode length: 198.08 +/- 47.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 785500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.41e-07 |
|    n_updates        | 193874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3884     |
|    fps              | 41       |
|    time_elapsed     | 19163    |
|    total_timesteps  | 785983   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.52e-06 |
|    n_updates        | 193995   |
----------------------------------
Eval num_timesteps=786000, episode_reward=-0.14 +/- 0.29
Episode length: 197.64 +/- 48.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 786000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000235 |
|    n_updates        | 193999   |
----------------------------------
Eval num_timesteps=786500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 786500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23e-06 |
|    n_updates        | 194124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3888     |
|    fps              | 41       |
|    time_elapsed     | 19176    |
|    total_timesteps  | 786823   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-07 |
|    n_updates        | 194205   |
----------------------------------
Eval num_timesteps=787000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 787000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00012  |
|    n_updates        | 194249   |
----------------------------------
Eval num_timesteps=787500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 787500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.95e-07 |
|    n_updates        | 194374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3892     |
|    fps              | 41       |
|    time_elapsed     | 19189    |
|    total_timesteps  | 787663   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13e-06 |
|    n_updates        | 194415   |
----------------------------------
Eval num_timesteps=788000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 788000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.87e-07 |
|    n_updates        | 194499   |
----------------------------------
Eval num_timesteps=788500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 788500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.75e-07 |
|    n_updates        | 194624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3896     |
|    fps              | 41       |
|    time_elapsed     | 19202    |
|    total_timesteps  | 788503   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-07  |
|    n_updates        | 194625   |
----------------------------------
Eval num_timesteps=789000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 789000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29e-08 |
|    n_updates        | 194749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3900     |
|    fps              | 41       |
|    time_elapsed     | 19209    |
|    total_timesteps  | 789201   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.38e-07 |
|    n_updates        | 194800   |
----------------------------------
Eval num_timesteps=789500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 789500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.62e-07 |
|    n_updates        | 194874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3904     |
|    fps              | 41       |
|    time_elapsed     | 19216    |
|    total_timesteps  | 789990   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000113 |
|    n_updates        | 194997   |
----------------------------------
Eval num_timesteps=790000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 790000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.96e-06 |
|    n_updates        | 194999   |
----------------------------------
Eval num_timesteps=790500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 790500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.05e-07 |
|    n_updates        | 195124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3908     |
|    fps              | 41       |
|    time_elapsed     | 19229    |
|    total_timesteps  | 790830   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63e-06 |
|    n_updates        | 195207   |
----------------------------------
Eval num_timesteps=791000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 791000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.24e-07 |
|    n_updates        | 195249   |
----------------------------------
Eval num_timesteps=791500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 791500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29e-07 |
|    n_updates        | 195374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3912     |
|    fps              | 41       |
|    time_elapsed     | 19242    |
|    total_timesteps  | 791670   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.66e-07 |
|    n_updates        | 195417   |
----------------------------------
Eval num_timesteps=792000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 792000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.64e-08 |
|    n_updates        | 195499   |
----------------------------------
Eval num_timesteps=792500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 792500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.88e-07 |
|    n_updates        | 195624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3916     |
|    fps              | 41       |
|    time_elapsed     | 19255    |
|    total_timesteps  | 792510   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.08e-07 |
|    n_updates        | 195627   |
----------------------------------
Eval num_timesteps=793000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 793000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.82e-07 |
|    n_updates        | 195749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3920     |
|    fps              | 41       |
|    time_elapsed     | 19262    |
|    total_timesteps  | 793350   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.3e-10  |
|    n_updates        | 195837   |
----------------------------------
Eval num_timesteps=793500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 793500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.45e-08 |
|    n_updates        | 195874   |
----------------------------------
Eval num_timesteps=794000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 794000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.06e-07 |
|    n_updates        | 195999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3924     |
|    fps              | 41       |
|    time_elapsed     | 19275    |
|    total_timesteps  | 794190   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.12e-07 |
|    n_updates        | 196047   |
----------------------------------
Eval num_timesteps=794500, episode_reward=-0.11 +/- 0.33
Episode length: 193.48 +/- 56.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 193      |
|    mean_reward      | -0.113   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 794500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.43e-07 |
|    n_updates        | 196124   |
----------------------------------
Eval num_timesteps=795000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 795000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-06 |
|    n_updates        | 196249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3928     |
|    fps              | 41       |
|    time_elapsed     | 19287    |
|    total_timesteps  | 795030   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.19e-06 |
|    n_updates        | 196257   |
----------------------------------
Eval num_timesteps=795500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 795500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.68e-07 |
|    n_updates        | 196374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3932     |
|    fps              | 41       |
|    time_elapsed     | 19294    |
|    total_timesteps  | 795870   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36e-06 |
|    n_updates        | 196467   |
----------------------------------
Eval num_timesteps=796000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 796000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.61e-06 |
|    n_updates        | 196499   |
----------------------------------
Eval num_timesteps=796500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 796500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.32e-06 |
|    n_updates        | 196624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3936     |
|    fps              | 41       |
|    time_elapsed     | 19307    |
|    total_timesteps  | 796710   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000133 |
|    n_updates        | 196677   |
----------------------------------
Eval num_timesteps=797000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 797000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.44e-07 |
|    n_updates        | 196749   |
----------------------------------
Eval num_timesteps=797500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 797500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.19e-07 |
|    n_updates        | 196874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3940     |
|    fps              | 41       |
|    time_elapsed     | 19320    |
|    total_timesteps  | 797550   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000135 |
|    n_updates        | 196887   |
----------------------------------
Eval num_timesteps=798000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 798000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.34e-07 |
|    n_updates        | 196999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3944     |
|    fps              | 41       |
|    time_elapsed     | 19327    |
|    total_timesteps  | 798390   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.83e-07 |
|    n_updates        | 197097   |
----------------------------------
Eval num_timesteps=798500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 798500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.07e-06 |
|    n_updates        | 197124   |
----------------------------------
Eval num_timesteps=799000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 799000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2e-07  |
|    n_updates        | 197249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3948     |
|    fps              | 41       |
|    time_elapsed     | 19340    |
|    total_timesteps  | 799230   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.37e-06 |
|    n_updates        | 197307   |
----------------------------------
Eval num_timesteps=799500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 799500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.86e-07 |
|    n_updates        | 197374   |
----------------------------------
Eval num_timesteps=800000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 800000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.04e-07 |
|    n_updates        | 197499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3952     |
|    fps              | 41       |
|    time_elapsed     | 19353    |
|    total_timesteps  | 800070   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55e-06 |
|    n_updates        | 197517   |
----------------------------------
Eval num_timesteps=800500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 800500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.19e-07 |
|    n_updates        | 197624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3956     |
|    fps              | 41       |
|    time_elapsed     | 19360    |
|    total_timesteps  | 800874   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-06 |
|    n_updates        | 197718   |
----------------------------------
Eval num_timesteps=801000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 801000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44e-08 |
|    n_updates        | 197749   |
----------------------------------
Eval num_timesteps=801500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 801500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.09e-06 |
|    n_updates        | 197874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3960     |
|    fps              | 41       |
|    time_elapsed     | 19373    |
|    total_timesteps  | 801714   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000123 |
|    n_updates        | 197928   |
----------------------------------
Eval num_timesteps=802000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 802000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.09e-07 |
|    n_updates        | 197999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3964     |
|    fps              | 41       |
|    time_elapsed     | 19380    |
|    total_timesteps  | 802483   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.39e-08 |
|    n_updates        | 198120   |
----------------------------------
Eval num_timesteps=802500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 802500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.83e-09 |
|    n_updates        | 198124   |
----------------------------------
Eval num_timesteps=803000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 803000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.9e-07  |
|    n_updates        | 198249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3968     |
|    fps              | 41       |
|    time_elapsed     | 19393    |
|    total_timesteps  | 803323   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0131   |
|    n_updates        | 198330   |
----------------------------------
Eval num_timesteps=803500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 803500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.5e-07  |
|    n_updates        | 198374   |
----------------------------------
Eval num_timesteps=804000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 804000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-07 |
|    n_updates        | 198499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3972     |
|    fps              | 41       |
|    time_elapsed     | 19406    |
|    total_timesteps  | 804101   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000134 |
|    n_updates        | 198525   |
----------------------------------
Eval num_timesteps=804500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 804500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.61e-07 |
|    n_updates        | 198624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3976     |
|    fps              | 41       |
|    time_elapsed     | 19413    |
|    total_timesteps  | 804941   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.9e-07  |
|    n_updates        | 198735   |
----------------------------------
Eval num_timesteps=805000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 805000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.53e-08 |
|    n_updates        | 198749   |
----------------------------------
Eval num_timesteps=805500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 805500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.13e-08 |
|    n_updates        | 198874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3980     |
|    fps              | 41       |
|    time_elapsed     | 19426    |
|    total_timesteps  | 805699   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000118 |
|    n_updates        | 198924   |
----------------------------------
Eval num_timesteps=806000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 806000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.73e-07 |
|    n_updates        | 198999   |
----------------------------------
Eval num_timesteps=806500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 806500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.16e-08 |
|    n_updates        | 199124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3984     |
|    fps              | 41       |
|    time_elapsed     | 19439    |
|    total_timesteps  | 806539   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.56e-07 |
|    n_updates        | 199134   |
----------------------------------
Eval num_timesteps=807000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 807000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.18e-06 |
|    n_updates        | 199249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3988     |
|    fps              | 41       |
|    time_elapsed     | 19445    |
|    total_timesteps  | 807247   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43e-07 |
|    n_updates        | 199311   |
----------------------------------
Eval num_timesteps=807500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 807500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.45e-08 |
|    n_updates        | 199374   |
----------------------------------
Eval num_timesteps=808000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 808000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.66e-07 |
|    n_updates        | 199499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3992     |
|    fps              | 41       |
|    time_elapsed     | 19458    |
|    total_timesteps  | 808087   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.8e-08  |
|    n_updates        | 199521   |
----------------------------------
Eval num_timesteps=808500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 808500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.3e-08  |
|    n_updates        | 199624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 3996     |
|    fps              | 41       |
|    time_elapsed     | 19465    |
|    total_timesteps  | 808927   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.65e-07 |
|    n_updates        | 199731   |
----------------------------------
Eval num_timesteps=809000, episode_reward=-0.19 +/- 0.16
Episode length: 207.24 +/- 19.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 809000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.71e-08 |
|    n_updates        | 199749   |
----------------------------------
Eval num_timesteps=809500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 809500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.71e-06 |
|    n_updates        | 199874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4000     |
|    fps              | 41       |
|    time_elapsed     | 19478    |
|    total_timesteps  | 809767   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.34e-06 |
|    n_updates        | 199941   |
----------------------------------
Eval num_timesteps=810000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 810000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44e-06 |
|    n_updates        | 199999   |
----------------------------------
Eval num_timesteps=810500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 810500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2e-06  |
|    n_updates        | 200124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4004     |
|    fps              | 41       |
|    time_elapsed     | 19491    |
|    total_timesteps  | 810607   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9e-07  |
|    n_updates        | 200151   |
----------------------------------
Eval num_timesteps=811000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 811000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000104 |
|    n_updates        | 200249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4008     |
|    fps              | 41       |
|    time_elapsed     | 19497    |
|    total_timesteps  | 811447   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.46e-06 |
|    n_updates        | 200361   |
----------------------------------
Eval num_timesteps=811500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 811500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000247 |
|    n_updates        | 200374   |
----------------------------------
Eval num_timesteps=812000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 812000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00012  |
|    n_updates        | 200499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4012     |
|    fps              | 41       |
|    time_elapsed     | 19510    |
|    total_timesteps  | 812125   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.29e-07 |
|    n_updates        | 200531   |
----------------------------------
Eval num_timesteps=812500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 812500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000113 |
|    n_updates        | 200624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4016     |
|    fps              | 41       |
|    time_elapsed     | 19517    |
|    total_timesteps  | 812843   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.31e-06 |
|    n_updates        | 200710   |
----------------------------------
Eval num_timesteps=813000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 813000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2e-07  |
|    n_updates        | 200749   |
----------------------------------
Eval num_timesteps=813500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 813500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.19e-08 |
|    n_updates        | 200874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4020     |
|    fps              | 41       |
|    time_elapsed     | 19530    |
|    total_timesteps  | 813683   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.89e-07 |
|    n_updates        | 200920   |
----------------------------------
Eval num_timesteps=814000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 814000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.18e-08 |
|    n_updates        | 200999   |
----------------------------------
Eval num_timesteps=814500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 814500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-06  |
|    n_updates        | 201124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4024     |
|    fps              | 41       |
|    time_elapsed     | 19542    |
|    total_timesteps  | 814523   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-06  |
|    n_updates        | 201130   |
----------------------------------
Eval num_timesteps=815000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 815000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.92e-06 |
|    n_updates        | 201249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4028     |
|    fps              | 41       |
|    time_elapsed     | 19549    |
|    total_timesteps  | 815363   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000125 |
|    n_updates        | 201340   |
----------------------------------
Eval num_timesteps=815500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 815500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.16e-07 |
|    n_updates        | 201374   |
----------------------------------
Eval num_timesteps=816000, episode_reward=-0.14 +/- 0.29
Episode length: 197.64 +/- 48.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 816000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.27e-06 |
|    n_updates        | 201499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4032     |
|    fps              | 41       |
|    time_elapsed     | 19562    |
|    total_timesteps  | 816203   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.59e-07 |
|    n_updates        | 201550   |
----------------------------------
Eval num_timesteps=816500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 816500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.03e-07 |
|    n_updates        | 201624   |
----------------------------------
Eval num_timesteps=817000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 817000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.26e-06 |
|    n_updates        | 201749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4036     |
|    fps              | 41       |
|    time_elapsed     | 19575    |
|    total_timesteps  | 817043   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-06 |
|    n_updates        | 201760   |
----------------------------------
Eval num_timesteps=817500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 817500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000126 |
|    n_updates        | 201874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4040     |
|    fps              | 41       |
|    time_elapsed     | 19581    |
|    total_timesteps  | 817883   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.57e-06 |
|    n_updates        | 201970   |
----------------------------------
Eval num_timesteps=818000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 818000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0129   |
|    n_updates        | 201999   |
----------------------------------
Eval num_timesteps=818500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 818500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.09e-07 |
|    n_updates        | 202124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4044     |
|    fps              | 41       |
|    time_elapsed     | 19595    |
|    total_timesteps  | 818723   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.53e-07 |
|    n_updates        | 202180   |
----------------------------------
Eval num_timesteps=819000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 819000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.1e-08  |
|    n_updates        | 202249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4048     |
|    fps              | 41       |
|    time_elapsed     | 19602    |
|    total_timesteps  | 819358   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.47e-06 |
|    n_updates        | 202339   |
----------------------------------
Eval num_timesteps=819500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 819500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.44e-07 |
|    n_updates        | 202374   |
----------------------------------
Eval num_timesteps=820000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 820000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.21e-07 |
|    n_updates        | 202499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0998  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4052     |
|    fps              | 41       |
|    time_elapsed     | 19614    |
|    total_timesteps  | 820057   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.43e-07 |
|    n_updates        | 202514   |
----------------------------------
Eval num_timesteps=820500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 820500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34e-07 |
|    n_updates        | 202624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.0983  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4056     |
|    fps              | 41       |
|    time_elapsed     | 19621    |
|    total_timesteps  | 820706   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.19e-07 |
|    n_updates        | 202676   |
----------------------------------
Eval num_timesteps=821000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 821000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.11e-07 |
|    n_updates        | 202749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | -0.076   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4060     |
|    fps              | 41       |
|    time_elapsed     | 19628    |
|    total_timesteps  | 821322   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1e-05    |
|    n_updates        | 202830   |
----------------------------------
Eval num_timesteps=821500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 821500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.97e-06 |
|    n_updates        | 202874   |
----------------------------------
Eval num_timesteps=822000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 822000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.95e-06 |
|    n_updates        | 202999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0968  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4064     |
|    fps              | 41       |
|    time_elapsed     | 19641    |
|    total_timesteps  | 822162   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.93e-06 |
|    n_updates        | 203040   |
----------------------------------
Eval num_timesteps=822500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 822500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.64e-07 |
|    n_updates        | 203124   |
----------------------------------
Eval num_timesteps=823000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 823000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000129 |
|    n_updates        | 203249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0968  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4068     |
|    fps              | 41       |
|    time_elapsed     | 19653    |
|    total_timesteps  | 823002   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.43e-07 |
|    n_updates        | 203250   |
----------------------------------
Eval num_timesteps=823500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 823500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-07  |
|    n_updates        | 203374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4072     |
|    fps              | 41       |
|    time_elapsed     | 19661    |
|    total_timesteps  | 823842   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.09e-07 |
|    n_updates        | 203460   |
----------------------------------
Eval num_timesteps=824000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 824000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.32e-05 |
|    n_updates        | 203499   |
----------------------------------
Eval num_timesteps=824500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 824500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59e-06 |
|    n_updates        | 203624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4076     |
|    fps              | 41       |
|    time_elapsed     | 19674    |
|    total_timesteps  | 824682   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.13e-05 |
|    n_updates        | 203670   |
----------------------------------
Eval num_timesteps=825000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 825000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.32e-07 |
|    n_updates        | 203749   |
----------------------------------
Eval num_timesteps=825500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 825500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54e-07 |
|    n_updates        | 203874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4080     |
|    fps              | 41       |
|    time_elapsed     | 19687    |
|    total_timesteps  | 825521   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.71e-07 |
|    n_updates        | 203880   |
----------------------------------
Eval num_timesteps=826000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 826000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000148 |
|    n_updates        | 203999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4084     |
|    fps              | 41       |
|    time_elapsed     | 19694    |
|    total_timesteps  | 826361   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.7e-07  |
|    n_updates        | 204090   |
----------------------------------
Eval num_timesteps=826500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 826500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2e-06  |
|    n_updates        | 204124   |
----------------------------------
Eval num_timesteps=827000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 827000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000167 |
|    n_updates        | 204249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4088     |
|    fps              | 41       |
|    time_elapsed     | 19707    |
|    total_timesteps  | 827201   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000208 |
|    n_updates        | 204300   |
----------------------------------
Eval num_timesteps=827500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 827500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.78e-07 |
|    n_updates        | 204374   |
----------------------------------
Eval num_timesteps=828000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 828000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.65e-07 |
|    n_updates        | 204499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4092     |
|    fps              | 41       |
|    time_elapsed     | 19720    |
|    total_timesteps  | 828041   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.36e-07 |
|    n_updates        | 204510   |
----------------------------------
Eval num_timesteps=828500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 828500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.82e-08 |
|    n_updates        | 204624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4096     |
|    fps              | 42       |
|    time_elapsed     | 19727    |
|    total_timesteps  | 828881   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18e-06 |
|    n_updates        | 204720   |
----------------------------------
Eval num_timesteps=829000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 829000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66e-07 |
|    n_updates        | 204749   |
----------------------------------
Eval num_timesteps=829500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 829500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.09e-08 |
|    n_updates        | 204874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4100     |
|    fps              | 42       |
|    time_elapsed     | 19740    |
|    total_timesteps  | 829671   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00016  |
|    n_updates        | 204917   |
----------------------------------
Eval num_timesteps=830000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 830000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.04e-07 |
|    n_updates        | 204999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0974  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4104     |
|    fps              | 42       |
|    time_elapsed     | 19747    |
|    total_timesteps  | 830351   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.42e-07 |
|    n_updates        | 205087   |
----------------------------------
Eval num_timesteps=830500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 830500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000155 |
|    n_updates        | 205124   |
----------------------------------
Eval num_timesteps=831000, episode_reward=-0.11 +/- 0.33
Episode length: 193.58 +/- 55.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | -0.114   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 831000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2e-06    |
|    n_updates        | 205249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4108     |
|    fps              | 42       |
|    time_elapsed     | 19760    |
|    total_timesteps  | 831179   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.82e-07 |
|    n_updates        | 205294   |
----------------------------------
Eval num_timesteps=831500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 831500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.94e-07 |
|    n_updates        | 205374   |
----------------------------------
Eval num_timesteps=832000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 832000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000165 |
|    n_updates        | 205499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0989  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4112     |
|    fps              | 42       |
|    time_elapsed     | 19772    |
|    total_timesteps  | 832019   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.14e-07 |
|    n_updates        | 205504   |
----------------------------------
Eval num_timesteps=832500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 832500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000209 |
|    n_updates        | 205624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4116     |
|    fps              | 42       |
|    time_elapsed     | 19779    |
|    total_timesteps  | 832859   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.02e-06 |
|    n_updates        | 205714   |
----------------------------------
Eval num_timesteps=833000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 833000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.44e-07 |
|    n_updates        | 205749   |
----------------------------------
Eval num_timesteps=833500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 833500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.52e-07 |
|    n_updates        | 205874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0999  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4120     |
|    fps              | 42       |
|    time_elapsed     | 19792    |
|    total_timesteps  | 833678   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.39e-07 |
|    n_updates        | 205919   |
----------------------------------
Eval num_timesteps=834000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 834000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.48e-06 |
|    n_updates        | 205999   |
----------------------------------
Eval num_timesteps=834500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 834500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.76e-07 |
|    n_updates        | 206124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0999  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4124     |
|    fps              | 42       |
|    time_elapsed     | 19805    |
|    total_timesteps  | 834518   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.43e-07 |
|    n_updates        | 206129   |
----------------------------------
Eval num_timesteps=835000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 835000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.78e-07 |
|    n_updates        | 206249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0999  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4128     |
|    fps              | 42       |
|    time_elapsed     | 19811    |
|    total_timesteps  | 835358   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.74e-08 |
|    n_updates        | 206339   |
----------------------------------
Eval num_timesteps=835500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 835500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.87e-07 |
|    n_updates        | 206374   |
----------------------------------
Eval num_timesteps=836000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 836000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.93e-07 |
|    n_updates        | 206499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0999  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4132     |
|    fps              | 42       |
|    time_elapsed     | 19824    |
|    total_timesteps  | 836198   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.71e-07 |
|    n_updates        | 206549   |
----------------------------------
Eval num_timesteps=836500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 836500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8e-07  |
|    n_updates        | 206624   |
----------------------------------
Eval num_timesteps=837000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 837000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.54e-07 |
|    n_updates        | 206749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0999  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4136     |
|    fps              | 42       |
|    time_elapsed     | 19837    |
|    total_timesteps  | 837038   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000155 |
|    n_updates        | 206759   |
----------------------------------
Eval num_timesteps=837500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 837500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000174 |
|    n_updates        | 206874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0999  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4140     |
|    fps              | 42       |
|    time_elapsed     | 19844    |
|    total_timesteps  | 837878   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000147 |
|    n_updates        | 206969   |
----------------------------------
Eval num_timesteps=838000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 838000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.02e-06 |
|    n_updates        | 206999   |
----------------------------------
Eval num_timesteps=838500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 838500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.82e-07 |
|    n_updates        | 207124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0999  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4144     |
|    fps              | 42       |
|    time_elapsed     | 19856    |
|    total_timesteps  | 838718   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000163 |
|    n_updates        | 207179   |
----------------------------------
Eval num_timesteps=839000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 839000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00019  |
|    n_updates        | 207249   |
----------------------------------
Eval num_timesteps=839500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 839500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.85e-06 |
|    n_updates        | 207374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4148     |
|    fps              | 42       |
|    time_elapsed     | 19869    |
|    total_timesteps  | 839558   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.42e-07 |
|    n_updates        | 207389   |
----------------------------------
Eval num_timesteps=840000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 840000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67e-06 |
|    n_updates        | 207499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4152     |
|    fps              | 42       |
|    time_elapsed     | 19876    |
|    total_timesteps  | 840398   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.32e-07 |
|    n_updates        | 207599   |
----------------------------------
Eval num_timesteps=840500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 840500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.1e-06  |
|    n_updates        | 207624   |
----------------------------------
Eval num_timesteps=841000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 841000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.06e-07 |
|    n_updates        | 207749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4156     |
|    fps              | 42       |
|    time_elapsed     | 19889    |
|    total_timesteps  | 841238   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.75e-07 |
|    n_updates        | 207809   |
----------------------------------
Eval num_timesteps=841500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 841500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.23e-07 |
|    n_updates        | 207874   |
----------------------------------
Eval num_timesteps=842000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 842000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000176 |
|    n_updates        | 207999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.158   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4160     |
|    fps              | 42       |
|    time_elapsed     | 19902    |
|    total_timesteps  | 842078   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63e-06 |
|    n_updates        | 208019   |
----------------------------------
Eval num_timesteps=842500, episode_reward=-0.16 +/- 0.23
Episode length: 202.36 +/- 37.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 842500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.71e-05 |
|    n_updates        | 208124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.158   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4164     |
|    fps              | 42       |
|    time_elapsed     | 19909    |
|    total_timesteps  | 842918   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36e-06 |
|    n_updates        | 208229   |
----------------------------------
Eval num_timesteps=843000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 843000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-06 |
|    n_updates        | 208249   |
----------------------------------
Eval num_timesteps=843500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 843500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13e-07 |
|    n_updates        | 208374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4168     |
|    fps              | 42       |
|    time_elapsed     | 19921    |
|    total_timesteps  | 843630   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000167 |
|    n_updates        | 208407   |
----------------------------------
Eval num_timesteps=844000, episode_reward=-0.19 +/- 0.16
Episode length: 206.62 +/- 23.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 844000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.63e-06 |
|    n_updates        | 208499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4172     |
|    fps              | 42       |
|    time_elapsed     | 19928    |
|    total_timesteps  | 844470   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.14e-08 |
|    n_updates        | 208617   |
----------------------------------
Eval num_timesteps=844500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 844500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.41e-07 |
|    n_updates        | 208624   |
----------------------------------
Eval num_timesteps=845000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 845000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.29e-06 |
|    n_updates        | 208749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4176     |
|    fps              | 42       |
|    time_elapsed     | 19941    |
|    total_timesteps  | 845310   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.39e-06 |
|    n_updates        | 208827   |
----------------------------------
Eval num_timesteps=845500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 845500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.53e-06 |
|    n_updates        | 208874   |
----------------------------------
Eval num_timesteps=846000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 846000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.66e-07 |
|    n_updates        | 208999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4180     |
|    fps              | 42       |
|    time_elapsed     | 19954    |
|    total_timesteps  | 846150   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.83e-06 |
|    n_updates        | 209037   |
----------------------------------
Eval num_timesteps=846500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 846500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.53e-07 |
|    n_updates        | 209124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4184     |
|    fps              | 42       |
|    time_elapsed     | 19961    |
|    total_timesteps  | 846990   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.01e-07 |
|    n_updates        | 209247   |
----------------------------------
Eval num_timesteps=847000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 847000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.81e-07 |
|    n_updates        | 209249   |
----------------------------------
Eval num_timesteps=847500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 847500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.14e-06 |
|    n_updates        | 209374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4188     |
|    fps              | 42       |
|    time_elapsed     | 19974    |
|    total_timesteps  | 847830   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35e-06 |
|    n_updates        | 209457   |
----------------------------------
Eval num_timesteps=848000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 848000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01e-06 |
|    n_updates        | 209499   |
----------------------------------
Eval num_timesteps=848500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 848500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.48e-06 |
|    n_updates        | 209624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4192     |
|    fps              | 42       |
|    time_elapsed     | 19986    |
|    total_timesteps  | 848670   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74e-06 |
|    n_updates        | 209667   |
----------------------------------
Eval num_timesteps=849000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 849000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7e-06  |
|    n_updates        | 209749   |
----------------------------------
Eval num_timesteps=849500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 849500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96e-06 |
|    n_updates        | 209874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4196     |
|    fps              | 42       |
|    time_elapsed     | 20000    |
|    total_timesteps  | 849510   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.71e-06 |
|    n_updates        | 209877   |
----------------------------------
Eval num_timesteps=850000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 850000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.24e-06 |
|    n_updates        | 209999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4200     |
|    fps              | 42       |
|    time_elapsed     | 20007    |
|    total_timesteps  | 850350   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.88e-07 |
|    n_updates        | 210087   |
----------------------------------
Eval num_timesteps=850500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 850500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47e-06 |
|    n_updates        | 210124   |
----------------------------------
Eval num_timesteps=851000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 851000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-06 |
|    n_updates        | 210249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4204     |
|    fps              | 42       |
|    time_elapsed     | 20020    |
|    total_timesteps  | 851190   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.81e-07 |
|    n_updates        | 210297   |
----------------------------------
Eval num_timesteps=851500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 851500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000175 |
|    n_updates        | 210374   |
----------------------------------
Eval num_timesteps=852000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 852000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-06 |
|    n_updates        | 210499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4208     |
|    fps              | 42       |
|    time_elapsed     | 20033    |
|    total_timesteps  | 852030   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000168 |
|    n_updates        | 210507   |
----------------------------------
Eval num_timesteps=852500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 852500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000178 |
|    n_updates        | 210624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4212     |
|    fps              | 42       |
|    time_elapsed     | 20039    |
|    total_timesteps  | 852870   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.55e-08 |
|    n_updates        | 210717   |
----------------------------------
Eval num_timesteps=853000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 853000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.73e-08 |
|    n_updates        | 210749   |
----------------------------------
Eval num_timesteps=853500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 853500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.06e-07 |
|    n_updates        | 210874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4216     |
|    fps              | 42       |
|    time_elapsed     | 20052    |
|    total_timesteps  | 853710   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9e-07  |
|    n_updates        | 210927   |
----------------------------------
Eval num_timesteps=854000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 854000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.42e-07 |
|    n_updates        | 210999   |
----------------------------------
Eval num_timesteps=854500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 854500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.25e-07 |
|    n_updates        | 211124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4220     |
|    fps              | 42       |
|    time_elapsed     | 20065    |
|    total_timesteps  | 854550   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.05e-07 |
|    n_updates        | 211137   |
----------------------------------
Eval num_timesteps=855000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 855000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.33e-06 |
|    n_updates        | 211249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4224     |
|    fps              | 42       |
|    time_elapsed     | 20072    |
|    total_timesteps  | 855390   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.35e-07 |
|    n_updates        | 211347   |
----------------------------------
Eval num_timesteps=855500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 855500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.59e-07 |
|    n_updates        | 211374   |
----------------------------------
Eval num_timesteps=856000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 856000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.66e-07 |
|    n_updates        | 211499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4228     |
|    fps              | 42       |
|    time_elapsed     | 20085    |
|    total_timesteps  | 856230   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.12e-08 |
|    n_updates        | 211557   |
----------------------------------
Eval num_timesteps=856500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 856500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.26e-06 |
|    n_updates        | 211624   |
----------------------------------
Eval num_timesteps=857000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 857000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.72e-06 |
|    n_updates        | 211749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4232     |
|    fps              | 42       |
|    time_elapsed     | 20098    |
|    total_timesteps  | 857070   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33e-06 |
|    n_updates        | 211767   |
----------------------------------
Eval num_timesteps=857500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 857500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.32e-07 |
|    n_updates        | 211874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4236     |
|    fps              | 42       |
|    time_elapsed     | 20105    |
|    total_timesteps  | 857910   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5e-06  |
|    n_updates        | 211977   |
----------------------------------
Eval num_timesteps=858000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 858000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.24e-08 |
|    n_updates        | 211999   |
----------------------------------
Eval num_timesteps=858500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 858500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.73e-07 |
|    n_updates        | 212124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4240     |
|    fps              | 42       |
|    time_elapsed     | 20118    |
|    total_timesteps  | 858750   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.56e-07 |
|    n_updates        | 212187   |
----------------------------------
Eval num_timesteps=859000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 859000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.45e-06 |
|    n_updates        | 212249   |
----------------------------------
Eval num_timesteps=859500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 859500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.52e-07 |
|    n_updates        | 212374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4244     |
|    fps              | 42       |
|    time_elapsed     | 20131    |
|    total_timesteps  | 859590   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91e-07 |
|    n_updates        | 212397   |
----------------------------------
Eval num_timesteps=860000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 860000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.81e-07 |
|    n_updates        | 212499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4248     |
|    fps              | 42       |
|    time_elapsed     | 20137    |
|    total_timesteps  | 860430   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.34e-06 |
|    n_updates        | 212607   |
----------------------------------
Eval num_timesteps=860500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 860500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.12e-06 |
|    n_updates        | 212624   |
----------------------------------
Eval num_timesteps=861000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 861000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000411 |
|    n_updates        | 212749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4252     |
|    fps              | 42       |
|    time_elapsed     | 20150    |
|    total_timesteps  | 861270   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000198 |
|    n_updates        | 212817   |
----------------------------------
Eval num_timesteps=861500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 861500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.66e-08 |
|    n_updates        | 212874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4256     |
|    fps              | 42       |
|    time_elapsed     | 20157    |
|    total_timesteps  | 861868   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.68e-07 |
|    n_updates        | 212966   |
----------------------------------
Eval num_timesteps=862000, episode_reward=-0.16 +/- 0.23
Episode length: 202.10 +/- 38.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 862000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.4e-07  |
|    n_updates        | 212999   |
----------------------------------
Eval num_timesteps=862500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 862500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000195 |
|    n_updates        | 213124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4260     |
|    fps              | 42       |
|    time_elapsed     | 20170    |
|    total_timesteps  | 862708   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.46e-07 |
|    n_updates        | 213176   |
----------------------------------
Eval num_timesteps=863000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 863000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.91e-07 |
|    n_updates        | 213249   |
----------------------------------
Eval num_timesteps=863500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 863500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.81e-07 |
|    n_updates        | 213374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4264     |
|    fps              | 42       |
|    time_elapsed     | 20182    |
|    total_timesteps  | 863548   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44e-07 |
|    n_updates        | 213386   |
----------------------------------
Eval num_timesteps=864000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 864000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.87e-07 |
|    n_updates        | 213499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4268     |
|    fps              | 42       |
|    time_elapsed     | 20189    |
|    total_timesteps  | 864388   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000192 |
|    n_updates        | 213596   |
----------------------------------
Eval num_timesteps=864500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 864500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.57e-05 |
|    n_updates        | 213624   |
----------------------------------
Eval num_timesteps=865000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 865000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.77e-07 |
|    n_updates        | 213749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4272     |
|    fps              | 42       |
|    time_elapsed     | 20202    |
|    total_timesteps  | 865080   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.57e-07 |
|    n_updates        | 213769   |
----------------------------------
Eval num_timesteps=865500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 865500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-07 |
|    n_updates        | 213874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4276     |
|    fps              | 42       |
|    time_elapsed     | 20209    |
|    total_timesteps  | 865920   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.38e-07 |
|    n_updates        | 213979   |
----------------------------------
Eval num_timesteps=866000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 866000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000367 |
|    n_updates        | 213999   |
----------------------------------
Eval num_timesteps=866500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 866500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000206 |
|    n_updates        | 214124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4280     |
|    fps              | 42       |
|    time_elapsed     | 20221    |
|    total_timesteps  | 866696   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.94e-07 |
|    n_updates        | 214173   |
----------------------------------
Eval num_timesteps=867000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 867000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.46e-06 |
|    n_updates        | 214249   |
----------------------------------
Eval num_timesteps=867500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 867500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.69e-08 |
|    n_updates        | 214374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4284     |
|    fps              | 42       |
|    time_elapsed     | 20234    |
|    total_timesteps  | 867536   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.47e-08 |
|    n_updates        | 214383   |
----------------------------------
Eval num_timesteps=868000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 868000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.81e-06 |
|    n_updates        | 214499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4288     |
|    fps              | 42       |
|    time_elapsed     | 20241    |
|    total_timesteps  | 868376   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.17e-06 |
|    n_updates        | 214593   |
----------------------------------
Eval num_timesteps=868500, episode_reward=-0.14 +/- 0.29
Episode length: 197.60 +/- 49.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 868500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67e-06 |
|    n_updates        | 214624   |
----------------------------------
Eval num_timesteps=869000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 869000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.34e-07 |
|    n_updates        | 214749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4292     |
|    fps              | 42       |
|    time_elapsed     | 20253    |
|    total_timesteps  | 869216   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.73e-06 |
|    n_updates        | 214803   |
----------------------------------
Eval num_timesteps=869500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 869500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0125   |
|    n_updates        | 214874   |
----------------------------------
Eval num_timesteps=870000, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 870000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.78e-07 |
|    n_updates        | 214999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4296     |
|    fps              | 42       |
|    time_elapsed     | 20266    |
|    total_timesteps  | 870056   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.43e-07 |
|    n_updates        | 215013   |
----------------------------------
Eval num_timesteps=870500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 870500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.79e-07 |
|    n_updates        | 215124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4300     |
|    fps              | 42       |
|    time_elapsed     | 20273    |
|    total_timesteps  | 870896   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.18e-06 |
|    n_updates        | 215223   |
----------------------------------
Eval num_timesteps=871000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 871000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.51e-07 |
|    n_updates        | 215249   |
----------------------------------
Eval num_timesteps=871500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 871500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.38e-07 |
|    n_updates        | 215374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4304     |
|    fps              | 42       |
|    time_elapsed     | 20286    |
|    total_timesteps  | 871736   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.54e-07 |
|    n_updates        | 215433   |
----------------------------------
Eval num_timesteps=872000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 872000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35e-06 |
|    n_updates        | 215499   |
----------------------------------
Eval num_timesteps=872500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 872500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.92e-07 |
|    n_updates        | 215624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4308     |
|    fps              | 42       |
|    time_elapsed     | 20299    |
|    total_timesteps  | 872576   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00018  |
|    n_updates        | 215643   |
----------------------------------
Eval num_timesteps=873000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 873000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000176 |
|    n_updates        | 215749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4312     |
|    fps              | 43       |
|    time_elapsed     | 20305    |
|    total_timesteps  | 873282   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000166 |
|    n_updates        | 215820   |
----------------------------------
Eval num_timesteps=873500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 873500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.74e-06 |
|    n_updates        | 215874   |
----------------------------------
Eval num_timesteps=874000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 874000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-06 |
|    n_updates        | 215999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4316     |
|    fps              | 43       |
|    time_elapsed     | 20318    |
|    total_timesteps  | 874122   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.95e-07 |
|    n_updates        | 216030   |
----------------------------------
Eval num_timesteps=874500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 874500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.54e-07 |
|    n_updates        | 216124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4320     |
|    fps              | 43       |
|    time_elapsed     | 20325    |
|    total_timesteps  | 874962   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.39e-07 |
|    n_updates        | 216240   |
----------------------------------
Eval num_timesteps=875000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 875000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.86e-08 |
|    n_updates        | 216249   |
----------------------------------
Eval num_timesteps=875500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 875500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.85e-07 |
|    n_updates        | 216374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4324     |
|    fps              | 43       |
|    time_elapsed     | 20337    |
|    total_timesteps  | 875802   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.47e-07 |
|    n_updates        | 216450   |
----------------------------------
Eval num_timesteps=876000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 876000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06e-05 |
|    n_updates        | 216499   |
----------------------------------
Eval num_timesteps=876500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 876500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000178 |
|    n_updates        | 216624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4328     |
|    fps              | 43       |
|    time_elapsed     | 20350    |
|    total_timesteps  | 876642   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94e-06 |
|    n_updates        | 216660   |
----------------------------------
Eval num_timesteps=877000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 877000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.97e-07 |
|    n_updates        | 216749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4332     |
|    fps              | 43       |
|    time_elapsed     | 20357    |
|    total_timesteps  | 877360   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.25e-07 |
|    n_updates        | 216839   |
----------------------------------
Eval num_timesteps=877500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 877500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.42e-07 |
|    n_updates        | 216874   |
----------------------------------
Eval num_timesteps=878000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 878000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.27e-07 |
|    n_updates        | 216999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4336     |
|    fps              | 43       |
|    time_elapsed     | 20370    |
|    total_timesteps  | 878043   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00017  |
|    n_updates        | 217010   |
----------------------------------
Eval num_timesteps=878500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 878500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67e-07 |
|    n_updates        | 217124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4340     |
|    fps              | 43       |
|    time_elapsed     | 20376    |
|    total_timesteps  | 878883   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16e-06 |
|    n_updates        | 217220   |
----------------------------------
Eval num_timesteps=879000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 879000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.75e-07 |
|    n_updates        | 217249   |
----------------------------------
Eval num_timesteps=879500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 879500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.4e-07  |
|    n_updates        | 217374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4344     |
|    fps              | 43       |
|    time_elapsed     | 20389    |
|    total_timesteps  | 879723   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000163 |
|    n_updates        | 217430   |
----------------------------------
Eval num_timesteps=880000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 880000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.31e-07 |
|    n_updates        | 217499   |
----------------------------------
Eval num_timesteps=880500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 880500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-07 |
|    n_updates        | 217624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4348     |
|    fps              | 43       |
|    time_elapsed     | 20402    |
|    total_timesteps  | 880517   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.76e-07 |
|    n_updates        | 217629   |
----------------------------------
Eval num_timesteps=881000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 881000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.24e-08 |
|    n_updates        | 217749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4352     |
|    fps              | 43       |
|    time_elapsed     | 20409    |
|    total_timesteps  | 881357   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-07 |
|    n_updates        | 217839   |
----------------------------------
Eval num_timesteps=881500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 881500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.59e-07 |
|    n_updates        | 217874   |
----------------------------------
Eval num_timesteps=882000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 882000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000152 |
|    n_updates        | 217999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4356     |
|    fps              | 43       |
|    time_elapsed     | 20422    |
|    total_timesteps  | 882197   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.93e-07 |
|    n_updates        | 218049   |
----------------------------------
Eval num_timesteps=882500, episode_reward=-0.14 +/- 0.29
Episode length: 197.74 +/- 48.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 882500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.44e-07 |
|    n_updates        | 218124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4360     |
|    fps              | 43       |
|    time_elapsed     | 20428    |
|    total_timesteps  | 882832   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.15e-07 |
|    n_updates        | 218207   |
----------------------------------
Eval num_timesteps=883000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 883000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.56e-07 |
|    n_updates        | 218249   |
----------------------------------
Eval num_timesteps=883500, episode_reward=-0.14 +/- 0.29
Episode length: 197.66 +/- 48.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 883500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.13e-06 |
|    n_updates        | 218374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4364     |
|    fps              | 43       |
|    time_elapsed     | 20441    |
|    total_timesteps  | 883672   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000162 |
|    n_updates        | 218417   |
----------------------------------
Eval num_timesteps=884000, episode_reward=-0.19 +/- 0.17
Episode length: 206.28 +/- 26.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 884000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.91e-06 |
|    n_updates        | 218499   |
----------------------------------
Eval num_timesteps=884500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 884500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.91e-07 |
|    n_updates        | 218624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4368     |
|    fps              | 43       |
|    time_elapsed     | 20454    |
|    total_timesteps  | 884512   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.77e-07 |
|    n_updates        | 218627   |
----------------------------------
Eval num_timesteps=885000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 885000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.85e-07 |
|    n_updates        | 218749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4372     |
|    fps              | 43       |
|    time_elapsed     | 20461    |
|    total_timesteps  | 885352   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.36e-06 |
|    n_updates        | 218837   |
----------------------------------
Eval num_timesteps=885500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 885500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00015  |
|    n_updates        | 218874   |
----------------------------------
Eval num_timesteps=886000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 886000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29e-07 |
|    n_updates        | 218999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4376     |
|    fps              | 43       |
|    time_elapsed     | 20474    |
|    total_timesteps  | 886192   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.03e-06 |
|    n_updates        | 219047   |
----------------------------------
Eval num_timesteps=886500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 886500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000152 |
|    n_updates        | 219124   |
----------------------------------
Eval num_timesteps=887000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 887000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000152 |
|    n_updates        | 219249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4380     |
|    fps              | 43       |
|    time_elapsed     | 20486    |
|    total_timesteps  | 887032   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.11e-07 |
|    n_updates        | 219257   |
----------------------------------
Eval num_timesteps=887500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 887500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.79e-07 |
|    n_updates        | 219374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4384     |
|    fps              | 43       |
|    time_elapsed     | 20493    |
|    total_timesteps  | 887727   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.15e-07 |
|    n_updates        | 219431   |
----------------------------------
Eval num_timesteps=888000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 888000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.28e-07 |
|    n_updates        | 219499   |
----------------------------------
Eval num_timesteps=888500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 888500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.6e-06  |
|    n_updates        | 219624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4388     |
|    fps              | 43       |
|    time_elapsed     | 20506    |
|    total_timesteps  | 888567   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.09e-06 |
|    n_updates        | 219641   |
----------------------------------
Eval num_timesteps=889000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 889000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.14e-06 |
|    n_updates        | 219749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4392     |
|    fps              | 43       |
|    time_elapsed     | 20513    |
|    total_timesteps  | 889407   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000135 |
|    n_updates        | 219851   |
----------------------------------
Eval num_timesteps=889500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 889500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.95e-07 |
|    n_updates        | 219874   |
----------------------------------
Eval num_timesteps=890000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 890000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.48e-07 |
|    n_updates        | 219999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4396     |
|    fps              | 43       |
|    time_elapsed     | 20526    |
|    total_timesteps  | 890040   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.96e-07 |
|    n_updates        | 220009   |
----------------------------------
Eval num_timesteps=890500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 890500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.11e-07 |
|    n_updates        | 220124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4400     |
|    fps              | 43       |
|    time_elapsed     | 20533    |
|    total_timesteps  | 890880   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00013  |
|    n_updates        | 220219   |
----------------------------------
Eval num_timesteps=891000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 891000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.6e-06  |
|    n_updates        | 220249   |
----------------------------------
Eval num_timesteps=891500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 891500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74e-07 |
|    n_updates        | 220374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4404     |
|    fps              | 43       |
|    time_elapsed     | 20546    |
|    total_timesteps  | 891720   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.18e-06 |
|    n_updates        | 220429   |
----------------------------------
Eval num_timesteps=892000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 892000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.29e-08 |
|    n_updates        | 220499   |
----------------------------------
Eval num_timesteps=892500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 892500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.29e-06 |
|    n_updates        | 220624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4408     |
|    fps              | 43       |
|    time_elapsed     | 20559    |
|    total_timesteps  | 892560   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.93e-06 |
|    n_updates        | 220639   |
----------------------------------
Eval num_timesteps=893000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 893000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.09e-06 |
|    n_updates        | 220749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4412     |
|    fps              | 43       |
|    time_elapsed     | 20566    |
|    total_timesteps  | 893400   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05e-06 |
|    n_updates        | 220849   |
----------------------------------
Eval num_timesteps=893500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 893500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.36e-07 |
|    n_updates        | 220874   |
----------------------------------
Eval num_timesteps=894000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 894000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.32e-06 |
|    n_updates        | 220999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4416     |
|    fps              | 43       |
|    time_elapsed     | 20578    |
|    total_timesteps  | 894240   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.23e-07 |
|    n_updates        | 221059   |
----------------------------------
Eval num_timesteps=894500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 894500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00013  |
|    n_updates        | 221124   |
----------------------------------
Eval num_timesteps=895000, episode_reward=-0.14 +/- 0.29
Episode length: 197.74 +/- 48.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 895000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.04e-07 |
|    n_updates        | 221249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4420     |
|    fps              | 43       |
|    time_elapsed     | 20591    |
|    total_timesteps  | 895080   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.64e-06 |
|    n_updates        | 221269   |
----------------------------------
Eval num_timesteps=895500, episode_reward=-0.19 +/- 0.16
Episode length: 206.66 +/- 23.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | -0.187   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 895500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000136 |
|    n_updates        | 221374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4424     |
|    fps              | 43       |
|    time_elapsed     | 20598    |
|    total_timesteps  | 895920   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.66e-07 |
|    n_updates        | 221479   |
----------------------------------
Eval num_timesteps=896000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 896000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.013    |
|    n_updates        | 221499   |
----------------------------------
Eval num_timesteps=896500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 896500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000115 |
|    n_updates        | 221624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4428     |
|    fps              | 43       |
|    time_elapsed     | 20611    |
|    total_timesteps  | 896760   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.25e-06 |
|    n_updates        | 221689   |
----------------------------------
Eval num_timesteps=897000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 897000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.24e-07 |
|    n_updates        | 221749   |
----------------------------------
Eval num_timesteps=897500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 897500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95e-06 |
|    n_updates        | 221874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4432     |
|    fps              | 43       |
|    time_elapsed     | 20624    |
|    total_timesteps  | 897518   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.86e-07 |
|    n_updates        | 221879   |
----------------------------------
Eval num_timesteps=898000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 898000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.22e-08 |
|    n_updates        | 221999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4436     |
|    fps              | 43       |
|    time_elapsed     | 20630    |
|    total_timesteps  | 898358   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.48e-06 |
|    n_updates        | 222089   |
----------------------------------
Eval num_timesteps=898500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 898500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.89e-07 |
|    n_updates        | 222124   |
----------------------------------
Eval num_timesteps=899000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 899000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.74e-07 |
|    n_updates        | 222249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4440     |
|    fps              | 43       |
|    time_elapsed     | 20643    |
|    total_timesteps  | 899198   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.65e-07 |
|    n_updates        | 222299   |
----------------------------------
Eval num_timesteps=899500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 899500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.52e-07 |
|    n_updates        | 222374   |
----------------------------------
Eval num_timesteps=900000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 900000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.91e-07 |
|    n_updates        | 222499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4444     |
|    fps              | 43       |
|    time_elapsed     | 20656    |
|    total_timesteps  | 900038   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.37e-07 |
|    n_updates        | 222509   |
----------------------------------
Eval num_timesteps=900500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 900500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.34e-07 |
|    n_updates        | 222624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4448     |
|    fps              | 43       |
|    time_elapsed     | 20663    |
|    total_timesteps  | 900774   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74e-06 |
|    n_updates        | 222693   |
----------------------------------
Eval num_timesteps=901000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 901000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.29e-07 |
|    n_updates        | 222749   |
----------------------------------
Eval num_timesteps=901500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 901500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.86e-06 |
|    n_updates        | 222874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4452     |
|    fps              | 43       |
|    time_elapsed     | 20676    |
|    total_timesteps  | 901614   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.76e-07 |
|    n_updates        | 222903   |
----------------------------------
Eval num_timesteps=902000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 902000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.42e-07 |
|    n_updates        | 222999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4456     |
|    fps              | 43       |
|    time_elapsed     | 20683    |
|    total_timesteps  | 902290   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.63e-07 |
|    n_updates        | 223072   |
----------------------------------
Eval num_timesteps=902500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 902500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.3e-07  |
|    n_updates        | 223124   |
----------------------------------
Eval num_timesteps=903000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 903000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000116 |
|    n_updates        | 223249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4460     |
|    fps              | 43       |
|    time_elapsed     | 20696    |
|    total_timesteps  | 903120   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.7e-06  |
|    n_updates        | 223279   |
----------------------------------
Eval num_timesteps=903500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 903500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.97e-07 |
|    n_updates        | 223374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4464     |
|    fps              | 43       |
|    time_elapsed     | 20703    |
|    total_timesteps  | 903960   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.44e-07 |
|    n_updates        | 223489   |
----------------------------------
Eval num_timesteps=904000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 904000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.77e-07 |
|    n_updates        | 223499   |
----------------------------------
Eval num_timesteps=904500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 904500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.83e-07 |
|    n_updates        | 223624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4468     |
|    fps              | 43       |
|    time_elapsed     | 20716    |
|    total_timesteps  | 904714   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.87e-07 |
|    n_updates        | 223678   |
----------------------------------
Eval num_timesteps=905000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 905000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-07 |
|    n_updates        | 223749   |
----------------------------------
Eval num_timesteps=905500, episode_reward=-0.16 +/- 0.23
Episode length: 202.08 +/- 38.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 905500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.32e-07 |
|    n_updates        | 223874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4472     |
|    fps              | 43       |
|    time_elapsed     | 20729    |
|    total_timesteps  | 905503   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.7e-08  |
|    n_updates        | 223875   |
----------------------------------
Eval num_timesteps=906000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 906000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.2e-07  |
|    n_updates        | 223999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4476     |
|    fps              | 43       |
|    time_elapsed     | 20735    |
|    total_timesteps  | 906222   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-06 |
|    n_updates        | 224055   |
----------------------------------
Eval num_timesteps=906500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 906500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.72e-06 |
|    n_updates        | 224124   |
----------------------------------
Eval num_timesteps=907000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 907000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3e-05  |
|    n_updates        | 224249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4480     |
|    fps              | 43       |
|    time_elapsed     | 20748    |
|    total_timesteps  | 907062   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.84e-06 |
|    n_updates        | 224265   |
----------------------------------
Eval num_timesteps=907500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 907500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.98e-08 |
|    n_updates        | 224374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4484     |
|    fps              | 43       |
|    time_elapsed     | 20755    |
|    total_timesteps  | 907696   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-06 |
|    n_updates        | 224423   |
----------------------------------
Eval num_timesteps=908000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 908000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04e-06 |
|    n_updates        | 224499   |
----------------------------------
Eval num_timesteps=908500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 908500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.76e-08 |
|    n_updates        | 224624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4488     |
|    fps              | 43       |
|    time_elapsed     | 20767    |
|    total_timesteps  | 908536   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.36e-06 |
|    n_updates        | 224633   |
----------------------------------
Eval num_timesteps=909000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 909000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.84e-07 |
|    n_updates        | 224749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4492     |
|    fps              | 43       |
|    time_elapsed     | 20774    |
|    total_timesteps  | 909376   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.43e-07 |
|    n_updates        | 224843   |
----------------------------------
Eval num_timesteps=909500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 909500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03e-05 |
|    n_updates        | 224874   |
----------------------------------
Eval num_timesteps=910000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 910000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.98e-06 |
|    n_updates        | 224999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4496     |
|    fps              | 43       |
|    time_elapsed     | 20787    |
|    total_timesteps  | 910216   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.1e-06  |
|    n_updates        | 225053   |
----------------------------------
Eval num_timesteps=910500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 910500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.1e-06  |
|    n_updates        | 225124   |
----------------------------------
Eval num_timesteps=911000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 911000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00015  |
|    n_updates        | 225249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4500     |
|    fps              | 43       |
|    time_elapsed     | 20800    |
|    total_timesteps  | 911023   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.56e-07 |
|    n_updates        | 225255   |
----------------------------------
Eval num_timesteps=911500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 911500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.87e-07 |
|    n_updates        | 225374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4504     |
|    fps              | 43       |
|    time_elapsed     | 20806    |
|    total_timesteps  | 911863   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000172 |
|    n_updates        | 225465   |
----------------------------------
Eval num_timesteps=912000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 912000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39e-07 |
|    n_updates        | 225499   |
----------------------------------
Eval num_timesteps=912500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 912500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.32e-06 |
|    n_updates        | 225624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4508     |
|    fps              | 43       |
|    time_elapsed     | 20819    |
|    total_timesteps  | 912703   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.13e-07 |
|    n_updates        | 225675   |
----------------------------------
Eval num_timesteps=913000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 913000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28e-06 |
|    n_updates        | 225749   |
----------------------------------
Eval num_timesteps=913500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 913500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.45e-06 |
|    n_updates        | 225874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4512     |
|    fps              | 43       |
|    time_elapsed     | 20832    |
|    total_timesteps  | 913543   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.63e-07 |
|    n_updates        | 225885   |
----------------------------------
Eval num_timesteps=914000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 914000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2e-06    |
|    n_updates        | 225999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0999  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4516     |
|    fps              | 43       |
|    time_elapsed     | 20839    |
|    total_timesteps  | 914234   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.1e-06  |
|    n_updates        | 226058   |
----------------------------------
Eval num_timesteps=914500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 914500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67e-07 |
|    n_updates        | 226124   |
----------------------------------
Eval num_timesteps=915000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 915000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.53e-08 |
|    n_updates        | 226249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.0895  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4520     |
|    fps              | 43       |
|    time_elapsed     | 20852    |
|    total_timesteps  | 915032   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.15e-07 |
|    n_updates        | 226257   |
----------------------------------
Eval num_timesteps=915500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 915500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000153 |
|    n_updates        | 226374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0792  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4524     |
|    fps              | 43       |
|    time_elapsed     | 20859    |
|    total_timesteps  | 915843   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8e-07  |
|    n_updates        | 226460   |
----------------------------------
Eval num_timesteps=916000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 916000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.54e-07 |
|    n_updates        | 226499   |
----------------------------------
Eval num_timesteps=916500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 916500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.78e-06 |
|    n_updates        | 226624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.0792  |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4528     |
|    fps              | 43       |
|    time_elapsed     | 20871    |
|    total_timesteps  | 916683   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-05 |
|    n_updates        | 226670   |
----------------------------------
Eval num_timesteps=917000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 917000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.23e-06 |
|    n_updates        | 226749   |
----------------------------------
Eval num_timesteps=917500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 917500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-07 |
|    n_updates        | 226874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.09    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4532     |
|    fps              | 43       |
|    time_elapsed     | 20884    |
|    total_timesteps  | 917523   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.74e-07 |
|    n_updates        | 226880   |
----------------------------------
Eval num_timesteps=918000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 918000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.27e-06 |
|    n_updates        | 226999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.09    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4536     |
|    fps              | 43       |
|    time_elapsed     | 20891    |
|    total_timesteps  | 918363   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18e-07 |
|    n_updates        | 227090   |
----------------------------------
Eval num_timesteps=918500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 918500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.02e-07 |
|    n_updates        | 227124   |
----------------------------------
Eval num_timesteps=919000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 919000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55e-07 |
|    n_updates        | 227249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.09    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4540     |
|    fps              | 43       |
|    time_elapsed     | 20904    |
|    total_timesteps  | 919203   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.02e-07 |
|    n_updates        | 227300   |
----------------------------------
Eval num_timesteps=919500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 919500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000153 |
|    n_updates        | 227374   |
----------------------------------
Eval num_timesteps=920000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 920000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1e-06    |
|    n_updates        | 227499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.09    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4544     |
|    fps              | 43       |
|    time_elapsed     | 20917    |
|    total_timesteps  | 920043   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000158 |
|    n_updates        | 227510   |
----------------------------------
Eval num_timesteps=920500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 920500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.51e-07 |
|    n_updates        | 227624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4548     |
|    fps              | 44       |
|    time_elapsed     | 20924    |
|    total_timesteps  | 920883   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.71e-07 |
|    n_updates        | 227720   |
----------------------------------
Eval num_timesteps=921000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 921000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47e-07 |
|    n_updates        | 227749   |
----------------------------------
Eval num_timesteps=921500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 921500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43e-06 |
|    n_updates        | 227874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4552     |
|    fps              | 44       |
|    time_elapsed     | 20937    |
|    total_timesteps  | 921723   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.64e-07 |
|    n_updates        | 227930   |
----------------------------------
Eval num_timesteps=922000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 922000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000306 |
|    n_updates        | 227999   |
----------------------------------
Eval num_timesteps=922500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 922500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-06 |
|    n_updates        | 228124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4556     |
|    fps              | 44       |
|    time_elapsed     | 20950    |
|    total_timesteps  | 922563   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.34e-07 |
|    n_updates        | 228140   |
----------------------------------
Eval num_timesteps=923000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 923000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.39e-07 |
|    n_updates        | 228249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4560     |
|    fps              | 44       |
|    time_elapsed     | 20957    |
|    total_timesteps  | 923403   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.02e-07 |
|    n_updates        | 228350   |
----------------------------------
Eval num_timesteps=923500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 923500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54e-05 |
|    n_updates        | 228374   |
----------------------------------
Eval num_timesteps=924000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 924000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15e-06 |
|    n_updates        | 228499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4564     |
|    fps              | 44       |
|    time_elapsed     | 20970    |
|    total_timesteps  | 924243   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87e-07 |
|    n_updates        | 228560   |
----------------------------------
Eval num_timesteps=924500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 924500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000139 |
|    n_updates        | 228624   |
----------------------------------
Eval num_timesteps=925000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 925000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55e-07 |
|    n_updates        | 228749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4568     |
|    fps              | 44       |
|    time_elapsed     | 20983    |
|    total_timesteps  | 925083   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.19e-07 |
|    n_updates        | 228770   |
----------------------------------
Eval num_timesteps=925500, episode_reward=-0.14 +/- 0.29
Episode length: 197.62 +/- 49.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 925500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.42e-07 |
|    n_updates        | 228874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4572     |
|    fps              | 44       |
|    time_elapsed     | 20990    |
|    total_timesteps  | 925812   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.49e-07 |
|    n_updates        | 228952   |
----------------------------------
Eval num_timesteps=926000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 926000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24e-06 |
|    n_updates        | 228999   |
----------------------------------
Eval num_timesteps=926500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 926500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.64e-06 |
|    n_updates        | 229124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4576     |
|    fps              | 44       |
|    time_elapsed     | 21003    |
|    total_timesteps  | 926652   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.18e-07 |
|    n_updates        | 229162   |
----------------------------------
Eval num_timesteps=927000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 927000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000142 |
|    n_updates        | 229249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4580     |
|    fps              | 44       |
|    time_elapsed     | 21010    |
|    total_timesteps  | 927342   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.49e-06 |
|    n_updates        | 229335   |
----------------------------------
Eval num_timesteps=927500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 927500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000133 |
|    n_updates        | 229374   |
----------------------------------
Eval num_timesteps=928000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 928000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.45e-07 |
|    n_updates        | 229499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4584     |
|    fps              | 44       |
|    time_elapsed     | 21023    |
|    total_timesteps  | 928123   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000136 |
|    n_updates        | 229530   |
----------------------------------
Eval num_timesteps=928500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 928500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.01e-07 |
|    n_updates        | 229624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4588     |
|    fps              | 44       |
|    time_elapsed     | 21030    |
|    total_timesteps  | 928963   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.05e-07 |
|    n_updates        | 229740   |
----------------------------------
Eval num_timesteps=929000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 929000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.87e-07 |
|    n_updates        | 229749   |
----------------------------------
Eval num_timesteps=929500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 929500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29e-06 |
|    n_updates        | 229874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4592     |
|    fps              | 44       |
|    time_elapsed     | 21043    |
|    total_timesteps  | 929803   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29e-06 |
|    n_updates        | 229950   |
----------------------------------
Eval num_timesteps=930000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 930000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.34e-06 |
|    n_updates        | 229999   |
----------------------------------
Eval num_timesteps=930500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 930500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.99e-07 |
|    n_updates        | 230124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4596     |
|    fps              | 44       |
|    time_elapsed     | 21056    |
|    total_timesteps  | 930643   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.74e-06 |
|    n_updates        | 230160   |
----------------------------------
Eval num_timesteps=931000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 931000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4e-07  |
|    n_updates        | 230249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4600     |
|    fps              | 44       |
|    time_elapsed     | 21063    |
|    total_timesteps  | 931313   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000136 |
|    n_updates        | 230328   |
----------------------------------
Eval num_timesteps=931500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 931500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.62e-06 |
|    n_updates        | 230374   |
----------------------------------
Eval num_timesteps=932000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 932000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02e-06 |
|    n_updates        | 230499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4604     |
|    fps              | 44       |
|    time_elapsed     | 21075    |
|    total_timesteps  | 932153   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.6e-07  |
|    n_updates        | 230538   |
----------------------------------
Eval num_timesteps=932500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 932500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.96e-06 |
|    n_updates        | 230624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4608     |
|    fps              | 44       |
|    time_elapsed     | 21082    |
|    total_timesteps  | 932993   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.51e-08 |
|    n_updates        | 230748   |
----------------------------------
Eval num_timesteps=933000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 933000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.12e-08 |
|    n_updates        | 230749   |
----------------------------------
Eval num_timesteps=933500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 933500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000121 |
|    n_updates        | 230874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4612     |
|    fps              | 44       |
|    time_elapsed     | 21095    |
|    total_timesteps  | 933833   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.97e-07 |
|    n_updates        | 230958   |
----------------------------------
Eval num_timesteps=934000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 934000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.78e-07 |
|    n_updates        | 230999   |
----------------------------------
Eval num_timesteps=934500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 934500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.8e-08  |
|    n_updates        | 231124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4616     |
|    fps              | 44       |
|    time_elapsed     | 21108    |
|    total_timesteps  | 934673   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.33e-07 |
|    n_updates        | 231168   |
----------------------------------
Eval num_timesteps=935000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 935000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.86e-08 |
|    n_updates        | 231249   |
----------------------------------
Eval num_timesteps=935500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 935500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.99e-07 |
|    n_updates        | 231374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4620     |
|    fps              | 44       |
|    time_elapsed     | 21121    |
|    total_timesteps  | 935513   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.14e-07 |
|    n_updates        | 231378   |
----------------------------------
Eval num_timesteps=936000, episode_reward=-0.16 +/- 0.23
Episode length: 202.96 +/- 34.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | -0.163   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 936000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16e-07 |
|    n_updates        | 231499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4624     |
|    fps              | 44       |
|    time_elapsed     | 21127    |
|    total_timesteps  | 936353   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.67e-07 |
|    n_updates        | 231588   |
----------------------------------
Eval num_timesteps=936500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 936500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01e-07 |
|    n_updates        | 231624   |
----------------------------------
Eval num_timesteps=937000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 937000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.88e-07 |
|    n_updates        | 231749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4628     |
|    fps              | 44       |
|    time_elapsed     | 21140    |
|    total_timesteps  | 937193   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-06 |
|    n_updates        | 231798   |
----------------------------------
Eval num_timesteps=937500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 937500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000212 |
|    n_updates        | 231874   |
----------------------------------
Eval num_timesteps=938000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 938000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.48e-07 |
|    n_updates        | 231999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4632     |
|    fps              | 44       |
|    time_elapsed     | 21153    |
|    total_timesteps  | 938033   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.9e-07  |
|    n_updates        | 232008   |
----------------------------------
Eval num_timesteps=938500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 938500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000225 |
|    n_updates        | 232124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4636     |
|    fps              | 44       |
|    time_elapsed     | 21160    |
|    total_timesteps  | 938873   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.54e-07 |
|    n_updates        | 232218   |
----------------------------------
Eval num_timesteps=939000, episode_reward=-0.14 +/- 0.29
Episode length: 197.62 +/- 49.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 939000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000112 |
|    n_updates        | 232249   |
----------------------------------
Eval num_timesteps=939500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 939500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.58e-07 |
|    n_updates        | 232374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4640     |
|    fps              | 44       |
|    time_elapsed     | 21172    |
|    total_timesteps  | 939713   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.94e-06 |
|    n_updates        | 232428   |
----------------------------------
Eval num_timesteps=940000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 940000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.54e-06 |
|    n_updates        | 232499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4644     |
|    fps              | 44       |
|    time_elapsed     | 21179    |
|    total_timesteps  | 940392   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.83e-07 |
|    n_updates        | 232597   |
----------------------------------
Eval num_timesteps=940500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 940500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.04e-07 |
|    n_updates        | 232624   |
----------------------------------
Eval num_timesteps=941000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 941000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.18e-08 |
|    n_updates        | 232749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4648     |
|    fps              | 44       |
|    time_elapsed     | 21192    |
|    total_timesteps  | 941232   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.75e-07 |
|    n_updates        | 232807   |
----------------------------------
Eval num_timesteps=941500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 941500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8e-06  |
|    n_updates        | 232874   |
----------------------------------
Eval num_timesteps=942000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 942000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28e-06 |
|    n_updates        | 232999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4652     |
|    fps              | 44       |
|    time_elapsed     | 21205    |
|    total_timesteps  | 942072   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2e-06  |
|    n_updates        | 233017   |
----------------------------------
Eval num_timesteps=942500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 942500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.37e-06 |
|    n_updates        | 233124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4656     |
|    fps              | 44       |
|    time_elapsed     | 21212    |
|    total_timesteps  | 942793   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.85e-07 |
|    n_updates        | 233198   |
----------------------------------
Eval num_timesteps=943000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 943000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000125 |
|    n_updates        | 233249   |
----------------------------------
Eval num_timesteps=943500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 943500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.27e-06 |
|    n_updates        | 233374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4660     |
|    fps              | 44       |
|    time_elapsed     | 21225    |
|    total_timesteps  | 943633   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1e-07  |
|    n_updates        | 233408   |
----------------------------------
Eval num_timesteps=944000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 944000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.36e-07 |
|    n_updates        | 233499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4664     |
|    fps              | 44       |
|    time_elapsed     | 21231    |
|    total_timesteps  | 944473   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.04e-07 |
|    n_updates        | 233618   |
----------------------------------
Eval num_timesteps=944500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 944500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01e-07 |
|    n_updates        | 233624   |
----------------------------------
Eval num_timesteps=945000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 945000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.7e-07  |
|    n_updates        | 233749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4668     |
|    fps              | 44       |
|    time_elapsed     | 21244    |
|    total_timesteps  | 945106   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-07 |
|    n_updates        | 233776   |
----------------------------------
Eval num_timesteps=945500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 945500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.16e-07 |
|    n_updates        | 233874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4672     |
|    fps              | 44       |
|    time_elapsed     | 21251    |
|    total_timesteps  | 945946   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.57e-06 |
|    n_updates        | 233986   |
----------------------------------
Eval num_timesteps=946000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 946000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.72e-07 |
|    n_updates        | 233999   |
----------------------------------
Eval num_timesteps=946500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 946500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.6e-06  |
|    n_updates        | 234124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4676     |
|    fps              | 44       |
|    time_elapsed     | 21264    |
|    total_timesteps  | 946786   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-07 |
|    n_updates        | 234196   |
----------------------------------
Eval num_timesteps=947000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 947000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.77e-07 |
|    n_updates        | 234249   |
----------------------------------
Eval num_timesteps=947500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 947500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08e-06 |
|    n_updates        | 234374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4680     |
|    fps              | 44       |
|    time_elapsed     | 21277    |
|    total_timesteps  | 947626   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.77e-07 |
|    n_updates        | 234406   |
----------------------------------
Eval num_timesteps=948000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 948000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07e-07 |
|    n_updates        | 234499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4684     |
|    fps              | 44       |
|    time_elapsed     | 21284    |
|    total_timesteps  | 948466   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.68e-06 |
|    n_updates        | 234616   |
----------------------------------
Eval num_timesteps=948500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 948500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000126 |
|    n_updates        | 234624   |
----------------------------------
Eval num_timesteps=949000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 949000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56e-07 |
|    n_updates        | 234749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4688     |
|    fps              | 44       |
|    time_elapsed     | 21297    |
|    total_timesteps  | 949306   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.48e-06 |
|    n_updates        | 234826   |
----------------------------------
Eval num_timesteps=949500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 949500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000118 |
|    n_updates        | 234874   |
----------------------------------
Eval num_timesteps=950000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 950000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.2e-08  |
|    n_updates        | 234999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4692     |
|    fps              | 44       |
|    time_elapsed     | 21310    |
|    total_timesteps  | 950146   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8e-07  |
|    n_updates        | 235036   |
----------------------------------
Eval num_timesteps=950500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 950500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000121 |
|    n_updates        | 235124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4696     |
|    fps              | 44       |
|    time_elapsed     | 21317    |
|    total_timesteps  | 950986   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.13e-06 |
|    n_updates        | 235246   |
----------------------------------
Eval num_timesteps=951000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 951000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.96e-07 |
|    n_updates        | 235249   |
----------------------------------
Eval num_timesteps=951500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 951500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.77e-07 |
|    n_updates        | 235374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4700     |
|    fps              | 44       |
|    time_elapsed     | 21330    |
|    total_timesteps  | 951826   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.59e-07 |
|    n_updates        | 235456   |
----------------------------------
Eval num_timesteps=952000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 952000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-06 |
|    n_updates        | 235499   |
----------------------------------
Eval num_timesteps=952500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 952500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.51e-07 |
|    n_updates        | 235624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4704     |
|    fps              | 44       |
|    time_elapsed     | 21343    |
|    total_timesteps  | 952666   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.49e-06 |
|    n_updates        | 235666   |
----------------------------------
Eval num_timesteps=953000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 953000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16e-07 |
|    n_updates        | 235749   |
----------------------------------
Eval num_timesteps=953500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 953500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.23e-07 |
|    n_updates        | 235874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4708     |
|    fps              | 44       |
|    time_elapsed     | 21356    |
|    total_timesteps  | 953506   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.48e-07 |
|    n_updates        | 235876   |
----------------------------------
Eval num_timesteps=954000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 954000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.85e-07 |
|    n_updates        | 235999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4712     |
|    fps              | 44       |
|    time_elapsed     | 21363    |
|    total_timesteps  | 954346   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.67e-07 |
|    n_updates        | 236086   |
----------------------------------
Eval num_timesteps=954500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 954500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.98e-07 |
|    n_updates        | 236124   |
----------------------------------
Eval num_timesteps=955000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 955000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95e-06 |
|    n_updates        | 236249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4716     |
|    fps              | 44       |
|    time_elapsed     | 21376    |
|    total_timesteps  | 955186   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.67e-05 |
|    n_updates        | 236296   |
----------------------------------
Eval num_timesteps=955500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 955500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.42e-08 |
|    n_updates        | 236374   |
----------------------------------
Eval num_timesteps=956000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 956000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51e-07 |
|    n_updates        | 236499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | -0.175   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4720     |
|    fps              | 44       |
|    time_elapsed     | 21389    |
|    total_timesteps  | 956026   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.68e-07 |
|    n_updates        | 236506   |
----------------------------------
Eval num_timesteps=956500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 956500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08e-05 |
|    n_updates        | 236624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4724     |
|    fps              | 44       |
|    time_elapsed     | 21395    |
|    total_timesteps  | 956765   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22e-07 |
|    n_updates        | 236691   |
----------------------------------
Eval num_timesteps=957000, episode_reward=-0.14 +/- 0.28
Episode length: 198.06 +/- 47.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 957000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25e-07 |
|    n_updates        | 236749   |
----------------------------------
Eval num_timesteps=957500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 957500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.72e-06 |
|    n_updates        | 236874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4728     |
|    fps              | 44       |
|    time_elapsed     | 21408    |
|    total_timesteps  | 957605   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.41e-08 |
|    n_updates        | 236901   |
----------------------------------
Eval num_timesteps=958000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 958000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.22e-07 |
|    n_updates        | 236999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4732     |
|    fps              | 44       |
|    time_elapsed     | 21414    |
|    total_timesteps  | 958445   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.29e-07 |
|    n_updates        | 237111   |
----------------------------------
Eval num_timesteps=958500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 958500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.76e-08 |
|    n_updates        | 237124   |
----------------------------------
Eval num_timesteps=959000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 959000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.51e-07 |
|    n_updates        | 237249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4736     |
|    fps              | 44       |
|    time_elapsed     | 21427    |
|    total_timesteps  | 959285   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.69e-06 |
|    n_updates        | 237321   |
----------------------------------
Eval num_timesteps=959500, episode_reward=-0.16 +/- 0.23
Episode length: 202.56 +/- 36.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | -0.163   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 959500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.48e-06 |
|    n_updates        | 237374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4740     |
|    fps              | 44       |
|    time_elapsed     | 21434    |
|    total_timesteps  | 959759   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.93e-07 |
|    n_updates        | 237439   |
----------------------------------
Eval num_timesteps=960000, episode_reward=-0.16 +/- 0.23
Episode length: 202.78 +/- 35.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | -0.163   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 960000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.99e-06 |
|    n_updates        | 237499   |
----------------------------------
Eval num_timesteps=960500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 960500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.97e-07 |
|    n_updates        | 237624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4744     |
|    fps              | 44       |
|    time_elapsed     | 21446    |
|    total_timesteps  | 960599   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28e-05 |
|    n_updates        | 237649   |
----------------------------------
Eval num_timesteps=961000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 961000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.67e-07 |
|    n_updates        | 237749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4748     |
|    fps              | 44       |
|    time_elapsed     | 21453    |
|    total_timesteps  | 961439   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.82e-07 |
|    n_updates        | 237859   |
----------------------------------
Eval num_timesteps=961500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 961500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.46e-07 |
|    n_updates        | 237874   |
----------------------------------
Eval num_timesteps=962000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 962000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.14e-07 |
|    n_updates        | 237999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4752     |
|    fps              | 44       |
|    time_elapsed     | 21466    |
|    total_timesteps  | 962279   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.7e-07  |
|    n_updates        | 238069   |
----------------------------------
Eval num_timesteps=962500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 962500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.06e-07 |
|    n_updates        | 238124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.151   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4756     |
|    fps              | 44       |
|    time_elapsed     | 21472    |
|    total_timesteps  | 962916   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95e-08 |
|    n_updates        | 238228   |
----------------------------------
Eval num_timesteps=963000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 963000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.53e-07 |
|    n_updates        | 238249   |
----------------------------------
Eval num_timesteps=963500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 963500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.55e-08 |
|    n_updates        | 238374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.151   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4760     |
|    fps              | 44       |
|    time_elapsed     | 21486    |
|    total_timesteps  | 963756   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02e-07 |
|    n_updates        | 238438   |
----------------------------------
Eval num_timesteps=964000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 964000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.79e-08 |
|    n_updates        | 238499   |
----------------------------------
Eval num_timesteps=964500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 964500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92e-06 |
|    n_updates        | 238624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.151   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4764     |
|    fps              | 44       |
|    time_elapsed     | 21499    |
|    total_timesteps  | 964596   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.42e-07 |
|    n_updates        | 238648   |
----------------------------------
Eval num_timesteps=965000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 965000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.7e-07  |
|    n_updates        | 238749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4768     |
|    fps              | 44       |
|    time_elapsed     | 21506    |
|    total_timesteps  | 965436   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.24e-06 |
|    n_updates        | 238858   |
----------------------------------
Eval num_timesteps=965500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 965500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.69e-06 |
|    n_updates        | 238874   |
----------------------------------
Eval num_timesteps=966000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 966000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.35e-06 |
|    n_updates        | 238999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4772     |
|    fps              | 44       |
|    time_elapsed     | 21519    |
|    total_timesteps  | 966276   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.41e-05 |
|    n_updates        | 239068   |
----------------------------------
Eval num_timesteps=966500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 966500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.51e-05 |
|    n_updates        | 239124   |
----------------------------------
Eval num_timesteps=967000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 967000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.17e-06 |
|    n_updates        | 239249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4776     |
|    fps              | 44       |
|    time_elapsed     | 21531    |
|    total_timesteps  | 967116   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1e-07  |
|    n_updates        | 239278   |
----------------------------------
Eval num_timesteps=967500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 967500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.13e-06 |
|    n_updates        | 239374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4780     |
|    fps              | 44       |
|    time_elapsed     | 21538    |
|    total_timesteps  | 967956   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55e-08 |
|    n_updates        | 239488   |
----------------------------------
Eval num_timesteps=968000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 968000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.31e-07 |
|    n_updates        | 239499   |
----------------------------------
Eval num_timesteps=968500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 968500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6e-06    |
|    n_updates        | 239624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4784     |
|    fps              | 44       |
|    time_elapsed     | 21551    |
|    total_timesteps  | 968796   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.2e-08  |
|    n_updates        | 239698   |
----------------------------------
Eval num_timesteps=969000, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 969000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.85e-06 |
|    n_updates        | 239749   |
----------------------------------
Eval num_timesteps=969500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 969500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000118 |
|    n_updates        | 239874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4788     |
|    fps              | 44       |
|    time_elapsed     | 21564    |
|    total_timesteps  | 969636   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.6e-07  |
|    n_updates        | 239908   |
----------------------------------
Eval num_timesteps=970000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 970000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.49e-08 |
|    n_updates        | 239999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4792     |
|    fps              | 44       |
|    time_elapsed     | 21571    |
|    total_timesteps  | 970476   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91e-07 |
|    n_updates        | 240118   |
----------------------------------
Eval num_timesteps=970500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 970500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41e-07 |
|    n_updates        | 240124   |
----------------------------------
Eval num_timesteps=971000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 971000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5e-07    |
|    n_updates        | 240249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4796     |
|    fps              | 45       |
|    time_elapsed     | 21584    |
|    total_timesteps  | 971316   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.98e-07 |
|    n_updates        | 240328   |
----------------------------------
Eval num_timesteps=971500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 971500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.2e-08  |
|    n_updates        | 240374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4800     |
|    fps              | 45       |
|    time_elapsed     | 21590    |
|    total_timesteps  | 971989   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01e-07 |
|    n_updates        | 240497   |
----------------------------------
Eval num_timesteps=972000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 972000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.62e-07 |
|    n_updates        | 240499   |
----------------------------------
Eval num_timesteps=972500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 972500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9e-08    |
|    n_updates        | 240624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4804     |
|    fps              | 45       |
|    time_elapsed     | 21603    |
|    total_timesteps  | 972693   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.76e-05 |
|    n_updates        | 240673   |
----------------------------------
Eval num_timesteps=973000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 973000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.94e-06 |
|    n_updates        | 240749   |
----------------------------------
Eval num_timesteps=973500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 973500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5e-06    |
|    n_updates        | 240874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4808     |
|    fps              | 45       |
|    time_elapsed     | 21616    |
|    total_timesteps  | 973533   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.62e-07 |
|    n_updates        | 240883   |
----------------------------------
Eval num_timesteps=974000, episode_reward=-0.19 +/- 0.17
Episode length: 206.32 +/- 25.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 974000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.14e-07 |
|    n_updates        | 240999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4812     |
|    fps              | 45       |
|    time_elapsed     | 21623    |
|    total_timesteps  | 974373   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.68e-07 |
|    n_updates        | 241093   |
----------------------------------
Eval num_timesteps=974500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 974500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000222 |
|    n_updates        | 241124   |
----------------------------------
Eval num_timesteps=975000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 975000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.45e-07 |
|    n_updates        | 241249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4816     |
|    fps              | 45       |
|    time_elapsed     | 21636    |
|    total_timesteps  | 975118   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.88e-07 |
|    n_updates        | 241279   |
----------------------------------
Eval num_timesteps=975500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 975500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.72e-08 |
|    n_updates        | 241374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4820     |
|    fps              | 45       |
|    time_elapsed     | 21643    |
|    total_timesteps  | 975958   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56e-07 |
|    n_updates        | 241489   |
----------------------------------
Eval num_timesteps=976000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 976000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25e-07 |
|    n_updates        | 241499   |
----------------------------------
Eval num_timesteps=976500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 976500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.02e-07 |
|    n_updates        | 241624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4824     |
|    fps              | 45       |
|    time_elapsed     | 21656    |
|    total_timesteps  | 976798   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3e-07  |
|    n_updates        | 241699   |
----------------------------------
Eval num_timesteps=977000, episode_reward=-0.19 +/- 0.16
Episode length: 207.74 +/- 15.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | -0.188   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 977000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.07e-06 |
|    n_updates        | 241749   |
----------------------------------
Eval num_timesteps=977500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 977500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.09e-07 |
|    n_updates        | 241874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4828     |
|    fps              | 45       |
|    time_elapsed     | 21669    |
|    total_timesteps  | 977638   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000103 |
|    n_updates        | 241909   |
----------------------------------
Eval num_timesteps=978000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 978000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.53e-07 |
|    n_updates        | 241999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4832     |
|    fps              | 45       |
|    time_elapsed     | 21676    |
|    total_timesteps  | 978275   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.98e-05 |
|    n_updates        | 242068   |
----------------------------------
Eval num_timesteps=978500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 978500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.07e-06 |
|    n_updates        | 242124   |
----------------------------------
Eval num_timesteps=979000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 979000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.75e-07 |
|    n_updates        | 242249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4836     |
|    fps              | 45       |
|    time_elapsed     | 21690    |
|    total_timesteps  | 979115   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.46e-05 |
|    n_updates        | 242278   |
----------------------------------
Eval num_timesteps=979500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 979500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.28e-07 |
|    n_updates        | 242374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4840     |
|    fps              | 45       |
|    time_elapsed     | 21696    |
|    total_timesteps  | 979749   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.67e-07 |
|    n_updates        | 242437   |
----------------------------------
Eval num_timesteps=980000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 980000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.89e-05 |
|    n_updates        | 242499   |
----------------------------------
Eval num_timesteps=980500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 980500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.52e-06 |
|    n_updates        | 242624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4844     |
|    fps              | 45       |
|    time_elapsed     | 21709    |
|    total_timesteps  | 980589   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.59e-07 |
|    n_updates        | 242647   |
----------------------------------
Eval num_timesteps=981000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 981000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.86e-07 |
|    n_updates        | 242749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4848     |
|    fps              | 45       |
|    time_elapsed     | 21716    |
|    total_timesteps  | 981429   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.34e-08 |
|    n_updates        | 242857   |
----------------------------------
Eval num_timesteps=981500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 981500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.98e-07 |
|    n_updates        | 242874   |
----------------------------------
Eval num_timesteps=982000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 982000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.96e-05 |
|    n_updates        | 242999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4852     |
|    fps              | 45       |
|    time_elapsed     | 21729    |
|    total_timesteps  | 982165   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.62e-06 |
|    n_updates        | 243041   |
----------------------------------
Eval num_timesteps=982500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 982500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.76e-05 |
|    n_updates        | 243124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4856     |
|    fps              | 45       |
|    time_elapsed     | 21735    |
|    total_timesteps  | 982816   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.21e-07 |
|    n_updates        | 243203   |
----------------------------------
Eval num_timesteps=983000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 983000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.16e-08 |
|    n_updates        | 243249   |
----------------------------------
Eval num_timesteps=983500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 983500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28e-07 |
|    n_updates        | 243374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4860     |
|    fps              | 45       |
|    time_elapsed     | 21748    |
|    total_timesteps  | 983656   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.72e-08 |
|    n_updates        | 243413   |
----------------------------------
Eval num_timesteps=984000, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 984000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.48e-06 |
|    n_updates        | 243499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4864     |
|    fps              | 45       |
|    time_elapsed     | 21755    |
|    total_timesteps  | 984496   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.19e-07 |
|    n_updates        | 243623   |
----------------------------------
Eval num_timesteps=984500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 984500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.74e-07 |
|    n_updates        | 243624   |
----------------------------------
Eval num_timesteps=985000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 985000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.72e-07 |
|    n_updates        | 243749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4868     |
|    fps              | 45       |
|    time_elapsed     | 21768    |
|    total_timesteps  | 985336   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.78e-06 |
|    n_updates        | 243833   |
----------------------------------
Eval num_timesteps=985500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 985500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.97e-07 |
|    n_updates        | 243874   |
----------------------------------
Eval num_timesteps=986000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 986000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.59e-05 |
|    n_updates        | 243999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4872     |
|    fps              | 45       |
|    time_elapsed     | 21781    |
|    total_timesteps  | 986176   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91e-07 |
|    n_updates        | 244043   |
----------------------------------
Eval num_timesteps=986500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 986500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.88e-06 |
|    n_updates        | 244124   |
----------------------------------
Eval num_timesteps=987000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 987000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.19e-07 |
|    n_updates        | 244249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4876     |
|    fps              | 45       |
|    time_elapsed     | 21794    |
|    total_timesteps  | 987016   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.93e-07 |
|    n_updates        | 244253   |
----------------------------------
Eval num_timesteps=987500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 987500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000163 |
|    n_updates        | 244374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4880     |
|    fps              | 45       |
|    time_elapsed     | 21800    |
|    total_timesteps  | 987856   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.55e-05 |
|    n_updates        | 244463   |
----------------------------------
Eval num_timesteps=988000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 988000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.16e-05 |
|    n_updates        | 244499   |
----------------------------------
Eval num_timesteps=988500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 988500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.07e-09 |
|    n_updates        | 244624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4884     |
|    fps              | 45       |
|    time_elapsed     | 21813    |
|    total_timesteps  | 988696   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.04e-07 |
|    n_updates        | 244673   |
----------------------------------
Eval num_timesteps=989000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 989000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.68e-05 |
|    n_updates        | 244749   |
----------------------------------
Eval num_timesteps=989500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 989500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.87e-07 |
|    n_updates        | 244874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4888     |
|    fps              | 45       |
|    time_elapsed     | 21826    |
|    total_timesteps  | 989536   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0135   |
|    n_updates        | 244883   |
----------------------------------
Eval num_timesteps=990000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 990000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.35e-07 |
|    n_updates        | 244999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4892     |
|    fps              | 45       |
|    time_elapsed     | 21833    |
|    total_timesteps  | 990376   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.38e-06 |
|    n_updates        | 245093   |
----------------------------------
Eval num_timesteps=990500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 990500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0135   |
|    n_updates        | 245124   |
----------------------------------
Eval num_timesteps=991000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 991000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.19e-08 |
|    n_updates        | 245249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4896     |
|    fps              | 45       |
|    time_elapsed     | 21846    |
|    total_timesteps  | 991216   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.6e-08  |
|    n_updates        | 245303   |
----------------------------------
Eval num_timesteps=991500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 991500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.66e-05 |
|    n_updates        | 245374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4900     |
|    fps              | 45       |
|    time_elapsed     | 21853    |
|    total_timesteps  | 991999   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.2e-07  |
|    n_updates        | 245499   |
----------------------------------
Eval num_timesteps=992000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 992000   |
---------------------------------
Eval num_timesteps=992500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 992500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.64e-07 |
|    n_updates        | 245624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4904     |
|    fps              | 45       |
|    time_elapsed     | 21865    |
|    total_timesteps  | 992839   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95e-07 |
|    n_updates        | 245709   |
----------------------------------
Eval num_timesteps=993000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 993000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.53e-05 |
|    n_updates        | 245749   |
----------------------------------
Eval num_timesteps=993500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 993500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.09e-07 |
|    n_updates        | 245874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4908     |
|    fps              | 45       |
|    time_elapsed     | 21878    |
|    total_timesteps  | 993679   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26e-07 |
|    n_updates        | 245919   |
----------------------------------
Eval num_timesteps=994000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 994000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88e-07 |
|    n_updates        | 245999   |
----------------------------------
Eval num_timesteps=994500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 994500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.27e-05 |
|    n_updates        | 246124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4912     |
|    fps              | 45       |
|    time_elapsed     | 21891    |
|    total_timesteps  | 994519   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.21e-05 |
|    n_updates        | 246129   |
----------------------------------
Eval num_timesteps=995000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 995000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.38e-07 |
|    n_updates        | 246249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4916     |
|    fps              | 45       |
|    time_elapsed     | 21898    |
|    total_timesteps  | 995359   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64e-06 |
|    n_updates        | 246339   |
----------------------------------
Eval num_timesteps=995500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 995500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.81e-08 |
|    n_updates        | 246374   |
----------------------------------
Eval num_timesteps=996000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 996000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.08e-07 |
|    n_updates        | 246499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4920     |
|    fps              | 45       |
|    time_elapsed     | 21911    |
|    total_timesteps  | 996199   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.89e-07 |
|    n_updates        | 246549   |
----------------------------------
Eval num_timesteps=996500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 996500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.91e-07 |
|    n_updates        | 246624   |
----------------------------------
Eval num_timesteps=997000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 997000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.6e-07  |
|    n_updates        | 246749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4924     |
|    fps              | 45       |
|    time_elapsed     | 21923    |
|    total_timesteps  | 997039   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.06e-07 |
|    n_updates        | 246759   |
----------------------------------
Eval num_timesteps=997500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 997500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.71e-07 |
|    n_updates        | 246874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4928     |
|    fps              | 45       |
|    time_elapsed     | 21930    |
|    total_timesteps  | 997879   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.43e-07 |
|    n_updates        | 246969   |
----------------------------------
Eval num_timesteps=998000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 998000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.93e-07 |
|    n_updates        | 246999   |
----------------------------------
Eval num_timesteps=998500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 998500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.27e-08 |
|    n_updates        | 247124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4932     |
|    fps              | 45       |
|    time_elapsed     | 21943    |
|    total_timesteps  | 998719   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.78e-07 |
|    n_updates        | 247179   |
----------------------------------
Eval num_timesteps=999000, episode_reward=-0.14 +/- 0.28
Episode length: 198.10 +/- 47.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 999000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.62e-06 |
|    n_updates        | 247249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 4936     |
|    fps              | 45       |
|    time_elapsed     | 21949    |
|    total_timesteps  | 999435   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.33e-07 |
|    n_updates        | 247358   |
----------------------------------
Eval num_timesteps=999500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 999500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58e-07 |
|    n_updates        | 247374   |
----------------------------------
Eval num_timesteps=1000000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 1000000  |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.29e-07 |
|    n_updates        | 247499   |
----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/my-way-home/dqn-3-fs(10)-steps(1000000)/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
