/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-9.35 +/- 18.61
Episode length: 47.62 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -9.35    |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-2.04 +/- 31.70
Episode length: 51.44 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=4.83 +/- 38.43
Episode length: 54.60 +/- 18.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | 4.83     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=1.49 +/- 35.83
Episode length: 52.12 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | 1.49     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 76.8     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 1        |
|    time_elapsed    | 14       |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=-18.11 +/- 23.85
Episode length: 47.70 +/- 16.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.7        |
|    mean_reward          | -18.1       |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.006125811 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.000358    |
|    learning_rate        | 0.001       |
|    loss                 | 206         |
|    n_updates            | 1           |
|    policy_gradient_loss | 0.00168     |
|    value_loss           | 305         |
-----------------------------------------
Eval num_timesteps=3000, episode_reward=-19.25 +/- 23.66
Episode length: 52.12 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -19.3    |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-22.01 +/- 20.73
Episode length: 54.68 +/- 18.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -22      |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-18.73 +/- 24.07
Episode length: 49.98 +/- 16.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 88.9     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 2        |
|    time_elapsed    | 27       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=99.37 +/- 56.97
Episode length: 188.46 +/- 210.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 188         |
|    mean_reward          | 99.4        |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.009175381 |
|    clip_fraction        | 0.0535      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.000283    |
|    learning_rate        | 0.001       |
|    loss                 | 70.8        |
|    n_updates            | 9           |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 186         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=5000, episode_reward=80.05 +/- 60.81
Episode length: 157.66 +/- 195.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 158      |
|    mean_reward     | 80       |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=90.07 +/- 54.68
Episode length: 166.10 +/- 202.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 90.1     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=88.41 +/- 59.97
Episode length: 175.62 +/- 207.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 88.4     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 83.8     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 3        |
|    time_elapsed    | 51       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=6500, episode_reward=65.51 +/- 23.35
Episode length: 65.70 +/- 24.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 65.7         |
|    mean_reward          | 65.5         |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.0063886843 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.00127      |
|    learning_rate        | 0.001        |
|    loss                 | 194          |
|    n_updates            | 10           |
|    policy_gradient_loss | 0.00289      |
|    value_loss           | 305          |
------------------------------------------
Eval num_timesteps=7000, episode_reward=69.57 +/- 18.84
Episode length: 58.52 +/- 23.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.5     |
|    mean_reward     | 69.6     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=66.13 +/- 22.89
Episode length: 62.06 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.1     |
|    mean_reward     | 66.1     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=62.09 +/- 30.70
Episode length: 55.36 +/- 21.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.4     |
|    mean_reward     | 62.1     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 80.2     |
| time/              |          |
|    fps             | 132      |
|    iterations      | 4        |
|    time_elapsed    | 61       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=8500, episode_reward=-24.41 +/- 4.71
Episode length: 49.06 +/- 16.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.1        |
|    mean_reward          | -24.4       |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.017773215 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.00213     |
|    learning_rate        | 0.001       |
|    loss                 | 190         |
|    n_updates            | 11          |
|    policy_gradient_loss | 0.0126      |
|    value_loss           | 327         |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=-14.99 +/- 30.28
Episode length: 50.86 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -15      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-20.59 +/- 19.17
Episode length: 49.90 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -20.6    |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-19.97 +/- 19.73
Episode length: 47.30 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -20      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 80.6     |
| time/              |          |
|    fps             | 145      |
|    iterations      | 5        |
|    time_elapsed    | 70       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=10500, episode_reward=-20.93 +/- 19.43
Episode length: 50.52 +/- 16.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.5        |
|    mean_reward          | -20.9       |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.011499359 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.00137     |
|    learning_rate        | 0.001       |
|    loss                 | 109         |
|    n_updates            | 12          |
|    policy_gradient_loss | 0.00345     |
|    value_loss           | 183         |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=-22.99 +/- 15.09
Episode length: 50.82 +/- 15.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -23      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-17.83 +/- 24.50
Episode length: 46.70 +/- 12.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -17.8    |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-25.40 +/- 14.66
Episode length: 59.96 +/- 20.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60       |
|    mean_reward     | -25.4    |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 81.7     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 6        |
|    time_elapsed    | 79       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=12500, episode_reward=-17.44 +/- 27.50
Episode length: 52.60 +/- 18.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.6        |
|    mean_reward          | -17.4       |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.029599788 |
|    clip_fraction        | 0.379       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.00419     |
|    learning_rate        | 0.001       |
|    loss                 | 105         |
|    n_updates            | 13          |
|    policy_gradient_loss | -0.00541    |
|    value_loss           | 336         |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=-21.15 +/- 19.88
Episode length: 51.64 +/- 19.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -21.2    |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-20.65 +/- 20.88
Episode length: 50.14 +/- 20.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -20.7    |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-18.92 +/- 23.13
Episode length: 50.90 +/- 15.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -18.9    |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 78.5     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 7        |
|    time_elapsed    | 87       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=14500, episode_reward=-23.25 +/- 14.61
Episode length: 51.50 +/- 15.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.5        |
|    mean_reward          | -23.3       |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.011678693 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.00241     |
|    learning_rate        | 0.001       |
|    loss                 | 129         |
|    n_updates            | 14          |
|    policy_gradient_loss | 0.00334     |
|    value_loss           | 330         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-18.82 +/- 23.64
Episode length: 50.58 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -18.8    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-14.94 +/- 29.36
Episode length: 51.20 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -14.9    |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-21.15 +/- 20.56
Episode length: 51.48 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -21.2    |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.6     |
|    ep_rew_mean     | 73       |
| time/              |          |
|    fps             | 169      |
|    iterations      | 8        |
|    time_elapsed    | 96       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=16500, episode_reward=73.99 +/- 5.01
Episode length: 56.44 +/- 18.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 56.4       |
|    mean_reward          | 74         |
| time/                   |            |
|    total_timesteps      | 16500      |
| train/                  |            |
|    approx_kl            | 0.03621458 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.795     |
|    explained_variance   | 0.00808    |
|    learning_rate        | 0.001      |
|    loss                 | 166        |
|    n_updates            | 15         |
|    policy_gradient_loss | 0.0142     |
|    value_loss           | 384        |
----------------------------------------
Eval num_timesteps=17000, episode_reward=72.33 +/- 4.96
Episode length: 63.84 +/- 22.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.8     |
|    mean_reward     | 72.3     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=72.23 +/- 5.86
Episode length: 63.18 +/- 21.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.2     |
|    mean_reward     | 72.2     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=72.23 +/- 5.58
Episode length: 63.70 +/- 22.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.7     |
|    mean_reward     | 72.2     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.5     |
|    ep_rew_mean     | 62.5     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 9        |
|    time_elapsed    | 106      |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=18500, episode_reward=111.89 +/- 73.14
Episode length: 304.98 +/- 238.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 305         |
|    mean_reward          | 112         |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.046782926 |
|    clip_fraction        | 0.496       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.0248      |
|    learning_rate        | 0.001       |
|    loss                 | 204         |
|    n_updates            | 16          |
|    policy_gradient_loss | 0.0314      |
|    value_loss           | 370         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=19000, episode_reward=106.37 +/- 79.00
Episode length: 302.88 +/- 240.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | 106      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=101.78 +/- 78.33
Episode length: 288.88 +/- 236.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | 102      |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=92.99 +/- 79.36
Episode length: 255.90 +/- 238.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 93       |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 61.9     |
|    ep_rew_mean     | 48.2     |
| time/              |          |
|    fps             | 140      |
|    iterations      | 10       |
|    time_elapsed    | 146      |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=20500, episode_reward=16.29 +/- 49.36
Episode length: 68.96 +/- 75.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 69         |
|    mean_reward          | 16.3       |
| time/                   |            |
|    total_timesteps      | 20500      |
| train/                  |            |
|    approx_kl            | 0.01771956 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.0303     |
|    learning_rate        | 0.001      |
|    loss                 | 98.1       |
|    n_updates            | 17         |
|    policy_gradient_loss | -0.000401  |
|    value_loss           | 279        |
----------------------------------------
Eval num_timesteps=21000, episode_reward=14.09 +/- 43.46
Episode length: 48.66 +/- 19.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=22.23 +/- 51.59
Episode length: 60.32 +/- 68.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=16.07 +/- 44.49
Episode length: 49.08 +/- 19.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=18.06 +/- 45.39
Episode length: 57.36 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.4     |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.4     |
|    ep_rew_mean     | 43.4     |
| time/              |          |
|    fps             | 143      |
|    iterations      | 11       |
|    time_elapsed    | 157      |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=23000, episode_reward=-7.63 +/- 19.40
Episode length: 48.88 +/- 17.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.9        |
|    mean_reward          | -7.63       |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.012899013 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.0386      |
|    learning_rate        | 0.001       |
|    loss                 | 141         |
|    n_updates            | 18          |
|    policy_gradient_loss | 0.0145      |
|    value_loss           | 268         |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=-9.61 +/- 13.86
Episode length: 50.82 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -9.61    |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-11.59 +/- 0.00
Episode length: 49.94 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-9.62 +/- 13.86
Episode length: 50.98 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -9.62    |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.5     |
|    ep_rew_mean     | 45.8     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 12       |
|    time_elapsed    | 165      |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=25000, episode_reward=33.11 +/- 71.77
Episode length: 108.60 +/- 155.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 33.1        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.014652516 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.0367      |
|    learning_rate        | 0.001       |
|    loss                 | 127         |
|    n_updates            | 19          |
|    policy_gradient_loss | 0.00302     |
|    value_loss           | 272         |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=23.99 +/- 47.52
Episode length: 56.02 +/- 22.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=27.95 +/- 48.55
Episode length: 55.86 +/- 20.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.9     |
|    mean_reward     | 27.9     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=28.09 +/- 52.98
Episode length: 62.80 +/- 73.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.8     |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.2     |
|    ep_rew_mean     | 43.1     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 13       |
|    time_elapsed    | 176      |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=27000, episode_reward=-11.92 +/- 0.55
Episode length: 50.58 +/- 16.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.6        |
|    mean_reward          | -11.9       |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.009153807 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.0405      |
|    learning_rate        | 0.001       |
|    loss                 | 113         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00216     |
|    value_loss           | 342         |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=-11.75 +/- 0.42
Episode length: 54.92 +/- 18.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.9     |
|    mean_reward     | -11.8    |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-11.81 +/- 0.46
Episode length: 50.84 +/- 17.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -11.8    |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-11.95 +/- 0.74
Episode length: 49.90 +/- 19.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -12      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.3     |
|    ep_rew_mean     | 53       |
| time/              |          |
|    fps             | 154      |
|    iterations      | 14       |
|    time_elapsed    | 185      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=29000, episode_reward=-14.15 +/- 1.27
Episode length: 51.22 +/- 18.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.2        |
|    mean_reward          | -14.2       |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.010852754 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.048       |
|    learning_rate        | 0.001       |
|    loss                 | 238         |
|    n_updates            | 21          |
|    policy_gradient_loss | 0.00405     |
|    value_loss           | 445         |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=-14.03 +/- 1.20
Episode length: 48.68 +/- 17.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -14      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-14.07 +/- 1.02
Episode length: 48.54 +/- 20.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -14.1    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-13.77 +/- 0.99
Episode length: 48.92 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -13.8    |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 54.6     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 15       |
|    time_elapsed    | 194      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=31000, episode_reward=-11.61 +/- 0.14
Episode length: 53.00 +/- 21.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53           |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0107877515 |
|    clip_fraction        | 0.195        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.0398       |
|    learning_rate        | 0.001        |
|    loss                 | 120          |
|    n_updates            | 22           |
|    policy_gradient_loss | 0.0052       |
|    value_loss           | 183          |
------------------------------------------
Eval num_timesteps=31500, episode_reward=-11.63 +/- 0.20
Episode length: 44.98 +/- 13.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-11.61 +/- 0.14
Episode length: 49.12 +/- 18.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-11.60 +/- 0.00
Episode length: 49.20 +/- 14.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.9     |
|    ep_rew_mean     | 58.1     |
| time/              |          |
|    fps             | 161      |
|    iterations      | 16       |
|    time_elapsed    | 203      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=33000, episode_reward=-11.60 +/- 0.00
Episode length: 47.20 +/- 15.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.2        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.007850558 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.0472      |
|    learning_rate        | 0.001       |
|    loss                 | 116         |
|    n_updates            | 23          |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 251         |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=-11.59 +/- 0.00
Episode length: 56.00 +/- 18.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-11.59 +/- 0.00
Episode length: 50.52 +/- 19.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-11.59 +/- 0.00
Episode length: 48.46 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.9     |
|    ep_rew_mean     | 60       |
| time/              |          |
|    fps             | 164      |
|    iterations      | 17       |
|    time_elapsed    | 211      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=35000, episode_reward=-11.59 +/- 0.01
Episode length: 49.84 +/- 19.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.8       |
|    mean_reward          | -11.6      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.01994764 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.939     |
|    explained_variance   | 0.046      |
|    learning_rate        | 0.001      |
|    loss                 | 170        |
|    n_updates            | 24         |
|    policy_gradient_loss | 0.00352    |
|    value_loss           | 301        |
----------------------------------------
Eval num_timesteps=35500, episode_reward=-11.59 +/- 0.00
Episode length: 50.56 +/- 18.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-11.59 +/- 0.01
Episode length: 50.98 +/- 19.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-11.60 +/- 0.00
Episode length: 52.70 +/- 14.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.8     |
|    ep_rew_mean     | 58.3     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 18       |
|    time_elapsed    | 220      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=37000, episode_reward=-11.59 +/- 0.01
Episode length: 47.02 +/- 15.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47         |
|    mean_reward          | -11.6      |
| time/                   |            |
|    total_timesteps      | 37000      |
| train/                  |            |
|    approx_kl            | 0.02370396 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.714     |
|    explained_variance   | 0.031      |
|    learning_rate        | 0.001      |
|    loss                 | 80.8       |
|    n_updates            | 25         |
|    policy_gradient_loss | 0.000999   |
|    value_loss           | 251        |
----------------------------------------
Eval num_timesteps=37500, episode_reward=-11.60 +/- 0.00
Episode length: 52.18 +/- 17.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-11.59 +/- 0.00
Episode length: 53.20 +/- 16.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-11.59 +/- 0.00
Episode length: 51.08 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.9     |
|    ep_rew_mean     | 39       |
| time/              |          |
|    fps             | 169      |
|    iterations      | 19       |
|    time_elapsed    | 229      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=39000, episode_reward=-11.59 +/- 0.00
Episode length: 49.70 +/- 15.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.7         |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0134203825 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.561       |
|    explained_variance   | 0.0106       |
|    learning_rate        | 0.001        |
|    loss                 | 115          |
|    n_updates            | 26           |
|    policy_gradient_loss | 0.013        |
|    value_loss           | 228          |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-11.59 +/- 0.00
Episode length: 48.50 +/- 18.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-11.59 +/- 0.00
Episode length: 51.00 +/- 16.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-11.59 +/- 0.00
Episode length: 51.74 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.3     |
|    ep_rew_mean     | 30.1     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 20       |
|    time_elapsed    | 237      |
|    total_timesteps | 40960    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=41000, episode_reward=-17.49 +/- 2.11
Episode length: 52.08 +/- 17.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.1        |
|    mean_reward          | -17.5       |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.040689517 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.832      |
|    explained_variance   | 0.0246      |
|    learning_rate        | 0.001       |
|    loss                 | 198         |
|    n_updates            | 27          |
|    policy_gradient_loss | 0.0176      |
|    value_loss           | 341         |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=-17.11 +/- 1.99
Episode length: 50.02 +/- 18.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -17.1    |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-16.91 +/- 2.29
Episode length: 49.24 +/- 17.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -16.9    |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-18.07 +/- 2.48
Episode length: 55.72 +/- 17.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.7     |
|    mean_reward     | -18.1    |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-17.35 +/- 2.29
Episode length: 50.78 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -17.4    |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57       |
|    ep_rew_mean     | 39.8     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 21       |
|    time_elapsed    | 248      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=43500, episode_reward=-20.87 +/- 14.79
Episode length: 51.86 +/- 15.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.9        |
|    mean_reward          | -20.9       |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.020769332 |
|    clip_fraction        | 0.367       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.115       |
|    learning_rate        | 0.001       |
|    loss                 | 237         |
|    n_updates            | 28          |
|    policy_gradient_loss | 0.0223      |
|    value_loss           | 401         |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=-20.28 +/- 14.26
Episode length: 50.46 +/- 18.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -20.3    |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-19.47 +/- 14.77
Episode length: 46.58 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -19.5    |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-22.15 +/- 4.49
Episode length: 49.24 +/- 19.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -22.2    |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 59.8     |
|    ep_rew_mean     | 54.9     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 22       |
|    time_elapsed    | 257      |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=45500, episode_reward=72.85 +/- 5.36
Episode length: 60.70 +/- 20.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60.7        |
|    mean_reward          | 72.8        |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.012463786 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.149       |
|    learning_rate        | 0.001       |
|    loss                 | 290         |
|    n_updates            | 29          |
|    policy_gradient_loss | 0.0118      |
|    value_loss           | 475         |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=72.23 +/- 6.03
Episode length: 63.26 +/- 23.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.3     |
|    mean_reward     | 72.2     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=72.25 +/- 6.15
Episode length: 65.42 +/- 28.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.4     |
|    mean_reward     | 72.2     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=71.99 +/- 5.64
Episode length: 65.04 +/- 24.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65       |
|    mean_reward     | 72       |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 61.4     |
|    ep_rew_mean     | 61.8     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 23       |
|    time_elapsed    | 267      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=47500, episode_reward=75.29 +/- 4.42
Episode length: 65.50 +/- 21.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 65.5        |
|    mean_reward          | 75.3        |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.008052672 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.167       |
|    learning_rate        | 0.001       |
|    loss                 | 190         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00552    |
|    value_loss           | 392         |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=75.99 +/- 4.80
Episode length: 62.26 +/- 23.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.3     |
|    mean_reward     | 76       |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=75.81 +/- 4.32
Episode length: 62.60 +/- 20.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.6     |
|    mean_reward     | 75.8     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=75.61 +/- 4.92
Episode length: 64.20 +/- 24.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.2     |
|    mean_reward     | 75.6     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 59.1     |
|    ep_rew_mean     | 61.4     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 24       |
|    time_elapsed    | 277      |
|    total_timesteps | 49152    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=49500, episode_reward=73.03 +/- 5.93
Episode length: 62.18 +/- 21.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 62.2         |
|    mean_reward          | 73           |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0057967165 |
|    clip_fraction        | 0.0737       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.001        |
|    loss                 | 145          |
|    n_updates            | 31           |
|    policy_gradient_loss | -0.00493     |
|    value_loss           | 435          |
------------------------------------------
Eval num_timesteps=50000, episode_reward=72.00 +/- 5.57
Episode length: 64.98 +/- 22.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65       |
|    mean_reward     | 72       |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=72.51 +/- 6.20
Episode length: 63.30 +/- 23.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.3     |
|    mean_reward     | 72.5     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=74.33 +/- 4.41
Episode length: 55.98 +/- 16.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 74.3     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.1     |
|    ep_rew_mean     | 61.6     |
| time/              |          |
|    fps             | 177      |
|    iterations      | 25       |
|    time_elapsed    | 287      |
|    total_timesteps | 51200    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=51500, episode_reward=71.71 +/- 6.03
Episode length: 65.66 +/- 23.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 65.7        |
|    mean_reward          | 71.7        |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.009123536 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.001       |
|    loss                 | 251         |
|    n_updates            | 32          |
|    policy_gradient_loss | 0.000819    |
|    value_loss           | 362         |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=72.41 +/- 5.90
Episode length: 63.42 +/- 24.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.4     |
|    mean_reward     | 72.4     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=72.31 +/- 5.59
Episode length: 64.60 +/- 25.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.6     |
|    mean_reward     | 72.3     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=71.57 +/- 5.92
Episode length: 65.88 +/- 22.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.9     |
|    mean_reward     | 71.6     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 61       |
|    ep_rew_mean     | 61.7     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 26       |
|    time_elapsed    | 298      |
|    total_timesteps | 53248    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=53500, episode_reward=66.99 +/- 23.77
Episode length: 61.80 +/- 22.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 61.8        |
|    mean_reward          | 67          |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.010021551 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.001       |
|    loss                 | 233         |
|    n_updates            | 33          |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 389         |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=58.90 +/- 33.97
Episode length: 61.80 +/- 18.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.8     |
|    mean_reward     | 58.9     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=62.61 +/- 30.05
Episode length: 64.72 +/- 25.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.7     |
|    mean_reward     | 62.6     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=65.61 +/- 23.93
Episode length: 67.60 +/- 20.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.6     |
|    mean_reward     | 65.6     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 62.3     |
|    ep_rew_mean     | 62.2     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 27       |
|    time_elapsed    | 308      |
|    total_timesteps | 55296    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=55500, episode_reward=63.71 +/- 26.59
Episode length: 68.56 +/- 30.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.6        |
|    mean_reward          | 63.7        |
| time/                   |             |
|    total_timesteps      | 55500       |
| train/                  |             |
|    approx_kl            | 0.009892882 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.001       |
|    loss                 | 137         |
|    n_updates            | 34          |
|    policy_gradient_loss | 0.00671     |
|    value_loss           | 325         |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=59.11 +/- 33.75
Episode length: 59.14 +/- 19.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 59.1     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=65.63 +/- 23.70
Episode length: 65.42 +/- 21.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.4     |
|    mean_reward     | 65.6     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=65.95 +/- 27.22
Episode length: 57.40 +/- 21.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.4     |
|    mean_reward     | 65.9     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 61.7     |
|    ep_rew_mean     | 65.7     |
| time/              |          |
|    fps             | 179      |
|    iterations      | 28       |
|    time_elapsed    | 319      |
|    total_timesteps | 57344    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=57500, episode_reward=62.15 +/- 32.84
Episode length: 59.18 +/- 27.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59.2        |
|    mean_reward          | 62.1        |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.007473211 |
|    clip_fraction        | 0.0996      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.001       |
|    loss                 | 127         |
|    n_updates            | 35          |
|    policy_gradient_loss | -6.9e-05    |
|    value_loss           | 299         |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=66.31 +/- 24.36
Episode length: 62.92 +/- 20.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.9     |
|    mean_reward     | 66.3     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=63.57 +/- 29.38
Episode length: 58.74 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.7     |
|    mean_reward     | 63.6     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=64.29 +/- 27.76
Episode length: 63.08 +/- 21.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.1     |
|    mean_reward     | 64.3     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 64.6     |
|    ep_rew_mean     | 67.7     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 29       |
|    time_elapsed    | 329      |
|    total_timesteps | 59392    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=59500, episode_reward=71.87 +/- 5.08
Episode length: 65.10 +/- 21.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 65.1         |
|    mean_reward          | 71.9         |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0093269665 |
|    clip_fraction        | 0.152        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.469        |
|    learning_rate        | 0.001        |
|    loss                 | 129          |
|    n_updates            | 36           |
|    policy_gradient_loss | -0.00616     |
|    value_loss           | 235          |
------------------------------------------
Eval num_timesteps=60000, episode_reward=72.37 +/- 5.51
Episode length: 62.68 +/- 21.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.7     |
|    mean_reward     | 72.4     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=71.49 +/- 5.65
Episode length: 66.36 +/- 22.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.4     |
|    mean_reward     | 71.5     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=73.39 +/- 5.73
Episode length: 58.14 +/- 20.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | 73.4     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 64.5     |
|    ep_rew_mean     | 66.5     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 30       |
|    time_elapsed    | 339      |
|    total_timesteps | 61440    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=61500, episode_reward=71.23 +/- 5.93
Episode length: 66.46 +/- 22.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 66.5        |
|    mean_reward          | 71.2        |
| time/                   |             |
|    total_timesteps      | 61500       |
| train/                  |             |
|    approx_kl            | 0.018209312 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.001       |
|    loss                 | 262         |
|    n_updates            | 37          |
|    policy_gradient_loss | 0.0111      |
|    value_loss           | 385         |
-----------------------------------------
Eval num_timesteps=62000, episode_reward=73.69 +/- 5.36
Episode length: 58.16 +/- 21.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.2     |
|    mean_reward     | 73.7     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=72.15 +/- 5.60
Episode length: 63.48 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.5     |
|    mean_reward     | 72.1     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=72.91 +/- 5.18
Episode length: 62.28 +/- 24.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.3     |
|    mean_reward     | 72.9     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.3     |
|    ep_rew_mean     | 70.9     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 31       |
|    time_elapsed    | 349      |
|    total_timesteps | 63488    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=63500, episode_reward=72.35 +/- 5.10
Episode length: 63.34 +/- 20.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 63.3         |
|    mean_reward          | 72.3         |
| time/                   |              |
|    total_timesteps      | 63500        |
| train/                  |              |
|    approx_kl            | 0.0080606565 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.438        |
|    learning_rate        | 0.001        |
|    loss                 | 115          |
|    n_updates            | 38           |
|    policy_gradient_loss | -0.000171    |
|    value_loss           | 271          |
------------------------------------------
Eval num_timesteps=64000, episode_reward=71.97 +/- 5.60
Episode length: 64.00 +/- 20.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64       |
|    mean_reward     | 72       |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=72.75 +/- 5.86
Episode length: 61.64 +/- 22.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.6     |
|    mean_reward     | 72.7     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=71.19 +/- 5.53
Episode length: 67.18 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.2     |
|    mean_reward     | 71.2     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=72.21 +/- 5.54
Episode length: 63.88 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.9     |
|    mean_reward     | 72.2     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.5     |
|    ep_rew_mean     | 74.5     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 32       |
|    time_elapsed    | 362      |
|    total_timesteps | 65536    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=66000, episode_reward=71.71 +/- 5.71
Episode length: 69.92 +/- 26.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 69.9        |
|    mean_reward          | 71.7        |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.016654082 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.001       |
|    loss                 | 22          |
|    n_updates            | 39          |
|    policy_gradient_loss | 0.00203     |
|    value_loss           | 109         |
-----------------------------------------
Eval num_timesteps=66500, episode_reward=74.79 +/- 5.76
Episode length: 57.70 +/- 23.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.7     |
|    mean_reward     | 74.8     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=73.81 +/- 5.32
Episode length: 59.74 +/- 22.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.7     |
|    mean_reward     | 73.8     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=71.66 +/- 5.57
Episode length: 69.20 +/- 24.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.2     |
|    mean_reward     | 71.7     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 79.4     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 33       |
|    time_elapsed    | 373      |
|    total_timesteps | 67584    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=68000, episode_reward=78.99 +/- 4.07
Episode length: 60.36 +/- 21.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60.4        |
|    mean_reward          | 79          |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.015825294 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.001       |
|    loss                 | 13.2        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00479    |
|    value_loss           | 108         |
-----------------------------------------
Eval num_timesteps=68500, episode_reward=77.97 +/- 3.18
Episode length: 63.56 +/- 20.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.6     |
|    mean_reward     | 78       |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=78.45 +/- 3.86
Episode length: 60.54 +/- 20.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.5     |
|    mean_reward     | 78.4     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=78.01 +/- 4.10
Episode length: 66.00 +/- 25.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66       |
|    mean_reward     | 78       |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 84.9     |
| time/              |          |
|    fps             | 181      |
|    iterations      | 34       |
|    time_elapsed    | 383      |
|    total_timesteps | 69632    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=70000, episode_reward=70.75 +/- 34.06
Episode length: 54.70 +/- 20.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.7        |
|    mean_reward          | 70.7        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.011355785 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.001       |
|    loss                 | 53.3        |
|    n_updates            | 41          |
|    policy_gradient_loss | 0.0108      |
|    value_loss           | 174         |
-----------------------------------------
Eval num_timesteps=70500, episode_reward=74.23 +/- 29.40
Episode length: 60.74 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.7     |
|    mean_reward     | 74.2     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=78.35 +/- 23.12
Episode length: 60.62 +/- 21.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.6     |
|    mean_reward     | 78.3     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=74.77 +/- 29.22
Episode length: 60.08 +/- 24.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.1     |
|    mean_reward     | 74.8     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 88.7     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 35       |
|    time_elapsed    | 393      |
|    total_timesteps | 71680    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=72000, episode_reward=86.05 +/- 0.59
Episode length: 61.30 +/- 24.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 61.3        |
|    mean_reward          | 86          |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.010981232 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.208       |
|    learning_rate        | 0.001       |
|    loss                 | 120         |
|    n_updates            | 42          |
|    policy_gradient_loss | 0.00563     |
|    value_loss           | 298         |
-----------------------------------------
Eval num_timesteps=72500, episode_reward=86.17 +/- 0.55
Episode length: 58.52 +/- 18.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.5     |
|    mean_reward     | 86.2     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=86.07 +/- 0.76
Episode length: 61.42 +/- 28.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.4     |
|    mean_reward     | 86.1     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=86.17 +/- 0.55
Episode length: 64.56 +/- 26.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.6     |
|    mean_reward     | 86.2     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 91.8     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 36       |
|    time_elapsed    | 403      |
|    total_timesteps | 73728    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=74000, episode_reward=90.36 +/- 20.82
Episode length: 82.06 +/- 92.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 82.1         |
|    mean_reward          | 90.4         |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0086945165 |
|    clip_fraction        | 0.0781       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.256        |
|    learning_rate        | 0.001        |
|    loss                 | 116          |
|    n_updates            | 43           |
|    policy_gradient_loss | 0.00244      |
|    value_loss           | 166          |
------------------------------------------
Eval num_timesteps=74500, episode_reward=91.42 +/- 20.97
Episode length: 95.14 +/- 111.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.1     |
|    mean_reward     | 91.4     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=88.14 +/- 15.44
Episode length: 74.38 +/- 68.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 88.1     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=87.94 +/- 12.33
Episode length: 79.04 +/- 68.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79       |
|    mean_reward     | 87.9     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 95.8     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 37       |
|    time_elapsed    | 416      |
|    total_timesteps | 75776    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=76000, episode_reward=87.41 +/- 0.00
Episode length: 58.76 +/- 21.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 58.8        |
|    mean_reward          | 87.4        |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.010864481 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.001       |
|    loss                 | 40.7        |
|    n_updates            | 44          |
|    policy_gradient_loss | 0.00924     |
|    value_loss           | 154         |
-----------------------------------------
Eval num_timesteps=76500, episode_reward=87.37 +/- 0.20
Episode length: 62.82 +/- 20.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.8     |
|    mean_reward     | 87.4     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=87.37 +/- 0.20
Episode length: 62.04 +/- 23.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62       |
|    mean_reward     | 87.4     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=87.39 +/- 0.14
Episode length: 61.70 +/- 19.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.7     |
|    mean_reward     | 87.4     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 99.8     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 38       |
|    time_elapsed    | 426      |
|    total_timesteps | 77824    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=78000, episode_reward=-11.59 +/- 0.00
Episode length: 49.04 +/- 18.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49           |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0074490486 |
|    clip_fraction        | 0.0918       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.213        |
|    learning_rate        | 0.001        |
|    loss                 | 27.5         |
|    n_updates            | 45           |
|    policy_gradient_loss | 0.00991      |
|    value_loss           | 73.6         |
------------------------------------------
Eval num_timesteps=78500, episode_reward=-11.60 +/- 0.00
Episode length: 47.68 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-11.59 +/- 0.00
Episode length: 50.54 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-11.59 +/- 0.00
Episode length: 49.28 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 99.3     |
| time/              |          |
|    fps             | 183      |
|    iterations      | 39       |
|    time_elapsed    | 434      |
|    total_timesteps | 79872    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=80000, episode_reward=-11.59 +/- 0.00
Episode length: 49.40 +/- 16.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.4         |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0071738265 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.0654       |
|    learning_rate        | 0.001        |
|    loss                 | 76.5         |
|    n_updates            | 46           |
|    policy_gradient_loss | 0.00911      |
|    value_loss           | 208          |
------------------------------------------
Eval num_timesteps=80500, episode_reward=-11.59 +/- 0.00
Episode length: 50.54 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-11.60 +/- 0.00
Episode length: 53.34 +/- 17.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-11.59 +/- 0.01
Episode length: 49.76 +/- 15.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 84.1     |
| time/              |          |
|    fps             | 184      |
|    iterations      | 40       |
|    time_elapsed    | 443      |
|    total_timesteps | 81920    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=82000, episode_reward=-11.59 +/- 0.00
Episode length: 50.20 +/- 15.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.2         |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0076392624 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.057        |
|    learning_rate        | 0.001        |
|    loss                 | 28.3         |
|    n_updates            | 47           |
|    policy_gradient_loss | 0.00595      |
|    value_loss           | 158          |
------------------------------------------
Eval num_timesteps=82500, episode_reward=-11.60 +/- 0.00
Episode length: 47.04 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-11.60 +/- 0.00
Episode length: 53.68 +/- 17.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-11.59 +/- 0.00
Episode length: 49.28 +/- 13.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 67.5     |
| time/              |          |
|    fps             | 185      |
|    iterations      | 41       |
|    time_elapsed    | 451      |
|    total_timesteps | 83968    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=84000, episode_reward=-11.60 +/- 0.00
Episode length: 52.48 +/- 18.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.5        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.006587035 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.137       |
|    learning_rate        | 0.001       |
|    loss                 | 149         |
|    n_updates            | 48          |
|    policy_gradient_loss | -0.00941    |
|    value_loss           | 292         |
-----------------------------------------
Eval num_timesteps=84500, episode_reward=-11.59 +/- 0.00
Episode length: 49.54 +/- 15.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-11.59 +/- 0.00
Episode length: 48.78 +/- 15.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-11.59 +/- 0.00
Episode length: 46.50 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-11.60 +/- 0.00
Episode length: 49.86 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 70.2     |
| time/              |          |
|    fps             | 186      |
|    iterations      | 42       |
|    time_elapsed    | 461      |
|    total_timesteps | 86016    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=86500, episode_reward=-11.60 +/- 0.00
Episode length: 50.16 +/- 16.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.2        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.006769049 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.0919      |
|    learning_rate        | 0.001       |
|    loss                 | 22.1        |
|    n_updates            | 49          |
|    policy_gradient_loss | 0.00437     |
|    value_loss           | 99          |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=-11.59 +/- 0.00
Episode length: 49.10 +/- 14.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-11.59 +/- 0.00
Episode length: 43.92 +/- 13.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-11.59 +/- 0.00
Episode length: 51.40 +/- 18.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 68.2     |
| time/              |          |
|    fps             | 187      |
|    iterations      | 43       |
|    time_elapsed    | 470      |
|    total_timesteps | 88064    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=88500, episode_reward=81.47 +/- 23.51
Episode length: 67.88 +/- 24.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 67.9         |
|    mean_reward          | 81.5         |
| time/                   |              |
|    total_timesteps      | 88500        |
| train/                  |              |
|    approx_kl            | 0.0064208107 |
|    clip_fraction        | 0.0797       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.196        |
|    learning_rate        | 0.001        |
|    loss                 | 62           |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00404     |
|    value_loss           | 147          |
------------------------------------------
Eval num_timesteps=89000, episode_reward=85.43 +/- 13.86
Episode length: 63.96 +/- 25.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64       |
|    mean_reward     | 85.4     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=85.43 +/- 13.86
Episode length: 67.04 +/- 24.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67       |
|    mean_reward     | 85.4     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=85.43 +/- 13.86
Episode length: 59.14 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 85.4     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 62.5     |
| time/              |          |
|    fps             | 187      |
|    iterations      | 44       |
|    time_elapsed    | 481      |
|    total_timesteps | 90112    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=90500, episode_reward=77.05 +/- 29.88
Episode length: 61.66 +/- 23.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 61.7       |
|    mean_reward          | 77         |
| time/                   |            |
|    total_timesteps      | 90500      |
| train/                  |            |
|    approx_kl            | 0.00944781 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.193      |
|    learning_rate        | 0.001      |
|    loss                 | 137        |
|    n_updates            | 51         |
|    policy_gradient_loss | 0.00022    |
|    value_loss           | 135        |
----------------------------------------
Eval num_timesteps=91000, episode_reward=74.95 +/- 32.39
Episode length: 58.18 +/- 23.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.2     |
|    mean_reward     | 74.9     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=83.07 +/- 19.53
Episode length: 67.90 +/- 24.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.9     |
|    mean_reward     | 83.1     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=84.91 +/- 13.94
Episode length: 58.34 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.3     |
|    mean_reward     | 84.9     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 58.7     |
| time/              |          |
|    fps             | 187      |
|    iterations      | 45       |
|    time_elapsed    | 491      |
|    total_timesteps | 92160    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=92500, episode_reward=97.83 +/- 32.73
Episode length: 106.24 +/- 141.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 97.8        |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.007500808 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.001       |
|    loss                 | 61.6        |
|    n_updates            | 52          |
|    policy_gradient_loss | 0.00608     |
|    value_loss           | 121         |
-----------------------------------------
Eval num_timesteps=93000, episode_reward=110.79 +/- 45.04
Episode length: 163.82 +/- 193.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 111      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=117.35 +/- 48.95
Episode length: 193.20 +/- 208.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 117      |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
New best mean reward!
Eval num_timesteps=94000, episode_reward=97.81 +/- 32.54
Episode length: 103.50 +/- 141.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 97.8     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 61.9     |
| time/              |          |
|    fps             | 183      |
|    iterations      | 46       |
|    time_elapsed    | 513      |
|    total_timesteps | 94208    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=94500, episode_reward=192.94 +/- 22.57
Episode length: 506.50 +/- 90.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 94500       |
| train/                  |             |
|    approx_kl            | 0.007267769 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.001       |
|    loss                 | 114         |
|    n_updates            | 53          |
|    policy_gradient_loss | 0.00396     |
|    value_loss           | 265         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=95000, episode_reward=195.56 +/- 15.60
Episode length: 515.90 +/- 63.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
New best mean reward!
Eval num_timesteps=95500, episode_reward=197.74 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
New best mean reward!
Eval num_timesteps=96000, episode_reward=197.49 +/- 1.04
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 66.4     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 47       |
|    time_elapsed    | 583      |
|    total_timesteps | 96256    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=96500, episode_reward=144.20 +/- 48.01
Episode length: 433.78 +/- 156.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 434          |
|    mean_reward          | 144          |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0073267394 |
|    clip_fraction        | 0.0791       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.238        |
|    learning_rate        | 0.001        |
|    loss                 | 26.6         |
|    n_updates            | 54           |
|    policy_gradient_loss | 0.00617      |
|    value_loss           | 113          |
------------------------------------------
Eval num_timesteps=97000, episode_reward=150.56 +/- 44.08
Episode length: 466.40 +/- 128.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 466      |
|    mean_reward     | 151      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=137.63 +/- 50.91
Episode length: 427.36 +/- 149.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | 138      |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=144.22 +/- 47.73
Episode length: 431.20 +/- 154.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | 144      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 71.5     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 48       |
|    time_elapsed    | 642      |
|    total_timesteps | 98304    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=98500, episode_reward=136.00 +/- 65.75
Episode length: 417.76 +/- 186.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 418         |
|    mean_reward          | 136         |
| time/                   |             |
|    total_timesteps      | 98500       |
| train/                  |             |
|    approx_kl            | 0.005900755 |
|    clip_fraction        | 0.0456      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.001       |
|    loss                 | 76.6        |
|    n_updates            | 55          |
|    policy_gradient_loss | 0.000833    |
|    value_loss           | 145         |
-----------------------------------------
Eval num_timesteps=99000, episode_reward=152.37 +/- 48.82
Episode length: 463.08 +/- 150.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 463      |
|    mean_reward     | 152      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=154.60 +/- 61.92
Episode length: 464.18 +/- 152.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 464      |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=138.99 +/- 69.98
Episode length: 402.78 +/- 199.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 403      |
|    mean_reward     | 139      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 79.9     |
| time/              |          |
|    fps             | 143      |
|    iterations      | 49       |
|    time_elapsed    | 701      |
|    total_timesteps | 100352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=100500, episode_reward=167.88 +/- 21.53
Episode length: 496.70 +/- 112.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 497         |
|    mean_reward          | 168         |
| time/                   |             |
|    total_timesteps      | 100500      |
| train/                  |             |
|    approx_kl            | 0.008418266 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.372       |
|    learning_rate        | 0.001       |
|    loss                 | 32.3        |
|    n_updates            | 56          |
|    policy_gradient_loss | -0.00372    |
|    value_loss           | 100         |
-----------------------------------------
Eval num_timesteps=101000, episode_reward=171.75 +/- 22.82
Episode length: 505.64 +/- 94.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=171.83 +/- 22.61
Episode length: 505.40 +/- 96.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=171.78 +/- 12.50
Episode length: 515.22 +/- 68.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 85.5     |
| time/              |          |
|    fps             | 132      |
|    iterations      | 50       |
|    time_elapsed    | 770      |
|    total_timesteps | 102400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=102500, episode_reward=170.29 +/- 17.54
Episode length: 506.58 +/- 90.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.009361408 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.001       |
|    loss                 | 108         |
|    n_updates            | 57          |
|    policy_gradient_loss | 0.00365     |
|    value_loss           | 98.8        |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=170.41 +/- 17.57
Episode length: 507.10 +/- 87.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=166.70 +/- 24.06
Episode length: 487.02 +/- 128.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=165.25 +/- 32.45
Episode length: 466.94 +/- 157.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 92.9     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 51       |
|    time_elapsed    | 836      |
|    total_timesteps | 104448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=104500, episode_reward=161.47 +/- 30.99
Episode length: 459.22 +/- 163.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 459          |
|    mean_reward          | 161          |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0071432055 |
|    clip_fraction        | 0.0797       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.28         |
|    learning_rate        | 0.001        |
|    loss                 | 12.9         |
|    n_updates            | 58           |
|    policy_gradient_loss | 0.0058       |
|    value_loss           | 87.5         |
------------------------------------------
Eval num_timesteps=105000, episode_reward=161.97 +/- 31.22
Episode length: 459.10 +/- 163.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 459      |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=157.22 +/- 36.04
Episode length: 430.62 +/- 188.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=162.28 +/- 31.00
Episode length: 458.48 +/- 164.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 458      |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 97.3     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 52       |
|    time_elapsed    | 898      |
|    total_timesteps | 106496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=106500, episode_reward=163.88 +/- 28.84
Episode length: 477.80 +/- 141.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 478         |
|    mean_reward          | 164         |
| time/                   |             |
|    total_timesteps      | 106500      |
| train/                  |             |
|    approx_kl            | 0.006209995 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.001       |
|    loss                 | 72.5        |
|    n_updates            | 59          |
|    policy_gradient_loss | 0.00961     |
|    value_loss           | 164         |
-----------------------------------------
Eval num_timesteps=107000, episode_reward=167.68 +/- 22.90
Episode length: 496.80 +/- 111.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=164.27 +/- 27.44
Episode length: 475.76 +/- 147.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=169.77 +/- 18.26
Episode length: 505.56 +/- 95.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=165.88 +/- 25.29
Episode length: 486.24 +/- 131.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | 104      |
| time/              |          |
|    fps             | 110      |
|    iterations      | 53       |
|    time_elapsed    | 980      |
|    total_timesteps | 108544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=109000, episode_reward=169.74 +/- 30.51
Episode length: 477.46 +/- 142.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 477         |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.007437555 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.166       |
|    learning_rate        | 0.001       |
|    loss                 | 96.1        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.000521   |
|    value_loss           | 182         |
-----------------------------------------
Eval num_timesteps=109500, episode_reward=178.90 +/- 20.56
Episode length: 515.64 +/- 65.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=174.76 +/- 20.44
Episode length: 505.90 +/- 93.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=169.11 +/- 26.99
Episode length: 487.88 +/- 126.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 263      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    fps             | 105      |
|    iterations      | 54       |
|    time_elapsed    | 1048     |
|    total_timesteps | 110592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=111000, episode_reward=147.96 +/- 40.28
Episode length: 391.92 +/- 213.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | 148         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.006620211 |
|    clip_fraction        | 0.0891      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.001       |
|    loss                 | 40.6        |
|    n_updates            | 61          |
|    policy_gradient_loss | 0.00689     |
|    value_loss           | 102         |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=148.06 +/- 44.88
Episode length: 383.44 +/- 216.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | 148      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=160.33 +/- 31.58
Episode length: 460.30 +/- 160.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 460      |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=155.20 +/- 41.34
Episode length: 414.50 +/- 197.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 414      |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    fps             | 101      |
|    iterations      | 55       |
|    time_elapsed    | 1104     |
|    total_timesteps | 112640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=113000, episode_reward=168.25 +/- 21.28
Episode length: 496.76 +/- 111.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 497         |
|    mean_reward          | 168         |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.007883511 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.001       |
|    loss                 | 93.5        |
|    n_updates            | 62          |
|    policy_gradient_loss | 0.0042      |
|    value_loss           | 112         |
-----------------------------------------
Eval num_timesteps=113500, episode_reward=171.86 +/- 12.23
Episode length: 515.18 +/- 68.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=168.49 +/- 21.00
Episode length: 496.00 +/- 114.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=164.61 +/- 38.68
Episode length: 478.46 +/- 139.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 272      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 97       |
|    iterations      | 56       |
|    time_elapsed    | 1172     |
|    total_timesteps | 114688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=115000, episode_reward=176.98 +/- 27.23
Episode length: 515.64 +/- 65.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.007865282 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.001       |
|    loss                 | 98.2        |
|    n_updates            | 63          |
|    policy_gradient_loss | 0.0136      |
|    value_loss           | 163         |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=177.06 +/- 33.41
Episode length: 506.58 +/- 90.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=174.88 +/- 13.95
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=174.91 +/- 13.94
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 276      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 93       |
|    iterations      | 57       |
|    time_elapsed    | 1243     |
|    total_timesteps | 116736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=117000, episode_reward=176.92 +/- 27.24
Episode length: 515.54 +/- 66.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.005458244 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.204       |
|    learning_rate        | 0.001       |
|    loss                 | 70.5        |
|    n_updates            | 64          |
|    policy_gradient_loss | 0.00601     |
|    value_loss           | 119         |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=175.02 +/- 23.90
Episode length: 516.20 +/- 61.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=178.86 +/- 23.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=182.92 +/- 29.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 281      |
|    ep_rew_mean     | 116      |
| time/              |          |
|    fps             | 90       |
|    iterations      | 58       |
|    time_elapsed    | 1314     |
|    total_timesteps | 118784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=119000, episode_reward=171.80 +/- 12.93
Episode length: 515.42 +/- 67.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.007253552 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.24        |
|    learning_rate        | 0.001       |
|    loss                 | 93.8        |
|    n_updates            | 65          |
|    policy_gradient_loss | 0.0121      |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=168.35 +/- 28.90
Episode length: 505.36 +/- 96.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=174.41 +/- 46.73
Episode length: 487.10 +/- 128.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=175.69 +/- 13.88
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 59       |
|    time_elapsed    | 1383     |
|    total_timesteps | 120832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=121000, episode_reward=155.75 +/- 47.63
Episode length: 460.20 +/- 149.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 460         |
|    mean_reward          | 156         |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.008854883 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.193       |
|    learning_rate        | 0.001       |
|    loss                 | 110         |
|    n_updates            | 66          |
|    policy_gradient_loss | 0.00532     |
|    value_loss           | 134         |
-----------------------------------------
Eval num_timesteps=121500, episode_reward=159.29 +/- 51.31
Episode length: 459.00 +/- 163.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 459      |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=163.21 +/- 44.25
Episode length: 468.22 +/- 154.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 468      |
|    mean_reward     | 163      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=146.51 +/- 58.79
Episode length: 420.64 +/- 188.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | 147      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 120      |
| time/              |          |
|    fps             | 85       |
|    iterations      | 60       |
|    time_elapsed    | 1444     |
|    total_timesteps | 122880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=123000, episode_reward=161.90 +/- 32.52
Episode length: 489.14 +/- 109.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 489         |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.006300203 |
|    clip_fraction        | 0.0844      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.001       |
|    loss                 | 23.4        |
|    n_updates            | 67          |
|    policy_gradient_loss | 0.00333     |
|    value_loss           | 146         |
-----------------------------------------
Eval num_timesteps=123500, episode_reward=164.10 +/- 29.26
Episode length: 494.94 +/- 103.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=163.93 +/- 29.50
Episode length: 497.54 +/- 97.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=168.26 +/- 21.61
Episode length: 510.82 +/- 69.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 511      |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 120      |
| time/              |          |
|    fps             | 82       |
|    iterations      | 61       |
|    time_elapsed    | 1511     |
|    total_timesteps | 124928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=125000, episode_reward=86.10 +/- 0.61
Episode length: 187.34 +/- 64.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 187         |
|    mean_reward          | 86.1        |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.011084599 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.001       |
|    loss                 | 65.7        |
|    n_updates            | 69          |
|    policy_gradient_loss | 0.00983     |
|    value_loss           | 145         |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=86.12 +/- 0.45
Episode length: 190.56 +/- 49.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 86.1     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=86.20 +/- 0.40
Episode length: 191.46 +/- 70.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 86.2     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=86.04 +/- 0.48
Episode length: 211.44 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 86       |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 82       |
|    iterations      | 62       |
|    time_elapsed    | 1538     |
|    total_timesteps | 126976   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=127000, episode_reward=149.57 +/- 55.58
Episode length: 340.22 +/- 219.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 340         |
|    mean_reward          | 150         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.011012768 |
|    clip_fraction        | 0.0722      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.146       |
|    learning_rate        | 0.001       |
|    loss                 | 201         |
|    n_updates            | 71          |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 203         |
-----------------------------------------
Eval num_timesteps=127500, episode_reward=142.66 +/- 59.81
Episode length: 310.14 +/- 233.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 310      |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=146.90 +/- 55.36
Episode length: 337.74 +/- 217.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 338      |
|    mean_reward     | 147      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=133.91 +/- 55.75
Episode length: 271.96 +/- 225.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 134      |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=143.11 +/- 55.64
Episode length: 311.06 +/- 224.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 311      |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 119      |
| time/              |          |
|    fps             | 80       |
|    iterations      | 63       |
|    time_elapsed    | 1592     |
|    total_timesteps | 129024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=129500, episode_reward=85.57 +/- 1.27
Episode length: 61.90 +/- 19.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 61.9         |
|    mean_reward          | 85.6         |
| time/                   |              |
|    total_timesteps      | 129500       |
| train/                  |              |
|    approx_kl            | 0.0061393655 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.187        |
|    learning_rate        | 0.001        |
|    loss                 | 98.6         |
|    n_updates            | 72           |
|    policy_gradient_loss | 0.00605      |
|    value_loss           | 114          |
------------------------------------------
Eval num_timesteps=130000, episode_reward=85.65 +/- 1.50
Episode length: 61.46 +/- 18.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.5     |
|    mean_reward     | 85.6     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=85.35 +/- 1.90
Episode length: 58.90 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.9     |
|    mean_reward     | 85.3     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=85.31 +/- 1.62
Episode length: 59.96 +/- 23.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60       |
|    mean_reward     | 85.3     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 277      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 81       |
|    iterations      | 64       |
|    time_elapsed    | 1602     |
|    total_timesteps | 131072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=131500, episode_reward=74.09 +/- 33.19
Episode length: 58.38 +/- 20.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 58.4        |
|    mean_reward          | 74.1        |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.007881977 |
|    clip_fraction        | 0.0894      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.001       |
|    loss                 | 32          |
|    n_updates            | 73          |
|    policy_gradient_loss | 0.000715    |
|    value_loss           | 183         |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=61.55 +/- 44.35
Episode length: 61.80 +/- 23.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.8     |
|    mean_reward     | 61.5     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=61.47 +/- 44.03
Episode length: 61.52 +/- 23.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.5     |
|    mean_reward     | 61.5     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=70.15 +/- 37.67
Episode length: 60.64 +/- 21.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.6     |
|    mean_reward     | 70.1     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 277      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 82       |
|    iterations      | 65       |
|    time_elapsed    | 1612     |
|    total_timesteps | 133120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=133500, episode_reward=145.37 +/- 54.09
Episode length: 308.84 +/- 234.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 309         |
|    mean_reward          | 145         |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.005234747 |
|    clip_fraction        | 0.057       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.001       |
|    loss                 | 53.5        |
|    n_updates            | 74          |
|    policy_gradient_loss | -0.000634   |
|    value_loss           | 124         |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=158.90 +/- 51.70
Episode length: 366.54 +/- 221.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 367      |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=158.97 +/- 51.71
Episode length: 368.12 +/- 218.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=145.04 +/- 53.65
Episode length: 312.74 +/- 230.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 313      |
|    mean_reward     | 145      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 281      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 81       |
|    iterations      | 66       |
|    time_elapsed    | 1659     |
|    total_timesteps | 135168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=135500, episode_reward=160.58 +/- 56.19
Episode length: 420.02 +/- 164.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 420          |
|    mean_reward          | 161          |
| time/                   |              |
|    total_timesteps      | 135500       |
| train/                  |              |
|    approx_kl            | 0.0064042844 |
|    clip_fraction        | 0.0614       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.415        |
|    learning_rate        | 0.001        |
|    loss                 | 66.1         |
|    n_updates            | 75           |
|    policy_gradient_loss | -0.0058      |
|    value_loss           | 73.7         |
------------------------------------------
Eval num_timesteps=136000, episode_reward=165.23 +/- 53.74
Episode length: 428.54 +/- 175.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=167.19 +/- 48.91
Episode length: 419.36 +/- 179.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 419      |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=155.90 +/- 57.58
Episode length: 403.68 +/- 176.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 404      |
|    mean_reward     | 156      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 67       |
|    time_elapsed    | 1717     |
|    total_timesteps | 137216   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=137500, episode_reward=183.89 +/- 33.65
Episode length: 478.70 +/- 139.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 479          |
|    mean_reward          | 184          |
| time/                   |              |
|    total_timesteps      | 137500       |
| train/                  |              |
|    approx_kl            | 0.0146805085 |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.376        |
|    learning_rate        | 0.001        |
|    loss                 | 77.8         |
|    n_updates            | 77           |
|    policy_gradient_loss | 0.00193      |
|    value_loss           | 135          |
------------------------------------------
Eval num_timesteps=138000, episode_reward=188.85 +/- 26.42
Episode length: 496.44 +/- 113.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=183.56 +/- 33.58
Episode length: 477.64 +/- 142.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=184.25 +/- 33.11
Episode length: 477.60 +/- 142.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 281      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 78       |
|    iterations      | 68       |
|    time_elapsed    | 1782     |
|    total_timesteps | 139264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=139500, episode_reward=178.54 +/- 28.79
Episode length: 486.74 +/- 129.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 487         |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 139500      |
| train/                  |             |
|    approx_kl            | 0.007395097 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.001       |
|    loss                 | 69.3        |
|    n_updates            | 78          |
|    policy_gradient_loss | 0.00903     |
|    value_loss           | 92.9        |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=179.38 +/- 28.99
Episode length: 486.60 +/- 130.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=177.44 +/- 31.88
Episode length: 477.34 +/- 143.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=173.70 +/- 36.44
Episode length: 457.50 +/- 167.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 458      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 281      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 69       |
|    time_elapsed    | 1846     |
|    total_timesteps | 141312   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=141500, episode_reward=139.45 +/- 54.58
Episode length: 284.58 +/- 231.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 285         |
|    mean_reward          | 139         |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.008634551 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.001       |
|    loss                 | 27.7        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 66.2        |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=124.26 +/- 51.86
Episode length: 221.94 +/- 218.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=122.11 +/- 50.98
Episode length: 205.40 +/- 219.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 122      |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=137.48 +/- 54.50
Episode length: 273.16 +/- 232.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 137      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 277      |
|    ep_rew_mean     | 113      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 70       |
|    time_elapsed    | 1880     |
|    total_timesteps | 143360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=143500, episode_reward=87.27 +/- 0.40
Episode length: 67.94 +/- 21.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 67.9        |
|    mean_reward          | 87.3        |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.010104459 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.534       |
|    learning_rate        | 0.001       |
|    loss                 | 20.8        |
|    n_updates            | 81          |
|    policy_gradient_loss | 0.00203     |
|    value_loss           | 127         |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=89.51 +/- 15.27
Episode length: 71.30 +/- 67.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.3     |
|    mean_reward     | 89.5     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=87.33 +/- 0.27
Episode length: 61.22 +/- 28.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.2     |
|    mean_reward     | 87.3     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=89.53 +/- 15.27
Episode length: 80.34 +/- 67.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 89.5     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 282      |
|    ep_rew_mean     | 116      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 71       |
|    time_elapsed    | 1891     |
|    total_timesteps | 145408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=145500, episode_reward=106.87 +/- 41.90
Episode length: 149.04 +/- 177.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 149         |
|    mean_reward          | 107         |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.006996791 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.001       |
|    loss                 | 10.8        |
|    n_updates            | 82          |
|    policy_gradient_loss | 0.00297     |
|    value_loss           | 69.8        |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=104.76 +/- 39.98
Episode length: 136.70 +/- 170.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 137      |
|    mean_reward     | 105      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=91.94 +/- 29.99
Episode length: 91.66 +/- 111.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.7     |
|    mean_reward     | 91.9     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=90.04 +/- 39.28
Episode length: 96.40 +/- 127.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 90       |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 269      |
|    ep_rew_mean     | 113      |
| time/              |          |
|    fps             | 77       |
|    iterations      | 72       |
|    time_elapsed    | 1909     |
|    total_timesteps | 147456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=147500, episode_reward=87.29 +/- 0.38
Episode length: 66.80 +/- 22.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 66.8         |
|    mean_reward          | 87.3         |
| time/                   |              |
|    total_timesteps      | 147500       |
| train/                  |              |
|    approx_kl            | 0.0057963724 |
|    clip_fraction        | 0.0693       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.503        |
|    learning_rate        | 0.001        |
|    loss                 | 90.5         |
|    n_updates            | 83           |
|    policy_gradient_loss | 0.00385      |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=148000, episode_reward=87.31 +/- 0.30
Episode length: 55.12 +/- 18.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | 87.3     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=89.45 +/- 15.28
Episode length: 73.50 +/- 67.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 89.4     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=87.27 +/- 0.35
Episode length: 62.82 +/- 23.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.8     |
|    mean_reward     | 87.3     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=87.33 +/- 0.34
Episode length: 63.06 +/- 21.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.1     |
|    mean_reward     | 87.3     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 277      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 77       |
|    iterations      | 73       |
|    time_elapsed    | 1921     |
|    total_timesteps | 149504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=150000, episode_reward=189.44 +/- 26.47
Episode length: 497.62 +/- 108.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 498         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.008257926 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.001       |
|    loss                 | 87          |
|    n_updates            | 84          |
|    policy_gradient_loss | 2.29e-05    |
|    value_loss           | 90.8        |
-----------------------------------------
Eval num_timesteps=150500, episode_reward=174.03 +/- 43.87
Episode length: 430.84 +/- 188.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=171.90 +/- 45.36
Episode length: 421.30 +/- 195.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=169.02 +/- 47.29
Episode length: 413.42 +/- 198.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 413      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | 116      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 74       |
|    time_elapsed    | 1983     |
|    total_timesteps | 151552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=152000, episode_reward=128.71 +/- 53.77
Episode length: 330.88 +/- 170.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 331         |
|    mean_reward          | 129         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.008881392 |
|    clip_fraction        | 0.0616      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.001       |
|    loss                 | 18.3        |
|    n_updates            | 85          |
|    policy_gradient_loss | 0.00655     |
|    value_loss           | 95.8        |
-----------------------------------------
Eval num_timesteps=152500, episode_reward=113.87 +/- 52.70
Episode length: 280.78 +/- 169.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 114      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=139.44 +/- 55.26
Episode length: 363.26 +/- 172.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 363      |
|    mean_reward     | 139      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=118.18 +/- 50.69
Episode length: 297.60 +/- 160.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | 118      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 289      |
|    ep_rew_mean     | 119      |
| time/              |          |
|    fps             | 75       |
|    iterations      | 75       |
|    time_elapsed    | 2027     |
|    total_timesteps | 153600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=154000, episode_reward=178.20 +/- 16.40
Episode length: 515.30 +/- 67.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 178          |
| time/                   |              |
|    total_timesteps      | 154000       |
| train/                  |              |
|    approx_kl            | 0.0065400135 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.597        |
|    learning_rate        | 0.001        |
|    loss                 | 46.9         |
|    n_updates            | 86           |
|    policy_gradient_loss | 0.00233      |
|    value_loss           | 76           |
------------------------------------------
Eval num_timesteps=154500, episode_reward=168.74 +/- 33.74
Episode length: 490.94 +/- 105.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 491      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=172.05 +/- 29.35
Episode length: 489.84 +/- 119.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=175.62 +/- 22.87
Episode length: 508.36 +/- 82.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 123      |
| time/              |          |
|    fps             | 74       |
|    iterations      | 76       |
|    time_elapsed    | 2095     |
|    total_timesteps | 155648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=156000, episode_reward=172.63 +/- 0.49
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 173         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.011033083 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.001       |
|    loss                 | 78.1        |
|    n_updates            | 87          |
|    policy_gradient_loss | -0.00142    |
|    value_loss           | 139         |
-----------------------------------------
Eval num_timesteps=156500, episode_reward=174.75 +/- 23.59
Episode length: 515.02 +/- 69.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=171.86 +/- 14.56
Episode length: 515.70 +/- 65.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=172.74 +/- 19.32
Episode length: 515.52 +/- 66.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 315      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 72       |
|    iterations      | 77       |
|    time_elapsed    | 2166     |
|    total_timesteps | 157696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=158000, episode_reward=170.88 +/- 23.16
Episode length: 505.36 +/- 96.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | 171          |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0066297986 |
|    clip_fraction        | 0.0566       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.558        |
|    learning_rate        | 0.001        |
|    loss                 | 50.4         |
|    n_updates            | 88           |
|    policy_gradient_loss | 0.0064       |
|    value_loss           | 105          |
------------------------------------------
Eval num_timesteps=158500, episode_reward=163.53 +/- 33.50
Episode length: 465.64 +/- 160.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 466      |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=170.87 +/- 23.49
Episode length: 505.62 +/- 94.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=163.24 +/- 28.03
Episode length: 476.28 +/- 146.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 163      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 326      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 71       |
|    iterations      | 78       |
|    time_elapsed    | 2232     |
|    total_timesteps | 159744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=160000, episode_reward=163.35 +/- 34.04
Episode length: 466.46 +/- 158.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 466         |
|    mean_reward          | 163         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.011757713 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.418       |
|    learning_rate        | 0.001       |
|    loss                 | 73.2        |
|    n_updates            | 89          |
|    policy_gradient_loss | 0.00915     |
|    value_loss           | 123         |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=161.34 +/- 30.54
Episode length: 466.74 +/- 157.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=165.34 +/- 32.19
Episode length: 476.48 +/- 145.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=165.26 +/- 31.45
Episode length: 475.78 +/- 147.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 325      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 70       |
|    iterations      | 79       |
|    time_elapsed    | 2297     |
|    total_timesteps | 161792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=162000, episode_reward=121.45 +/- 49.37
Episode length: 274.26 +/- 231.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 121         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.011620626 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.001       |
|    loss                 | 77.3        |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.018       |
|    value_loss           | 191         |
-----------------------------------------
Eval num_timesteps=162500, episode_reward=135.59 +/- 47.45
Episode length: 333.40 +/- 234.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 333      |
|    mean_reward     | 136      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=118.47 +/- 56.03
Episode length: 242.80 +/- 230.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 118      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=117.77 +/- 51.57
Episode length: 238.86 +/- 233.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 118      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 329      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 70       |
|    iterations      | 80       |
|    time_elapsed    | 2335     |
|    total_timesteps | 163840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=164000, episode_reward=175.66 +/- 6.57
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 176          |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0068117203 |
|    clip_fraction        | 0.0694       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.522        |
|    learning_rate        | 0.001        |
|    loss                 | 93.7         |
|    n_updates            | 91           |
|    policy_gradient_loss | 0.00383      |
|    value_loss           | 108          |
------------------------------------------
Eval num_timesteps=164500, episode_reward=176.45 +/- 6.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=178.65 +/- 8.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=176.92 +/- 7.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 334      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 68       |
|    iterations      | 81       |
|    time_elapsed    | 2407     |
|    total_timesteps | 165888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=166000, episode_reward=189.54 +/- 8.64
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.004967287 |
|    clip_fraction        | 0.0385      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.001       |
|    loss                 | 96.5        |
|    n_updates            | 92          |
|    policy_gradient_loss | 0.00273     |
|    value_loss           | 116         |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=189.22 +/- 8.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=188.91 +/- 8.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=186.11 +/- 9.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 82       |
|    time_elapsed    | 2479     |
|    total_timesteps | 167936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=168000, episode_reward=186.64 +/- 8.65
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 187          |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 0.0052581467 |
|    clip_fraction        | 0.0456       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.432        |
|    learning_rate        | 0.001        |
|    loss                 | 69           |
|    n_updates            | 93           |
|    policy_gradient_loss | 0.0197       |
|    value_loss           | 52.9         |
------------------------------------------
Eval num_timesteps=168500, episode_reward=183.97 +/- 9.98
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=184.87 +/- 9.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=187.41 +/- 9.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 348      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 83       |
|    time_elapsed    | 2550     |
|    total_timesteps | 169984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=170000, episode_reward=187.65 +/- 26.73
Episode length: 497.44 +/- 109.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 497          |
|    mean_reward          | 188          |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0051547876 |
|    clip_fraction        | 0.051        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.345        |
|    learning_rate        | 0.001        |
|    loss                 | 41.9         |
|    n_updates            | 94           |
|    policy_gradient_loss | 0.00595      |
|    value_loss           | 105          |
------------------------------------------
Eval num_timesteps=170500, episode_reward=189.38 +/- 21.88
Episode length: 507.18 +/- 87.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=180.79 +/- 36.32
Episode length: 470.68 +/- 147.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=191.97 +/- 15.45
Episode length: 515.82 +/- 64.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=192.52 +/- 15.48
Episode length: 515.86 +/- 63.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 352      |
|    ep_rew_mean     | 135      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 84       |
|    time_elapsed    | 2635     |
|    total_timesteps | 172032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=172500, episode_reward=193.84 +/- 15.68
Episode length: 515.58 +/- 65.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 172500      |
| train/                  |             |
|    approx_kl            | 0.006100327 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.466       |
|    learning_rate        | 0.001       |
|    loss                 | 31          |
|    n_updates            | 95          |
|    policy_gradient_loss | -0.000439   |
|    value_loss           | 72.6        |
-----------------------------------------
Eval num_timesteps=173000, episode_reward=193.53 +/- 15.66
Episode length: 515.86 +/- 63.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=193.81 +/- 15.82
Episode length: 515.46 +/- 66.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=194.05 +/- 15.55
Episode length: 515.76 +/- 64.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 85       |
|    time_elapsed    | 2705     |
|    total_timesteps | 174080   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=174500, episode_reward=196.90 +/- 0.94
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.013494604 |
|    clip_fraction        | 0.0612      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.001       |
|    loss                 | 44          |
|    n_updates            | 97          |
|    policy_gradient_loss | 0.00188     |
|    value_loss           | 88.8        |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=196.37 +/- 3.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=196.59 +/- 1.18
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=196.53 +/- 1.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 363      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 63       |
|    iterations      | 86       |
|    time_elapsed    | 2777     |
|    total_timesteps | 176128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=176500, episode_reward=185.25 +/- 28.04
Episode length: 496.10 +/- 99.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 496         |
|    mean_reward          | 185         |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.010881441 |
|    clip_fraction        | 0.0605      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.001       |
|    loss                 | 46.4        |
|    n_updates            | 98          |
|    policy_gradient_loss | 0.0131      |
|    value_loss           | 79.1        |
-----------------------------------------
Eval num_timesteps=177000, episode_reward=183.23 +/- 30.81
Episode length: 493.92 +/- 105.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=182.32 +/- 34.96
Episode length: 496.16 +/- 99.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=187.79 +/- 23.47
Episode length: 507.82 +/- 84.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 368      |
|    ep_rew_mean     | 138      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 87       |
|    time_elapsed    | 2845     |
|    total_timesteps | 178176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=178500, episode_reward=189.99 +/- 8.82
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 178500      |
| train/                  |             |
|    approx_kl            | 0.008863056 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.001       |
|    loss                 | 7.44        |
|    n_updates            | 99          |
|    policy_gradient_loss | 0.0138      |
|    value_loss           | 75.2        |
-----------------------------------------
Eval num_timesteps=179000, episode_reward=186.84 +/- 10.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=187.08 +/- 10.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=188.43 +/- 9.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 373      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 88       |
|    time_elapsed    | 2917     |
|    total_timesteps | 180224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=180500, episode_reward=188.39 +/- 10.01
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.007051925 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.001       |
|    loss                 | 40.8        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00376    |
|    value_loss           | 80.3        |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=190.29 +/- 8.88
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=191.44 +/- 6.98
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=189.13 +/- 9.21
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 386      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 89       |
|    time_elapsed    | 2989     |
|    total_timesteps | 182272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=182500, episode_reward=195.29 +/- 4.46
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 195          |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 0.0075723557 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.001        |
|    loss                 | 48.3         |
|    n_updates            | 101          |
|    policy_gradient_loss | 0.00273      |
|    value_loss           | 72.5         |
------------------------------------------
Eval num_timesteps=183000, episode_reward=195.96 +/- 3.25
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=193.74 +/- 6.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=193.60 +/- 7.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 90       |
|    time_elapsed    | 3062     |
|    total_timesteps | 184320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=184500, episode_reward=195.81 +/- 3.58
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 184500      |
| train/                  |             |
|    approx_kl            | 0.007594282 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.001       |
|    loss                 | 12.6        |
|    n_updates            | 102         |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 44.5        |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=195.93 +/- 1.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=196.42 +/- 1.06
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=196.29 +/- 1.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 91       |
|    time_elapsed    | 3135     |
|    total_timesteps | 186368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=186500, episode_reward=196.88 +/- 0.62
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.007325725 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.001       |
|    loss                 | 23.9        |
|    n_updates            | 103         |
|    policy_gradient_loss | 0.00354     |
|    value_loss           | 77.1        |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=196.33 +/- 3.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=196.34 +/- 3.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=196.43 +/- 3.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 409      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 92       |
|    time_elapsed    | 3207     |
|    total_timesteps | 188416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=188500, episode_reward=191.89 +/- 9.02
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 192          |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 0.0067564393 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.697        |
|    learning_rate        | 0.001        |
|    loss                 | 52.6         |
|    n_updates            | 104          |
|    policy_gradient_loss | 0.0219       |
|    value_loss           | 103          |
------------------------------------------
Eval num_timesteps=189000, episode_reward=192.88 +/- 8.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=194.91 +/- 5.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=192.28 +/- 8.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 93       |
|    time_elapsed    | 3278     |
|    total_timesteps | 190464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=190500, episode_reward=184.87 +/- 9.98
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 185         |
| time/                   |             |
|    total_timesteps      | 190500      |
| train/                  |             |
|    approx_kl            | 0.008933182 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.001       |
|    loss                 | 39.8        |
|    n_updates            | 105         |
|    policy_gradient_loss | 0.0147      |
|    value_loss           | 48.7        |
-----------------------------------------
Eval num_timesteps=191000, episode_reward=185.32 +/- 10.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=184.23 +/- 10.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=185.90 +/- 10.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=188.25 +/- 9.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 419      |
|    ep_rew_mean     | 150      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 94       |
|    time_elapsed    | 3368     |
|    total_timesteps | 192512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=193000, episode_reward=189.71 +/- 10.06
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.018438676 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.644       |
|    learning_rate        | 0.001       |
|    loss                 | 51.3        |
|    n_updates            | 107         |
|    policy_gradient_loss | 0.00616     |
|    value_loss           | 58.6        |
-----------------------------------------
Eval num_timesteps=193500, episode_reward=187.89 +/- 10.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=192.47 +/- 8.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=190.31 +/- 9.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 95       |
|    time_elapsed    | 3439     |
|    total_timesteps | 194560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=195000, episode_reward=195.57 +/- 4.95
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.008185729 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.001       |
|    loss                 | 54.9        |
|    n_updates            | 108         |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 67.5        |
-----------------------------------------
Eval num_timesteps=195500, episode_reward=195.69 +/- 5.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=196.11 +/- 4.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=196.54 +/- 3.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | 150      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 96       |
|    time_elapsed    | 3511     |
|    total_timesteps | 196608   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=197000, episode_reward=197.08 +/- 0.87
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.012857394 |
|    clip_fraction        | 0.0723      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.001       |
|    loss                 | 28.9        |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00931     |
|    value_loss           | 61.2        |
-----------------------------------------
Eval num_timesteps=197500, episode_reward=196.71 +/- 1.07
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=196.90 +/- 0.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=196.86 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 409      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 97       |
|    time_elapsed    | 3583     |
|    total_timesteps | 198656   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=199000, episode_reward=192.02 +/- 9.17
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 192        |
| time/                   |            |
|    total_timesteps      | 199000     |
| train/                  |            |
|    approx_kl            | 0.01068427 |
|    clip_fraction        | 0.0629     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.001      |
|    loss                 | 53.3       |
|    n_updates            | 112        |
|    policy_gradient_loss | 0.00793    |
|    value_loss           | 74.4       |
----------------------------------------
Eval num_timesteps=199500, episode_reward=192.47 +/- 17.08
Episode length: 515.54 +/- 66.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=192.74 +/- 8.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=192.98 +/- 8.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 407      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 98       |
|    time_elapsed    | 3655     |
|    total_timesteps | 200704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=201000, episode_reward=197.30 +/- 3.60
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.003641582 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.001       |
|    loss                 | 71.2        |
|    n_updates            | 113         |
|    policy_gradient_loss | 0.0259      |
|    value_loss           | 91.6        |
-----------------------------------------
Eval num_timesteps=201500, episode_reward=197.28 +/- 3.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=194.31 +/- 16.90
Episode length: 515.36 +/- 67.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=197.25 +/- 2.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 417      |
|    ep_rew_mean     | 150      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 99       |
|    time_elapsed    | 3725     |
|    total_timesteps | 202752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=203000, episode_reward=98.85 +/- 63.97
Episode length: 297.86 +/- 171.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 298          |
|    mean_reward          | 98.8         |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 0.0037017695 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.656        |
|    learning_rate        | 0.001        |
|    loss                 | 57.1         |
|    n_updates            | 114          |
|    policy_gradient_loss | 0.00093      |
|    value_loss           | 52.8         |
------------------------------------------
Eval num_timesteps=203500, episode_reward=98.50 +/- 72.96
Episode length: 278.38 +/- 187.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 98.5     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=99.08 +/- 77.68
Episode length: 293.00 +/- 194.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | 99.1     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=99.05 +/- 73.07
Episode length: 297.08 +/- 184.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | 99.1     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 424      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 100      |
|    time_elapsed    | 3765     |
|    total_timesteps | 204800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=205000, episode_reward=66.86 +/- 15.55
Episode length: 192.08 +/- 76.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 192          |
|    mean_reward          | 66.9         |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0045945775 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.566        |
|    learning_rate        | 0.001        |
|    loss                 | 121          |
|    n_updates            | 115          |
|    policy_gradient_loss | 0.00322      |
|    value_loss           | 94.2         |
------------------------------------------
Eval num_timesteps=205500, episode_reward=71.92 +/- 26.13
Episode length: 197.26 +/- 102.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 71.9     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=68.24 +/- 21.49
Episode length: 226.46 +/- 97.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | 68.2     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=67.36 +/- 15.80
Episode length: 191.84 +/- 81.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 67.4     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 416      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 101      |
|    time_elapsed    | 3795     |
|    total_timesteps | 206848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=207000, episode_reward=62.68 +/- 1.15
Episode length: 205.78 +/- 68.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | 62.7        |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.007926487 |
|    clip_fraction        | 0.0794      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.114       |
|    learning_rate        | 0.001       |
|    loss                 | 80.8        |
|    n_updates            | 116         |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 221         |
-----------------------------------------
Eval num_timesteps=207500, episode_reward=62.54 +/- 0.85
Episode length: 205.64 +/- 58.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 62.5     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=62.50 +/- 0.70
Episode length: 196.78 +/- 55.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 62.5     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=62.46 +/- 0.42
Episode length: 197.04 +/- 50.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 62.5     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 408      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 102      |
|    time_elapsed    | 3824     |
|    total_timesteps | 208896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=209000, episode_reward=62.60 +/- 1.15
Episode length: 196.92 +/- 63.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 197         |
|    mean_reward          | 62.6        |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.006910488 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.001       |
|    loss                 | 97.7        |
|    n_updates            | 117         |
|    policy_gradient_loss | 0.00213     |
|    value_loss           | 184         |
-----------------------------------------
Eval num_timesteps=209500, episode_reward=62.98 +/- 1.94
Episode length: 194.48 +/- 81.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 63       |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=62.82 +/- 1.99
Episode length: 212.06 +/- 72.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 62.8     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=62.64 +/- 1.32
Episode length: 205.08 +/- 81.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 62.6     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 403      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 103      |
|    time_elapsed    | 3854     |
|    total_timesteps | 210944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=211000, episode_reward=90.88 +/- 53.91
Episode length: 250.14 +/- 169.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 250         |
|    mean_reward          | 90.9        |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.008396722 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.254       |
|    learning_rate        | 0.001       |
|    loss                 | 58.4        |
|    n_updates            | 118         |
|    policy_gradient_loss | 0.000755    |
|    value_loss           | 141         |
-----------------------------------------
Eval num_timesteps=211500, episode_reward=83.01 +/- 42.50
Episode length: 235.62 +/- 150.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | 83       |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=93.91 +/- 48.73
Episode length: 270.00 +/- 156.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 93.9     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=82.70 +/- 46.70
Episode length: 239.12 +/- 144.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 82.7     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 398      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 104      |
|    time_elapsed    | 3889     |
|    total_timesteps | 212992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=213000, episode_reward=177.12 +/- 19.46
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.004098355 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.001       |
|    loss                 | 56.8        |
|    n_updates            | 119         |
|    policy_gradient_loss | 0.00139     |
|    value_loss           | 94.2        |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=173.63 +/- 19.10
Episode length: 515.00 +/- 70.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=173.16 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=175.14 +/- 13.92
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=179.25 +/- 23.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 393      |
|    ep_rew_mean     | 135      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 105      |
|    time_elapsed    | 3978     |
|    total_timesteps | 215040   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=215500, episode_reward=169.37 +/- 42.77
Episode length: 457.12 +/- 168.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 457        |
|    mean_reward          | 169        |
| time/                   |            |
|    total_timesteps      | 215500     |
| train/                  |            |
|    approx_kl            | 0.01364181 |
|    clip_fraction        | 0.0785     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.435      |
|    learning_rate        | 0.001      |
|    loss                 | 47.7       |
|    n_updates            | 121        |
|    policy_gradient_loss | 0.00529    |
|    value_loss           | 109        |
----------------------------------------
Eval num_timesteps=216000, episode_reward=165.40 +/- 37.36
Episode length: 466.48 +/- 158.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 466      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=166.28 +/- 42.63
Episode length: 472.34 +/- 143.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 472      |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=165.08 +/- 37.99
Episode length: 467.54 +/- 155.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 468      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 401      |
|    ep_rew_mean     | 138      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 106      |
|    time_elapsed    | 4041     |
|    total_timesteps | 217088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=217500, episode_reward=74.45 +/- 14.67
Episode length: 69.14 +/- 66.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 69.1         |
|    mean_reward          | 74.4         |
| time/                   |              |
|    total_timesteps      | 217500       |
| train/                  |              |
|    approx_kl            | 0.0056564966 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.397        |
|    learning_rate        | 0.001        |
|    loss                 | 2.24         |
|    n_updates            | 122          |
|    policy_gradient_loss | 0.0154       |
|    value_loss           | 136          |
------------------------------------------
Eval num_timesteps=218000, episode_reward=72.11 +/- 5.72
Episode length: 62.28 +/- 22.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.3     |
|    mean_reward     | 72.1     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=71.07 +/- 5.77
Episode length: 67.70 +/- 25.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.7     |
|    mean_reward     | 71.1     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=72.41 +/- 6.13
Episode length: 62.56 +/- 27.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.6     |
|    mean_reward     | 72.4     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 373      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 107      |
|    time_elapsed    | 4052     |
|    total_timesteps | 219136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=219500, episode_reward=71.11 +/- 5.65
Episode length: 66.44 +/- 23.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 66.4        |
|    mean_reward          | 71.1        |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.009801041 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.001       |
|    loss                 | 91.1        |
|    n_updates            | 123         |
|    policy_gradient_loss | -0.0053     |
|    value_loss           | 134         |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=71.97 +/- 5.83
Episode length: 62.50 +/- 23.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.5     |
|    mean_reward     | 72       |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=71.43 +/- 5.66
Episode length: 64.14 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.1     |
|    mean_reward     | 71.4     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=70.89 +/- 5.70
Episode length: 67.00 +/- 23.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67       |
|    mean_reward     | 70.9     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | 121      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 108      |
|    time_elapsed    | 4062     |
|    total_timesteps | 221184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=221500, episode_reward=113.35 +/- 50.51
Episode length: 222.96 +/- 226.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 223          |
|    mean_reward          | 113          |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 0.0071763555 |
|    clip_fraction        | 0.0859       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.772        |
|    learning_rate        | 0.001        |
|    loss                 | 53           |
|    n_updates            | 124          |
|    policy_gradient_loss | 0.000984     |
|    value_loss           | 98.9         |
------------------------------------------
Eval num_timesteps=222000, episode_reward=102.43 +/- 41.63
Episode length: 171.34 +/- 209.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 102      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=114.61 +/- 58.70
Episode length: 214.36 +/- 223.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 115      |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=107.62 +/- 51.06
Episode length: 200.74 +/- 213.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 108      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 109      |
|    time_elapsed    | 4091     |
|    total_timesteps | 223232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=223500, episode_reward=135.01 +/- 57.68
Episode length: 287.68 +/- 237.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | 135          |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 0.0076460554 |
|    clip_fraction        | 0.0781       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.748        |
|    learning_rate        | 0.001        |
|    loss                 | 78.9         |
|    n_updates            | 125          |
|    policy_gradient_loss | 0.00807      |
|    value_loss           | 98.6         |
------------------------------------------
Eval num_timesteps=224000, episode_reward=114.73 +/- 45.76
Episode length: 199.48 +/- 223.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | 115      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=143.09 +/- 65.77
Episode length: 280.34 +/- 235.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=128.05 +/- 58.50
Episode length: 240.14 +/- 232.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 128      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | 113      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 110      |
|    time_elapsed    | 4126     |
|    total_timesteps | 225280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=225500, episode_reward=174.52 +/- 4.64
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.005531042 |
|    clip_fraction        | 0.0658      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.001       |
|    loss                 | 57.8        |
|    n_updates            | 126         |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 84.2        |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=173.24 +/- 0.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=173.60 +/- 3.15
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=177.28 +/- 19.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 270      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 111      |
|    time_elapsed    | 4198     |
|    total_timesteps | 227328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=227500, episode_reward=173.22 +/- 0.76
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 173          |
| time/                   |              |
|    total_timesteps      | 227500       |
| train/                  |              |
|    approx_kl            | 0.0053328467 |
|    clip_fraction        | 0.0535       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.707        |
|    learning_rate        | 0.001        |
|    loss                 | 14.4         |
|    n_updates            | 127          |
|    policy_gradient_loss | 0.00778      |
|    value_loss           | 59.1         |
------------------------------------------
Eval num_timesteps=228000, episode_reward=175.30 +/- 14.01
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=177.35 +/- 14.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=173.77 +/- 3.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 270      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 112      |
|    time_elapsed    | 4270     |
|    total_timesteps | 229376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=229500, episode_reward=178.02 +/- 8.83
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.005515225 |
|    clip_fraction        | 0.0215      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.001       |
|    loss                 | 47.6        |
|    n_updates            | 128         |
|    policy_gradient_loss | 0.00814     |
|    value_loss           | 114         |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=183.03 +/- 16.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=178.85 +/- 9.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=183.14 +/- 10.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 277      |
|    ep_rew_mean     | 113      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 113      |
|    time_elapsed    | 4341     |
|    total_timesteps | 231424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=231500, episode_reward=187.01 +/- 18.10
Episode length: 516.32 +/- 60.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 187          |
| time/                   |              |
|    total_timesteps      | 231500       |
| train/                  |              |
|    approx_kl            | 0.0051734224 |
|    clip_fraction        | 0.0654       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.49         |
|    learning_rate        | 0.001        |
|    loss                 | 60           |
|    n_updates            | 129          |
|    policy_gradient_loss | -0.00378     |
|    value_loss           | 76.3         |
------------------------------------------
Eval num_timesteps=232000, episode_reward=189.06 +/- 10.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=191.10 +/- 9.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=191.91 +/- 17.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 281      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 114      |
|    time_elapsed    | 4413     |
|    total_timesteps | 233472   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=233500, episode_reward=190.95 +/- 16.45
Episode length: 515.66 +/- 65.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 233500      |
| train/                  |             |
|    approx_kl            | 0.009639514 |
|    clip_fraction        | 0.0421      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.001       |
|    loss                 | 41.3        |
|    n_updates            | 131         |
|    policy_gradient_loss | 0.00571     |
|    value_loss           | 64          |
-----------------------------------------
Eval num_timesteps=234000, episode_reward=194.07 +/- 2.21
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=191.91 +/- 6.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=191.64 +/- 6.93
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=192.42 +/- 5.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 119      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 115      |
|    time_elapsed    | 4501     |
|    total_timesteps | 235520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=236000, episode_reward=183.98 +/- 16.88
Episode length: 515.16 +/- 68.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 184          |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0043253526 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.732        |
|    learning_rate        | 0.001        |
|    loss                 | 40.8         |
|    n_updates            | 132          |
|    policy_gradient_loss | -0.00142     |
|    value_loss           | 70.6         |
------------------------------------------
Eval num_timesteps=236500, episode_reward=186.02 +/- 8.05
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=184.66 +/- 15.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=180.63 +/- 22.33
Episode length: 505.80 +/- 94.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 124      |
| time/              |          |
|    fps             | 51       |
|    iterations      | 116      |
|    time_elapsed    | 4571     |
|    total_timesteps | 237568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=238000, episode_reward=181.31 +/- 9.80
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 0.0054414375 |
|    clip_fraction        | 0.0449       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.624        |
|    learning_rate        | 0.001        |
|    loss                 | 31.6         |
|    n_updates            | 133          |
|    policy_gradient_loss | 0.015        |
|    value_loss           | 88           |
------------------------------------------
Eval num_timesteps=238500, episode_reward=183.42 +/- 11.02
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=181.76 +/- 9.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=182.12 +/- 9.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 325      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 51       |
|    iterations      | 117      |
|    time_elapsed    | 4642     |
|    total_timesteps | 239616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=240000, episode_reward=181.40 +/- 9.54
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.009036105 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.001       |
|    loss                 | 0.383       |
|    n_updates            | 134         |
|    policy_gradient_loss | -0.0027     |
|    value_loss           | 72.3        |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=183.07 +/- 10.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=184.75 +/- 10.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=183.37 +/- 11.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 332      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 51       |
|    iterations      | 118      |
|    time_elapsed    | 4714     |
|    total_timesteps | 241664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=242000, episode_reward=191.80 +/- 29.38
Episode length: 514.88 +/- 70.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 192          |
| time/                   |              |
|    total_timesteps      | 242000       |
| train/                  |              |
|    approx_kl            | 0.0072963336 |
|    clip_fraction        | 0.0781       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.001        |
|    loss                 | 4.91         |
|    n_updates            | 135          |
|    policy_gradient_loss | 0.00177      |
|    value_loss           | 32.8         |
------------------------------------------
Eval num_timesteps=242500, episode_reward=195.94 +/- 1.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=195.32 +/- 2.03
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=195.59 +/- 1.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 338      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 50       |
|    iterations      | 119      |
|    time_elapsed    | 4785     |
|    total_timesteps | 243712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=244000, episode_reward=195.50 +/- 1.64
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 196        |
| time/                   |            |
|    total_timesteps      | 244000     |
| train/                  |            |
|    approx_kl            | 0.00527271 |
|    clip_fraction        | 0.0305     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.001      |
|    loss                 | 38.6       |
|    n_updates            | 136        |
|    policy_gradient_loss | 0.00312    |
|    value_loss           | 77.2       |
----------------------------------------
Eval num_timesteps=244500, episode_reward=195.14 +/- 2.01
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=195.52 +/- 1.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=195.43 +/- 1.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 345      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 50       |
|    iterations      | 120      |
|    time_elapsed    | 4857     |
|    total_timesteps | 245760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=246000, episode_reward=195.51 +/- 2.14
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0038690052 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.568        |
|    learning_rate        | 0.001        |
|    loss                 | 29           |
|    n_updates            | 137          |
|    policy_gradient_loss | 0.00861      |
|    value_loss           | 80.2         |
------------------------------------------
Eval num_timesteps=246500, episode_reward=195.41 +/- 2.16
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=195.28 +/- 2.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=194.96 +/- 3.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 345      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 50       |
|    iterations      | 121      |
|    time_elapsed    | 4930     |
|    total_timesteps | 247808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=248000, episode_reward=192.05 +/- 16.18
Episode length: 516.40 +/- 60.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 192          |
| time/                   |              |
|    total_timesteps      | 248000       |
| train/                  |              |
|    approx_kl            | 0.0048436904 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.001        |
|    loss                 | 25.9         |
|    n_updates            | 138          |
|    policy_gradient_loss | 0.00529      |
|    value_loss           | 45.2         |
------------------------------------------
Eval num_timesteps=248500, episode_reward=193.76 +/- 4.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=192.51 +/- 15.65
Episode length: 515.60 +/- 65.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=193.73 +/- 5.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 345      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 122      |
|    time_elapsed    | 5002     |
|    total_timesteps | 249856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=250000, episode_reward=184.36 +/- 25.87
Episode length: 511.18 +/- 68.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 511         |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.006626861 |
|    clip_fraction        | 0.053       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.001       |
|    loss                 | 43.3        |
|    n_updates            | 139         |
|    policy_gradient_loss | 0.000789    |
|    value_loss           | 134         |
-----------------------------------------
Eval num_timesteps=250500, episode_reward=186.19 +/- 30.21
Episode length: 515.30 +/- 67.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=182.80 +/- 33.60
Episode length: 506.62 +/- 90.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=179.78 +/- 41.55
Episode length: 493.06 +/- 110.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 341      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 123      |
|    time_elapsed    | 5071     |
|    total_timesteps | 251904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=252000, episode_reward=188.00 +/- 17.12
Episode length: 515.52 +/- 66.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.004243685 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.001       |
|    loss                 | 55.3        |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.005       |
|    value_loss           | 78.4        |
-----------------------------------------
Eval num_timesteps=252500, episode_reward=189.38 +/- 7.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=190.51 +/- 6.90
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=190.59 +/- 6.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 350      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 124      |
|    time_elapsed    | 5143     |
|    total_timesteps | 253952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=254000, episode_reward=187.44 +/- 10.36
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 187          |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 0.0041192197 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.736        |
|    learning_rate        | 0.001        |
|    loss                 | 20.5         |
|    n_updates            | 141          |
|    policy_gradient_loss | 0.00845      |
|    value_loss           | 45.1         |
------------------------------------------
Eval num_timesteps=254500, episode_reward=186.94 +/- 10.88
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=188.51 +/- 10.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=190.30 +/- 10.11
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=185.93 +/- 10.94
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 365      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 125      |
|    time_elapsed    | 5232     |
|    total_timesteps | 256000   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=256500, episode_reward=194.82 +/- 5.68
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 195        |
| time/                   |            |
|    total_timesteps      | 256500     |
| train/                  |            |
|    approx_kl            | 0.01405268 |
|    clip_fraction        | 0.0321     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.001      |
|    loss                 | 37         |
|    n_updates            | 143        |
|    policy_gradient_loss | 0.00448    |
|    value_loss           | 61         |
----------------------------------------
Eval num_timesteps=257000, episode_reward=195.86 +/- 3.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=195.99 +/- 3.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=196.32 +/- 1.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 384      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 126      |
|    time_elapsed    | 5304     |
|    total_timesteps | 258048   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=258500, episode_reward=195.41 +/- 1.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.009556172 |
|    clip_fraction        | 0.016       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.001       |
|    loss                 | 30.4        |
|    n_updates            | 145         |
|    policy_gradient_loss | -0.00324    |
|    value_loss           | 48          |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=194.13 +/- 3.89
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=193.49 +/- 5.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=193.31 +/- 5.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 397      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 127      |
|    time_elapsed    | 5375     |
|    total_timesteps | 260096   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=260500, episode_reward=191.36 +/- 16.06
Episode length: 514.92 +/- 70.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 260500       |
| train/                  |              |
|    approx_kl            | 0.0062669083 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.619        |
|    learning_rate        | 0.001        |
|    loss                 | 39.7         |
|    n_updates            | 147          |
|    policy_gradient_loss | 0.00225      |
|    value_loss           | 96.2         |
------------------------------------------
Eval num_timesteps=261000, episode_reward=194.25 +/- 4.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=192.78 +/- 7.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=189.87 +/- 22.62
Episode length: 505.56 +/- 95.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 128      |
|    time_elapsed    | 5446     |
|    total_timesteps | 262144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=262500, episode_reward=197.07 +/- 1.09
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 262500     |
| train/                  |            |
|    approx_kl            | 0.00543502 |
|    clip_fraction        | 0.00156    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.689      |
|    learning_rate        | 0.001      |
|    loss                 | 89.4       |
|    n_updates            | 148        |
|    policy_gradient_loss | -0.00386   |
|    value_loss           | 72.1       |
----------------------------------------
Eval num_timesteps=263000, episode_reward=197.00 +/- 1.06
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=196.70 +/- 3.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=196.09 +/- 4.95
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 429      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 129      |
|    time_elapsed    | 5519     |
|    total_timesteps | 264192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=264500, episode_reward=190.70 +/- 33.65
Episode length: 511.74 +/- 71.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 512          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 0.0048676385 |
|    clip_fraction        | 0.00651      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.84         |
|    learning_rate        | 0.001        |
|    loss                 | 27           |
|    n_updates            | 149          |
|    policy_gradient_loss | -0.000471    |
|    value_loss           | 45           |
------------------------------------------
Eval num_timesteps=265000, episode_reward=188.32 +/- 41.78
Episode length: 505.52 +/- 95.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=188.79 +/- 41.52
Episode length: 505.56 +/- 95.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=192.96 +/- 29.66
Episode length: 515.02 +/- 69.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 439      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 130      |
|    time_elapsed    | 5589     |
|    total_timesteps | 266240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=266500, episode_reward=195.13 +/- 1.57
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 195          |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0069457227 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.73         |
|    learning_rate        | 0.001        |
|    loss                 | 47.8         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00208     |
|    value_loss           | 59.2         |
------------------------------------------
Eval num_timesteps=267000, episode_reward=194.90 +/- 2.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=190.68 +/- 21.54
Episode length: 507.14 +/- 87.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=195.17 +/- 1.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 454      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 131      |
|    time_elapsed    | 5659     |
|    total_timesteps | 268288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=268500, episode_reward=90.56 +/- 86.50
Episode length: 226.90 +/- 233.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 227          |
|    mean_reward          | 90.6         |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 0.0072572036 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.588        |
|    learning_rate        | 0.001        |
|    loss                 | 62.2         |
|    n_updates            | 151          |
|    policy_gradient_loss | 0.01         |
|    value_loss           | 56.5         |
------------------------------------------
Eval num_timesteps=269000, episode_reward=113.08 +/- 86.59
Episode length: 296.94 +/- 237.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | 113      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=125.70 +/- 84.78
Episode length: 334.82 +/- 233.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 335      |
|    mean_reward     | 126      |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=134.01 +/- 71.70
Episode length: 339.50 +/- 228.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 340      |
|    mean_reward     | 134      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 451      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 132      |
|    time_elapsed    | 5700     |
|    total_timesteps | 270336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=270500, episode_reward=171.03 +/- 23.34
Episode length: 501.78 +/- 91.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 502        |
|    mean_reward          | 171        |
| time/                   |            |
|    total_timesteps      | 270500     |
| train/                  |            |
|    approx_kl            | 0.00590757 |
|    clip_fraction        | 0.0439     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.3        |
|    learning_rate        | 0.001      |
|    loss                 | 140        |
|    n_updates            | 152        |
|    policy_gradient_loss | 0.00397    |
|    value_loss           | 160        |
----------------------------------------
Eval num_timesteps=271000, episode_reward=160.84 +/- 41.21
Episode length: 466.72 +/- 145.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=161.12 +/- 41.34
Episode length: 469.70 +/- 139.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 470      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=176.53 +/- 25.02
Episode length: 508.96 +/- 79.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 509      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 449      |
|    ep_rew_mean     | 150      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 133      |
|    time_elapsed    | 5766     |
|    total_timesteps | 272384   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=272500, episode_reward=113.81 +/- 55.95
Episode length: 289.56 +/- 163.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 290         |
|    mean_reward          | 114         |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.021418076 |
|    clip_fraction        | 0.0551      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.001       |
|    loss                 | 36.7        |
|    n_updates            | 154         |
|    policy_gradient_loss | 0.00765     |
|    value_loss           | 112         |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=100.68 +/- 48.19
Episode length: 263.68 +/- 146.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 101      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=104.95 +/- 48.39
Episode length: 265.06 +/- 155.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 105      |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=92.13 +/- 39.86
Episode length: 268.72 +/- 121.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 92.1     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 442      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 134      |
|    time_elapsed    | 5804     |
|    total_timesteps | 274432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=274500, episode_reward=120.30 +/- 69.16
Episode length: 334.62 +/- 179.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 335          |
|    mean_reward          | 120          |
| time/                   |              |
|    total_timesteps      | 274500       |
| train/                  |              |
|    approx_kl            | 0.0047007077 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.289        |
|    learning_rate        | 0.001        |
|    loss                 | 60.8         |
|    n_updates            | 155          |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 120          |
------------------------------------------
Eval num_timesteps=275000, episode_reward=115.57 +/- 74.00
Episode length: 313.38 +/- 181.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 313      |
|    mean_reward     | 116      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=106.43 +/- 61.24
Episode length: 302.66 +/- 169.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | 106      |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=123.81 +/- 65.82
Episode length: 345.34 +/- 167.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 345      |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 431      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 135      |
|    time_elapsed    | 5850     |
|    total_timesteps | 276480   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=276500, episode_reward=189.48 +/- 7.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 276500      |
| train/                  |             |
|    approx_kl            | 0.007892744 |
|    clip_fraction        | 0.0357      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.406       |
|    learning_rate        | 0.001       |
|    loss                 | 11.3        |
|    n_updates            | 157         |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 118         |
-----------------------------------------
Eval num_timesteps=277000, episode_reward=191.37 +/- 13.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=190.06 +/- 6.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=190.39 +/- 5.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=189.65 +/- 6.92
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 431      |
|    ep_rew_mean     | 147      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 136      |
|    time_elapsed    | 5939     |
|    total_timesteps | 278528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=279000, episode_reward=172.24 +/- 38.77
Episode length: 475.44 +/- 136.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 475         |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.006238511 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.001       |
|    loss                 | 99.7        |
|    n_updates            | 158         |
|    policy_gradient_loss | 0.0166      |
|    value_loss           | 146         |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=161.00 +/- 53.46
Episode length: 446.40 +/- 161.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 446      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=181.32 +/- 29.21
Episode length: 506.14 +/- 76.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=179.80 +/- 38.66
Episode length: 494.68 +/- 104.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 425      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 137      |
|    time_elapsed    | 6004     |
|    total_timesteps | 280576   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=281000, episode_reward=168.43 +/- 30.02
Episode length: 495.32 +/- 102.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | 168         |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.008991798 |
|    clip_fraction        | 0.0553      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.001       |
|    loss                 | 51.5        |
|    n_updates            | 160         |
|    policy_gradient_loss | 0.00443     |
|    value_loss           | 115         |
-----------------------------------------
Eval num_timesteps=281500, episode_reward=168.47 +/- 29.40
Episode length: 499.62 +/- 100.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=170.28 +/- 26.07
Episode length: 511.76 +/- 65.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 512      |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=170.89 +/- 15.54
Episode length: 517.16 +/- 54.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 425      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 138      |
|    time_elapsed    | 6073     |
|    total_timesteps | 282624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=283000, episode_reward=184.04 +/- 27.48
Episode length: 495.98 +/- 114.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 496         |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 283000      |
| train/                  |             |
|    approx_kl            | 0.004324533 |
|    clip_fraction        | 0.0169      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.443       |
|    learning_rate        | 0.001       |
|    loss                 | 12.6        |
|    n_updates            | 161         |
|    policy_gradient_loss | 0.00294     |
|    value_loss           | 78.8        |
-----------------------------------------
Eval num_timesteps=283500, episode_reward=186.87 +/- 17.92
Episode length: 515.32 +/- 67.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=189.08 +/- 10.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=184.43 +/- 27.34
Episode length: 495.92 +/- 115.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 420      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 139      |
|    time_elapsed    | 6142     |
|    total_timesteps | 284672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=285000, episode_reward=191.36 +/- 21.57
Episode length: 505.74 +/- 94.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 0.0031330942 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.413        |
|    learning_rate        | 0.001        |
|    loss                 | 91.9         |
|    n_updates            | 162          |
|    policy_gradient_loss | 0.0156       |
|    value_loss           | 120          |
------------------------------------------
Eval num_timesteps=285500, episode_reward=195.82 +/- 1.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=195.96 +/- 1.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=196.02 +/- 1.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 416      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 140      |
|    time_elapsed    | 6213     |
|    total_timesteps | 286720   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=287000, episode_reward=180.37 +/- 9.38
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 180         |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.019534482 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.001       |
|    loss                 | 89.2        |
|    n_updates            | 164         |
|    policy_gradient_loss | 0.00448     |
|    value_loss           | 97          |
-----------------------------------------
Eval num_timesteps=287500, episode_reward=178.18 +/- 8.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=180.02 +/- 9.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=182.27 +/- 9.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 417      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 141      |
|    time_elapsed    | 6286     |
|    total_timesteps | 288768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=289000, episode_reward=174.00 +/- 0.81
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 174          |
| time/                   |              |
|    total_timesteps      | 289000       |
| train/                  |              |
|    approx_kl            | 0.0038065317 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.664        |
|    learning_rate        | 0.001        |
|    loss                 | 66.7         |
|    n_updates            | 165          |
|    policy_gradient_loss | 0.000566     |
|    value_loss           | 87.3         |
------------------------------------------
Eval num_timesteps=289500, episode_reward=173.78 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=173.95 +/- 1.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=173.62 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 421      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 142      |
|    time_elapsed    | 6358     |
|    total_timesteps | 290816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=291000, episode_reward=195.74 +/- 4.03
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0061999536 |
|    clip_fraction        | 0.043        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.824        |
|    learning_rate        | 0.001        |
|    loss                 | 7.01         |
|    n_updates            | 166          |
|    policy_gradient_loss | 0.00482      |
|    value_loss           | 33.1         |
------------------------------------------
Eval num_timesteps=291500, episode_reward=191.66 +/- 17.00
Episode length: 517.16 +/- 54.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=194.01 +/- 16.09
Episode length: 515.86 +/- 63.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=195.77 +/- 3.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 417      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 143      |
|    time_elapsed    | 6429     |
|    total_timesteps | 292864   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=293000, episode_reward=197.01 +/- 0.68
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.015794195 |
|    clip_fraction        | 0.00873     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.001       |
|    loss                 | 13.8        |
|    n_updates            | 168         |
|    policy_gradient_loss | 0.0112      |
|    value_loss           | 43.6        |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=197.08 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=196.84 +/- 0.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=197.07 +/- 0.93
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 421      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 144      |
|    time_elapsed    | 6501     |
|    total_timesteps | 294912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=295000, episode_reward=197.03 +/- 0.68
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.003554115 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.001       |
|    loss                 | 10.9        |
|    n_updates            | 169         |
|    policy_gradient_loss | 0.00702     |
|    value_loss           | 28.5        |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=197.10 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=197.04 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=197.11 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 421      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 145      |
|    time_elapsed    | 6573     |
|    total_timesteps | 296960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=297000, episode_reward=197.10 +/- 0.71
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 297000     |
| train/                  |            |
|    approx_kl            | 0.00525281 |
|    clip_fraction        | 0.0167     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.001      |
|    loss                 | 2.95       |
|    n_updates            | 170        |
|    policy_gradient_loss | 0.035      |
|    value_loss           | 24.7       |
----------------------------------------
Eval num_timesteps=297500, episode_reward=197.12 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=197.24 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=196.97 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=197.03 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 421      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 146      |
|    time_elapsed    | 6662     |
|    total_timesteps | 299008   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=299500, episode_reward=196.90 +/- 0.65
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 0.0071651414 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.855        |
|    learning_rate        | 0.001        |
|    loss                 | 33.1         |
|    n_updates            | 172          |
|    policy_gradient_loss | 0.0044       |
|    value_loss           | 35.7         |
------------------------------------------
Eval num_timesteps=300000, episode_reward=197.14 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=196.88 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=196.88 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 421      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 147      |
|    time_elapsed    | 6734     |
|    total_timesteps | 301056   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=301500, episode_reward=197.43 +/- 0.87
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 301500      |
| train/                  |             |
|    approx_kl            | 0.008733929 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.001       |
|    loss                 | 28.4        |
|    n_updates            | 174         |
|    policy_gradient_loss | 0.00209     |
|    value_loss           | 50.7        |
-----------------------------------------
Eval num_timesteps=302000, episode_reward=197.24 +/- 0.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=197.54 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=197.62 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 425      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 148      |
|    time_elapsed    | 6805     |
|    total_timesteps | 303104   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=303500, episode_reward=197.64 +/- 0.64
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 303500       |
| train/                  |              |
|    approx_kl            | 0.0097844815 |
|    clip_fraction        | 0.0615       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.841        |
|    learning_rate        | 0.001        |
|    loss                 | 17.1         |
|    n_updates            | 179          |
|    policy_gradient_loss | -0.002       |
|    value_loss           | 38.6         |
------------------------------------------
Eval num_timesteps=304000, episode_reward=197.61 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=197.57 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=197.40 +/- 0.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 425      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 149      |
|    time_elapsed    | 6877     |
|    total_timesteps | 305152   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=305500, episode_reward=197.06 +/- 0.47
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 305500      |
| train/                  |             |
|    approx_kl            | 0.017299538 |
|    clip_fraction        | 0.0239      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.001       |
|    loss                 | 24.4        |
|    n_updates            | 181         |
|    policy_gradient_loss | -0.000652   |
|    value_loss           | 31.5        |
-----------------------------------------
Eval num_timesteps=306000, episode_reward=196.97 +/- 0.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=196.92 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=197.05 +/- 0.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 425      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 150      |
|    time_elapsed    | 6949     |
|    total_timesteps | 307200   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=307500, episode_reward=197.26 +/- 0.83
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 307500     |
| train/                  |            |
|    approx_kl            | 0.00952952 |
|    clip_fraction        | 0.0234     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.001      |
|    loss                 | 2.92       |
|    n_updates            | 183        |
|    policy_gradient_loss | 0.0038     |
|    value_loss           | 28.6       |
----------------------------------------
Eval num_timesteps=308000, episode_reward=197.39 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=197.43 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=197.44 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 425      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 151      |
|    time_elapsed    | 7021     |
|    total_timesteps | 309248   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=309500, episode_reward=196.85 +/- 0.55
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 309500      |
| train/                  |             |
|    approx_kl            | 0.015650427 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.001       |
|    loss                 | 1.66        |
|    n_updates            | 185         |
|    policy_gradient_loss | -0.000793   |
|    value_loss           | 16.4        |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=196.89 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=197.12 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=196.75 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 428      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 152      |
|    time_elapsed    | 7093     |
|    total_timesteps | 311296   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=311500, episode_reward=197.14 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 311500     |
| train/                  |            |
|    approx_kl            | 0.00802528 |
|    clip_fraction        | 0.032      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.001      |
|    loss                 | 12         |
|    n_updates            | 187        |
|    policy_gradient_loss | -0.00132   |
|    value_loss           | 19         |
----------------------------------------
Eval num_timesteps=312000, episode_reward=197.13 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=197.12 +/- 0.85
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=197.19 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 439      |
|    ep_rew_mean     | 150      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 153      |
|    time_elapsed    | 7165     |
|    total_timesteps | 313344   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=313500, episode_reward=197.30 +/- 0.73
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 313500      |
| train/                  |             |
|    approx_kl            | 0.016113687 |
|    clip_fraction        | 0.0578      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 6.18        |
|    n_updates            | 192         |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 16.7        |
-----------------------------------------
Eval num_timesteps=314000, episode_reward=197.52 +/- 0.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=197.45 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=197.33 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 454      |
|    ep_rew_mean     | 156      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 154      |
|    time_elapsed    | 7236     |
|    total_timesteps | 315392   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=315500, episode_reward=197.00 +/- 0.67
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 315500     |
| train/                  |            |
|    approx_kl            | 0.01198858 |
|    clip_fraction        | 0.0625     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.001      |
|    loss                 | 11.2       |
|    n_updates            | 194        |
|    policy_gradient_loss | 0.00263    |
|    value_loss           | 28.4       |
----------------------------------------
Eval num_timesteps=316000, episode_reward=197.09 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=197.27 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=196.99 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 462      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 155      |
|    time_elapsed    | 7308     |
|    total_timesteps | 317440   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=317500, episode_reward=197.00 +/- 0.80
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 317500      |
| train/                  |             |
|    approx_kl            | 0.015117602 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.001       |
|    loss                 | 8.54        |
|    n_updates            | 196         |
|    policy_gradient_loss | -0.000517   |
|    value_loss           | 27.9        |
-----------------------------------------
Eval num_timesteps=318000, episode_reward=197.08 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=197.00 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=197.00 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 156      |
|    time_elapsed    | 7380     |
|    total_timesteps | 319488   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=319500, episode_reward=197.42 +/- 0.65
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 319500      |
| train/                  |             |
|    approx_kl            | 0.010992179 |
|    clip_fraction        | 0.049       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.001       |
|    loss                 | 19.7        |
|    n_updates            | 199         |
|    policy_gradient_loss | -0.000473   |
|    value_loss           | 25.5        |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=197.32 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=197.12 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=197.16 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=197.17 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 157      |
|    time_elapsed    | 7470     |
|    total_timesteps | 321536   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=322000, episode_reward=194.65 +/- 15.35
Episode length: 515.86 +/- 63.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 322000      |
| train/                  |             |
|    approx_kl            | 0.011852554 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.001       |
|    loss                 | 20.9        |
|    n_updates            | 201         |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 34.1        |
-----------------------------------------
Eval num_timesteps=322500, episode_reward=194.67 +/- 15.35
Episode length: 515.76 +/- 64.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=188.24 +/- 29.74
Episode length: 487.24 +/- 128.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=196.96 +/- 0.97
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 158      |
|    time_elapsed    | 7540     |
|    total_timesteps | 323584   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=324000, episode_reward=196.85 +/- 0.51
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.018682152 |
|    clip_fraction        | 0.0423      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.001       |
|    loss                 | 4.91        |
|    n_updates            | 203         |
|    policy_gradient_loss | 0.00136     |
|    value_loss           | 39          |
-----------------------------------------
Eval num_timesteps=324500, episode_reward=196.78 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=196.86 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=196.84 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 159      |
|    time_elapsed    | 7612     |
|    total_timesteps | 325632   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.03
Eval num_timesteps=326000, episode_reward=196.71 +/- 0.67
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 326000      |
| train/                  |             |
|    approx_kl            | 0.018359983 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.001       |
|    loss                 | 22.1        |
|    n_updates            | 209         |
|    policy_gradient_loss | -0.00285    |
|    value_loss           | 31.1        |
-----------------------------------------
Eval num_timesteps=326500, episode_reward=196.81 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=196.96 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=196.82 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 160      |
|    time_elapsed    | 7684     |
|    total_timesteps | 327680   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=328000, episode_reward=197.12 +/- 0.62
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.028770944 |
|    clip_fraction        | 0.0175      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.001       |
|    loss                 | 17.6        |
|    n_updates            | 211         |
|    policy_gradient_loss | -0.00162    |
|    value_loss           | 27.7        |
-----------------------------------------
Eval num_timesteps=328500, episode_reward=197.29 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=197.19 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=197.13 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 161      |
|    time_elapsed    | 7755     |
|    total_timesteps | 329728   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=330000, episode_reward=197.44 +/- 0.67
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.022744806 |
|    clip_fraction        | 0.0362      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 4.81        |
|    n_updates            | 215         |
|    policy_gradient_loss | -0.0009     |
|    value_loss           | 18.1        |
-----------------------------------------
Eval num_timesteps=330500, episode_reward=197.35 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=197.18 +/- 0.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=197.35 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 162      |
|    time_elapsed    | 7828     |
|    total_timesteps | 331776   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=332000, episode_reward=197.80 +/- 0.63
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0072591887 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 14.5         |
|    n_updates            | 217          |
|    policy_gradient_loss | -0.00332     |
|    value_loss           | 12.6         |
------------------------------------------
New best mean reward!
Eval num_timesteps=332500, episode_reward=195.60 +/- 15.61
Episode length: 515.42 +/- 67.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=197.92 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
New best mean reward!
Eval num_timesteps=333500, episode_reward=197.81 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 163      |
|    time_elapsed    | 7898     |
|    total_timesteps | 333824   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=334000, episode_reward=197.57 +/- 0.72
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 334000      |
| train/                  |             |
|    approx_kl            | 0.008591318 |
|    clip_fraction        | 0.00967     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.001       |
|    loss                 | 15.3        |
|    n_updates            | 219         |
|    policy_gradient_loss | -0.000592   |
|    value_loss           | 33          |
-----------------------------------------
Eval num_timesteps=334500, episode_reward=197.63 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=197.69 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=197.66 +/- 0.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 164      |
|    time_elapsed    | 7970     |
|    total_timesteps | 335872   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=336000, episode_reward=196.88 +/- 0.61
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 336000       |
| train/                  |              |
|    approx_kl            | 0.0063958582 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.001        |
|    loss                 | 16.9         |
|    n_updates            | 221          |
|    policy_gradient_loss | -0.00252     |
|    value_loss           | 21           |
------------------------------------------
Eval num_timesteps=336500, episode_reward=197.01 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=197.03 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=197.04 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 165      |
|    time_elapsed    | 8042     |
|    total_timesteps | 337920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=338000, episode_reward=197.55 +/- 0.76
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.005210812 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 21.5        |
|    n_updates            | 222         |
|    policy_gradient_loss | 0.000487    |
|    value_loss           | 14          |
-----------------------------------------
Eval num_timesteps=338500, episode_reward=197.38 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=197.41 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=197.46 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 166      |
|    time_elapsed    | 8113     |
|    total_timesteps | 339968   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=340000, episode_reward=170.02 +/- 46.86
Episode length: 446.34 +/- 148.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 446         |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.011116172 |
|    clip_fraction        | 0.0632      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.001       |
|    loss                 | 0.43        |
|    n_updates            | 225         |
|    policy_gradient_loss | -0.00349    |
|    value_loss           | 21.5        |
-----------------------------------------
Eval num_timesteps=340500, episode_reward=176.90 +/- 42.10
Episode length: 457.72 +/- 146.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 458      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=178.98 +/- 40.25
Episode length: 476.76 +/- 115.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=179.12 +/- 40.32
Episode length: 466.58 +/- 135.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=183.33 +/- 35.62
Episode length: 484.66 +/- 109.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 485      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 167      |
|    time_elapsed    | 8192     |
|    total_timesteps | 342016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=342500, episode_reward=196.85 +/- 1.16
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 342500       |
| train/                  |              |
|    approx_kl            | 0.0058144736 |
|    clip_fraction        | 0.0534       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.507        |
|    learning_rate        | 0.001        |
|    loss                 | 70.5         |
|    n_updates            | 226          |
|    policy_gradient_loss | 0.00695      |
|    value_loss           | 83.6         |
------------------------------------------
Eval num_timesteps=343000, episode_reward=196.73 +/- 0.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=196.78 +/- 1.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=197.04 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 168      |
|    time_elapsed    | 8264     |
|    total_timesteps | 344064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=344500, episode_reward=196.38 +/- 1.13
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 344500      |
| train/                  |             |
|    approx_kl            | 0.004371879 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.001       |
|    loss                 | 50.9        |
|    n_updates            | 227         |
|    policy_gradient_loss | -0.00614    |
|    value_loss           | 92.5        |
-----------------------------------------
Eval num_timesteps=345000, episode_reward=196.41 +/- 1.03
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=196.56 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=196.59 +/- 1.11
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 169      |
|    time_elapsed    | 8334     |
|    total_timesteps | 346112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=346500, episode_reward=197.26 +/- 0.96
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 346500      |
| train/                  |             |
|    approx_kl            | 0.004286714 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.001       |
|    loss                 | 70.7        |
|    n_updates            | 228         |
|    policy_gradient_loss | 0.00677     |
|    value_loss           | 59.1        |
-----------------------------------------
Eval num_timesteps=347000, episode_reward=196.95 +/- 1.11
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=197.19 +/- 0.98
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=197.18 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 170      |
|    time_elapsed    | 8406     |
|    total_timesteps | 348160   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=348500, episode_reward=196.96 +/- 1.16
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 348500       |
| train/                  |              |
|    approx_kl            | 0.0059655057 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.662        |
|    learning_rate        | 0.001        |
|    loss                 | 72.7         |
|    n_updates            | 229          |
|    policy_gradient_loss | 0.00342      |
|    value_loss           | 63.1         |
------------------------------------------
Eval num_timesteps=349000, episode_reward=196.86 +/- 1.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=197.01 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=197.34 +/- 0.89
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 171      |
|    time_elapsed    | 8478     |
|    total_timesteps | 350208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=350500, episode_reward=197.28 +/- 0.71
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 350500       |
| train/                  |              |
|    approx_kl            | 0.0064444733 |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.72         |
|    learning_rate        | 0.001        |
|    loss                 | 56.8         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00224     |
|    value_loss           | 51.2         |
------------------------------------------
Eval num_timesteps=351000, episode_reward=197.23 +/- 0.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=197.38 +/- 0.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=197.37 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 172      |
|    time_elapsed    | 8549     |
|    total_timesteps | 352256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=352500, episode_reward=197.40 +/- 0.71
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 352500       |
| train/                  |              |
|    approx_kl            | 0.0033216272 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 43.7         |
|    n_updates            | 231          |
|    policy_gradient_loss | 0.00232      |
|    value_loss           | 41.7         |
------------------------------------------
Eval num_timesteps=353000, episode_reward=197.54 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=197.51 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=197.48 +/- 0.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 173      |
|    time_elapsed    | 8619     |
|    total_timesteps | 354304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=354500, episode_reward=197.47 +/- 0.54
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 354500      |
| train/                  |             |
|    approx_kl            | 0.003915499 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.001       |
|    loss                 | 50.7        |
|    n_updates            | 232         |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 77.7        |
-----------------------------------------
Eval num_timesteps=355000, episode_reward=197.37 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=197.53 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=197.24 +/- 0.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 174      |
|    time_elapsed    | 8690     |
|    total_timesteps | 356352   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=356500, episode_reward=197.60 +/- 0.54
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 356500     |
| train/                  |            |
|    approx_kl            | 0.01254273 |
|    clip_fraction        | 0.0261     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.001      |
|    loss                 | 28         |
|    n_updates            | 234        |
|    policy_gradient_loss | 0.00393    |
|    value_loss           | 49.2       |
----------------------------------------
Eval num_timesteps=357000, episode_reward=197.59 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=197.50 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=197.46 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 175      |
|    time_elapsed    | 8761     |
|    total_timesteps | 358400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=358500, episode_reward=197.33 +/- 0.60
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 358500      |
| train/                  |             |
|    approx_kl            | 0.004238865 |
|    clip_fraction        | 0.038       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.001       |
|    loss                 | 11.6        |
|    n_updates            | 235         |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 34.8        |
-----------------------------------------
Eval num_timesteps=359000, episode_reward=197.30 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=197.36 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=197.13 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 176      |
|    time_elapsed    | 8834     |
|    total_timesteps | 360448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=360500, episode_reward=197.12 +/- 1.08
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 360500       |
| train/                  |              |
|    approx_kl            | 0.0062578646 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0.842        |
|    learning_rate        | 0.001        |
|    loss                 | 27.1         |
|    n_updates            | 236          |
|    policy_gradient_loss | 9.45e-06     |
|    value_loss           | 45.4         |
------------------------------------------
Eval num_timesteps=361000, episode_reward=196.97 +/- 1.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=196.91 +/- 1.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=194.96 +/- 15.85
Episode length: 515.00 +/- 70.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 177      |
|    time_elapsed    | 8905     |
|    total_timesteps | 362496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=362500, episode_reward=197.11 +/- 0.99
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 0.0049009556 |
|    clip_fraction        | 0.0433       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.889        |
|    learning_rate        | 0.001        |
|    loss                 | 39.3         |
|    n_updates            | 237          |
|    policy_gradient_loss | 0.00369      |
|    value_loss           | 37           |
------------------------------------------
Eval num_timesteps=363000, episode_reward=197.19 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=197.06 +/- 0.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=197.29 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=197.14 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 178      |
|    time_elapsed    | 8993     |
|    total_timesteps | 364544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=365000, episode_reward=197.70 +/- 0.59
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 365000       |
| train/                  |              |
|    approx_kl            | 0.0027427399 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.769        |
|    learning_rate        | 0.001        |
|    loss                 | 67.6         |
|    n_updates            | 238          |
|    policy_gradient_loss | 0.00742      |
|    value_loss           | 61.6         |
------------------------------------------
Eval num_timesteps=365500, episode_reward=197.72 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=197.63 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=197.48 +/- 0.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 179      |
|    time_elapsed    | 9065     |
|    total_timesteps | 366592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=367000, episode_reward=181.95 +/- 38.21
Episode length: 460.16 +/- 160.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 460          |
|    mean_reward          | 182          |
| time/                   |              |
|    total_timesteps      | 367000       |
| train/                  |              |
|    approx_kl            | 0.0038936005 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.872        |
|    learning_rate        | 0.001        |
|    loss                 | 6.68         |
|    n_updates            | 239          |
|    policy_gradient_loss | 0.00503      |
|    value_loss           | 33.8         |
------------------------------------------
Eval num_timesteps=367500, episode_reward=182.17 +/- 38.30
Episode length: 460.42 +/- 160.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 460      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=175.61 +/- 44.20
Episode length: 431.12 +/- 188.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=188.62 +/- 29.93
Episode length: 488.40 +/- 124.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 180      |
|    time_elapsed    | 9127     |
|    total_timesteps | 368640   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=369000, episode_reward=197.34 +/- 0.94
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 369000      |
| train/                  |             |
|    approx_kl            | 0.020012932 |
|    clip_fraction        | 0.0379      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.001       |
|    loss                 | 55.7        |
|    n_updates            | 241         |
|    policy_gradient_loss | 0.00148     |
|    value_loss           | 34.6        |
-----------------------------------------
Eval num_timesteps=369500, episode_reward=197.12 +/- 1.12
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=197.07 +/- 1.21
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=197.01 +/- 1.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 181      |
|    time_elapsed    | 9199     |
|    total_timesteps | 370688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=371000, episode_reward=193.05 +/- 15.62
Episode length: 515.02 +/- 69.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 371000       |
| train/                  |              |
|    approx_kl            | 0.0041658953 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.798        |
|    learning_rate        | 0.001        |
|    loss                 | 50.2         |
|    n_updates            | 242          |
|    policy_gradient_loss | 0.0125       |
|    value_loss           | 36.4         |
------------------------------------------
Eval num_timesteps=371500, episode_reward=191.09 +/- 29.72
Episode length: 515.06 +/- 69.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=195.42 +/- 2.03
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=191.16 +/- 22.14
Episode length: 505.26 +/- 96.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 182      |
|    time_elapsed    | 9269     |
|    total_timesteps | 372736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=373000, episode_reward=196.89 +/- 0.79
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 373000      |
| train/                  |             |
|    approx_kl            | 0.004756251 |
|    clip_fraction        | 0.048       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.001       |
|    loss                 | 101         |
|    n_updates            | 243         |
|    policy_gradient_loss | 0.00207     |
|    value_loss           | 116         |
-----------------------------------------
Eval num_timesteps=373500, episode_reward=196.98 +/- 0.91
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=197.01 +/- 1.06
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=196.83 +/- 0.89
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 183      |
|    time_elapsed    | 9341     |
|    total_timesteps | 374784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=375000, episode_reward=192.64 +/- 29.77
Episode length: 515.02 +/- 69.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0030462597 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.801        |
|    learning_rate        | 0.001        |
|    loss                 | 33.8         |
|    n_updates            | 244          |
|    policy_gradient_loss | 0.00465      |
|    value_loss           | 47           |
------------------------------------------
Eval num_timesteps=375500, episode_reward=196.83 +/- 1.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=196.67 +/- 1.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=192.45 +/- 29.61
Episode length: 514.94 +/- 70.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 184      |
|    time_elapsed    | 9411     |
|    total_timesteps | 376832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=377000, episode_reward=197.18 +/- 1.07
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 377000     |
| train/                  |            |
|    approx_kl            | 0.00338866 |
|    clip_fraction        | 0.0186     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.001      |
|    loss                 | 26.1       |
|    n_updates            | 245        |
|    policy_gradient_loss | -6.36e-05  |
|    value_loss           | 49         |
----------------------------------------
Eval num_timesteps=377500, episode_reward=197.21 +/- 1.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=197.67 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=193.01 +/- 29.53
Episode length: 514.90 +/- 70.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 185      |
|    time_elapsed    | 9482     |
|    total_timesteps | 378880   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=379000, episode_reward=197.65 +/- 0.70
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.009396279 |
|    clip_fraction        | 0.0331      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 2.04        |
|    n_updates            | 247         |
|    policy_gradient_loss | -0.00353    |
|    value_loss           | 11.5        |
-----------------------------------------
Eval num_timesteps=379500, episode_reward=197.63 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=197.71 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=197.66 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 186      |
|    time_elapsed    | 9554     |
|    total_timesteps | 380928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=381000, episode_reward=193.34 +/- 21.64
Episode length: 505.44 +/- 95.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 505         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.015340001 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.001       |
|    loss                 | 7.1         |
|    n_updates            | 249         |
|    policy_gradient_loss | 0.00305     |
|    value_loss           | 18          |
-----------------------------------------
Eval num_timesteps=381500, episode_reward=192.96 +/- 21.77
Episode length: 506.68 +/- 89.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=184.17 +/- 35.86
Episode length: 466.80 +/- 157.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=195.51 +/- 15.46
Episode length: 516.24 +/- 61.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 187      |
|    time_elapsed    | 9623     |
|    total_timesteps | 382976   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=383000, episode_reward=196.45 +/- 1.01
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 383000      |
| train/                  |             |
|    approx_kl            | 0.006743058 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.001       |
|    loss                 | 55.8        |
|    n_updates            | 251         |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 27.3        |
-----------------------------------------
Eval num_timesteps=383500, episode_reward=191.89 +/- 21.35
Episode length: 504.90 +/- 98.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=191.99 +/- 29.25
Episode length: 515.12 +/- 69.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=187.97 +/- 41.26
Episode length: 505.78 +/- 94.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=175.28 +/- 63.03
Episode length: 475.28 +/- 149.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 475      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 188      |
|    time_elapsed    | 9709     |
|    total_timesteps | 385024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=385500, episode_reward=194.51 +/- 16.23
Episode length: 515.70 +/- 65.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 385500      |
| train/                  |             |
|    approx_kl            | 0.005349152 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.001       |
|    loss                 | 94.3        |
|    n_updates            | 252         |
|    policy_gradient_loss | 0.0197      |
|    value_loss           | 88.3        |
-----------------------------------------
Eval num_timesteps=386000, episode_reward=188.30 +/- 36.51
Episode length: 496.16 +/- 114.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=194.65 +/- 15.70
Episode length: 515.44 +/- 66.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=194.51 +/- 15.53
Episode length: 516.00 +/- 63.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 189      |
|    time_elapsed    | 9779     |
|    total_timesteps | 387072   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=387500, episode_reward=197.39 +/- 1.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 387500      |
| train/                  |             |
|    approx_kl            | 0.010936627 |
|    clip_fraction        | 0.0473      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.001       |
|    loss                 | 22.5        |
|    n_updates            | 254         |
|    policy_gradient_loss | 0.00846     |
|    value_loss           | 43.4        |
-----------------------------------------
Eval num_timesteps=388000, episode_reward=197.56 +/- 0.91
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=197.49 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=193.48 +/- 29.59
Episode length: 515.28 +/- 68.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 190      |
|    time_elapsed    | 9851     |
|    total_timesteps | 389120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=389500, episode_reward=197.54 +/- 0.75
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 389500       |
| train/                  |              |
|    approx_kl            | 0.0057676113 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.774        |
|    learning_rate        | 0.001        |
|    loss                 | 15.6         |
|    n_updates            | 255          |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 70.8         |
------------------------------------------
Eval num_timesteps=390000, episode_reward=197.60 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=197.67 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=197.80 +/- 0.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 191      |
|    time_elapsed    | 9923     |
|    total_timesteps | 391168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=391500, episode_reward=197.76 +/- 0.53
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 391500      |
| train/                  |             |
|    approx_kl            | 0.002689753 |
|    clip_fraction        | 0.0121      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.001       |
|    loss                 | 12.5        |
|    n_updates            | 256         |
|    policy_gradient_loss | 0.000363    |
|    value_loss           | 6.97        |
-----------------------------------------
Eval num_timesteps=392000, episode_reward=197.40 +/- 0.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=197.80 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=197.62 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 192      |
|    time_elapsed    | 9993     |
|    total_timesteps | 393216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=393500, episode_reward=197.73 +/- 0.64
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 393500      |
| train/                  |             |
|    approx_kl            | 0.003025237 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 5.69        |
|    n_updates            | 257         |
|    policy_gradient_loss | -0.00274    |
|    value_loss           | 12.6        |
-----------------------------------------
Eval num_timesteps=394000, episode_reward=197.73 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=197.77 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=195.58 +/- 15.75
Episode length: 515.84 +/- 64.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 193      |
|    time_elapsed    | 10063    |
|    total_timesteps | 395264   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=395500, episode_reward=197.69 +/- 0.84
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 395500      |
| train/                  |             |
|    approx_kl            | 0.009371945 |
|    clip_fraction        | 0.0588      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 27.4        |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.000807    |
|    value_loss           | 14          |
-----------------------------------------
Eval num_timesteps=396000, episode_reward=195.41 +/- 15.60
Episode length: 517.66 +/- 51.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=195.23 +/- 15.57
Episode length: 518.84 +/- 43.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=195.52 +/- 15.60
Episode length: 518.06 +/- 48.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 194      |
|    time_elapsed    | 10134    |
|    total_timesteps | 397312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=397500, episode_reward=197.68 +/- 0.63
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 397500      |
| train/                  |             |
|    approx_kl            | 0.008826728 |
|    clip_fraction        | 0.0164      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.001       |
|    loss                 | 58.9        |
|    n_updates            | 261         |
|    policy_gradient_loss | -0.00255    |
|    value_loss           | 32.6        |
-----------------------------------------
Eval num_timesteps=398000, episode_reward=197.79 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=197.70 +/- 0.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=197.69 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 195      |
|    time_elapsed    | 10206    |
|    total_timesteps | 399360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=399500, episode_reward=197.73 +/- 0.58
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 399500      |
| train/                  |             |
|    approx_kl            | 0.003924803 |
|    clip_fraction        | 0.00703     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.001       |
|    loss                 | 47.8        |
|    n_updates            | 262         |
|    policy_gradient_loss | 0.00999     |
|    value_loss           | 53.7        |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=197.69 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=197.76 +/- 0.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=197.82 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 196      |
|    time_elapsed    | 10278    |
|    total_timesteps | 401408   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=401500, episode_reward=197.33 +/- 0.68
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 401500      |
| train/                  |             |
|    approx_kl            | 0.010899206 |
|    clip_fraction        | 0.0359      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 1.09        |
|    n_updates            | 266         |
|    policy_gradient_loss | 0.00134     |
|    value_loss           | 13.6        |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=197.46 +/- 0.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=197.62 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=197.57 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 197      |
|    time_elapsed    | 10350    |
|    total_timesteps | 403456   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=403500, episode_reward=197.27 +/- 0.69
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 403500      |
| train/                  |             |
|    approx_kl            | 0.008911897 |
|    clip_fraction        | 0.0169      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 0.762       |
|    n_updates            | 268         |
|    policy_gradient_loss | 0.00226     |
|    value_loss           | 9.22        |
-----------------------------------------
Eval num_timesteps=404000, episode_reward=197.15 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=197.30 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=197.20 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=197.38 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 198      |
|    time_elapsed    | 10439    |
|    total_timesteps | 405504   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=406000, episode_reward=197.56 +/- 0.61
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.007836296 |
|    clip_fraction        | 0.0115      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 9.62        |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.00578     |
|    value_loss           | 10.8        |
-----------------------------------------
Eval num_timesteps=406500, episode_reward=197.49 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=197.59 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=197.46 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 199      |
|    time_elapsed    | 10514    |
|    total_timesteps | 407552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=408000, episode_reward=197.60 +/- 0.66
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.010637651 |
|    clip_fraction        | 0.0207      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | 2.58        |
|    n_updates            | 272         |
|    policy_gradient_loss | -0.000525   |
|    value_loss           | 12.4        |
-----------------------------------------
Eval num_timesteps=408500, episode_reward=197.73 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=197.69 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=197.72 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 200      |
|    time_elapsed    | 10586    |
|    total_timesteps | 409600   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=410000, episode_reward=197.82 +/- 0.55
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.009272791 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 2.58        |
|    n_updates            | 274         |
|    policy_gradient_loss | 0.00377     |
|    value_loss           | 16.6        |
-----------------------------------------
Eval num_timesteps=410500, episode_reward=197.86 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=197.73 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=197.72 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 201      |
|    time_elapsed    | 10657    |
|    total_timesteps | 411648   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=412000, episode_reward=197.82 +/- 0.55
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 412000      |
| train/                  |             |
|    approx_kl            | 0.011210812 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.001       |
|    loss                 | 10.9        |
|    n_updates            | 276         |
|    policy_gradient_loss | -0.000934   |
|    value_loss           | 12.4        |
-----------------------------------------
Eval num_timesteps=412500, episode_reward=197.75 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=197.76 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=197.86 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 202      |
|    time_elapsed    | 10729    |
|    total_timesteps | 413696   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=414000, episode_reward=197.60 +/- 0.66
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.008869805 |
|    clip_fraction        | 0.0272      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 4.05        |
|    n_updates            | 281         |
|    policy_gradient_loss | 0.00133     |
|    value_loss           | 26          |
-----------------------------------------
Eval num_timesteps=414500, episode_reward=195.42 +/- 15.59
Episode length: 515.16 +/- 68.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=195.35 +/- 15.58
Episode length: 515.46 +/- 66.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=197.72 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 203      |
|    time_elapsed    | 10800    |
|    total_timesteps | 415744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=416000, episode_reward=195.65 +/- 15.61
Episode length: 515.18 +/- 68.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 416000      |
| train/                  |             |
|    approx_kl            | 0.005595767 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.001       |
|    loss                 | 69.4        |
|    n_updates            | 282         |
|    policy_gradient_loss | 0.0056      |
|    value_loss           | 76.6        |
-----------------------------------------
Eval num_timesteps=416500, episode_reward=197.83 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=197.83 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=197.87 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 204      |
|    time_elapsed    | 10871    |
|    total_timesteps | 417792   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=418000, episode_reward=195.55 +/- 15.60
Episode length: 515.04 +/- 69.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 418000      |
| train/                  |             |
|    approx_kl            | 0.011365711 |
|    clip_fraction        | 0.0665      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.001       |
|    loss                 | 3.79        |
|    n_updates            | 284         |
|    policy_gradient_loss | 0.000819    |
|    value_loss           | 50.6        |
-----------------------------------------
Eval num_timesteps=418500, episode_reward=197.76 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=195.52 +/- 15.60
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=197.72 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 205      |
|    time_elapsed    | 10941    |
|    total_timesteps | 419840   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=420000, episode_reward=197.86 +/- 0.40
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.009457018 |
|    clip_fraction        | 0.0535      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 0.338       |
|    n_updates            | 288         |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 16.5        |
-----------------------------------------
Eval num_timesteps=420500, episode_reward=197.79 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=197.76 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=197.88 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 206      |
|    time_elapsed    | 11013    |
|    total_timesteps | 421888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=422000, episode_reward=197.47 +/- 0.70
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.004441557 |
|    clip_fraction        | 0.0449      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.001       |
|    loss                 | 17.8        |
|    n_updates            | 289         |
|    policy_gradient_loss | 0.00319     |
|    value_loss           | 26.3        |
-----------------------------------------
Eval num_timesteps=422500, episode_reward=197.42 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=197.56 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=197.54 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 207      |
|    time_elapsed    | 11085    |
|    total_timesteps | 423936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=424000, episode_reward=197.68 +/- 0.93
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.005639903 |
|    clip_fraction        | 0.0733      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 5.45        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 9.17        |
-----------------------------------------
Eval num_timesteps=424500, episode_reward=197.76 +/- 0.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=197.66 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=197.65 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 208      |
|    time_elapsed    | 11157    |
|    total_timesteps | 425984   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=426000, episode_reward=183.48 +/- 35.69
Episode length: 483.48 +/- 112.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 483          |
|    mean_reward          | 183          |
| time/                   |              |
|    total_timesteps      | 426000       |
| train/                  |              |
|    approx_kl            | 0.0073911003 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 7.19         |
|    n_updates            | 293          |
|    policy_gradient_loss | -0.00302     |
|    value_loss           | 12           |
------------------------------------------
Eval num_timesteps=426500, episode_reward=179.04 +/- 40.52
Episode length: 473.52 +/- 122.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=172.38 +/- 45.55
Episode length: 451.76 +/- 140.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 452      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=179.09 +/- 40.43
Episode length: 472.62 +/- 123.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 473      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=176.66 +/- 42.27
Episode length: 459.86 +/- 140.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 460      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 209      |
|    time_elapsed    | 11237    |
|    total_timesteps | 428032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=428500, episode_reward=131.76 +/- 53.76
Episode length: 328.52 +/- 171.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 329          |
|    mean_reward          | 132          |
| time/                   |              |
|    total_timesteps      | 428500       |
| train/                  |              |
|    approx_kl            | 0.0038820494 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 0.446        |
|    learning_rate        | 0.001        |
|    loss                 | 58           |
|    n_updates            | 294          |
|    policy_gradient_loss | 0.0305       |
|    value_loss           | 133          |
------------------------------------------
Eval num_timesteps=429000, episode_reward=138.18 +/- 54.50
Episode length: 354.74 +/- 169.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 355      |
|    mean_reward     | 138      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=142.46 +/- 54.32
Episode length: 370.78 +/- 172.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 371      |
|    mean_reward     | 142      |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=146.71 +/- 53.92
Episode length: 385.96 +/- 160.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 386      |
|    mean_reward     | 147      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 210      |
|    time_elapsed    | 11287    |
|    total_timesteps | 430080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=430500, episode_reward=160.11 +/- 50.52
Episode length: 426.64 +/- 158.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 427          |
|    mean_reward          | 160          |
| time/                   |              |
|    total_timesteps      | 430500       |
| train/                  |              |
|    approx_kl            | 0.0077297767 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.497        |
|    learning_rate        | 0.001        |
|    loss                 | 22.2         |
|    n_updates            | 295          |
|    policy_gradient_loss | 0.0161       |
|    value_loss           | 103          |
------------------------------------------
Eval num_timesteps=431000, episode_reward=150.84 +/- 52.99
Episode length: 393.14 +/- 169.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 393      |
|    mean_reward     | 151      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=155.14 +/- 52.20
Episode length: 397.46 +/- 180.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 397      |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=153.25 +/- 60.36
Episode length: 402.36 +/- 177.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 402      |
|    mean_reward     | 153      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 211      |
|    time_elapsed    | 11343    |
|    total_timesteps | 432128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=432500, episode_reward=170.89 +/- 47.57
Episode length: 411.94 +/- 201.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 412         |
|    mean_reward          | 171         |
| time/                   |             |
|    total_timesteps      | 432500      |
| train/                  |             |
|    approx_kl            | 0.006890178 |
|    clip_fraction        | 0.0301      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.001       |
|    loss                 | 47.2        |
|    n_updates            | 296         |
|    policy_gradient_loss | 0.00257     |
|    value_loss           | 80.7        |
-----------------------------------------
Eval num_timesteps=433000, episode_reward=172.91 +/- 50.53
Episode length: 433.36 +/- 183.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=161.40 +/- 51.93
Episode length: 374.78 +/- 219.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 375      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=170.24 +/- 47.64
Episode length: 411.90 +/- 201.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 212      |
|    time_elapsed    | 11399    |
|    total_timesteps | 434176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=434500, episode_reward=190.51 +/- 26.43
Episode length: 496.26 +/- 113.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 434500       |
| train/                  |              |
|    approx_kl            | 0.0058528674 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.554        |
|    learning_rate        | 0.001        |
|    loss                 | 36.1         |
|    n_updates            | 297          |
|    policy_gradient_loss | 0.00215      |
|    value_loss           | 95           |
------------------------------------------
Eval num_timesteps=435000, episode_reward=193.11 +/- 22.00
Episode length: 506.42 +/- 91.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=193.03 +/- 22.10
Episode length: 505.96 +/- 93.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=197.39 +/- 0.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 213      |
|    time_elapsed    | 11469    |
|    total_timesteps | 436224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=436500, episode_reward=193.60 +/- 29.46
Episode length: 515.08 +/- 69.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 436500      |
| train/                  |             |
|    approx_kl            | 0.010111052 |
|    clip_fraction        | 0.0557      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.001       |
|    loss                 | 28.9        |
|    n_updates            | 298         |
|    policy_gradient_loss | 0.00245     |
|    value_loss           | 61.6        |
-----------------------------------------
Eval num_timesteps=437000, episode_reward=193.43 +/- 29.59
Episode length: 515.32 +/- 67.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=195.48 +/- 15.74
Episode length: 515.30 +/- 67.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=193.44 +/- 29.58
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 214      |
|    time_elapsed    | 11538    |
|    total_timesteps | 438272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=438500, episode_reward=197.75 +/- 0.56
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 438500      |
| train/                  |             |
|    approx_kl            | 0.003631182 |
|    clip_fraction        | 0.0218      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.001       |
|    loss                 | 24.6        |
|    n_updates            | 299         |
|    policy_gradient_loss | 0.00202     |
|    value_loss           | 79.3        |
-----------------------------------------
Eval num_timesteps=439000, episode_reward=197.72 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=197.76 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=195.53 +/- 15.60
Episode length: 515.24 +/- 68.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 215      |
|    time_elapsed    | 11609    |
|    total_timesteps | 440320   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.04
Eval num_timesteps=440500, episode_reward=197.79 +/- 0.59
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 440500      |
| train/                  |             |
|    approx_kl            | 0.011268601 |
|    clip_fraction        | 0.0672      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.001       |
|    loss                 | 39          |
|    n_updates            | 302         |
|    policy_gradient_loss | -0.00162    |
|    value_loss           | 44.1        |
-----------------------------------------
Eval num_timesteps=441000, episode_reward=197.90 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=197.92 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
New best mean reward!
Eval num_timesteps=442000, episode_reward=197.70 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 216      |
|    time_elapsed    | 11680    |
|    total_timesteps | 442368   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=442500, episode_reward=197.51 +/- 0.85
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 442500      |
| train/                  |             |
|    approx_kl            | 0.008033841 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.001       |
|    loss                 | 10.7        |
|    n_updates            | 305         |
|    policy_gradient_loss | -0.000612   |
|    value_loss           | 26.8        |
-----------------------------------------
Eval num_timesteps=443000, episode_reward=197.59 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=197.41 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=197.67 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 217      |
|    time_elapsed    | 11752    |
|    total_timesteps | 444416   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=444500, episode_reward=190.00 +/- 26.03
Episode length: 497.08 +/- 110.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 497         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 444500      |
| train/                  |             |
|    approx_kl            | 0.018114677 |
|    clip_fraction        | 0.0175      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.001       |
|    loss                 | 11.2        |
|    n_updates            | 308         |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 23.3        |
-----------------------------------------
Eval num_timesteps=445000, episode_reward=190.25 +/- 32.78
Episode length: 505.80 +/- 94.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=187.90 +/- 35.63
Episode length: 496.58 +/- 112.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=185.54 +/- 32.79
Episode length: 477.48 +/- 142.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 218      |
|    time_elapsed    | 11820    |
|    total_timesteps | 446464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=446500, episode_reward=197.36 +/- 0.91
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 446500       |
| train/                  |              |
|    approx_kl            | 0.0065754284 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.531        |
|    learning_rate        | 0.001        |
|    loss                 | 88           |
|    n_updates            | 309          |
|    policy_gradient_loss | 0.0226       |
|    value_loss           | 113          |
------------------------------------------
Eval num_timesteps=447000, episode_reward=197.31 +/- 0.94
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=197.25 +/- 1.01
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=197.07 +/- 1.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=197.13 +/- 1.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 219      |
|    time_elapsed    | 11909    |
|    total_timesteps | 448512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=449000, episode_reward=196.60 +/- 1.09
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 0.0046193996 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.65         |
|    learning_rate        | 0.001        |
|    loss                 | 16           |
|    n_updates            | 310          |
|    policy_gradient_loss | 0.00675      |
|    value_loss           | 81.8         |
------------------------------------------
Eval num_timesteps=449500, episode_reward=196.57 +/- 1.03
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=196.72 +/- 0.97
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=196.49 +/- 1.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 220      |
|    time_elapsed    | 11980    |
|    total_timesteps | 450560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=451000, episode_reward=189.88 +/- 32.65
Episode length: 505.48 +/- 95.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | 190          |
| time/                   |              |
|    total_timesteps      | 451000       |
| train/                  |              |
|    approx_kl            | 0.0054861694 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 0.699        |
|    learning_rate        | 0.001        |
|    loss                 | 67.8         |
|    n_updates            | 311          |
|    policy_gradient_loss | -0.00202     |
|    value_loss           | 70.6         |
------------------------------------------
Eval num_timesteps=451500, episode_reward=188.28 +/- 41.12
Episode length: 504.92 +/- 98.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=190.03 +/- 32.87
Episode length: 505.04 +/- 97.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=192.11 +/- 21.70
Episode length: 505.52 +/- 95.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 221      |
|    time_elapsed    | 12049    |
|    total_timesteps | 452608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=453000, episode_reward=197.33 +/- 0.90
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 453000      |
| train/                  |             |
|    approx_kl            | 0.005071665 |
|    clip_fraction        | 0.00586     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.001       |
|    loss                 | 52.6        |
|    n_updates            | 312         |
|    policy_gradient_loss | -0.000119   |
|    value_loss           | 68.3        |
-----------------------------------------
Eval num_timesteps=453500, episode_reward=197.66 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=197.51 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=197.58 +/- 0.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 222      |
|    time_elapsed    | 12122    |
|    total_timesteps | 454656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=455000, episode_reward=197.11 +/- 0.91
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 455000      |
| train/                  |             |
|    approx_kl            | 0.006189743 |
|    clip_fraction        | 0.0643      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 5.31        |
|    n_updates            | 313         |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 8.17        |
-----------------------------------------
Eval num_timesteps=455500, episode_reward=197.19 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=197.42 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=197.51 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 223      |
|    time_elapsed    | 12193    |
|    total_timesteps | 456704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=457000, episode_reward=197.70 +/- 0.51
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 457000      |
| train/                  |             |
|    approx_kl            | 0.010760493 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.001       |
|    loss                 | 22.4        |
|    n_updates            | 314         |
|    policy_gradient_loss | 0.0882      |
|    value_loss           | 42.1        |
-----------------------------------------
Eval num_timesteps=457500, episode_reward=197.57 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=197.70 +/- 0.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=197.66 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 224      |
|    time_elapsed    | 12265    |
|    total_timesteps | 458752   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=459000, episode_reward=197.59 +/- 0.77
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 459000      |
| train/                  |             |
|    approx_kl            | 0.009313027 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.001       |
|    loss                 | 36.3        |
|    n_updates            | 316         |
|    policy_gradient_loss | -0.0022     |
|    value_loss           | 43.3        |
-----------------------------------------
Eval num_timesteps=459500, episode_reward=197.52 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=197.70 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=197.63 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 225      |
|    time_elapsed    | 12336    |
|    total_timesteps | 460800   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=461000, episode_reward=197.36 +/- 0.65
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 461000       |
| train/                  |              |
|    approx_kl            | 0.0072866124 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.001        |
|    loss                 | 8.37         |
|    n_updates            | 318          |
|    policy_gradient_loss | -0.000243    |
|    value_loss           | 18           |
------------------------------------------
Eval num_timesteps=461500, episode_reward=197.50 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=197.44 +/- 0.88
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=197.54 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 226      |
|    time_elapsed    | 12408    |
|    total_timesteps | 462848   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=463000, episode_reward=197.78 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 463000     |
| train/                  |            |
|    approx_kl            | 0.00999561 |
|    clip_fraction        | 0.034      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.001      |
|    loss                 | 6.29       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.00186   |
|    value_loss           | 17.3       |
----------------------------------------
Eval num_timesteps=463500, episode_reward=195.48 +/- 15.45
Episode length: 515.32 +/- 67.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=197.76 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=197.78 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 227      |
|    time_elapsed    | 12479    |
|    total_timesteps | 464896   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=465000, episode_reward=197.80 +/- 0.53
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.008307184 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 8.25        |
|    n_updates            | 322         |
|    policy_gradient_loss | -0.00105    |
|    value_loss           | 12.7        |
-----------------------------------------
Eval num_timesteps=465500, episode_reward=197.75 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=197.78 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=197.68 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 228      |
|    time_elapsed    | 12551    |
|    total_timesteps | 466944   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=467000, episode_reward=197.46 +/- 0.76
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | 197       |
| time/                   |           |
|    total_timesteps      | 467000    |
| train/                  |           |
|    approx_kl            | 0.0253231 |
|    clip_fraction        | 0.0207    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18     |
|    explained_variance   | 0.946     |
|    learning_rate        | 0.001     |
|    loss                 | 10.1      |
|    n_updates            | 324       |
|    policy_gradient_loss | 0.00252   |
|    value_loss           | 16.1      |
---------------------------------------
Eval num_timesteps=467500, episode_reward=197.49 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=197.48 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=197.22 +/- 0.85
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 229      |
|    time_elapsed    | 12623    |
|    total_timesteps | 468992   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=469000, episode_reward=197.63 +/- 0.61
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 469000      |
| train/                  |             |
|    approx_kl            | 0.011858159 |
|    clip_fraction        | 0.0462      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 10.5        |
|    n_updates            | 326         |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 10.2        |
-----------------------------------------
Eval num_timesteps=469500, episode_reward=195.33 +/- 15.43
Episode length: 515.22 +/- 68.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=197.49 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=197.61 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=197.50 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 230      |
|    time_elapsed    | 12711    |
|    total_timesteps | 471040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=471500, episode_reward=197.72 +/- 0.59
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 471500     |
| train/                  |            |
|    approx_kl            | 0.00469425 |
|    clip_fraction        | 0.0026     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.001      |
|    loss                 | 5.88       |
|    n_updates            | 327        |
|    policy_gradient_loss | -0.00536   |
|    value_loss           | 12.6       |
----------------------------------------
Eval num_timesteps=472000, episode_reward=197.62 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=197.82 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=197.75 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 231      |
|    time_elapsed    | 12783    |
|    total_timesteps | 473088   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=473500, episode_reward=197.75 +/- 0.57
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 473500      |
| train/                  |             |
|    approx_kl            | 0.013480124 |
|    clip_fraction        | 0.0481      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 23.8        |
|    n_updates            | 329         |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 20.9        |
-----------------------------------------
Eval num_timesteps=474000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
New best mean reward!
Eval num_timesteps=474500, episode_reward=197.74 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=197.84 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 232      |
|    time_elapsed    | 12854    |
|    total_timesteps | 475136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=475500, episode_reward=197.85 +/- 0.49
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 475500      |
| train/                  |             |
|    approx_kl            | 0.007021617 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.001       |
|    loss                 | 10.6        |
|    n_updates            | 330         |
|    policy_gradient_loss | 0.0114      |
|    value_loss           | 31.1        |
-----------------------------------------
Eval num_timesteps=476000, episode_reward=197.64 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=197.78 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=197.74 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 233      |
|    time_elapsed    | 12925    |
|    total_timesteps | 477184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=477500, episode_reward=197.83 +/- 0.53
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 477500       |
| train/                  |              |
|    approx_kl            | 0.0042273565 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 7.87         |
|    n_updates            | 331          |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 11.2         |
------------------------------------------
Eval num_timesteps=478000, episode_reward=197.84 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=197.68 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=197.82 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 234      |
|    time_elapsed    | 12996    |
|    total_timesteps | 479232   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=479500, episode_reward=197.28 +/- 0.96
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 479500      |
| train/                  |             |
|    approx_kl            | 0.007652404 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.001       |
|    loss                 | 7.99        |
|    n_updates            | 333         |
|    policy_gradient_loss | 0.00748     |
|    value_loss           | 8.04        |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=184.14 +/- 35.73
Episode length: 466.92 +/- 157.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=188.57 +/- 29.84
Episode length: 486.14 +/- 131.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=190.65 +/- 26.10
Episode length: 496.32 +/- 113.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 235      |
|    time_elapsed    | 13064    |
|    total_timesteps | 481280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=481500, episode_reward=171.37 +/- 45.42
Episode length: 422.48 +/- 193.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 422          |
|    mean_reward          | 171          |
| time/                   |              |
|    total_timesteps      | 481500       |
| train/                  |              |
|    approx_kl            | 0.0052063297 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.634        |
|    learning_rate        | 0.001        |
|    loss                 | 55.5         |
|    n_updates            | 334          |
|    policy_gradient_loss | 0.00106      |
|    value_loss           | 95.6         |
------------------------------------------
Eval num_timesteps=482000, episode_reward=167.40 +/- 48.25
Episode length: 402.56 +/- 206.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 403      |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=184.59 +/- 32.88
Episode length: 478.00 +/- 141.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=167.45 +/- 47.86
Episode length: 399.88 +/- 211.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 236      |
|    time_elapsed    | 13123    |
|    total_timesteps | 483328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=483500, episode_reward=197.86 +/- 0.44
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 483500       |
| train/                  |              |
|    approx_kl            | 0.0057588643 |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.001        |
|    loss                 | 22.7         |
|    n_updates            | 335          |
|    policy_gradient_loss | 0.00503      |
|    value_loss           | 37.4         |
------------------------------------------
Eval num_timesteps=484000, episode_reward=197.75 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=197.70 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=197.70 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 237      |
|    time_elapsed    | 13194    |
|    total_timesteps | 485376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=485500, episode_reward=197.76 +/- 0.54
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 485500       |
| train/                  |              |
|    approx_kl            | 0.0036159693 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.001        |
|    loss                 | 8.3          |
|    n_updates            | 336          |
|    policy_gradient_loss | 0.00289      |
|    value_loss           | 42.6         |
------------------------------------------
Eval num_timesteps=486000, episode_reward=197.57 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=197.75 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=197.74 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 238      |
|    time_elapsed    | 13265    |
|    total_timesteps | 487424   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=487500, episode_reward=197.66 +/- 0.72
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 487500     |
| train/                  |            |
|    approx_kl            | 0.01295435 |
|    clip_fraction        | 0.0484     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.001      |
|    loss                 | 39.7       |
|    n_updates            | 338        |
|    policy_gradient_loss | -0.000365  |
|    value_loss           | 20.6       |
----------------------------------------
Eval num_timesteps=488000, episode_reward=197.83 +/- 0.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=197.81 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=197.77 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 239      |
|    time_elapsed    | 13336    |
|    total_timesteps | 489472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=489500, episode_reward=197.74 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 489500     |
| train/                  |            |
|    approx_kl            | 0.00684409 |
|    clip_fraction        | 0.0195     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.001      |
|    loss                 | 11.3       |
|    n_updates            | 339        |
|    policy_gradient_loss | -0.00187   |
|    value_loss           | 9.28       |
----------------------------------------
Eval num_timesteps=490000, episode_reward=197.69 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=197.66 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=197.68 +/- 0.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=197.77 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 240      |
|    time_elapsed    | 13426    |
|    total_timesteps | 491520   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=492000, episode_reward=197.78 +/- 0.49
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 492000     |
| train/                  |            |
|    approx_kl            | 0.00874925 |
|    clip_fraction        | 0.0315     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.001      |
|    loss                 | 0.168      |
|    n_updates            | 343        |
|    policy_gradient_loss | -9.16e-06  |
|    value_loss           | 3.86       |
----------------------------------------
Eval num_timesteps=492500, episode_reward=197.78 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=197.72 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=197.82 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 241      |
|    time_elapsed    | 13499    |
|    total_timesteps | 493568   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=494000, episode_reward=197.77 +/- 0.65
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 494000      |
| train/                  |             |
|    approx_kl            | 0.015585514 |
|    clip_fraction        | 0.0387      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 11.3        |
|    n_updates            | 345         |
|    policy_gradient_loss | -0.0003     |
|    value_loss           | 14.2        |
-----------------------------------------
Eval num_timesteps=494500, episode_reward=197.73 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=197.77 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=197.85 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 242      |
|    time_elapsed    | 13570    |
|    total_timesteps | 495616   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=496000, episode_reward=197.47 +/- 0.73
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 496000      |
| train/                  |             |
|    approx_kl            | 0.006589495 |
|    clip_fraction        | 0.0051      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 2.26        |
|    n_updates            | 347         |
|    policy_gradient_loss | 0.00458     |
|    value_loss           | 9.44        |
-----------------------------------------
Eval num_timesteps=496500, episode_reward=197.51 +/- 0.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=197.73 +/- 0.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=197.65 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 243      |
|    time_elapsed    | 13642    |
|    total_timesteps | 497664   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=498000, episode_reward=197.70 +/- 0.70
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.009598497 |
|    clip_fraction        | 0.00699     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.001       |
|    loss                 | 7.06        |
|    n_updates            | 349         |
|    policy_gradient_loss | 0.00229     |
|    value_loss           | 15.8        |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=197.55 +/- 0.88
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=197.68 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=197.57 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 244      |
|    time_elapsed    | 13714    |
|    total_timesteps | 499712   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=500000, episode_reward=183.41 +/- 38.32
Episode length: 485.74 +/- 133.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 486         |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.007862678 |
|    clip_fraction        | 0.0103      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 1.75        |
|    n_updates            | 351         |
|    policy_gradient_loss | -0.000459   |
|    value_loss           | 8.88        |
-----------------------------------------
Eval num_timesteps=500500, episode_reward=175.73 +/- 40.63
Episode length: 457.20 +/- 156.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=182.74 +/- 33.53
Episode length: 482.30 +/- 129.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 482      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=167.64 +/- 54.48
Episode length: 446.20 +/- 161.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 446      |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 245      |
|    time_elapsed    | 13780    |
|    total_timesteps | 501760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=502000, episode_reward=188.98 +/- 26.02
Episode length: 495.44 +/- 117.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | 189          |
| time/                   |              |
|    total_timesteps      | 502000       |
| train/                  |              |
|    approx_kl            | 0.0068643694 |
|    clip_fraction        | 0.049        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.371        |
|    learning_rate        | 0.001        |
|    loss                 | 70.5         |
|    n_updates            | 352          |
|    policy_gradient_loss | 0.00801      |
|    value_loss           | 113          |
------------------------------------------
Eval num_timesteps=502500, episode_reward=186.48 +/- 36.15
Episode length: 495.94 +/- 115.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
Eval num_timesteps=503000, episode_reward=191.32 +/- 29.30
Episode length: 514.96 +/- 70.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=503500, episode_reward=193.20 +/- 15.20
Episode length: 515.08 +/- 69.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 246      |
|    time_elapsed    | 13851    |
|    total_timesteps | 503808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=504000, episode_reward=195.68 +/- 1.39
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 504000       |
| train/                  |              |
|    approx_kl            | 0.0062169917 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.2         |
|    explained_variance   | 0.424        |
|    learning_rate        | 0.001        |
|    loss                 | 46.4         |
|    n_updates            | 353          |
|    policy_gradient_loss | 5.24e-05     |
|    value_loss           | 111          |
------------------------------------------
Eval num_timesteps=504500, episode_reward=196.37 +/- 1.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
Eval num_timesteps=505000, episode_reward=195.86 +/- 1.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=505500, episode_reward=196.28 +/- 1.25
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 247      |
|    time_elapsed    | 13923    |
|    total_timesteps | 505856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=506000, episode_reward=197.70 +/- 0.81
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 506000       |
| train/                  |              |
|    approx_kl            | 0.0061939503 |
|    clip_fraction        | 0.0612       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.75         |
|    learning_rate        | 0.001        |
|    loss                 | 21           |
|    n_updates            | 354          |
|    policy_gradient_loss | 0.00809      |
|    value_loss           | 49.2         |
------------------------------------------
Eval num_timesteps=506500, episode_reward=197.90 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
Eval num_timesteps=507000, episode_reward=197.64 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=507500, episode_reward=197.84 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 248      |
|    time_elapsed    | 13995    |
|    total_timesteps | 507904   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=508000, episode_reward=197.43 +/- 0.87
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 508000     |
| train/                  |            |
|    approx_kl            | 0.01561339 |
|    clip_fraction        | 0.02       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.001      |
|    loss                 | 47.1       |
|    n_updates            | 356        |
|    policy_gradient_loss | 0.00146    |
|    value_loss           | 53.5       |
----------------------------------------
Eval num_timesteps=508500, episode_reward=197.39 +/- 0.92
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
Eval num_timesteps=509000, episode_reward=188.94 +/- 36.11
Episode length: 495.62 +/- 116.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=509500, episode_reward=197.30 +/- 1.03
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 249      |
|    time_elapsed    | 14069    |
|    total_timesteps | 509952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=510000, episode_reward=193.47 +/- 29.45
Episode length: 515.26 +/- 68.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 510000       |
| train/                  |              |
|    approx_kl            | 0.0045808335 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.733        |
|    learning_rate        | 0.001        |
|    loss                 | 28.2         |
|    n_updates            | 357          |
|    policy_gradient_loss | 0.00133      |
|    value_loss           | 48.6         |
------------------------------------------
Eval num_timesteps=510500, episode_reward=197.71 +/- 0.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
Eval num_timesteps=511000, episode_reward=193.51 +/- 29.45
Episode length: 515.12 +/- 69.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=511500, episode_reward=197.46 +/- 1.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=193.24 +/- 29.57
Episode length: 515.02 +/- 69.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 250      |
|    time_elapsed    | 14156    |
|    total_timesteps | 512000   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=512500, episode_reward=197.74 +/- 0.72
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 512500       |
| train/                  |              |
|    approx_kl            | 0.0069701285 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.001        |
|    loss                 | 6.33         |
|    n_updates            | 359          |
|    policy_gradient_loss | 0.00229      |
|    value_loss           | 40.4         |
------------------------------------------
Eval num_timesteps=513000, episode_reward=197.84 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
Eval num_timesteps=513500, episode_reward=197.77 +/- 0.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 513500   |
---------------------------------
Eval num_timesteps=514000, episode_reward=193.55 +/- 29.60
Episode length: 515.04 +/- 69.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 251      |
|    time_elapsed    | 14227    |
|    total_timesteps | 514048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=514500, episode_reward=197.57 +/- 0.78
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 514500       |
| train/                  |              |
|    approx_kl            | 0.0022386666 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.2         |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.001        |
|    loss                 | 54.2         |
|    n_updates            | 360          |
|    policy_gradient_loss | 0.00351      |
|    value_loss           | 27.8         |
------------------------------------------
Eval num_timesteps=515000, episode_reward=197.61 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
Eval num_timesteps=515500, episode_reward=197.66 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 515500   |
---------------------------------
Eval num_timesteps=516000, episode_reward=197.70 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 252      |
|    time_elapsed    | 14299    |
|    total_timesteps | 516096   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=516500, episode_reward=197.79 +/- 0.44
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 516500       |
| train/                  |              |
|    approx_kl            | 0.0065154373 |
|    clip_fraction        | 0.00725      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.2         |
|    explained_variance   | 0.856        |
|    learning_rate        | 0.001        |
|    loss                 | 7.87         |
|    n_updates            | 362          |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 37.1         |
------------------------------------------
Eval num_timesteps=517000, episode_reward=197.74 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
Eval num_timesteps=517500, episode_reward=197.68 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 517500   |
---------------------------------
Eval num_timesteps=518000, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 253      |
|    time_elapsed    | 14371    |
|    total_timesteps | 518144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=518500, episode_reward=197.14 +/- 0.69
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 518500      |
| train/                  |             |
|    approx_kl            | 0.007016312 |
|    clip_fraction        | 0.00347     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.001       |
|    loss                 | 13.4        |
|    n_updates            | 363         |
|    policy_gradient_loss | 0.0188      |
|    value_loss           | 18.6        |
-----------------------------------------
Eval num_timesteps=519000, episode_reward=197.00 +/- 1.16
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
Eval num_timesteps=519500, episode_reward=197.17 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 519500   |
---------------------------------
Eval num_timesteps=520000, episode_reward=197.44 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 254      |
|    time_elapsed    | 14443    |
|    total_timesteps | 520192   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=520500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 520500      |
| train/                  |             |
|    approx_kl            | 0.012963663 |
|    clip_fraction        | 0.0109      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.001       |
|    loss                 | 0.529       |
|    n_updates            | 365         |
|    policy_gradient_loss | 0.000738    |
|    value_loss           | 14.2        |
-----------------------------------------
Eval num_timesteps=521000, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
Eval num_timesteps=521500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 521500   |
---------------------------------
Eval num_timesteps=522000, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 255      |
|    time_elapsed    | 14515    |
|    total_timesteps | 522240   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=522500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 522500     |
| train/                  |            |
|    approx_kl            | 0.01017833 |
|    clip_fraction        | 0.0227     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.001      |
|    loss                 | -0.00742   |
|    n_updates            | 371        |
|    policy_gradient_loss | 5.78e-05   |
|    value_loss           | 19.8       |
----------------------------------------
Eval num_timesteps=523000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
Eval num_timesteps=523500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 523500   |
---------------------------------
Eval num_timesteps=524000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 256      |
|    time_elapsed    | 14586    |
|    total_timesteps | 524288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=524500, episode_reward=197.39 +/- 0.69
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 524500       |
| train/                  |              |
|    approx_kl            | 0.0023342844 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 11.4         |
|    n_updates            | 372          |
|    policy_gradient_loss | -5.68e-05    |
|    value_loss           | 17.6         |
------------------------------------------
Eval num_timesteps=525000, episode_reward=197.50 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
Eval num_timesteps=525500, episode_reward=197.58 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 525500   |
---------------------------------
Eval num_timesteps=526000, episode_reward=197.46 +/- 0.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 257      |
|    time_elapsed    | 14657    |
|    total_timesteps | 526336   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=526500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 526500     |
| train/                  |            |
|    approx_kl            | 0.00991728 |
|    clip_fraction        | 0.00994    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.001      |
|    loss                 | 4.44       |
|    n_updates            | 374        |
|    policy_gradient_loss | -0.00393   |
|    value_loss           | 22.4       |
----------------------------------------
Eval num_timesteps=527000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
Eval num_timesteps=527500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 527500   |
---------------------------------
Eval num_timesteps=528000, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 258      |
|    time_elapsed    | 14727    |
|    total_timesteps | 528384   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=528500, episode_reward=197.83 +/- 0.43
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 528500       |
| train/                  |              |
|    approx_kl            | 0.0065370076 |
|    clip_fraction        | 0.00451      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 27.2         |
|    n_updates            | 377          |
|    policy_gradient_loss | 0.000856     |
|    value_loss           | 21.1         |
------------------------------------------
Eval num_timesteps=529000, episode_reward=197.85 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
Eval num_timesteps=529500, episode_reward=197.81 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 529500   |
---------------------------------
Eval num_timesteps=530000, episode_reward=197.79 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 259      |
|    time_elapsed    | 14798    |
|    total_timesteps | 530432   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=530500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 530500      |
| train/                  |             |
|    approx_kl            | 0.008738039 |
|    clip_fraction        | 0.0555      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.001       |
|    loss                 | 12          |
|    n_updates            | 379         |
|    policy_gradient_loss | 0.00297     |
|    value_loss           | 29          |
-----------------------------------------
Eval num_timesteps=531000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
Eval num_timesteps=531500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 531500   |
---------------------------------
Eval num_timesteps=532000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 260      |
|    time_elapsed    | 14869    |
|    total_timesteps | 532480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=532500, episode_reward=197.93 +/- 0.31
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 532500       |
| train/                  |              |
|    approx_kl            | 0.0038980918 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.001        |
|    loss                 | 15.7         |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.000614    |
|    value_loss           | 24.4         |
------------------------------------------
Eval num_timesteps=533000, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=197.88 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
Eval num_timesteps=534000, episode_reward=197.92 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=534500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 261      |
|    time_elapsed    | 14956    |
|    total_timesteps | 534528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=535000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 0.0046747653 |
|    clip_fraction        | 0.0405       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 23.7         |
|    n_updates            | 381          |
|    policy_gradient_loss | 0.00184      |
|    value_loss           | 15.4         |
------------------------------------------
Eval num_timesteps=535500, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
Eval num_timesteps=536000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=536500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 262      |
|    time_elapsed    | 15027    |
|    total_timesteps | 536576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=537000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 537000       |
| train/                  |              |
|    approx_kl            | 0.0047973706 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.001        |
|    loss                 | 12.6         |
|    n_updates            | 382          |
|    policy_gradient_loss | -0.00245     |
|    value_loss           | 24.1         |
------------------------------------------
Eval num_timesteps=537500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
Eval num_timesteps=538000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=538500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 263      |
|    time_elapsed    | 15098    |
|    total_timesteps | 538624   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=539000, episode_reward=197.77 +/- 0.51
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 539000      |
| train/                  |             |
|    approx_kl            | 0.021953499 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 3           |
|    n_updates            | 384         |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 8.49        |
-----------------------------------------
Eval num_timesteps=539500, episode_reward=197.83 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
Eval num_timesteps=540000, episode_reward=197.81 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=540500, episode_reward=197.80 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 264      |
|    time_elapsed    | 15169    |
|    total_timesteps | 540672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=541000, episode_reward=197.76 +/- 0.54
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 541000       |
| train/                  |              |
|    approx_kl            | 0.0020736256 |
|    clip_fraction        | 0.00502      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.902        |
|    learning_rate        | 0.001        |
|    loss                 | 10.8         |
|    n_updates            | 385          |
|    policy_gradient_loss | 0.00115      |
|    value_loss           | 29.6         |
------------------------------------------
Eval num_timesteps=541500, episode_reward=197.87 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
Eval num_timesteps=542000, episode_reward=197.76 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=542500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 265      |
|    time_elapsed    | 15240    |
|    total_timesteps | 542720   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=543000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 543000      |
| train/                  |             |
|    approx_kl            | 0.011466864 |
|    clip_fraction        | 0.0632      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.001       |
|    loss                 | 25.8        |
|    n_updates            | 387         |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 44.4        |
-----------------------------------------
Eval num_timesteps=543500, episode_reward=197.82 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
Eval num_timesteps=544000, episode_reward=197.84 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=544500, episode_reward=197.91 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 266      |
|    time_elapsed    | 15311    |
|    total_timesteps | 544768   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=545000, episode_reward=197.93 +/- 0.28
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 545000      |
| train/                  |             |
|    approx_kl            | 0.014941576 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.001       |
|    loss                 | 1.21        |
|    n_updates            | 389         |
|    policy_gradient_loss | 0.00403     |
|    value_loss           | 39.6        |
-----------------------------------------
Eval num_timesteps=545500, episode_reward=197.83 +/- 0.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
Eval num_timesteps=546000, episode_reward=197.89 +/- 0.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=546500, episode_reward=197.86 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 267      |
|    time_elapsed    | 15383    |
|    total_timesteps | 546816   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=547000, episode_reward=197.83 +/- 0.46
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 547000      |
| train/                  |             |
|    approx_kl            | 0.011186152 |
|    clip_fraction        | 0.0456      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 15.9        |
|    n_updates            | 391         |
|    policy_gradient_loss | 0.00169     |
|    value_loss           | 33.1        |
-----------------------------------------
Eval num_timesteps=547500, episode_reward=197.93 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
Eval num_timesteps=548000, episode_reward=197.80 +/- 0.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=548500, episode_reward=197.83 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 268      |
|    time_elapsed    | 15454    |
|    total_timesteps | 548864   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=549000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 549000      |
| train/                  |             |
|    approx_kl            | 0.014407003 |
|    clip_fraction        | 0.0169      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.001       |
|    loss                 | 0.611       |
|    n_updates            | 393         |
|    policy_gradient_loss | 7.01e-06    |
|    value_loss           | 21.5        |
-----------------------------------------
Eval num_timesteps=549500, episode_reward=197.97 +/- 0.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=550500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 269      |
|    time_elapsed    | 15525    |
|    total_timesteps | 550912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=551000, episode_reward=197.85 +/- 0.38
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 551000      |
| train/                  |             |
|    approx_kl            | 0.003947921 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 3.66        |
|    n_updates            | 394         |
|    policy_gradient_loss | -0.00142    |
|    value_loss           | 11.9        |
-----------------------------------------
Eval num_timesteps=551500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
Eval num_timesteps=552000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=552500, episode_reward=197.91 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 270      |
|    time_elapsed    | 15597    |
|    total_timesteps | 552960   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=553000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 553000     |
| train/                  |            |
|    approx_kl            | 0.00906621 |
|    clip_fraction        | 0.00547    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.001      |
|    loss                 | 27         |
|    n_updates            | 396        |
|    policy_gradient_loss | -0.00134   |
|    value_loss           | 19.9       |
----------------------------------------
Eval num_timesteps=553500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
Eval num_timesteps=554000, episode_reward=197.94 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=554500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 271      |
|    time_elapsed    | 15685    |
|    total_timesteps | 555008   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=555500, episode_reward=197.96 +/- 0.28
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 555500      |
| train/                  |             |
|    approx_kl            | 0.008985766 |
|    clip_fraction        | 0.00411     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 1.01        |
|    n_updates            | 398         |
|    policy_gradient_loss | 0.0012      |
|    value_loss           | 20.5        |
-----------------------------------------
Eval num_timesteps=556000, episode_reward=197.93 +/- 0.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
Eval num_timesteps=556500, episode_reward=197.89 +/- 0.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 556500   |
---------------------------------
Eval num_timesteps=557000, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 272      |
|    time_elapsed    | 15757    |
|    total_timesteps | 557056   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=557500, episode_reward=197.88 +/- 0.32
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 557500      |
| train/                  |             |
|    approx_kl            | 0.010515968 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.001       |
|    loss                 | 24.5        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.000893   |
|    value_loss           | 27.1        |
-----------------------------------------
Eval num_timesteps=558000, episode_reward=197.82 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=558500, episode_reward=197.74 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 558500   |
---------------------------------
Eval num_timesteps=559000, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 273      |
|    time_elapsed    | 15829    |
|    total_timesteps | 559104   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=559500, episode_reward=197.80 +/- 0.40
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 559500      |
| train/                  |             |
|    approx_kl            | 0.012358689 |
|    clip_fraction        | 0.0632      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 0.375       |
|    n_updates            | 402         |
|    policy_gradient_loss | -0.00126    |
|    value_loss           | 10.2        |
-----------------------------------------
Eval num_timesteps=560000, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=560500, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 560500   |
---------------------------------
Eval num_timesteps=561000, episode_reward=197.78 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 274      |
|    time_elapsed    | 15900    |
|    total_timesteps | 561152   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=561500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 561500      |
| train/                  |             |
|    approx_kl            | 0.019676711 |
|    clip_fraction        | 0.00827     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 0.275       |
|    n_updates            | 404         |
|    policy_gradient_loss | 1.86e-05    |
|    value_loss           | 6.91        |
-----------------------------------------
Eval num_timesteps=562000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
Eval num_timesteps=562500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 562500   |
---------------------------------
Eval num_timesteps=563000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 275      |
|    time_elapsed    | 15971    |
|    total_timesteps | 563200   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=563500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 563500     |
| train/                  |            |
|    approx_kl            | 0.01044285 |
|    clip_fraction        | 0.0337     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.001      |
|    loss                 | 13.3       |
|    n_updates            | 408        |
|    policy_gradient_loss | -0.000327  |
|    value_loss           | 17.6       |
----------------------------------------
Eval num_timesteps=564000, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=564500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 564500   |
---------------------------------
Eval num_timesteps=565000, episode_reward=197.89 +/- 0.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 276      |
|    time_elapsed    | 16044    |
|    total_timesteps | 565248   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=565500, episode_reward=197.80 +/- 0.40
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 565500      |
| train/                  |             |
|    approx_kl            | 0.008949751 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.001       |
|    loss                 | 19.7        |
|    n_updates            | 411         |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 25.3        |
-----------------------------------------
Eval num_timesteps=566000, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
Eval num_timesteps=566500, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 566500   |
---------------------------------
Eval num_timesteps=567000, episode_reward=197.82 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 277      |
|    time_elapsed    | 16116    |
|    total_timesteps | 567296   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=567500, episode_reward=197.87 +/- 0.37
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 567500       |
| train/                  |              |
|    approx_kl            | 0.0042780573 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 5.64         |
|    n_updates            | 413          |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 14.6         |
------------------------------------------
Eval num_timesteps=568000, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=568500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 568500   |
---------------------------------
Eval num_timesteps=569000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 278      |
|    time_elapsed    | 16188    |
|    total_timesteps | 569344   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=569500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 569500       |
| train/                  |              |
|    approx_kl            | 0.0041552717 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.001        |
|    loss                 | 0.368        |
|    n_updates            | 417          |
|    policy_gradient_loss | -0.00387     |
|    value_loss           | 5.32         |
------------------------------------------
Eval num_timesteps=570000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
Eval num_timesteps=570500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 570500   |
---------------------------------
Eval num_timesteps=571000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 279      |
|    time_elapsed    | 16259    |
|    total_timesteps | 571392   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=571500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 571500      |
| train/                  |             |
|    approx_kl            | 0.009863621 |
|    clip_fraction        | 0.00703     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.001       |
|    loss                 | 6.02        |
|    n_updates            | 419         |
|    policy_gradient_loss | -0.000701   |
|    value_loss           | 15.3        |
-----------------------------------------
Eval num_timesteps=572000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=572500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 572500   |
---------------------------------
Eval num_timesteps=573000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 280      |
|    time_elapsed    | 16330    |
|    total_timesteps | 573440   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=573500, episode_reward=186.19 +/- 33.08
Episode length: 477.74 +/- 141.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 478          |
|    mean_reward          | 186          |
| time/                   |              |
|    total_timesteps      | 573500       |
| train/                  |              |
|    approx_kl            | 0.0036478564 |
|    clip_fraction        | 0.00251      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | 10.7         |
|    n_updates            | 421          |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 18.3         |
------------------------------------------
Eval num_timesteps=574000, episode_reward=181.41 +/- 38.12
Episode length: 460.06 +/- 161.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 460      |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
Eval num_timesteps=574500, episode_reward=181.56 +/- 38.24
Episode length: 457.86 +/- 166.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 458      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 574500   |
---------------------------------
Eval num_timesteps=575000, episode_reward=183.62 +/- 36.05
Episode length: 467.88 +/- 154.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 468      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 281      |
|    time_elapsed    | 16393    |
|    total_timesteps | 575488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=575500, episode_reward=196.91 +/- 1.24
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 575500       |
| train/                  |              |
|    approx_kl            | 0.0060952795 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.847        |
|    learning_rate        | 0.001        |
|    loss                 | 53.4         |
|    n_updates            | 422          |
|    policy_gradient_loss | 0.000325     |
|    value_loss           | 46.1         |
------------------------------------------
Eval num_timesteps=576000, episode_reward=197.19 +/- 1.11
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=197.30 +/- 1.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
Eval num_timesteps=577000, episode_reward=197.03 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=577500, episode_reward=197.19 +/- 1.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 282      |
|    time_elapsed    | 16482    |
|    total_timesteps | 577536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=578000, episode_reward=192.62 +/- 29.63
Episode length: 515.10 +/- 69.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0054969313 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.001        |
|    loss                 | 5.82         |
|    n_updates            | 423          |
|    policy_gradient_loss | 0.00325      |
|    value_loss           | 38.7         |
------------------------------------------
Eval num_timesteps=578500, episode_reward=186.22 +/- 43.83
Episode length: 495.30 +/- 117.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
Eval num_timesteps=579000, episode_reward=196.42 +/- 1.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=579500, episode_reward=196.38 +/- 1.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 283      |
|    time_elapsed    | 16554    |
|    total_timesteps | 579584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=580000, episode_reward=197.67 +/- 1.02
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.015584831 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.001       |
|    loss                 | 15.4        |
|    n_updates            | 424         |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 96.5        |
-----------------------------------------
Eval num_timesteps=580500, episode_reward=197.95 +/- 0.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
Eval num_timesteps=581000, episode_reward=197.82 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=581500, episode_reward=197.84 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 284      |
|    time_elapsed    | 16627    |
|    total_timesteps | 581632   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=582000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 582000      |
| train/                  |             |
|    approx_kl            | 0.005262356 |
|    clip_fraction        | 0.00521     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.001       |
|    loss                 | 1.46        |
|    n_updates            | 425         |
|    policy_gradient_loss | 0.00812     |
|    value_loss           | 21.6        |
-----------------------------------------
Eval num_timesteps=582500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
Eval num_timesteps=583000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=583500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 285      |
|    time_elapsed    | 16698    |
|    total_timesteps | 583680   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=584000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 584000      |
| train/                  |             |
|    approx_kl            | 0.014425626 |
|    clip_fraction        | 0.0461      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | 6.12        |
|    n_updates            | 427         |
|    policy_gradient_loss | -0.000696   |
|    value_loss           | 13.1        |
-----------------------------------------
Eval num_timesteps=584500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
Eval num_timesteps=585000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=585500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 286      |
|    time_elapsed    | 16768    |
|    total_timesteps | 585728   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=586000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 586000     |
| train/                  |            |
|    approx_kl            | 0.01549391 |
|    clip_fraction        | 0.00391    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.001      |
|    loss                 | 2.92       |
|    n_updates            | 429        |
|    policy_gradient_loss | 0.00185    |
|    value_loss           | 5.17       |
----------------------------------------
Eval num_timesteps=586500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
Eval num_timesteps=587000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=587500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 287      |
|    time_elapsed    | 16839    |
|    total_timesteps | 587776   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=588000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 588000       |
| train/                  |              |
|    approx_kl            | 0.0054643783 |
|    clip_fraction        | 0.0075       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 9.14         |
|    n_updates            | 437          |
|    policy_gradient_loss | 0.000304     |
|    value_loss           | 12.5         |
------------------------------------------
Eval num_timesteps=588500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
Eval num_timesteps=589000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
Eval num_timesteps=589500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 288      |
|    time_elapsed    | 16910    |
|    total_timesteps | 589824   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=590000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.005158643 |
|    clip_fraction        | 0.0028      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.001       |
|    loss                 | 9.92        |
|    n_updates            | 440         |
|    policy_gradient_loss | 0.000712    |
|    value_loss           | 28.7        |
-----------------------------------------
Eval num_timesteps=590500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
Eval num_timesteps=591000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=591500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 289      |
|    time_elapsed    | 16981    |
|    total_timesteps | 591872   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=592000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 592000      |
| train/                  |             |
|    approx_kl            | 0.008060578 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0311      |
|    n_updates            | 444         |
|    policy_gradient_loss | 0.00071     |
|    value_loss           | 20.8        |
-----------------------------------------
Eval num_timesteps=592500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
Eval num_timesteps=593000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
Eval num_timesteps=593500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 290      |
|    time_elapsed    | 17053    |
|    total_timesteps | 593920   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=594000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 594000     |
| train/                  |            |
|    approx_kl            | 0.04431621 |
|    clip_fraction        | 0.0152     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.001      |
|    loss                 | -0.00124   |
|    n_updates            | 446        |
|    policy_gradient_loss | 0.00715    |
|    value_loss           | 14.7       |
----------------------------------------
Eval num_timesteps=594500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
Eval num_timesteps=595000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=595500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 291      |
|    time_elapsed    | 17125    |
|    total_timesteps | 595968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=596000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 596000       |
| train/                  |              |
|    approx_kl            | 0.0036683998 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | -0.00404     |
|    n_updates            | 447          |
|    policy_gradient_loss | 0.00115      |
|    value_loss           | 15           |
------------------------------------------
Eval num_timesteps=596500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
Eval num_timesteps=597000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=597500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 292      |
|    time_elapsed    | 17213    |
|    total_timesteps | 598016   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=598500, episode_reward=197.85 +/- 0.43
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 598500       |
| train/                  |              |
|    approx_kl            | 0.0030314957 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 11.7         |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 10.8         |
------------------------------------------
Eval num_timesteps=599000, episode_reward=197.88 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
Eval num_timesteps=599500, episode_reward=197.82 +/- 0.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 599500   |
---------------------------------
Eval num_timesteps=600000, episode_reward=197.87 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 293      |
|    time_elapsed    | 17284    |
|    total_timesteps | 600064   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=600500, episode_reward=197.92 +/- 0.32
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 600500       |
| train/                  |              |
|    approx_kl            | 0.0061218105 |
|    clip_fraction        | 0.00859      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.925        |
|    learning_rate        | 0.001        |
|    loss                 | 6.76         |
|    n_updates            | 452          |
|    policy_gradient_loss | 0.000417     |
|    value_loss           | 26.2         |
------------------------------------------
Eval num_timesteps=601000, episode_reward=197.97 +/- 0.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 601000   |
---------------------------------
Eval num_timesteps=601500, episode_reward=197.90 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 601500   |
---------------------------------
Eval num_timesteps=602000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 602000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 294      |
|    time_elapsed    | 17355    |
|    total_timesteps | 602112   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=602500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 602500      |
| train/                  |             |
|    approx_kl            | 0.010662767 |
|    clip_fraction        | 0.0487      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0404      |
|    n_updates            | 454         |
|    policy_gradient_loss | -0.00231    |
|    value_loss           | 14.2        |
-----------------------------------------
Eval num_timesteps=603000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 603000   |
---------------------------------
Eval num_timesteps=603500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 603500   |
---------------------------------
Eval num_timesteps=604000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 604000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 295      |
|    time_elapsed    | 17426    |
|    total_timesteps | 604160   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=604500, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 604500      |
| train/                  |             |
|    approx_kl            | 0.032297492 |
|    clip_fraction        | 0.00689     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 15.3        |
|    n_updates            | 456         |
|    policy_gradient_loss | 0.0029      |
|    value_loss           | 13.8        |
-----------------------------------------
Eval num_timesteps=605000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 605000   |
---------------------------------
Eval num_timesteps=605500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 605500   |
---------------------------------
Eval num_timesteps=606000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 606000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 296      |
|    time_elapsed    | 17497    |
|    total_timesteps | 606208   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=606500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 606500       |
| train/                  |              |
|    approx_kl            | 0.0055288277 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 0.485        |
|    n_updates            | 461          |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 7.3          |
------------------------------------------
Eval num_timesteps=607000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 607000   |
---------------------------------
Eval num_timesteps=607500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 607500   |
---------------------------------
Eval num_timesteps=608000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 608000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 297      |
|    time_elapsed    | 17568    |
|    total_timesteps | 608256   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=608500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 608500      |
| train/                  |             |
|    approx_kl            | 0.019539505 |
|    clip_fraction        | 0.002       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 0.407       |
|    n_updates            | 469         |
|    policy_gradient_loss | 0.00136     |
|    value_loss           | 11.7        |
-----------------------------------------
Eval num_timesteps=609000, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 609000   |
---------------------------------
Eval num_timesteps=609500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 609500   |
---------------------------------
Eval num_timesteps=610000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 610000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 298      |
|    time_elapsed    | 17640    |
|    total_timesteps | 610304   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=610500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 610500       |
| train/                  |              |
|    approx_kl            | 0.0138562005 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.001        |
|    loss                 | 20.4         |
|    n_updates            | 473          |
|    policy_gradient_loss | 0.000831     |
|    value_loss           | 21           |
------------------------------------------
Eval num_timesteps=611000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 611000   |
---------------------------------
Eval num_timesteps=611500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 611500   |
---------------------------------
Eval num_timesteps=612000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 612000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 299      |
|    time_elapsed    | 17711    |
|    total_timesteps | 612352   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=612500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 612500      |
| train/                  |             |
|    approx_kl            | 0.012656539 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.001       |
|    loss                 | -0.113      |
|    n_updates            | 475         |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 4.05        |
-----------------------------------------
Eval num_timesteps=613000, episode_reward=197.91 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 613000   |
---------------------------------
Eval num_timesteps=613500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 613500   |
---------------------------------
Eval num_timesteps=614000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 614000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 300      |
|    time_elapsed    | 17783    |
|    total_timesteps | 614400   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=614500, episode_reward=197.63 +/- 0.68
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 614500      |
| train/                  |             |
|    approx_kl            | 0.034558892 |
|    clip_fraction        | 0.00174     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 3.18        |
|    n_updates            | 477         |
|    policy_gradient_loss | -0.000297   |
|    value_loss           | 8.49        |
-----------------------------------------
Eval num_timesteps=615000, episode_reward=197.68 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 615000   |
---------------------------------
Eval num_timesteps=615500, episode_reward=197.78 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 615500   |
---------------------------------
Eval num_timesteps=616000, episode_reward=197.89 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 616000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 301      |
|    time_elapsed    | 17855    |
|    total_timesteps | 616448   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=616500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 616500      |
| train/                  |             |
|    approx_kl            | 0.008473205 |
|    clip_fraction        | 0.00987     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 3.94        |
|    n_updates            | 479         |
|    policy_gradient_loss | 0.00502     |
|    value_loss           | 12.2        |
-----------------------------------------
Eval num_timesteps=617000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 617000   |
---------------------------------
Eval num_timesteps=617500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 617500   |
---------------------------------
Eval num_timesteps=618000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 618000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 302      |
|    time_elapsed    | 17927    |
|    total_timesteps | 618496   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.04
Eval num_timesteps=618500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 618500       |
| train/                  |              |
|    approx_kl            | 0.0069609494 |
|    clip_fraction        | 0.00404      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.001        |
|    loss                 | 7.65         |
|    n_updates            | 483          |
|    policy_gradient_loss | 0.00062      |
|    value_loss           | 34           |
------------------------------------------
Eval num_timesteps=619000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 619000   |
---------------------------------
Eval num_timesteps=619500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 619500   |
---------------------------------
Eval num_timesteps=620000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=620500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 620500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 303      |
|    time_elapsed    | 18015    |
|    total_timesteps | 620544   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=621000, episode_reward=197.72 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 621000     |
| train/                  |            |
|    approx_kl            | 0.00779686 |
|    clip_fraction        | 0.00472    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.001      |
|    loss                 | 2.51       |
|    n_updates            | 487        |
|    policy_gradient_loss | -0.00096   |
|    value_loss           | 5.57       |
----------------------------------------
Eval num_timesteps=621500, episode_reward=197.19 +/- 1.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 621500   |
---------------------------------
Eval num_timesteps=622000, episode_reward=197.34 +/- 1.25
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 622000   |
---------------------------------
Eval num_timesteps=622500, episode_reward=197.50 +/- 0.98
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 622500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 304      |
|    time_elapsed    | 18087    |
|    total_timesteps | 622592   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=623000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 623000      |
| train/                  |             |
|    approx_kl            | 0.008176544 |
|    clip_fraction        | 0.00474     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.001       |
|    loss                 | 34.8        |
|    n_updates            | 489         |
|    policy_gradient_loss | -0.00072    |
|    value_loss           | 49.8        |
-----------------------------------------
Eval num_timesteps=623500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 623500   |
---------------------------------
Eval num_timesteps=624000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 624000   |
---------------------------------
Eval num_timesteps=624500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 624500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 305      |
|    time_elapsed    | 18158    |
|    total_timesteps | 624640   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=625000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.008850065 |
|    clip_fraction        | 0.0188      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 0.569       |
|    n_updates            | 497         |
|    policy_gradient_loss | -0.000968   |
|    value_loss           | 8.67        |
-----------------------------------------
Eval num_timesteps=625500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 625500   |
---------------------------------
Eval num_timesteps=626000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 626000   |
---------------------------------
Eval num_timesteps=626500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 626500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 306      |
|    time_elapsed    | 18230    |
|    total_timesteps | 626688   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=627000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 627000      |
| train/                  |             |
|    approx_kl            | 0.013912233 |
|    clip_fraction        | 0.00955     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.001       |
|    loss                 | 0.239       |
|    n_updates            | 499         |
|    policy_gradient_loss | 0.00588     |
|    value_loss           | 19.9        |
-----------------------------------------
Eval num_timesteps=627500, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 627500   |
---------------------------------
Eval num_timesteps=628000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 628000   |
---------------------------------
Eval num_timesteps=628500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 628500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 307      |
|    time_elapsed    | 18301    |
|    total_timesteps | 628736   |
---------------------------------
Eval num_timesteps=629000, episode_reward=197.89 +/- 0.41
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 629000       |
| train/                  |              |
|    approx_kl            | 0.0053922157 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.861        |
|    learning_rate        | 0.001        |
|    loss                 | 22.2         |
|    n_updates            | 509          |
|    policy_gradient_loss | -0.0022      |
|    value_loss           | 33.9         |
------------------------------------------
Eval num_timesteps=629500, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 629500   |
---------------------------------
Eval num_timesteps=630000, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 630000   |
---------------------------------
Eval num_timesteps=630500, episode_reward=197.89 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 630500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 308      |
|    time_elapsed    | 18373    |
|    total_timesteps | 630784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=631000, episode_reward=190.84 +/- 26.14
Episode length: 506.26 +/- 74.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 631000      |
| train/                  |             |
|    approx_kl            | 0.006758825 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 11          |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.00238    |
|    value_loss           | 12.7        |
-----------------------------------------
Eval num_timesteps=631500, episode_reward=193.17 +/- 21.60
Episode length: 512.42 +/- 67.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 512      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 631500   |
---------------------------------
Eval num_timesteps=632000, episode_reward=191.00 +/- 26.18
Episode length: 505.06 +/- 79.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 632000   |
---------------------------------
Eval num_timesteps=632500, episode_reward=193.02 +/- 21.57
Episode length: 515.36 +/- 47.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 632500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 309      |
|    time_elapsed    | 18443    |
|    total_timesteps | 632832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=633000, episode_reward=197.95 +/- 0.27
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 633000      |
| train/                  |             |
|    approx_kl            | 0.003347138 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.001       |
|    loss                 | 32          |
|    n_updates            | 511         |
|    policy_gradient_loss | 0.000243    |
|    value_loss           | 52.4        |
-----------------------------------------
Eval num_timesteps=633500, episode_reward=195.74 +/- 15.48
Episode length: 520.36 +/- 32.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 520      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 633500   |
---------------------------------
Eval num_timesteps=634000, episode_reward=193.46 +/- 21.65
Episode length: 512.64 +/- 60.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 513      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 634000   |
---------------------------------
Eval num_timesteps=634500, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 634500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 310      |
|    time_elapsed    | 18514    |
|    total_timesteps | 634880   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=635000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 635000      |
| train/                  |             |
|    approx_kl            | 0.007691132 |
|    clip_fraction        | 0.00479     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 7.31        |
|    n_updates            | 514         |
|    policy_gradient_loss | -0.000121   |
|    value_loss           | 14.3        |
-----------------------------------------
Eval num_timesteps=635500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 635500   |
---------------------------------
Eval num_timesteps=636000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 636000   |
---------------------------------
Eval num_timesteps=636500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 636500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 311      |
|    time_elapsed    | 18586    |
|    total_timesteps | 636928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=637000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 637000      |
| train/                  |             |
|    approx_kl            | 0.005114966 |
|    clip_fraction        | 0.00204     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.001       |
|    loss                 | 8.13        |
|    n_updates            | 516         |
|    policy_gradient_loss | -0.000589   |
|    value_loss           | 24.5        |
-----------------------------------------
Eval num_timesteps=637500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 637500   |
---------------------------------
Eval num_timesteps=638000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 638000   |
---------------------------------
Eval num_timesteps=638500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 638500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 312      |
|    time_elapsed    | 18657    |
|    total_timesteps | 638976   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=639000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 639000       |
| train/                  |              |
|    approx_kl            | 0.0076863244 |
|    clip_fraction        | 0.0071       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 14.2         |
|    n_updates            | 518          |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 14.9         |
------------------------------------------
Eval num_timesteps=639500, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 639500   |
---------------------------------
Eval num_timesteps=640000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=640500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 640500   |
---------------------------------
Eval num_timesteps=641000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 641000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 313      |
|    time_elapsed    | 18745    |
|    total_timesteps | 641024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=641500, episode_reward=197.80 +/- 0.48
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 641500       |
| train/                  |              |
|    approx_kl            | 0.0033962424 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 4.28         |
|    n_updates            | 519          |
|    policy_gradient_loss | 0.000261     |
|    value_loss           | 15.9         |
------------------------------------------
Eval num_timesteps=642000, episode_reward=197.86 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 642000   |
---------------------------------
Eval num_timesteps=642500, episode_reward=197.92 +/- 0.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 642500   |
---------------------------------
Eval num_timesteps=643000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 643000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 314      |
|    time_elapsed    | 18816    |
|    total_timesteps | 643072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=643500, episode_reward=197.83 +/- 0.50
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 643500       |
| train/                  |              |
|    approx_kl            | 0.0061112093 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 7.11         |
|    n_updates            | 520          |
|    policy_gradient_loss | 0.00206      |
|    value_loss           | 20.6         |
------------------------------------------
Eval num_timesteps=644000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 644000   |
---------------------------------
Eval num_timesteps=644500, episode_reward=197.73 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 644500   |
---------------------------------
Eval num_timesteps=645000, episode_reward=197.87 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 645000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 315      |
|    time_elapsed    | 18887    |
|    total_timesteps | 645120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=645500, episode_reward=197.79 +/- 0.64
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 645500      |
| train/                  |             |
|    approx_kl            | 0.003642082 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.001       |
|    loss                 | 18.6        |
|    n_updates            | 521         |
|    policy_gradient_loss | 0.00612     |
|    value_loss           | 51.2        |
-----------------------------------------
Eval num_timesteps=646000, episode_reward=197.91 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 646000   |
---------------------------------
Eval num_timesteps=646500, episode_reward=197.87 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 646500   |
---------------------------------
Eval num_timesteps=647000, episode_reward=197.82 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 647000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 316      |
|    time_elapsed    | 18958    |
|    total_timesteps | 647168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=647500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 647500       |
| train/                  |              |
|    approx_kl            | 0.0043195104 |
|    clip_fraction        | 0.0269       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.7          |
|    learning_rate        | 0.001        |
|    loss                 | 53.9         |
|    n_updates            | 522          |
|    policy_gradient_loss | -0.00514     |
|    value_loss           | 61.7         |
------------------------------------------
Eval num_timesteps=648000, episode_reward=197.97 +/- 0.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 648000   |
---------------------------------
Eval num_timesteps=648500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 648500   |
---------------------------------
Eval num_timesteps=649000, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 649000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 317      |
|    time_elapsed    | 19028    |
|    total_timesteps | 649216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=649500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 649500      |
| train/                  |             |
|    approx_kl            | 0.012043053 |
|    clip_fraction        | 0.0449      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.001       |
|    loss                 | 16.2        |
|    n_updates            | 523         |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 28.4        |
-----------------------------------------
Eval num_timesteps=650000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 650000   |
---------------------------------
Eval num_timesteps=650500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 650500   |
---------------------------------
Eval num_timesteps=651000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 651000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 318      |
|    time_elapsed    | 19100    |
|    total_timesteps | 651264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=651500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 651500      |
| train/                  |             |
|    approx_kl            | 0.006481701 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 6.92        |
|    n_updates            | 524         |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 6.66        |
-----------------------------------------
Eval num_timesteps=652000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 652000   |
---------------------------------
Eval num_timesteps=652500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 652500   |
---------------------------------
Eval num_timesteps=653000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 653000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 319      |
|    time_elapsed    | 19172    |
|    total_timesteps | 653312   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=653500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 653500      |
| train/                  |             |
|    approx_kl            | 0.013055322 |
|    clip_fraction        | 0.0269      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0715     |
|    n_updates            | 526         |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 18.8        |
-----------------------------------------
Eval num_timesteps=654000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 654000   |
---------------------------------
Eval num_timesteps=654500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 654500   |
---------------------------------
Eval num_timesteps=655000, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 655000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 320      |
|    time_elapsed    | 19244    |
|    total_timesteps | 655360   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=655500, episode_reward=197.84 +/- 0.42
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 655500      |
| train/                  |             |
|    approx_kl            | 0.015121339 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 7.4         |
|    n_updates            | 528         |
|    policy_gradient_loss | -0.00413    |
|    value_loss           | 3.85        |
-----------------------------------------
Eval num_timesteps=656000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 656000   |
---------------------------------
Eval num_timesteps=656500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 656500   |
---------------------------------
Eval num_timesteps=657000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 657000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 321      |
|    time_elapsed    | 19316    |
|    total_timesteps | 657408   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
Eval num_timesteps=657500, episode_reward=197.74 +/- 0.44
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 657500      |
| train/                  |             |
|    approx_kl            | 0.009800275 |
|    clip_fraction        | 0.0342      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 10.6        |
|    n_updates            | 538         |
|    policy_gradient_loss | 1.14e-05    |
|    value_loss           | 10.9        |
-----------------------------------------
Eval num_timesteps=658000, episode_reward=197.88 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 658000   |
---------------------------------
Eval num_timesteps=658500, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 658500   |
---------------------------------
Eval num_timesteps=659000, episode_reward=197.71 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 659000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 34       |
|    iterations      | 322      |
|    time_elapsed    | 19388    |
|    total_timesteps | 659456   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=659500, episode_reward=197.39 +/- 1.28
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 659500      |
| train/                  |             |
|    approx_kl            | 0.015538463 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.001       |
|    loss                 | 6.52        |
|    n_updates            | 542         |
|    policy_gradient_loss | 0.000379    |
|    value_loss           | 29.9        |
-----------------------------------------
Eval num_timesteps=660000, episode_reward=197.07 +/- 1.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=660500, episode_reward=197.22 +/- 1.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 660500   |
---------------------------------
Eval num_timesteps=661000, episode_reward=197.30 +/- 1.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 661000   |
---------------------------------
Eval num_timesteps=661500, episode_reward=197.36 +/- 1.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 661500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 323      |
|    time_elapsed    | 19476    |
|    total_timesteps | 661504   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=662000, episode_reward=197.87 +/- 0.37
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 662000       |
| train/                  |              |
|    approx_kl            | 0.0061659515 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 14.5         |
|    n_updates            | 545          |
|    policy_gradient_loss | -0.00432     |
|    value_loss           | 20.3         |
------------------------------------------
Eval num_timesteps=662500, episode_reward=197.89 +/- 0.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 662500   |
---------------------------------
Eval num_timesteps=663000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 663000   |
---------------------------------
Eval num_timesteps=663500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 663500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 324      |
|    time_elapsed    | 19547    |
|    total_timesteps | 663552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=664000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 664000       |
| train/                  |              |
|    approx_kl            | 0.0048585585 |
|    clip_fraction        | 0.037        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 8.88         |
|    n_updates            | 546          |
|    policy_gradient_loss | 0.00361      |
|    value_loss           | 10.8         |
------------------------------------------
Eval num_timesteps=664500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 664500   |
---------------------------------
Eval num_timesteps=665000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 665000   |
---------------------------------
Eval num_timesteps=665500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 665500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 325      |
|    time_elapsed    | 19618    |
|    total_timesteps | 665600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=666000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 666000      |
| train/                  |             |
|    approx_kl            | 0.009785437 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.001       |
|    loss                 | 16.8        |
|    n_updates            | 547         |
|    policy_gradient_loss | -0.00338    |
|    value_loss           | 11          |
-----------------------------------------
Eval num_timesteps=666500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 666500   |
---------------------------------
Eval num_timesteps=667000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 667000   |
---------------------------------
Eval num_timesteps=667500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 667500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 326      |
|    time_elapsed    | 19689    |
|    total_timesteps | 667648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=668000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 668000      |
| train/                  |             |
|    approx_kl            | 0.005526161 |
|    clip_fraction        | 0.0384      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 7.12        |
|    n_updates            | 548         |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 7.4         |
-----------------------------------------
Eval num_timesteps=668500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 668500   |
---------------------------------
Eval num_timesteps=669000, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 669000   |
---------------------------------
Eval num_timesteps=669500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 669500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 327      |
|    time_elapsed    | 19760    |
|    total_timesteps | 669696   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=670000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.042221524 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 3.43        |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.000471   |
|    value_loss           | 7.44        |
-----------------------------------------
Eval num_timesteps=670500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 670500   |
---------------------------------
Eval num_timesteps=671000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 671000   |
---------------------------------
Eval num_timesteps=671500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 671500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 328      |
|    time_elapsed    | 19831    |
|    total_timesteps | 671744   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=672000, episode_reward=178.39 +/- 46.30
Episode length: 486.48 +/- 130.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 486         |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 672000      |
| train/                  |             |
|    approx_kl            | 0.029248577 |
|    clip_fraction        | 0.00477     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.001       |
|    loss                 | 7.88        |
|    n_updates            | 552         |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 7.98        |
-----------------------------------------
Eval num_timesteps=672500, episode_reward=160.00 +/- 73.64
Episode length: 426.54 +/- 196.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 672500   |
---------------------------------
Eval num_timesteps=673000, episode_reward=178.08 +/- 49.43
Episode length: 475.32 +/- 149.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 475      |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 673000   |
---------------------------------
Eval num_timesteps=673500, episode_reward=170.12 +/- 56.83
Episode length: 456.72 +/- 169.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 673500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 329      |
|    time_elapsed    | 19893    |
|    total_timesteps | 673792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.18
Eval num_timesteps=674000, episode_reward=178.21 +/- 55.50
Episode length: 475.58 +/- 148.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 674000      |
| train/                  |             |
|    approx_kl            | 0.021359485 |
|    clip_fraction        | 0.0174      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.001       |
|    loss                 | 83.9        |
|    n_updates            | 553         |
|    policy_gradient_loss | -0.000507   |
|    value_loss           | 104         |
-----------------------------------------
Eval num_timesteps=674500, episode_reward=180.25 +/- 46.49
Episode length: 486.24 +/- 131.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 674500   |
---------------------------------
Eval num_timesteps=675000, episode_reward=185.23 +/- 41.24
Episode length: 505.04 +/- 97.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 675000   |
---------------------------------
Eval num_timesteps=675500, episode_reward=163.45 +/- 70.27
Episode length: 427.50 +/- 195.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 428      |
|    mean_reward     | 163      |
| time/              |          |
|    total_timesteps | 675500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 330      |
|    time_elapsed    | 19958    |
|    total_timesteps | 675840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=676000, episode_reward=182.47 +/- 36.20
Episode length: 467.24 +/- 156.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 467         |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 676000      |
| train/                  |             |
|    approx_kl            | 0.006917553 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.001       |
|    loss                 | 78          |
|    n_updates            | 554         |
|    policy_gradient_loss | -6.38e-07   |
|    value_loss           | 178         |
-----------------------------------------
Eval num_timesteps=676500, episode_reward=186.61 +/- 36.42
Episode length: 495.84 +/- 115.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 676500   |
---------------------------------
Eval num_timesteps=677000, episode_reward=184.90 +/- 43.66
Episode length: 494.80 +/- 119.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 677000   |
---------------------------------
Eval num_timesteps=677500, episode_reward=182.10 +/- 46.11
Episode length: 485.68 +/- 133.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 677500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 331      |
|    time_elapsed    | 20024    |
|    total_timesteps | 677888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=678000, episode_reward=185.19 +/- 44.39
Episode length: 495.52 +/- 116.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 185          |
| time/                   |              |
|    total_timesteps      | 678000       |
| train/                  |              |
|    approx_kl            | 0.0075956583 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.3          |
|    learning_rate        | 0.001        |
|    loss                 | 80.1         |
|    n_updates            | 555          |
|    policy_gradient_loss | -0.00972     |
|    value_loss           | 162          |
------------------------------------------
Eval num_timesteps=678500, episode_reward=193.50 +/- 15.68
Episode length: 515.04 +/- 69.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 678500   |
---------------------------------
Eval num_timesteps=679000, episode_reward=191.41 +/- 22.84
Episode length: 505.64 +/- 94.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 679000   |
---------------------------------
Eval num_timesteps=679500, episode_reward=195.72 +/- 1.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 679500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 332      |
|    time_elapsed    | 20093    |
|    total_timesteps | 679936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=680000, episode_reward=192.81 +/- 16.31
Episode length: 515.32 +/- 67.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 0.0077116895 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.494        |
|    learning_rate        | 0.001        |
|    loss                 | 91.9         |
|    n_updates            | 556          |
|    policy_gradient_loss | 0.00683      |
|    value_loss           | 147          |
------------------------------------------
Eval num_timesteps=680500, episode_reward=188.13 +/- 33.60
Episode length: 505.46 +/- 95.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 680500   |
---------------------------------
Eval num_timesteps=681000, episode_reward=190.46 +/- 30.05
Episode length: 515.20 +/- 68.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 681000   |
---------------------------------
Eval num_timesteps=681500, episode_reward=190.76 +/- 30.33
Episode length: 515.42 +/- 67.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 681500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 333      |
|    time_elapsed    | 20162    |
|    total_timesteps | 681984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=682000, episode_reward=196.63 +/- 1.86
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 682000      |
| train/                  |             |
|    approx_kl            | 0.009303523 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.001       |
|    loss                 | 30.5        |
|    n_updates            | 557         |
|    policy_gradient_loss | 0.011       |
|    value_loss           | 62.6        |
-----------------------------------------
Eval num_timesteps=682500, episode_reward=196.87 +/- 1.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 682500   |
---------------------------------
Eval num_timesteps=683000, episode_reward=196.53 +/- 1.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 683000   |
---------------------------------
Eval num_timesteps=683500, episode_reward=196.67 +/- 1.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 683500   |
---------------------------------
Eval num_timesteps=684000, episode_reward=196.84 +/- 1.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 334      |
|    time_elapsed    | 20251    |
|    total_timesteps | 684032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=684500, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 684500       |
| train/                  |              |
|    approx_kl            | 0.0035638148 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.824        |
|    learning_rate        | 0.001        |
|    loss                 | 23.6         |
|    n_updates            | 558          |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 47           |
------------------------------------------
Eval num_timesteps=685000, episode_reward=197.82 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 685000   |
---------------------------------
Eval num_timesteps=685500, episode_reward=197.77 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 685500   |
---------------------------------
Eval num_timesteps=686000, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 686000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 335      |
|    time_elapsed    | 20323    |
|    total_timesteps | 686080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=686500, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 686500      |
| train/                  |             |
|    approx_kl            | 0.005238255 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.001       |
|    loss                 | 3.35        |
|    n_updates            | 559         |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 23          |
-----------------------------------------
Eval num_timesteps=687000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 687000   |
---------------------------------
Eval num_timesteps=687500, episode_reward=197.79 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 687500   |
---------------------------------
Eval num_timesteps=688000, episode_reward=197.80 +/- 0.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 688000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 336      |
|    time_elapsed    | 20395    |
|    total_timesteps | 688128   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=688500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 688500     |
| train/                  |            |
|    approx_kl            | 0.05877035 |
|    clip_fraction        | 0.0225     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.001      |
|    loss                 | 26         |
|    n_updates            | 561        |
|    policy_gradient_loss | -0.00192   |
|    value_loss           | 19.2       |
----------------------------------------
Eval num_timesteps=689000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 689000   |
---------------------------------
Eval num_timesteps=689500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 689500   |
---------------------------------
Eval num_timesteps=690000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 690000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 337      |
|    time_elapsed    | 20467    |
|    total_timesteps | 690176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=690500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 690500       |
| train/                  |              |
|    approx_kl            | 0.0063776365 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.001        |
|    loss                 | 7.55         |
|    n_updates            | 562          |
|    policy_gradient_loss | -0.00207     |
|    value_loss           | 17.8         |
------------------------------------------
Eval num_timesteps=691000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 691000   |
---------------------------------
Eval num_timesteps=691500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 691500   |
---------------------------------
Eval num_timesteps=692000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 692000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 338      |
|    time_elapsed    | 20537    |
|    total_timesteps | 692224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=692500, episode_reward=99.88 +/- 36.24
Episode length: 118.40 +/- 152.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 99.9        |
| time/                   |             |
|    total_timesteps      | 692500      |
| train/                  |             |
|    approx_kl            | 0.006434407 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.001       |
|    loss                 | 11.4        |
|    n_updates            | 563         |
|    policy_gradient_loss | 0.00134     |
|    value_loss           | 30.2        |
-----------------------------------------
Eval num_timesteps=693000, episode_reward=106.79 +/- 47.57
Episode length: 155.06 +/- 186.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 693000   |
---------------------------------
Eval num_timesteps=693500, episode_reward=95.65 +/- 36.77
Episode length: 108.88 +/- 140.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 95.6     |
| time/              |          |
|    total_timesteps | 693500   |
---------------------------------
Eval num_timesteps=694000, episode_reward=100.12 +/- 36.15
Episode length: 116.28 +/- 151.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 100      |
| time/              |          |
|    total_timesteps | 694000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 440      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 339      |
|    time_elapsed    | 20555    |
|    total_timesteps | 694272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=694500, episode_reward=168.30 +/- 48.37
Episode length: 404.96 +/- 203.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 405          |
|    mean_reward          | 168          |
| time/                   |              |
|    total_timesteps      | 694500       |
| train/                  |              |
|    approx_kl            | 0.0052020163 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.554        |
|    learning_rate        | 0.001        |
|    loss                 | 63.1         |
|    n_updates            | 564          |
|    policy_gradient_loss | 0.0112       |
|    value_loss           | 154          |
------------------------------------------
Eval num_timesteps=695000, episode_reward=166.03 +/- 49.58
Episode length: 395.84 +/- 207.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 396      |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 695000   |
---------------------------------
Eval num_timesteps=695500, episode_reward=155.13 +/- 53.65
Episode length: 351.98 +/- 222.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 352      |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 695500   |
---------------------------------
Eval num_timesteps=696000, episode_reward=166.33 +/- 49.54
Episode length: 392.64 +/- 212.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 393      |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 696000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 340      |
|    time_elapsed    | 20609    |
|    total_timesteps | 696320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=696500, episode_reward=195.02 +/- 15.56
Episode length: 516.14 +/- 62.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 696500      |
| train/                  |             |
|    approx_kl            | 0.003778542 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.001       |
|    loss                 | 48          |
|    n_updates            | 565         |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 88.8        |
-----------------------------------------
Eval num_timesteps=697000, episode_reward=184.34 +/- 36.17
Episode length: 468.54 +/- 153.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 697000   |
---------------------------------
Eval num_timesteps=697500, episode_reward=195.45 +/- 15.60
Episode length: 515.44 +/- 66.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 697500   |
---------------------------------
Eval num_timesteps=698000, episode_reward=193.16 +/- 21.81
Episode length: 505.68 +/- 94.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 698000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 341      |
|    time_elapsed    | 20677    |
|    total_timesteps | 698368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=698500, episode_reward=193.25 +/- 21.82
Episode length: 507.04 +/- 87.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 507          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 698500       |
| train/                  |              |
|    approx_kl            | 0.0037044778 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.2         |
|    explained_variance   | 0.814        |
|    learning_rate        | 0.001        |
|    loss                 | 6.26         |
|    n_updates            | 566          |
|    policy_gradient_loss | -0.00371     |
|    value_loss           | 30.5         |
------------------------------------------
Eval num_timesteps=699000, episode_reward=195.51 +/- 15.47
Episode length: 515.48 +/- 66.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 699000   |
---------------------------------
Eval num_timesteps=699500, episode_reward=193.43 +/- 21.65
Episode length: 506.00 +/- 93.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 699500   |
---------------------------------
Eval num_timesteps=700000, episode_reward=193.28 +/- 22.13
Episode length: 507.60 +/- 85.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 342      |
|    time_elapsed    | 20747    |
|    total_timesteps | 700416   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=700500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 700500      |
| train/                  |             |
|    approx_kl            | 0.007734373 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 1.83        |
|    n_updates            | 569         |
|    policy_gradient_loss | -0.00285    |
|    value_loss           | 5.97        |
-----------------------------------------
Eval num_timesteps=701000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 701000   |
---------------------------------
Eval num_timesteps=701500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 701500   |
---------------------------------
Eval num_timesteps=702000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 702000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 343      |
|    time_elapsed    | 20819    |
|    total_timesteps | 702464   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=702500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 702500      |
| train/                  |             |
|    approx_kl            | 0.010868105 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0839     |
|    n_updates            | 571         |
|    policy_gradient_loss | -0.000773   |
|    value_loss           | 12.9        |
-----------------------------------------
Eval num_timesteps=703000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 703000   |
---------------------------------
Eval num_timesteps=703500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 703500   |
---------------------------------
Eval num_timesteps=704000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 704000   |
---------------------------------
Eval num_timesteps=704500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 704500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 344      |
|    time_elapsed    | 20907    |
|    total_timesteps | 704512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=705000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 705000     |
| train/                  |            |
|    approx_kl            | 0.01037346 |
|    clip_fraction        | 0.0581     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0469    |
|    n_updates            | 573        |
|    policy_gradient_loss | 5.89e-05   |
|    value_loss           | 7.43       |
----------------------------------------
Eval num_timesteps=705500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 705500   |
---------------------------------
Eval num_timesteps=706000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 706000   |
---------------------------------
Eval num_timesteps=706500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 706500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 345      |
|    time_elapsed    | 20979    |
|    total_timesteps | 706560   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
Eval num_timesteps=707000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 707000      |
| train/                  |             |
|    approx_kl            | 0.007959011 |
|    clip_fraction        | 0.0053      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.001       |
|    loss                 | 16.3        |
|    n_updates            | 582         |
|    policy_gradient_loss | -0.000398   |
|    value_loss           | 23.6        |
-----------------------------------------
Eval num_timesteps=707500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 707500   |
---------------------------------
Eval num_timesteps=708000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 708000   |
---------------------------------
Eval num_timesteps=708500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 708500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 346      |
|    time_elapsed    | 21050    |
|    total_timesteps | 708608   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=709000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 709000       |
| train/                  |              |
|    approx_kl            | 0.0048506972 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0325       |
|    n_updates            | 589          |
|    policy_gradient_loss | 0.00143      |
|    value_loss           | 12.5         |
------------------------------------------
Eval num_timesteps=709500, episode_reward=197.82 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 709500   |
---------------------------------
Eval num_timesteps=710000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 710000   |
---------------------------------
Eval num_timesteps=710500, episode_reward=197.88 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 710500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 347      |
|    time_elapsed    | 21122    |
|    total_timesteps | 710656   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=711000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 711000       |
| train/                  |              |
|    approx_kl            | 0.0064432635 |
|    clip_fraction        | 0.00318      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 10.9         |
|    n_updates            | 591          |
|    policy_gradient_loss | 0.000476     |
|    value_loss           | 9.41         |
------------------------------------------
Eval num_timesteps=711500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 711500   |
---------------------------------
Eval num_timesteps=712000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 712000   |
---------------------------------
Eval num_timesteps=712500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 712500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 348      |
|    time_elapsed    | 21195    |
|    total_timesteps | 712704   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=713000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 713000       |
| train/                  |              |
|    approx_kl            | 0.0070667514 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.901        |
|    learning_rate        | 0.001        |
|    loss                 | 22.1         |
|    n_updates            | 594          |
|    policy_gradient_loss | 0.000374     |
|    value_loss           | 28           |
------------------------------------------
Eval num_timesteps=713500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 713500   |
---------------------------------
Eval num_timesteps=714000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 714000   |
---------------------------------
Eval num_timesteps=714500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 714500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 349      |
|    time_elapsed    | 21266    |
|    total_timesteps | 714752   |
---------------------------------
Eval num_timesteps=715000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 715000      |
| train/                  |             |
|    approx_kl            | 0.005781669 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 1.16        |
|    n_updates            | 604         |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 8.82        |
-----------------------------------------
Eval num_timesteps=715500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 715500   |
---------------------------------
Eval num_timesteps=716000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 716000   |
---------------------------------
Eval num_timesteps=716500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 716500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 350      |
|    time_elapsed    | 21338    |
|    total_timesteps | 716800   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=717000, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 717000      |
| train/                  |             |
|    approx_kl            | 0.011421967 |
|    clip_fraction        | 0.0238      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 0.182       |
|    n_updates            | 606         |
|    policy_gradient_loss | -0.00103    |
|    value_loss           | 10.9        |
-----------------------------------------
Eval num_timesteps=717500, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 717500   |
---------------------------------
Eval num_timesteps=718000, episode_reward=197.91 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 718000   |
---------------------------------
Eval num_timesteps=718500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 718500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 441      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 351      |
|    time_elapsed    | 21410    |
|    total_timesteps | 718848   |
---------------------------------
Eval num_timesteps=719000, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 719000      |
| train/                  |             |
|    approx_kl            | 0.003320071 |
|    clip_fraction        | 0.0215      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 5.91        |
|    n_updates            | 616         |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 12.5        |
-----------------------------------------
Eval num_timesteps=719500, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 719500   |
---------------------------------
Eval num_timesteps=720000, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=720500, episode_reward=197.88 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 720500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 455      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 352      |
|    time_elapsed    | 21481    |
|    total_timesteps | 720896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=721000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 721000       |
| train/                  |              |
|    approx_kl            | 0.0031993107 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.83         |
|    learning_rate        | 0.001        |
|    loss                 | 2.99         |
|    n_updates            | 617          |
|    policy_gradient_loss | 0.00138      |
|    value_loss           | 76.9         |
------------------------------------------
Eval num_timesteps=721500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 721500   |
---------------------------------
Eval num_timesteps=722000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 722000   |
---------------------------------
Eval num_timesteps=722500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 722500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 353      |
|    time_elapsed    | 21553    |
|    total_timesteps | 722944   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=723000, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 723000      |
| train/                  |             |
|    approx_kl            | 0.007324163 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 38.3        |
|    n_updates            | 619         |
|    policy_gradient_loss | -0.000161   |
|    value_loss           | 24.3        |
-----------------------------------------
Eval num_timesteps=723500, episode_reward=197.86 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 723500   |
---------------------------------
Eval num_timesteps=724000, episode_reward=197.89 +/- 0.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 724000   |
---------------------------------
Eval num_timesteps=724500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 724500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 354      |
|    time_elapsed    | 21624    |
|    total_timesteps | 724992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=725000, episode_reward=197.93 +/- 0.31
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 725000       |
| train/                  |              |
|    approx_kl            | 0.0034981049 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 36           |
|    n_updates            | 620          |
|    policy_gradient_loss | 0.0128       |
|    value_loss           | 18.6         |
------------------------------------------
Eval num_timesteps=725500, episode_reward=197.88 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 725500   |
---------------------------------
Eval num_timesteps=726000, episode_reward=197.97 +/- 0.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 726000   |
---------------------------------
Eval num_timesteps=726500, episode_reward=197.93 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 726500   |
---------------------------------
Eval num_timesteps=727000, episode_reward=197.90 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 727000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 355      |
|    time_elapsed    | 21712    |
|    total_timesteps | 727040   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=727500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 727500     |
| train/                  |            |
|    approx_kl            | 0.00907556 |
|    clip_fraction        | 0.026      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.001      |
|    loss                 | 2.41       |
|    n_updates            | 622        |
|    policy_gradient_loss | -0.00321   |
|    value_loss           | 25.9       |
----------------------------------------
Eval num_timesteps=728000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 728000   |
---------------------------------
Eval num_timesteps=728500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 728500   |
---------------------------------
Eval num_timesteps=729000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 729000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 356      |
|    time_elapsed    | 21784    |
|    total_timesteps | 729088   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=729500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 729500      |
| train/                  |             |
|    approx_kl            | 0.010129166 |
|    clip_fraction        | 0.0542      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.001       |
|    loss                 | 3.67        |
|    n_updates            | 625         |
|    policy_gradient_loss | -0.00438    |
|    value_loss           | 6.16        |
-----------------------------------------
Eval num_timesteps=730000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 730000   |
---------------------------------
Eval num_timesteps=730500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 730500   |
---------------------------------
Eval num_timesteps=731000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 731000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 357      |
|    time_elapsed    | 21855    |
|    total_timesteps | 731136   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=731500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 731500      |
| train/                  |             |
|    approx_kl            | 0.013104779 |
|    clip_fraction        | 0.0419      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 0.576       |
|    n_updates            | 631         |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 7.45        |
-----------------------------------------
Eval num_timesteps=732000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 732000   |
---------------------------------
Eval num_timesteps=732500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 732500   |
---------------------------------
Eval num_timesteps=733000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 733000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 358      |
|    time_elapsed    | 21926    |
|    total_timesteps | 733184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=733500, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 733500       |
| train/                  |              |
|    approx_kl            | 0.0037005458 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | 20.2         |
|    n_updates            | 632          |
|    policy_gradient_loss | 0.000795     |
|    value_loss           | 22.4         |
------------------------------------------
Eval num_timesteps=734000, episode_reward=197.81 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 734000   |
---------------------------------
Eval num_timesteps=734500, episode_reward=197.82 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 734500   |
---------------------------------
Eval num_timesteps=735000, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 735000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 359      |
|    time_elapsed    | 21998    |
|    total_timesteps | 735232   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=735500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 735500      |
| train/                  |             |
|    approx_kl            | 0.004532172 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 3.37        |
|    n_updates            | 634         |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 10          |
-----------------------------------------
Eval num_timesteps=736000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 736000   |
---------------------------------
Eval num_timesteps=736500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 736500   |
---------------------------------
Eval num_timesteps=737000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 737000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 360      |
|    time_elapsed    | 22069    |
|    total_timesteps | 737280   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=737500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 737500       |
| train/                  |              |
|    approx_kl            | 0.0086378995 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 3.52         |
|    n_updates            | 637          |
|    policy_gradient_loss | -0.00368     |
|    value_loss           | 8.66         |
------------------------------------------
Eval num_timesteps=738000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 738000   |
---------------------------------
Eval num_timesteps=738500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 738500   |
---------------------------------
Eval num_timesteps=739000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 739000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 361      |
|    time_elapsed    | 22139    |
|    total_timesteps | 739328   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=739500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 739500      |
| train/                  |             |
|    approx_kl            | 0.013266534 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0823      |
|    n_updates            | 644         |
|    policy_gradient_loss | -0.00107    |
|    value_loss           | 3.85        |
-----------------------------------------
Eval num_timesteps=740000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=740500, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 740500   |
---------------------------------
Eval num_timesteps=741000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 741000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 362      |
|    time_elapsed    | 22211    |
|    total_timesteps | 741376   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=741500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 741500      |
| train/                  |             |
|    approx_kl            | 0.006475071 |
|    clip_fraction        | 0.0257      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.001       |
|    loss                 | 29.3        |
|    n_updates            | 648         |
|    policy_gradient_loss | -0.000969   |
|    value_loss           | 19.3        |
-----------------------------------------
Eval num_timesteps=742000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 742000   |
---------------------------------
Eval num_timesteps=742500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 742500   |
---------------------------------
Eval num_timesteps=743000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 743000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 363      |
|    time_elapsed    | 22281    |
|    total_timesteps | 743424   |
---------------------------------
Eval num_timesteps=743500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 743500       |
| train/                  |              |
|    approx_kl            | 0.0032024481 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.001        |
|    loss                 | 0.972        |
|    n_updates            | 658          |
|    policy_gradient_loss | -0.000109    |
|    value_loss           | 3.63         |
------------------------------------------
Eval num_timesteps=744000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 744000   |
---------------------------------
Eval num_timesteps=744500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 744500   |
---------------------------------
Eval num_timesteps=745000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 745000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 364      |
|    time_elapsed    | 22352    |
|    total_timesteps | 745472   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
Eval num_timesteps=745500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 745500      |
| train/                  |             |
|    approx_kl            | 0.010192393 |
|    clip_fraction        | 0.0585      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 9           |
|    n_updates            | 667         |
|    policy_gradient_loss | -0.000518   |
|    value_loss           | 12.8        |
-----------------------------------------
Eval num_timesteps=746000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 746000   |
---------------------------------
Eval num_timesteps=746500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 746500   |
---------------------------------
Eval num_timesteps=747000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=747500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 747500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 365      |
|    time_elapsed    | 22442    |
|    total_timesteps | 747520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=748000, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 748000      |
| train/                  |             |
|    approx_kl            | 0.005997694 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.001       |
|    loss                 | 12.2        |
|    n_updates            | 668         |
|    policy_gradient_loss | 0.00563     |
|    value_loss           | 22          |
-----------------------------------------
Eval num_timesteps=748500, episode_reward=191.26 +/- 26.33
Episode length: 497.08 +/- 110.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 748500   |
---------------------------------
Eval num_timesteps=749000, episode_reward=193.50 +/- 21.76
Episode length: 506.22 +/- 92.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 749000   |
---------------------------------
Eval num_timesteps=749500, episode_reward=191.30 +/- 26.42
Episode length: 497.88 +/- 107.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 749500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 366      |
|    time_elapsed    | 22511    |
|    total_timesteps | 749568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=750000, episode_reward=162.61 +/- 51.59
Episode length: 376.34 +/- 217.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 376          |
|    mean_reward          | 163          |
| time/                   |              |
|    total_timesteps      | 750000       |
| train/                  |              |
|    approx_kl            | 0.0058940365 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.001        |
|    loss                 | 15.7         |
|    n_updates            | 669          |
|    policy_gradient_loss | 0.0119       |
|    value_loss           | 41.1         |
------------------------------------------
Eval num_timesteps=750500, episode_reward=175.85 +/- 44.22
Episode length: 432.68 +/- 184.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 750500   |
---------------------------------
Eval num_timesteps=751000, episode_reward=169.21 +/- 48.49
Episode length: 402.72 +/- 206.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 403      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 751000   |
---------------------------------
Eval num_timesteps=751500, episode_reward=171.46 +/- 47.23
Episode length: 414.52 +/- 196.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 415      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 751500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 367      |
|    time_elapsed    | 22567    |
|    total_timesteps | 751616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=752000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 752000      |
| train/                  |             |
|    approx_kl            | 0.013854523 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.001       |
|    loss                 | 46.1        |
|    n_updates            | 670         |
|    policy_gradient_loss | 0.00214     |
|    value_loss           | 65.3        |
-----------------------------------------
Eval num_timesteps=752500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 752500   |
---------------------------------
Eval num_timesteps=753000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 753000   |
---------------------------------
Eval num_timesteps=753500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 753500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 368      |
|    time_elapsed    | 22638    |
|    total_timesteps | 753664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=754000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 754000       |
| train/                  |              |
|    approx_kl            | 0.0047522117 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.886        |
|    learning_rate        | 0.001        |
|    loss                 | 50.9         |
|    n_updates            | 671          |
|    policy_gradient_loss | 0.00435      |
|    value_loss           | 32.2         |
------------------------------------------
Eval num_timesteps=754500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 754500   |
---------------------------------
Eval num_timesteps=755000, episode_reward=197.92 +/- 0.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 755000   |
---------------------------------
Eval num_timesteps=755500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 755500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 369      |
|    time_elapsed    | 22708    |
|    total_timesteps | 755712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=756000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 756000      |
| train/                  |             |
|    approx_kl            | 0.010474757 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | 22.7        |
|    n_updates            | 672         |
|    policy_gradient_loss | 0.00132     |
|    value_loss           | 22.2        |
-----------------------------------------
Eval num_timesteps=756500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 756500   |
---------------------------------
Eval num_timesteps=757000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 757000   |
---------------------------------
Eval num_timesteps=757500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 757500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 370      |
|    time_elapsed    | 22780    |
|    total_timesteps | 757760   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=758000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 758000       |
| train/                  |              |
|    approx_kl            | 0.0041860514 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.001        |
|    loss                 | 10           |
|    n_updates            | 674          |
|    policy_gradient_loss | -0.000929    |
|    value_loss           | 20.6         |
------------------------------------------
Eval num_timesteps=758500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 758500   |
---------------------------------
Eval num_timesteps=759000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 759000   |
---------------------------------
Eval num_timesteps=759500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 759500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 371      |
|    time_elapsed    | 22852    |
|    total_timesteps | 759808   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=760000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 760000      |
| train/                  |             |
|    approx_kl            | 0.009918744 |
|    clip_fraction        | 0.00835     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 18.2        |
|    n_updates            | 676         |
|    policy_gradient_loss | -0.000954   |
|    value_loss           | 14.7        |
-----------------------------------------
Eval num_timesteps=760500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 760500   |
---------------------------------
Eval num_timesteps=761000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 761000   |
---------------------------------
Eval num_timesteps=761500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 761500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 372      |
|    time_elapsed    | 22923    |
|    total_timesteps | 761856   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=762000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 762000      |
| train/                  |             |
|    approx_kl            | 0.010118476 |
|    clip_fraction        | 0.0151      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 2.38        |
|    n_updates            | 679         |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 9.8         |
-----------------------------------------
Eval num_timesteps=762500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 762500   |
---------------------------------
Eval num_timesteps=763000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 763000   |
---------------------------------
Eval num_timesteps=763500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 763500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 373      |
|    time_elapsed    | 22994    |
|    total_timesteps | 763904   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=764000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 764000      |
| train/                  |             |
|    approx_kl            | 0.005212684 |
|    clip_fraction        | 0.00713     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.001       |
|    loss                 | 16.5        |
|    n_updates            | 681         |
|    policy_gradient_loss | -0.000288   |
|    value_loss           | 26.4        |
-----------------------------------------
Eval num_timesteps=764500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 764500   |
---------------------------------
Eval num_timesteps=765000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 765000   |
---------------------------------
Eval num_timesteps=765500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 765500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 374      |
|    time_elapsed    | 23066    |
|    total_timesteps | 765952   |
---------------------------------
Eval num_timesteps=766000, episode_reward=197.95 +/- 0.34
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 766000       |
| train/                  |              |
|    approx_kl            | 0.0067758877 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 2.94         |
|    n_updates            | 691          |
|    policy_gradient_loss | -4.9e-05     |
|    value_loss           | 9.98         |
------------------------------------------
Eval num_timesteps=766500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 766500   |
---------------------------------
Eval num_timesteps=767000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 767000   |
---------------------------------
Eval num_timesteps=767500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 767500   |
---------------------------------
Eval num_timesteps=768000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 375      |
|    time_elapsed    | 23157    |
|    total_timesteps | 768000   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.04
Eval num_timesteps=768500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 768500     |
| train/                  |            |
|    approx_kl            | 0.00922436 |
|    clip_fraction        | 0.00279    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.001      |
|    loss                 | 8.41       |
|    n_updates            | 695        |
|    policy_gradient_loss | 0.000149   |
|    value_loss           | 21.1       |
----------------------------------------
Eval num_timesteps=769000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 769000   |
---------------------------------
Eval num_timesteps=769500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 769500   |
---------------------------------
Eval num_timesteps=770000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 770000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 376      |
|    time_elapsed    | 23227    |
|    total_timesteps | 770048   |
---------------------------------
Eval num_timesteps=770500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 770500       |
| train/                  |              |
|    approx_kl            | 0.0063717705 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.001        |
|    loss                 | 2.19         |
|    n_updates            | 705          |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 14.8         |
------------------------------------------
Eval num_timesteps=771000, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 771000   |
---------------------------------
Eval num_timesteps=771500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 771500   |
---------------------------------
Eval num_timesteps=772000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 772000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 377      |
|    time_elapsed    | 23300    |
|    total_timesteps | 772096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=772500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 772500       |
| train/                  |              |
|    approx_kl            | 0.0046533193 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 11.7         |
|    n_updates            | 706          |
|    policy_gradient_loss | -0.0036      |
|    value_loss           | 5.44         |
------------------------------------------
Eval num_timesteps=773000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 773000   |
---------------------------------
Eval num_timesteps=773500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 773500   |
---------------------------------
Eval num_timesteps=774000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 774000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 378      |
|    time_elapsed    | 23372    |
|    total_timesteps | 774144   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=774500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 774500      |
| train/                  |             |
|    approx_kl            | 0.012995116 |
|    clip_fraction        | 0.055       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.001       |
|    loss                 | 23.8        |
|    n_updates            | 708         |
|    policy_gradient_loss | -0.000371   |
|    value_loss           | 29.7        |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 775000   |
---------------------------------
Eval num_timesteps=775500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 775500   |
---------------------------------
Eval num_timesteps=776000, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 776000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 379      |
|    time_elapsed    | 23443    |
|    total_timesteps | 776192   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=776500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 776500      |
| train/                  |             |
|    approx_kl            | 0.012068321 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.001       |
|    loss                 | 0.469       |
|    n_updates            | 710         |
|    policy_gradient_loss | 0.00807     |
|    value_loss           | 30.8        |
-----------------------------------------
Eval num_timesteps=777000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 777000   |
---------------------------------
Eval num_timesteps=777500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 777500   |
---------------------------------
Eval num_timesteps=778000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 778000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 380      |
|    time_elapsed    | 23514    |
|    total_timesteps | 778240   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=778500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 778500      |
| train/                  |             |
|    approx_kl            | 0.020504162 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.001       |
|    loss                 | 33.1        |
|    n_updates            | 712         |
|    policy_gradient_loss | 0.000112    |
|    value_loss           | 184         |
-----------------------------------------
Eval num_timesteps=779000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 779000   |
---------------------------------
Eval num_timesteps=779500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 779500   |
---------------------------------
Eval num_timesteps=780000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 780000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 381      |
|    time_elapsed    | 23585    |
|    total_timesteps | 780288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=780500, episode_reward=71.61 +/- 46.80
Episode length: 113.32 +/- 116.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 71.6        |
| time/                   |             |
|    total_timesteps      | 780500      |
| train/                  |             |
|    approx_kl            | 0.003923695 |
|    clip_fraction        | 0.0141      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 4.67        |
|    n_updates            | 713         |
|    policy_gradient_loss | 0.00461     |
|    value_loss           | 10.6        |
-----------------------------------------
Eval num_timesteps=781000, episode_reward=71.42 +/- 41.94
Episode length: 118.52 +/- 107.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 71.4     |
| time/              |          |
|    total_timesteps | 781000   |
---------------------------------
Eval num_timesteps=781500, episode_reward=83.78 +/- 41.12
Episode length: 147.04 +/- 124.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 147      |
|    mean_reward     | 83.8     |
| time/              |          |
|    total_timesteps | 781500   |
---------------------------------
Eval num_timesteps=782000, episode_reward=73.32 +/- 40.30
Episode length: 121.78 +/- 100.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 73.3     |
| time/              |          |
|    total_timesteps | 782000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 473      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 382      |
|    time_elapsed    | 23603    |
|    total_timesteps | 782336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=782500, episode_reward=197.82 +/- 0.50
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 782500      |
| train/                  |             |
|    approx_kl            | 0.015051508 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.477       |
|    learning_rate        | 0.001       |
|    loss                 | 51.3        |
|    n_updates            | 714         |
|    policy_gradient_loss | 0.0118      |
|    value_loss           | 154         |
-----------------------------------------
Eval num_timesteps=783000, episode_reward=195.49 +/- 15.46
Episode length: 517.52 +/- 52.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 783000   |
---------------------------------
Eval num_timesteps=783500, episode_reward=197.89 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 783500   |
---------------------------------
Eval num_timesteps=784000, episode_reward=197.88 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 784000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 470      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 383      |
|    time_elapsed    | 23674    |
|    total_timesteps | 784384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=784500, episode_reward=197.84 +/- 0.61
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 784500       |
| train/                  |              |
|    approx_kl            | 0.0069930237 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.711        |
|    learning_rate        | 0.001        |
|    loss                 | 43.5         |
|    n_updates            | 715          |
|    policy_gradient_loss | 0.00437      |
|    value_loss           | 55.7         |
------------------------------------------
Eval num_timesteps=785000, episode_reward=197.87 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 785000   |
---------------------------------
Eval num_timesteps=785500, episode_reward=197.86 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 785500   |
---------------------------------
Eval num_timesteps=786000, episode_reward=197.94 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 786000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 470      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 384      |
|    time_elapsed    | 23744    |
|    total_timesteps | 786432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.16
Eval num_timesteps=786500, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 786500      |
| train/                  |             |
|    approx_kl            | 0.013213179 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.001       |
|    loss                 | 43.9        |
|    n_updates            | 716         |
|    policy_gradient_loss | 0.00489     |
|    value_loss           | 57.7        |
-----------------------------------------
Eval num_timesteps=787000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 787000   |
---------------------------------
Eval num_timesteps=787500, episode_reward=197.93 +/- 0.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 787500   |
---------------------------------
Eval num_timesteps=788000, episode_reward=197.91 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 788000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 470      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 385      |
|    time_elapsed    | 23815    |
|    total_timesteps | 788480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=788500, episode_reward=81.35 +/- 102.99
Episode length: 238.80 +/- 234.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 239         |
|    mean_reward          | 81.3        |
| time/                   |             |
|    total_timesteps      | 788500      |
| train/                  |             |
|    approx_kl            | 0.009784547 |
|    clip_fraction        | 0.0071      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.001       |
|    loss                 | 21.1        |
|    n_updates            | 717         |
|    policy_gradient_loss | -0.00627    |
|    value_loss           | 20.1        |
-----------------------------------------
Eval num_timesteps=789000, episode_reward=85.86 +/- 106.04
Episode length: 240.20 +/- 233.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 85.9     |
| time/              |          |
|    total_timesteps | 789000   |
---------------------------------
Eval num_timesteps=789500, episode_reward=72.89 +/- 97.44
Episode length: 179.30 +/- 215.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 72.9     |
| time/              |          |
|    total_timesteps | 789500   |
---------------------------------
Eval num_timesteps=790000, episode_reward=96.75 +/- 96.37
Episode length: 247.34 +/- 236.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 96.7     |
| time/              |          |
|    total_timesteps | 790000   |
---------------------------------
Eval num_timesteps=790500, episode_reward=97.29 +/- 99.40
Episode length: 256.28 +/- 238.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 97.3     |
| time/              |          |
|    total_timesteps | 790500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 386      |
|    time_elapsed    | 23856    |
|    total_timesteps | 790528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=791000, episode_reward=34.45 +/- 84.04
Episode length: 134.42 +/- 183.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 34.4        |
| time/                   |             |
|    total_timesteps      | 791000      |
| train/                  |             |
|    approx_kl            | 0.007265675 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.0816      |
|    learning_rate        | 0.001       |
|    loss                 | 307         |
|    n_updates            | 718         |
|    policy_gradient_loss | 0.00378     |
|    value_loss           | 661         |
-----------------------------------------
Eval num_timesteps=791500, episode_reward=16.77 +/- 65.53
Episode length: 89.96 +/- 129.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90       |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 791500   |
---------------------------------
Eval num_timesteps=792000, episode_reward=8.37 +/- 62.88
Episode length: 88.06 +/- 130.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.1     |
|    mean_reward     | 8.37     |
| time/              |          |
|    total_timesteps | 792000   |
---------------------------------
Eval num_timesteps=792500, episode_reward=26.13 +/- 72.64
Episode length: 105.98 +/- 155.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 26.1     |
| time/              |          |
|    total_timesteps | 792500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 424      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 387      |
|    time_elapsed    | 23872    |
|    total_timesteps | 792576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=793000, episode_reward=64.78 +/- 97.01
Episode length: 203.60 +/- 221.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 204         |
|    mean_reward          | 64.8        |
| time/                   |             |
|    total_timesteps      | 793000      |
| train/                  |             |
|    approx_kl            | 0.013687649 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.863      |
|    explained_variance   | -0.55       |
|    learning_rate        | 0.001       |
|    loss                 | 482         |
|    n_updates            | 719         |
|    policy_gradient_loss | 0.00063     |
|    value_loss           | 1.26e+03    |
-----------------------------------------
Eval num_timesteps=793500, episode_reward=88.27 +/- 100.64
Episode length: 235.90 +/- 229.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | 88.3     |
| time/              |          |
|    total_timesteps | 793500   |
---------------------------------
Eval num_timesteps=794000, episode_reward=93.45 +/- 88.24
Episode length: 231.72 +/- 229.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 93.5     |
| time/              |          |
|    total_timesteps | 794000   |
---------------------------------
Eval num_timesteps=794500, episode_reward=57.16 +/- 86.12
Episode length: 178.66 +/- 208.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 57.2     |
| time/              |          |
|    total_timesteps | 794500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 388      |
|    time_elapsed    | 23902    |
|    total_timesteps | 794624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=795000, episode_reward=105.12 +/- 56.82
Episode length: 263.08 +/- 176.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 263         |
|    mean_reward          | 105         |
| time/                   |             |
|    total_timesteps      | 795000      |
| train/                  |             |
|    approx_kl            | 0.022619393 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -0.217      |
|    learning_rate        | 0.001       |
|    loss                 | 234         |
|    n_updates            | 720         |
|    policy_gradient_loss | 0.0225      |
|    value_loss           | 349         |
-----------------------------------------
Eval num_timesteps=795500, episode_reward=106.56 +/- 53.27
Episode length: 267.30 +/- 175.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 795500   |
---------------------------------
Eval num_timesteps=796000, episode_reward=105.70 +/- 60.46
Episode length: 265.20 +/- 180.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 106      |
| time/              |          |
|    total_timesteps | 796000   |
---------------------------------
Eval num_timesteps=796500, episode_reward=120.97 +/- 69.25
Episode length: 294.72 +/- 174.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 121      |
| time/              |          |
|    total_timesteps | 796500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 399      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 389      |
|    time_elapsed    | 23940    |
|    total_timesteps | 796672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=797000, episode_reward=76.50 +/- 26.26
Episode length: 167.00 +/- 99.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 167         |
|    mean_reward          | 76.5        |
| time/                   |             |
|    total_timesteps      | 797000      |
| train/                  |             |
|    approx_kl            | 0.006186107 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.13        |
|    learning_rate        | 0.001       |
|    loss                 | 93.5        |
|    n_updates            | 721         |
|    policy_gradient_loss | 0.00341     |
|    value_loss           | 269         |
-----------------------------------------
Eval num_timesteps=797500, episode_reward=75.28 +/- 21.34
Episode length: 152.88 +/- 61.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 153      |
|    mean_reward     | 75.3     |
| time/              |          |
|    total_timesteps | 797500   |
---------------------------------
Eval num_timesteps=798000, episode_reward=69.80 +/- 20.41
Episode length: 164.94 +/- 81.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 69.8     |
| time/              |          |
|    total_timesteps | 798000   |
---------------------------------
Eval num_timesteps=798500, episode_reward=78.06 +/- 35.86
Episode length: 202.52 +/- 103.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | 78.1     |
| time/              |          |
|    total_timesteps | 798500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 384      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 390      |
|    time_elapsed    | 23965    |
|    total_timesteps | 798720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=799000, episode_reward=73.82 +/- 38.60
Episode length: 163.04 +/- 92.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 163        |
|    mean_reward          | 73.8       |
| time/                   |            |
|    total_timesteps      | 799000     |
| train/                  |            |
|    approx_kl            | 0.01624407 |
|    clip_fraction        | 0.0938     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.277      |
|    learning_rate        | 0.001      |
|    loss                 | 130        |
|    n_updates            | 722        |
|    policy_gradient_loss | 0.0182     |
|    value_loss           | 199        |
----------------------------------------
Eval num_timesteps=799500, episode_reward=79.54 +/- 23.62
Episode length: 185.70 +/- 77.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 79.5     |
| time/              |          |
|    total_timesteps | 799500   |
---------------------------------
Eval num_timesteps=800000, episode_reward=75.46 +/- 29.73
Episode length: 188.26 +/- 85.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 188      |
|    mean_reward     | 75.5     |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=800500, episode_reward=69.96 +/- 36.59
Episode length: 184.66 +/- 78.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 70       |
| time/              |          |
|    total_timesteps | 800500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 376      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 391      |
|    time_elapsed    | 23991    |
|    total_timesteps | 800768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=801000, episode_reward=69.76 +/- 35.96
Episode length: 164.40 +/- 72.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 164          |
|    mean_reward          | 69.8         |
| time/                   |              |
|    total_timesteps      | 801000       |
| train/                  |              |
|    approx_kl            | 0.0112783825 |
|    clip_fraction        | 0.0527       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.353        |
|    learning_rate        | 0.001        |
|    loss                 | 73.1         |
|    n_updates            | 723          |
|    policy_gradient_loss | 0.00527      |
|    value_loss           | 88.9         |
------------------------------------------
Eval num_timesteps=801500, episode_reward=56.14 +/- 45.36
Episode length: 141.16 +/- 88.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 141      |
|    mean_reward     | 56.1     |
| time/              |          |
|    total_timesteps | 801500   |
---------------------------------
Eval num_timesteps=802000, episode_reward=55.74 +/- 45.01
Episode length: 148.44 +/- 87.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 148      |
|    mean_reward     | 55.7     |
| time/              |          |
|    total_timesteps | 802000   |
---------------------------------
Eval num_timesteps=802500, episode_reward=59.74 +/- 43.07
Episode length: 135.86 +/- 71.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 136      |
|    mean_reward     | 59.7     |
| time/              |          |
|    total_timesteps | 802500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 363      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 392      |
|    time_elapsed    | 24012    |
|    total_timesteps | 802816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=803000, episode_reward=86.22 +/- 0.43
Episode length: 208.52 +/- 85.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 86.2        |
| time/                   |             |
|    total_timesteps      | 803000      |
| train/                  |             |
|    approx_kl            | 0.009530107 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.001       |
|    loss                 | 81.1        |
|    n_updates            | 724         |
|    policy_gradient_loss | 0.00779     |
|    value_loss           | 120         |
-----------------------------------------
Eval num_timesteps=803500, episode_reward=86.24 +/- 0.37
Episode length: 186.84 +/- 69.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | 86.2     |
| time/              |          |
|    total_timesteps | 803500   |
---------------------------------
Eval num_timesteps=804000, episode_reward=86.16 +/- 0.43
Episode length: 191.80 +/- 63.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 86.2     |
| time/              |          |
|    total_timesteps | 804000   |
---------------------------------
Eval num_timesteps=804500, episode_reward=86.30 +/- 0.30
Episode length: 186.38 +/- 60.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 86.3     |
| time/              |          |
|    total_timesteps | 804500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 314      |
|    ep_rew_mean     | 120      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 393      |
|    time_elapsed    | 24039    |
|    total_timesteps | 804864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=805000, episode_reward=184.50 +/- 26.65
Episode length: 500.22 +/- 88.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 500        |
|    mean_reward          | 184        |
| time/                   |            |
|    total_timesteps      | 805000     |
| train/                  |            |
|    approx_kl            | 0.01262068 |
|    clip_fraction        | 0.082      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.42       |
|    learning_rate        | 0.001      |
|    loss                 | 79.9       |
|    n_updates            | 725        |
|    policy_gradient_loss | -0.0126    |
|    value_loss           | 142        |
----------------------------------------
Eval num_timesteps=805500, episode_reward=183.93 +/- 27.50
Episode length: 496.26 +/- 97.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 805500   |
---------------------------------
Eval num_timesteps=806000, episode_reward=175.81 +/- 43.55
Episode length: 458.86 +/- 143.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 459      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 806000   |
---------------------------------
Eval num_timesteps=806500, episode_reward=183.42 +/- 36.22
Episode length: 475.86 +/- 123.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 806500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 394      |
|    time_elapsed    | 24105    |
|    total_timesteps | 806912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=807000, episode_reward=182.69 +/- 53.45
Episode length: 464.60 +/- 134.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 465         |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 807000      |
| train/                  |             |
|    approx_kl            | 0.011448652 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.001       |
|    loss                 | 79.3        |
|    n_updates            | 726         |
|    policy_gradient_loss | 0.0105      |
|    value_loss           | 129         |
-----------------------------------------
Eval num_timesteps=807500, episode_reward=164.51 +/- 50.24
Episode length: 451.82 +/- 143.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 452      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 807500   |
---------------------------------
Eval num_timesteps=808000, episode_reward=164.60 +/- 47.60
Episode length: 450.98 +/- 142.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 451      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 808000   |
---------------------------------
Eval num_timesteps=808500, episode_reward=157.16 +/- 58.76
Episode length: 433.24 +/- 153.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 433      |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 808500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 395      |
|    time_elapsed    | 24166    |
|    total_timesteps | 808960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=809000, episode_reward=183.79 +/- 29.20
Episode length: 500.82 +/- 83.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 501         |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 809000      |
| train/                  |             |
|    approx_kl            | 0.009127126 |
|    clip_fraction        | 0.0693      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.001       |
|    loss                 | 44.9        |
|    n_updates            | 727         |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 115         |
-----------------------------------------
Eval num_timesteps=809500, episode_reward=191.88 +/- 20.08
Episode length: 517.92 +/- 49.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 809500   |
---------------------------------
Eval num_timesteps=810000, episode_reward=187.84 +/- 21.13
Episode length: 510.66 +/- 72.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 511      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 810000   |
---------------------------------
Eval num_timesteps=810500, episode_reward=189.61 +/- 25.71
Episode length: 512.34 +/- 62.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 512      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 810500   |
---------------------------------
Eval num_timesteps=811000, episode_reward=190.22 +/- 32.99
Episode length: 505.08 +/- 80.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 811000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 396      |
|    time_elapsed    | 24252    |
|    total_timesteps | 811008   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=811500, episode_reward=191.22 +/- 25.97
Episode length: 508.50 +/- 82.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 508        |
|    mean_reward          | 191        |
| time/                   |            |
|    total_timesteps      | 811500     |
| train/                  |            |
|    approx_kl            | 0.00609814 |
|    clip_fraction        | 0.0352     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.001      |
|    loss                 | 55.1       |
|    n_updates            | 728        |
|    policy_gradient_loss | 0.0105     |
|    value_loss           | 87.3       |
----------------------------------------
Eval num_timesteps=812000, episode_reward=190.45 +/- 15.05
Episode length: 517.78 +/- 50.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 812000   |
---------------------------------
Eval num_timesteps=812500, episode_reward=192.76 +/- 3.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 812500   |
---------------------------------
Eval num_timesteps=813000, episode_reward=193.56 +/- 2.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 813000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 113      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 397      |
|    time_elapsed    | 24324    |
|    total_timesteps | 813056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=813500, episode_reward=177.38 +/- 52.12
Episode length: 466.28 +/- 159.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 466         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 813500      |
| train/                  |             |
|    approx_kl            | 0.013325031 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.001       |
|    loss                 | 15.9        |
|    n_updates            | 729         |
|    policy_gradient_loss | 0.00606     |
|    value_loss           | 80.2        |
-----------------------------------------
Eval num_timesteps=814000, episode_reward=169.40 +/- 53.78
Episode length: 438.16 +/- 185.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 814000   |
---------------------------------
Eval num_timesteps=814500, episode_reward=160.77 +/- 68.63
Episode length: 428.98 +/- 192.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 814500   |
---------------------------------
Eval num_timesteps=815000, episode_reward=168.92 +/- 61.97
Episode length: 440.86 +/- 179.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 441      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 815000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 329      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 398      |
|    time_elapsed    | 24386    |
|    total_timesteps | 815104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=815500, episode_reward=163.51 +/- 61.06
Episode length: 421.94 +/- 194.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 422         |
|    mean_reward          | 164         |
| time/                   |             |
|    total_timesteps      | 815500      |
| train/                  |             |
|    approx_kl            | 0.007576147 |
|    clip_fraction        | 0.0977      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.001       |
|    loss                 | 27          |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.000389   |
|    value_loss           | 80.2        |
-----------------------------------------
Eval num_timesteps=816000, episode_reward=157.65 +/- 72.39
Episode length: 421.04 +/- 196.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 816000   |
---------------------------------
Eval num_timesteps=816500, episode_reward=173.99 +/- 58.52
Episode length: 457.82 +/- 166.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 458      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 816500   |
---------------------------------
Eval num_timesteps=817000, episode_reward=136.40 +/- 89.39
Episode length: 350.38 +/- 232.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 350      |
|    mean_reward     | 136      |
| time/              |          |
|    total_timesteps | 817000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 399      |
|    time_elapsed    | 24441    |
|    total_timesteps | 817152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=817500, episode_reward=161.92 +/- 70.45
Episode length: 417.42 +/- 202.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 417         |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 817500      |
| train/                  |             |
|    approx_kl            | 0.010622669 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.982      |
|    explained_variance   | 0.387       |
|    learning_rate        | 0.001       |
|    loss                 | 90.3        |
|    n_updates            | 731         |
|    policy_gradient_loss | 0.00407     |
|    value_loss           | 164         |
-----------------------------------------
Eval num_timesteps=818000, episode_reward=138.83 +/- 80.00
Episode length: 352.18 +/- 230.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 352      |
|    mean_reward     | 139      |
| time/              |          |
|    total_timesteps | 818000   |
---------------------------------
Eval num_timesteps=818500, episode_reward=167.83 +/- 55.53
Episode length: 430.76 +/- 188.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 818500   |
---------------------------------
Eval num_timesteps=819000, episode_reward=140.32 +/- 77.20
Episode length: 350.84 +/- 232.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | 140      |
| time/              |          |
|    total_timesteps | 819000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 289      |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 400      |
|    time_elapsed    | 24494    |
|    total_timesteps | 819200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=819500, episode_reward=119.42 +/- 84.37
Episode length: 295.04 +/- 239.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 295        |
|    mean_reward          | 119        |
| time/                   |            |
|    total_timesteps      | 819500     |
| train/                  |            |
|    approx_kl            | 0.03151972 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.869     |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.001      |
|    loss                 | 80.2       |
|    n_updates            | 732        |
|    policy_gradient_loss | 0.0245     |
|    value_loss           | 203        |
----------------------------------------
Eval num_timesteps=820000, episode_reward=119.30 +/- 81.79
Episode length: 283.96 +/- 241.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | 119      |
| time/              |          |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=820500, episode_reward=119.49 +/- 87.14
Episode length: 305.56 +/- 238.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | 119      |
| time/              |          |
|    total_timesteps | 820500   |
---------------------------------
Eval num_timesteps=821000, episode_reward=123.79 +/- 80.50
Episode length: 301.00 +/- 233.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 301      |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 821000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 401      |
|    time_elapsed    | 24535    |
|    total_timesteps | 821248   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=821500, episode_reward=110.99 +/- 79.74
Episode length: 250.86 +/- 233.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | 111         |
| time/                   |             |
|    total_timesteps      | 821500      |
| train/                  |             |
|    approx_kl            | 0.010152455 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.55        |
|    learning_rate        | 0.001       |
|    loss                 | 70.8        |
|    n_updates            | 733         |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 108         |
-----------------------------------------
Eval num_timesteps=822000, episode_reward=134.59 +/- 76.69
Episode length: 324.52 +/- 235.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 325      |
|    mean_reward     | 135      |
| time/              |          |
|    total_timesteps | 822000   |
---------------------------------
Eval num_timesteps=822500, episode_reward=113.30 +/- 80.47
Episode length: 261.70 +/- 233.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 113      |
| time/              |          |
|    total_timesteps | 822500   |
---------------------------------
Eval num_timesteps=823000, episode_reward=131.99 +/- 70.58
Episode length: 300.64 +/- 233.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 301      |
|    mean_reward     | 132      |
| time/              |          |
|    total_timesteps | 823000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 287      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 402      |
|    time_elapsed    | 24574    |
|    total_timesteps | 823296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=823500, episode_reward=138.74 +/- 77.15
Episode length: 343.90 +/- 231.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 344         |
|    mean_reward          | 139         |
| time/                   |             |
|    total_timesteps      | 823500      |
| train/                  |             |
|    approx_kl            | 0.010854518 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.772      |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.001       |
|    loss                 | 61.7        |
|    n_updates            | 734         |
|    policy_gradient_loss | 0.00603     |
|    value_loss           | 120         |
-----------------------------------------
Eval num_timesteps=824000, episode_reward=136.75 +/- 73.59
Episode length: 314.56 +/- 237.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 315      |
|    mean_reward     | 137      |
| time/              |          |
|    total_timesteps | 824000   |
---------------------------------
Eval num_timesteps=824500, episode_reward=125.65 +/- 69.60
Episode length: 268.98 +/- 237.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 126      |
| time/              |          |
|    total_timesteps | 824500   |
---------------------------------
Eval num_timesteps=825000, episode_reward=132.77 +/- 82.27
Episode length: 335.56 +/- 232.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 336      |
|    mean_reward     | 133      |
| time/              |          |
|    total_timesteps | 825000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 281      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 403      |
|    time_elapsed    | 24618    |
|    total_timesteps | 825344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=825500, episode_reward=151.63 +/- 67.24
Episode length: 369.02 +/- 227.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 369         |
|    mean_reward          | 152         |
| time/                   |             |
|    total_timesteps      | 825500      |
| train/                  |             |
|    approx_kl            | 0.009586146 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.001       |
|    loss                 | 39.1        |
|    n_updates            | 735         |
|    policy_gradient_loss | -0.00623    |
|    value_loss           | 110         |
-----------------------------------------
Eval num_timesteps=826000, episode_reward=162.01 +/- 57.75
Episode length: 398.40 +/- 213.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 398      |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 826000   |
---------------------------------
Eval num_timesteps=826500, episode_reward=157.94 +/- 68.98
Episode length: 387.04 +/- 221.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 387      |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 826500   |
---------------------------------
Eval num_timesteps=827000, episode_reward=158.26 +/- 65.65
Episode length: 397.44 +/- 215.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 397      |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 827000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 272      |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 404      |
|    time_elapsed    | 24671    |
|    total_timesteps | 827392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=827500, episode_reward=169.39 +/- 55.09
Episode length: 430.74 +/- 189.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 431        |
|    mean_reward          | 169        |
| time/                   |            |
|    total_timesteps      | 827500     |
| train/                  |            |
|    approx_kl            | 0.00752083 |
|    clip_fraction        | 0.0625     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.468      |
|    learning_rate        | 0.001      |
|    loss                 | 68.5       |
|    n_updates            | 736        |
|    policy_gradient_loss | -0.000718  |
|    value_loss           | 121        |
----------------------------------------
Eval num_timesteps=828000, episode_reward=175.49 +/- 46.56
Episode length: 445.94 +/- 181.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 446      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 828000   |
---------------------------------
Eval num_timesteps=828500, episode_reward=177.73 +/- 44.80
Episode length: 457.40 +/- 167.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 828500   |
---------------------------------
Eval num_timesteps=829000, episode_reward=171.40 +/- 49.93
Episode length: 429.00 +/- 192.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 829000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 405      |
|    time_elapsed    | 24731    |
|    total_timesteps | 829440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=829500, episode_reward=178.07 +/- 49.60
Episode length: 466.14 +/- 159.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 178          |
| time/                   |              |
|    total_timesteps      | 829500       |
| train/                  |              |
|    approx_kl            | 0.0061752596 |
|    clip_fraction        | 0.0651       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.491        |
|    learning_rate        | 0.001        |
|    loss                 | 45.9         |
|    n_updates            | 737          |
|    policy_gradient_loss | -0.00921     |
|    value_loss           | 103          |
------------------------------------------
Eval num_timesteps=830000, episode_reward=175.18 +/- 42.83
Episode length: 437.52 +/- 186.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 830000   |
---------------------------------
Eval num_timesteps=830500, episode_reward=175.41 +/- 54.89
Episode length: 446.52 +/- 179.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 447      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 830500   |
---------------------------------
Eval num_timesteps=831000, episode_reward=184.22 +/- 32.80
Episode length: 475.76 +/- 147.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 831000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 282      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 406      |
|    time_elapsed    | 24793    |
|    total_timesteps | 831488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=831500, episode_reward=196.11 +/- 1.51
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 831500      |
| train/                  |             |
|    approx_kl            | 0.009403171 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.001       |
|    loss                 | 63.5        |
|    n_updates            | 738         |
|    policy_gradient_loss | -0.00434    |
|    value_loss           | 156         |
-----------------------------------------
Eval num_timesteps=832000, episode_reward=196.13 +/- 1.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 832000   |
---------------------------------
Eval num_timesteps=832500, episode_reward=195.53 +/- 29.84
Episode length: 505.80 +/- 94.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 832500   |
---------------------------------
Eval num_timesteps=833000, episode_reward=194.21 +/- 15.73
Episode length: 515.16 +/- 68.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 833000   |
---------------------------------
Eval num_timesteps=833500, episode_reward=195.99 +/- 1.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 833500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 113      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 407      |
|    time_elapsed    | 24880    |
|    total_timesteps | 833536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=834000, episode_reward=196.80 +/- 0.76
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 834000      |
| train/                  |             |
|    approx_kl            | 0.013024581 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.001       |
|    loss                 | 27.2        |
|    n_updates            | 739         |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 66.3        |
-----------------------------------------
Eval num_timesteps=834500, episode_reward=196.63 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 834500   |
---------------------------------
Eval num_timesteps=835000, episode_reward=196.83 +/- 0.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 835000   |
---------------------------------
Eval num_timesteps=835500, episode_reward=196.65 +/- 0.97
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 835500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 408      |
|    time_elapsed    | 24950    |
|    total_timesteps | 835584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=836000, episode_reward=197.78 +/- 0.50
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 836000     |
| train/                  |            |
|    approx_kl            | 0.00522373 |
|    clip_fraction        | 0.0547     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.001      |
|    loss                 | 2.66       |
|    n_updates            | 740        |
|    policy_gradient_loss | 0.00492    |
|    value_loss           | 54.9       |
----------------------------------------
Eval num_timesteps=836500, episode_reward=197.88 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 836500   |
---------------------------------
Eval num_timesteps=837000, episode_reward=197.87 +/- 0.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 837000   |
---------------------------------
Eval num_timesteps=837500, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 837500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 321      |
|    ep_rew_mean     | 121      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 409      |
|    time_elapsed    | 25020    |
|    total_timesteps | 837632   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=838000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 838000      |
| train/                  |             |
|    approx_kl            | 0.009134337 |
|    clip_fraction        | 0.0775      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.001       |
|    loss                 | 40.8        |
|    n_updates            | 741         |
|    policy_gradient_loss | 0.00442     |
|    value_loss           | 41.8        |
-----------------------------------------
Eval num_timesteps=838500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 838500   |
---------------------------------
Eval num_timesteps=839000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 839000   |
---------------------------------
Eval num_timesteps=839500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 839500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 330      |
|    ep_rew_mean     | 124      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 410      |
|    time_elapsed    | 25091    |
|    total_timesteps | 839680   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=840000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.005096048 |
|    clip_fraction        | 0.0568      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 4.84        |
|    n_updates            | 742         |
|    policy_gradient_loss | -0.000507   |
|    value_loss           | 18.7        |
-----------------------------------------
Eval num_timesteps=840500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 840500   |
---------------------------------
Eval num_timesteps=841000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 841000   |
---------------------------------
Eval num_timesteps=841500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 841500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 337      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 411      |
|    time_elapsed    | 25161    |
|    total_timesteps | 841728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=842000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 842000      |
| train/                  |             |
|    approx_kl            | 0.007306632 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.001       |
|    loss                 | 24.5        |
|    n_updates            | 743         |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 28.2        |
-----------------------------------------
Eval num_timesteps=842500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 842500   |
---------------------------------
Eval num_timesteps=843000, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 843000   |
---------------------------------
Eval num_timesteps=843500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 843500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 344      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 412      |
|    time_elapsed    | 25232    |
|    total_timesteps | 843776   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=844000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 844000      |
| train/                  |             |
|    approx_kl            | 0.009829698 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 24.8        |
|    n_updates            | 744         |
|    policy_gradient_loss | -2.17e-05   |
|    value_loss           | 55.4        |
-----------------------------------------
Eval num_timesteps=844500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 844500   |
---------------------------------
Eval num_timesteps=845000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 845000   |
---------------------------------
Eval num_timesteps=845500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 845500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 344      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 413      |
|    time_elapsed    | 25303    |
|    total_timesteps | 845824   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=846000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 846000      |
| train/                  |             |
|    approx_kl            | 0.013075786 |
|    clip_fraction        | 0.0576      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.001       |
|    loss                 | 27.7        |
|    n_updates            | 747         |
|    policy_gradient_loss | 0.000996    |
|    value_loss           | 32.5        |
-----------------------------------------
Eval num_timesteps=846500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 846500   |
---------------------------------
Eval num_timesteps=847000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 847000   |
---------------------------------
Eval num_timesteps=847500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 847500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 344      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 414      |
|    time_elapsed    | 25374    |
|    total_timesteps | 847872   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=848000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 848000      |
| train/                  |             |
|    approx_kl            | 0.016886089 |
|    clip_fraction        | 0.0358      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 13.9        |
|    n_updates            | 749         |
|    policy_gradient_loss | -0.000633   |
|    value_loss           | 11.8        |
-----------------------------------------
Eval num_timesteps=848500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 848500   |
---------------------------------
Eval num_timesteps=849000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 849000   |
---------------------------------
Eval num_timesteps=849500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 849500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 344      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 415      |
|    time_elapsed    | 25444    |
|    total_timesteps | 849920   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=850000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.007404128 |
|    clip_fraction        | 0.0202      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 0.00697     |
|    n_updates            | 751         |
|    policy_gradient_loss | -0.000403   |
|    value_loss           | 18.5        |
-----------------------------------------
Eval num_timesteps=850500, episode_reward=197.91 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 850500   |
---------------------------------
Eval num_timesteps=851000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 851000   |
---------------------------------
Eval num_timesteps=851500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 851500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 416      |
|    time_elapsed    | 25516    |
|    total_timesteps | 851968   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=852000, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 852000      |
| train/                  |             |
|    approx_kl            | 0.008943108 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.001       |
|    loss                 | 10.7        |
|    n_updates            | 757         |
|    policy_gradient_loss | 3.45e-05    |
|    value_loss           | 22.2        |
-----------------------------------------
Eval num_timesteps=852500, episode_reward=197.90 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 852500   |
---------------------------------
Eval num_timesteps=853000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 853000   |
---------------------------------
Eval num_timesteps=853500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 853500   |
---------------------------------
Eval num_timesteps=854000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 368      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 417      |
|    time_elapsed    | 25605    |
|    total_timesteps | 854016   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=854500, episode_reward=197.96 +/- 0.22
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 854500     |
| train/                  |            |
|    approx_kl            | 0.01318548 |
|    clip_fraction        | 0.0359     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.001      |
|    loss                 | 2.25       |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.00136   |
|    value_loss           | 13.6       |
----------------------------------------
Eval num_timesteps=855000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 855000   |
---------------------------------
Eval num_timesteps=855500, episode_reward=197.93 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 855500   |
---------------------------------
Eval num_timesteps=856000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 856000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 372      |
|    ep_rew_mean     | 138      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 418      |
|    time_elapsed    | 25677    |
|    total_timesteps | 856064   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=856500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 856500       |
| train/                  |              |
|    approx_kl            | 0.0140359625 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.939        |
|    learning_rate        | 0.001        |
|    loss                 | 0.49         |
|    n_updates            | 762          |
|    policy_gradient_loss | -0.00552     |
|    value_loss           | 17.7         |
------------------------------------------
Eval num_timesteps=857000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 857000   |
---------------------------------
Eval num_timesteps=857500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 857500   |
---------------------------------
Eval num_timesteps=858000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 858000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 392      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 419      |
|    time_elapsed    | 25748    |
|    total_timesteps | 858112   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=858500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 858500      |
| train/                  |             |
|    approx_kl            | 0.013410292 |
|    clip_fraction        | 0.0395      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 6.06        |
|    n_updates            | 765         |
|    policy_gradient_loss | -0.00121    |
|    value_loss           | 11.8        |
-----------------------------------------
Eval num_timesteps=859000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 859000   |
---------------------------------
Eval num_timesteps=859500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 859500   |
---------------------------------
Eval num_timesteps=860000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 401      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 420      |
|    time_elapsed    | 25819    |
|    total_timesteps | 860160   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.21
Eval num_timesteps=860500, episode_reward=196.53 +/- 0.87
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 860500      |
| train/                  |             |
|    approx_kl            | 0.016002906 |
|    clip_fraction        | 0.00713     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 7.95        |
|    n_updates            | 768         |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 8.58        |
-----------------------------------------
Eval num_timesteps=861000, episode_reward=198.56 +/- 14.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 861000   |
---------------------------------
New best mean reward!
Eval num_timesteps=861500, episode_reward=196.49 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 861500   |
---------------------------------
Eval num_timesteps=862000, episode_reward=196.53 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 862000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 411      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 421      |
|    time_elapsed    | 25891    |
|    total_timesteps | 862208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=862500, episode_reward=193.35 +/- 2.33
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 862500      |
| train/                  |             |
|    approx_kl            | 0.018119697 |
|    clip_fraction        | 0.0067      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.001       |
|    loss                 | 46.7        |
|    n_updates            | 769         |
|    policy_gradient_loss | 0.00966     |
|    value_loss           | 82.6        |
-----------------------------------------
Eval num_timesteps=863000, episode_reward=193.64 +/- 1.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 863000   |
---------------------------------
Eval num_timesteps=863500, episode_reward=193.25 +/- 1.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 863500   |
---------------------------------
Eval num_timesteps=864000, episode_reward=192.87 +/- 2.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 864000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 426      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 422      |
|    time_elapsed    | 25962    |
|    total_timesteps | 864256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=864500, episode_reward=157.28 +/- 53.65
Episode length: 455.44 +/- 127.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 455          |
|    mean_reward          | 157          |
| time/                   |              |
|    total_timesteps      | 864500       |
| train/                  |              |
|    approx_kl            | 0.0061765467 |
|    clip_fraction        | 0.00868      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.635        |
|    learning_rate        | 0.001        |
|    loss                 | 77.6         |
|    n_updates            | 770          |
|    policy_gradient_loss | 0.0119       |
|    value_loss           | 95.1         |
------------------------------------------
Eval num_timesteps=865000, episode_reward=162.39 +/- 50.41
Episode length: 462.58 +/- 126.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 463      |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 865000   |
---------------------------------
Eval num_timesteps=865500, episode_reward=177.75 +/- 40.81
Episode length: 497.78 +/- 82.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 865500   |
---------------------------------
Eval num_timesteps=866000, episode_reward=174.68 +/- 44.40
Episode length: 491.24 +/- 105.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 491      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 866000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 433      |
|    ep_rew_mean     | 155      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 423      |
|    time_elapsed    | 26027    |
|    total_timesteps | 866304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=866500, episode_reward=187.70 +/- 6.10
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 866500      |
| train/                  |             |
|    approx_kl            | 0.009121146 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.949      |
|    explained_variance   | 0.506       |
|    learning_rate        | 0.001       |
|    loss                 | 25.4        |
|    n_updates            | 771         |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 208         |
-----------------------------------------
Eval num_timesteps=867000, episode_reward=184.89 +/- 25.42
Episode length: 516.28 +/- 48.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 867000   |
---------------------------------
Eval num_timesteps=867500, episode_reward=170.39 +/- 43.93
Episode length: 490.12 +/- 90.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 867500   |
---------------------------------
Eval num_timesteps=868000, episode_reward=170.46 +/- 44.05
Episode length: 492.38 +/- 83.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 492      |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 868000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 447      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 424      |
|    time_elapsed    | 26095    |
|    total_timesteps | 868352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=868500, episode_reward=194.92 +/- 1.45
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 195          |
| time/                   |              |
|    total_timesteps      | 868500       |
| train/                  |              |
|    approx_kl            | 0.0055937236 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.602        |
|    learning_rate        | 0.001        |
|    loss                 | 73           |
|    n_updates            | 772          |
|    policy_gradient_loss | 0.00454      |
|    value_loss           | 120          |
------------------------------------------
Eval num_timesteps=869000, episode_reward=195.27 +/- 1.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 869000   |
---------------------------------
Eval num_timesteps=869500, episode_reward=195.12 +/- 1.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 869500   |
---------------------------------
Eval num_timesteps=870000, episode_reward=195.21 +/- 1.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 870000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 425      |
|    time_elapsed    | 26166    |
|    total_timesteps | 870400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=870500, episode_reward=196.22 +/- 1.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 870500      |
| train/                  |             |
|    approx_kl            | 0.007247375 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.001       |
|    loss                 | 30          |
|    n_updates            | 773         |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 77.7        |
-----------------------------------------
Eval num_timesteps=871000, episode_reward=196.65 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 871000   |
---------------------------------
Eval num_timesteps=871500, episode_reward=196.42 +/- 0.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 871500   |
---------------------------------
Eval num_timesteps=872000, episode_reward=196.36 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 872000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 426      |
|    time_elapsed    | 26237    |
|    total_timesteps | 872448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=872500, episode_reward=197.03 +/- 0.72
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 872500      |
| train/                  |             |
|    approx_kl            | 0.004234306 |
|    clip_fraction        | 0.00488     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.001       |
|    loss                 | 23          |
|    n_updates            | 774         |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 59.6        |
-----------------------------------------
Eval num_timesteps=873000, episode_reward=196.92 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 873000   |
---------------------------------
Eval num_timesteps=873500, episode_reward=196.85 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 873500   |
---------------------------------
Eval num_timesteps=874000, episode_reward=197.10 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 874000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 481      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 427      |
|    time_elapsed    | 26308    |
|    total_timesteps | 874496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=874500, episode_reward=197.69 +/- 0.48
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 874500       |
| train/                  |              |
|    approx_kl            | 0.0020143264 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.789        |
|    learning_rate        | 0.001        |
|    loss                 | 35.1         |
|    n_updates            | 775          |
|    policy_gradient_loss | 0.0023       |
|    value_loss           | 42.3         |
------------------------------------------
Eval num_timesteps=875000, episode_reward=197.64 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 875000   |
---------------------------------
Eval num_timesteps=875500, episode_reward=197.63 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 875500   |
---------------------------------
Eval num_timesteps=876000, episode_reward=197.74 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 876000   |
---------------------------------
Eval num_timesteps=876500, episode_reward=199.68 +/- 13.92
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 876500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 428      |
|    time_elapsed    | 26395    |
|    total_timesteps | 876544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=877000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 877000       |
| train/                  |              |
|    approx_kl            | 0.0026788237 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.766        |
|    learning_rate        | 0.001        |
|    loss                 | 16.7         |
|    n_updates            | 776          |
|    policy_gradient_loss | 0.0052       |
|    value_loss           | 41.8         |
------------------------------------------
Eval num_timesteps=877500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 877500   |
---------------------------------
Eval num_timesteps=878000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 878000   |
---------------------------------
Eval num_timesteps=878500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 878500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 429      |
|    time_elapsed    | 26467    |
|    total_timesteps | 878592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=879000, episode_reward=190.99 +/- 26.18
Episode length: 504.36 +/- 84.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 879000       |
| train/                  |              |
|    approx_kl            | 0.0024766766 |
|    clip_fraction        | 0.00497      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.001        |
|    loss                 | 2.17         |
|    n_updates            | 777          |
|    policy_gradient_loss | -0.000499    |
|    value_loss           | 28.8         |
------------------------------------------
Eval num_timesteps=879500, episode_reward=192.99 +/- 21.57
Episode length: 514.48 +/- 52.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 879500   |
---------------------------------
Eval num_timesteps=880000, episode_reward=190.98 +/- 26.17
Episode length: 502.22 +/- 90.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 502      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=880500, episode_reward=197.60 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 880500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 430      |
|    time_elapsed    | 26537    |
|    total_timesteps | 880640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=881000, episode_reward=177.86 +/- 42.75
Episode length: 465.86 +/- 129.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 466          |
|    mean_reward          | 178          |
| time/                   |              |
|    total_timesteps      | 881000       |
| train/                  |              |
|    approx_kl            | 0.0032800166 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.001        |
|    loss                 | 13.6         |
|    n_updates            | 778          |
|    policy_gradient_loss | 0.000944     |
|    value_loss           | 18.2         |
------------------------------------------
Eval num_timesteps=881500, episode_reward=184.58 +/- 36.01
Episode length: 483.86 +/- 112.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 484      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 881500   |
---------------------------------
Eval num_timesteps=882000, episode_reward=180.11 +/- 40.74
Episode length: 479.52 +/- 107.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 480      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 882000   |
---------------------------------
Eval num_timesteps=882500, episode_reward=184.57 +/- 36.13
Episode length: 489.70 +/- 97.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 882500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 518      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 431      |
|    time_elapsed    | 26602    |
|    total_timesteps | 882688   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=883000, episode_reward=197.93 +/- 0.28
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 883000       |
| train/                  |              |
|    approx_kl            | 0.0060275863 |
|    clip_fraction        | 0.00841      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.001        |
|    loss                 | 14.4         |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.000877    |
|    value_loss           | 30.9         |
------------------------------------------
Eval num_timesteps=883500, episode_reward=197.97 +/- 0.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 883500   |
---------------------------------
Eval num_timesteps=884000, episode_reward=197.93 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 884000   |
---------------------------------
Eval num_timesteps=884500, episode_reward=197.94 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 884500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 432      |
|    time_elapsed    | 26674    |
|    total_timesteps | 884736   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=885000, episode_reward=197.84 +/- 0.49
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 885000      |
| train/                  |             |
|    approx_kl            | 0.010228119 |
|    clip_fraction        | 0.0388      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.001       |
|    loss                 | 1.44        |
|    n_updates            | 782         |
|    policy_gradient_loss | 0.00407     |
|    value_loss           | 27.1        |
-----------------------------------------
Eval num_timesteps=885500, episode_reward=197.89 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 885500   |
---------------------------------
Eval num_timesteps=886000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 886000   |
---------------------------------
Eval num_timesteps=886500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 886500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 433      |
|    time_elapsed    | 26744    |
|    total_timesteps | 886784   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.16
Eval num_timesteps=887000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 887000      |
| train/                  |             |
|    approx_kl            | 0.018991753 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 1.68        |
|    n_updates            | 786         |
|    policy_gradient_loss | 7.83e-05    |
|    value_loss           | 7.23        |
-----------------------------------------
Eval num_timesteps=887500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 887500   |
---------------------------------
Eval num_timesteps=888000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 888000   |
---------------------------------
Eval num_timesteps=888500, episode_reward=197.91 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 888500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 434      |
|    time_elapsed    | 26815    |
|    total_timesteps | 888832   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=889000, episode_reward=197.81 +/- 0.44
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 889000      |
| train/                  |             |
|    approx_kl            | 0.013509405 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0523     |
|    n_updates            | 788         |
|    policy_gradient_loss | -0.000332   |
|    value_loss           | 7.38        |
-----------------------------------------
Eval num_timesteps=889500, episode_reward=197.86 +/- 0.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 889500   |
---------------------------------
Eval num_timesteps=890000, episode_reward=197.81 +/- 0.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 890000   |
---------------------------------
Eval num_timesteps=890500, episode_reward=197.84 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 890500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 435      |
|    time_elapsed    | 26885    |
|    total_timesteps | 890880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=891000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 891000       |
| train/                  |              |
|    approx_kl            | 0.0050593102 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 1.59         |
|    n_updates            | 789          |
|    policy_gradient_loss | 0.00211      |
|    value_loss           | 10.9         |
------------------------------------------
Eval num_timesteps=891500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 891500   |
---------------------------------
Eval num_timesteps=892000, episode_reward=197.93 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 892000   |
---------------------------------
Eval num_timesteps=892500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 892500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 436      |
|    time_elapsed    | 26956    |
|    total_timesteps | 892928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=893000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 893000       |
| train/                  |              |
|    approx_kl            | 0.0061175795 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 1.52         |
|    n_updates            | 790          |
|    policy_gradient_loss | 0.000975     |
|    value_loss           | 8.21         |
------------------------------------------
Eval num_timesteps=893500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 893500   |
---------------------------------
Eval num_timesteps=894000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 894000   |
---------------------------------
Eval num_timesteps=894500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 894500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 437      |
|    time_elapsed    | 27027    |
|    total_timesteps | 894976   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
Eval num_timesteps=895000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 895000       |
| train/                  |              |
|    approx_kl            | 0.0059208144 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.001        |
|    loss                 | 2.3          |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 6.83         |
------------------------------------------
Eval num_timesteps=895500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 895500   |
---------------------------------
Eval num_timesteps=896000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 896000   |
---------------------------------
Eval num_timesteps=896500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 896500   |
---------------------------------
Eval num_timesteps=897000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 897000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 438      |
|    time_elapsed    | 27117    |
|    total_timesteps | 897024   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=897500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 897500      |
| train/                  |             |
|    approx_kl            | 0.010152767 |
|    clip_fraction        | 0.00823     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.001       |
|    loss                 | 22.6        |
|    n_updates            | 803         |
|    policy_gradient_loss | -0.000153   |
|    value_loss           | 28.6        |
-----------------------------------------
Eval num_timesteps=898000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 898000   |
---------------------------------
Eval num_timesteps=898500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 898500   |
---------------------------------
Eval num_timesteps=899000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 899000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 439      |
|    time_elapsed    | 27189    |
|    total_timesteps | 899072   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=899500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 899500       |
| train/                  |              |
|    approx_kl            | 0.0062832283 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0901       |
|    n_updates            | 809          |
|    policy_gradient_loss | 0.00193      |
|    value_loss           | 16.1         |
------------------------------------------
Eval num_timesteps=900000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=900500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 900500   |
---------------------------------
Eval num_timesteps=901000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 901000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 440      |
|    time_elapsed    | 27260    |
|    total_timesteps | 901120   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
Eval num_timesteps=901500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 901500      |
| train/                  |             |
|    approx_kl            | 0.011945963 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | -0.096      |
|    n_updates            | 818         |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 17.1        |
-----------------------------------------
Eval num_timesteps=902000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 902000   |
---------------------------------
Eval num_timesteps=902500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 902500   |
---------------------------------
Eval num_timesteps=903000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 903000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 441      |
|    time_elapsed    | 27331    |
|    total_timesteps | 903168   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=903500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 903500      |
| train/                  |             |
|    approx_kl            | 0.015176946 |
|    clip_fraction        | 0.0506      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | 8.89        |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.000841   |
|    value_loss           | 15.6        |
-----------------------------------------
Eval num_timesteps=904000, episode_reward=193.58 +/- 21.67
Episode length: 512.98 +/- 58.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 513      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 904000   |
---------------------------------
Eval num_timesteps=904500, episode_reward=195.77 +/- 15.48
Episode length: 518.00 +/- 49.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 904500   |
---------------------------------
Eval num_timesteps=905000, episode_reward=195.79 +/- 15.48
Episode length: 516.86 +/- 56.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 905000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 442      |
|    time_elapsed    | 27402    |
|    total_timesteps | 905216   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=905500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 905500      |
| train/                  |             |
|    approx_kl            | 0.013097437 |
|    clip_fraction        | 0.00954     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.001       |
|    loss                 | 7.19        |
|    n_updates            | 825         |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 34.8        |
-----------------------------------------
Eval num_timesteps=906000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 906000   |
---------------------------------
Eval num_timesteps=906500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 906500   |
---------------------------------
Eval num_timesteps=907000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 907000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 443      |
|    time_elapsed    | 27473    |
|    total_timesteps | 907264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=907500, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 907500      |
| train/                  |             |
|    approx_kl            | 0.005028406 |
|    clip_fraction        | 0.0144      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.001       |
|    loss                 | 5.31        |
|    n_updates            | 826         |
|    policy_gradient_loss | 0.000199    |
|    value_loss           | 16          |
-----------------------------------------
Eval num_timesteps=908000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 908000   |
---------------------------------
Eval num_timesteps=908500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 908500   |
---------------------------------
Eval num_timesteps=909000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 909000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 33       |
|    iterations      | 444      |
|    time_elapsed    | 27545    |
|    total_timesteps | 909312   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=909500, episode_reward=196.92 +/- 1.23
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 909500      |
| train/                  |             |
|    approx_kl            | 0.010894498 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 8.52        |
|    n_updates            | 829         |
|    policy_gradient_loss | -0.000951   |
|    value_loss           | 16.6        |
-----------------------------------------
Eval num_timesteps=910000, episode_reward=197.08 +/- 1.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 910000   |
---------------------------------
Eval num_timesteps=910500, episode_reward=196.62 +/- 1.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 910500   |
---------------------------------
Eval num_timesteps=911000, episode_reward=197.28 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 911000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 445      |
|    time_elapsed    | 27617    |
|    total_timesteps | 911360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=911500, episode_reward=197.91 +/- 0.31
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 911500       |
| train/                  |              |
|    approx_kl            | 0.0051169787 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.001        |
|    loss                 | 9.8          |
|    n_updates            | 830          |
|    policy_gradient_loss | 0.00373      |
|    value_loss           | 17.6         |
------------------------------------------
Eval num_timesteps=912000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 912000   |
---------------------------------
Eval num_timesteps=912500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 912500   |
---------------------------------
Eval num_timesteps=913000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 913000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 446      |
|    time_elapsed    | 27689    |
|    total_timesteps | 913408   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=913500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 913500     |
| train/                  |            |
|    approx_kl            | 0.04556571 |
|    clip_fraction        | 0.0547     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.001      |
|    loss                 | 11.7       |
|    n_updates            | 832        |
|    policy_gradient_loss | 0.000187   |
|    value_loss           | 16.6       |
----------------------------------------
Eval num_timesteps=914000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 914000   |
---------------------------------
Eval num_timesteps=914500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 914500   |
---------------------------------
Eval num_timesteps=915000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 915000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 447      |
|    time_elapsed    | 27761    |
|    total_timesteps | 915456   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=915500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 915500      |
| train/                  |             |
|    approx_kl            | 0.015226097 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.001       |
|    loss                 | 3.73        |
|    n_updates            | 837         |
|    policy_gradient_loss | -0.000515   |
|    value_loss           | 53.5        |
-----------------------------------------
Eval num_timesteps=916000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 916000   |
---------------------------------
Eval num_timesteps=916500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 916500   |
---------------------------------
Eval num_timesteps=917000, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 917000   |
---------------------------------
Eval num_timesteps=917500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 917500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 448      |
|    time_elapsed    | 27851    |
|    total_timesteps | 917504   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=918000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 918000     |
| train/                  |            |
|    approx_kl            | 0.00668858 |
|    clip_fraction        | 0.0435     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.001      |
|    loss                 | 16.3       |
|    n_updates            | 841        |
|    policy_gradient_loss | 0.00194    |
|    value_loss           | 13.4       |
----------------------------------------
Eval num_timesteps=918500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 918500   |
---------------------------------
Eval num_timesteps=919000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 919000   |
---------------------------------
Eval num_timesteps=919500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 919500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 449      |
|    time_elapsed    | 27923    |
|    total_timesteps | 919552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=920000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.015813963 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 3.69        |
|    n_updates            | 843         |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 10.3        |
-----------------------------------------
Eval num_timesteps=920500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 920500   |
---------------------------------
Eval num_timesteps=921000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 921000   |
---------------------------------
Eval num_timesteps=921500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 921500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 450      |
|    time_elapsed    | 27994    |
|    total_timesteps | 921600   |
---------------------------------
Eval num_timesteps=922000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 922000      |
| train/                  |             |
|    approx_kl            | 0.008489532 |
|    clip_fraction        | 0.02        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 0.336       |
|    n_updates            | 853         |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 6.02        |
-----------------------------------------
Eval num_timesteps=922500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 922500   |
---------------------------------
Eval num_timesteps=923000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 923000   |
---------------------------------
Eval num_timesteps=923500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 923500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 451      |
|    time_elapsed    | 28065    |
|    total_timesteps | 923648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=924000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 924000      |
| train/                  |             |
|    approx_kl            | 0.003263548 |
|    clip_fraction        | 0.000651    |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 5.13        |
|    n_updates            | 854         |
|    policy_gradient_loss | -1.25e-05   |
|    value_loss           | 20.4        |
-----------------------------------------
Eval num_timesteps=924500, episode_reward=197.93 +/- 0.25
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 924500   |
---------------------------------
Eval num_timesteps=925000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 925000   |
---------------------------------
Eval num_timesteps=925500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 925500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 452      |
|    time_elapsed    | 28137    |
|    total_timesteps | 925696   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=926000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 926000      |
| train/                  |             |
|    approx_kl            | 0.006716587 |
|    clip_fraction        | 0.00767     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 14          |
|    n_updates            | 861         |
|    policy_gradient_loss | 0.00123     |
|    value_loss           | 23.7        |
-----------------------------------------
Eval num_timesteps=926500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 926500   |
---------------------------------
Eval num_timesteps=927000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 927000   |
---------------------------------
Eval num_timesteps=927500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 927500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 453      |
|    time_elapsed    | 28208    |
|    total_timesteps | 927744   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=928000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 928000      |
| train/                  |             |
|    approx_kl            | 0.012225715 |
|    clip_fraction        | 0.0144      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.001       |
|    loss                 | 10.3        |
|    n_updates            | 864         |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 14.7        |
-----------------------------------------
Eval num_timesteps=928500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 928500   |
---------------------------------
Eval num_timesteps=929000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 929000   |
---------------------------------
Eval num_timesteps=929500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 929500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 454      |
|    time_elapsed    | 28279    |
|    total_timesteps | 929792   |
---------------------------------
Eval num_timesteps=930000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.005704509 |
|    clip_fraction        | 0.00645     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 17.3        |
|    n_updates            | 874         |
|    policy_gradient_loss | -0.000179   |
|    value_loss           | 17.4        |
-----------------------------------------
Eval num_timesteps=930500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 930500   |
---------------------------------
Eval num_timesteps=931000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 931000   |
---------------------------------
Eval num_timesteps=931500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 931500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 455      |
|    time_elapsed    | 28350    |
|    total_timesteps | 931840   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=932000, episode_reward=197.80 +/- 0.40
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 932000      |
| train/                  |             |
|    approx_kl            | 0.007603826 |
|    clip_fraction        | 0.0126      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 15.4        |
|    n_updates            | 881         |
|    policy_gradient_loss | -0.000976   |
|    value_loss           | 21.3        |
-----------------------------------------
Eval num_timesteps=932500, episode_reward=197.75 +/- 0.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 932500   |
---------------------------------
Eval num_timesteps=933000, episode_reward=197.80 +/- 0.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 933000   |
---------------------------------
Eval num_timesteps=933500, episode_reward=197.80 +/- 0.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 933500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 456      |
|    time_elapsed    | 28422    |
|    total_timesteps | 933888   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=934000, episode_reward=197.91 +/- 0.32
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 934000      |
| train/                  |             |
|    approx_kl            | 0.006784983 |
|    clip_fraction        | 0.0728      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.001       |
|    loss                 | 4.96        |
|    n_updates            | 886         |
|    policy_gradient_loss | -0.000683   |
|    value_loss           | 9.5         |
-----------------------------------------
Eval num_timesteps=934500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 934500   |
---------------------------------
Eval num_timesteps=935000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 935000   |
---------------------------------
Eval num_timesteps=935500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 935500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 457      |
|    time_elapsed    | 28495    |
|    total_timesteps | 935936   |
---------------------------------
Eval num_timesteps=936000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 936000       |
| train/                  |              |
|    approx_kl            | 0.0025285836 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 0.629        |
|    n_updates            | 896          |
|    policy_gradient_loss | 0.000727     |
|    value_loss           | 18.1         |
------------------------------------------
Eval num_timesteps=936500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 936500   |
---------------------------------
Eval num_timesteps=937000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 937000   |
---------------------------------
Eval num_timesteps=937500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 937500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 458      |
|    time_elapsed    | 28567    |
|    total_timesteps | 937984   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=938000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 938000      |
| train/                  |             |
|    approx_kl            | 0.019631943 |
|    clip_fraction        | 0.00112     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 2.89        |
|    n_updates            | 898         |
|    policy_gradient_loss | -0.000362   |
|    value_loss           | 7.99        |
-----------------------------------------
Eval num_timesteps=938500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 938500   |
---------------------------------
Eval num_timesteps=939000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 939000   |
---------------------------------
Eval num_timesteps=939500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 939500   |
---------------------------------
Eval num_timesteps=940000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 459      |
|    time_elapsed    | 28657    |
|    total_timesteps | 940032   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=940500, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 940500       |
| train/                  |              |
|    approx_kl            | 0.0039858026 |
|    clip_fraction        | 0.000901     |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 1.48         |
|    n_updates            | 900          |
|    policy_gradient_loss | 0.000999     |
|    value_loss           | 12.6         |
------------------------------------------
Eval num_timesteps=941000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 941000   |
---------------------------------
Eval num_timesteps=941500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 941500   |
---------------------------------
Eval num_timesteps=942000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 942000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 460      |
|    time_elapsed    | 28729    |
|    total_timesteps | 942080   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=942500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 942500       |
| train/                  |              |
|    approx_kl            | 0.0031351997 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 0.11         |
|    n_updates            | 902          |
|    policy_gradient_loss | 0.00736      |
|    value_loss           | 14.6         |
------------------------------------------
Eval num_timesteps=943000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 943000   |
---------------------------------
Eval num_timesteps=943500, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 943500   |
---------------------------------
Eval num_timesteps=944000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 944000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 461      |
|    time_elapsed    | 28801    |
|    total_timesteps | 944128   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=944500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 944500     |
| train/                  |            |
|    approx_kl            | 0.00475598 |
|    clip_fraction        | 0.00728    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.93       |
|    learning_rate        | 0.001      |
|    loss                 | 2.26       |
|    n_updates            | 907        |
|    policy_gradient_loss | -0.000779  |
|    value_loss           | 18.4       |
----------------------------------------
Eval num_timesteps=945000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 945000   |
---------------------------------
Eval num_timesteps=945500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 945500   |
---------------------------------
Eval num_timesteps=946000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 946000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 462      |
|    time_elapsed    | 28872    |
|    total_timesteps | 946176   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=946500, episode_reward=197.70 +/- 0.64
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 946500       |
| train/                  |              |
|    approx_kl            | 0.0083053075 |
|    clip_fraction        | 0.00404      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 21.3         |
|    n_updates            | 911          |
|    policy_gradient_loss | 0.000356     |
|    value_loss           | 12.6         |
------------------------------------------
Eval num_timesteps=947000, episode_reward=197.66 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 947000   |
---------------------------------
Eval num_timesteps=947500, episode_reward=197.60 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 947500   |
---------------------------------
Eval num_timesteps=948000, episode_reward=197.74 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 948000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 463      |
|    time_elapsed    | 28944    |
|    total_timesteps | 948224   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=948500, episode_reward=197.60 +/- 0.77
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 948500      |
| train/                  |             |
|    approx_kl            | 0.013942817 |
|    clip_fraction        | 0.0244      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | -0.136      |
|    n_updates            | 915         |
|    policy_gradient_loss | -0.000825   |
|    value_loss           | 6.77        |
-----------------------------------------
Eval num_timesteps=949000, episode_reward=197.41 +/- 0.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 949000   |
---------------------------------
Eval num_timesteps=949500, episode_reward=197.27 +/- 0.95
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 949500   |
---------------------------------
Eval num_timesteps=950000, episode_reward=197.42 +/- 0.90
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 464      |
|    time_elapsed    | 29015    |
|    total_timesteps | 950272   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=950500, episode_reward=197.62 +/- 0.72
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 950500      |
| train/                  |             |
|    approx_kl            | 0.009753705 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.001       |
|    loss                 | 15.8        |
|    n_updates            | 921         |
|    policy_gradient_loss | 0.000178    |
|    value_loss           | 32          |
-----------------------------------------
Eval num_timesteps=951000, episode_reward=197.50 +/- 0.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 951000   |
---------------------------------
Eval num_timesteps=951500, episode_reward=197.51 +/- 0.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 951500   |
---------------------------------
Eval num_timesteps=952000, episode_reward=197.50 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 952000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 465      |
|    time_elapsed    | 29088    |
|    total_timesteps | 952320   |
---------------------------------
Eval num_timesteps=952500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 952500      |
| train/                  |             |
|    approx_kl            | 0.008598737 |
|    clip_fraction        | 0.0267      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.001       |
|    loss                 | 0.54        |
|    n_updates            | 931         |
|    policy_gradient_loss | -0.00103    |
|    value_loss           | 16.6        |
-----------------------------------------
Eval num_timesteps=953000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 953000   |
---------------------------------
Eval num_timesteps=953500, episode_reward=197.90 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 953500   |
---------------------------------
Eval num_timesteps=954000, episode_reward=197.90 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 954000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 466      |
|    time_elapsed    | 29159    |
|    total_timesteps | 954368   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=954500, episode_reward=197.81 +/- 0.41
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 954500       |
| train/                  |              |
|    approx_kl            | 0.0092087975 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.001        |
|    loss                 | 5.96         |
|    n_updates            | 934          |
|    policy_gradient_loss | -0.00318     |
|    value_loss           | 6.7          |
------------------------------------------
Eval num_timesteps=955000, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 955000   |
---------------------------------
Eval num_timesteps=955500, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 955500   |
---------------------------------
Eval num_timesteps=956000, episode_reward=197.82 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 956000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 467      |
|    time_elapsed    | 29230    |
|    total_timesteps | 956416   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=956500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 956500      |
| train/                  |             |
|    approx_kl            | 0.009681879 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.001       |
|    loss                 | 7.84        |
|    n_updates            | 936         |
|    policy_gradient_loss | -0.00214    |
|    value_loss           | 35          |
-----------------------------------------
Eval num_timesteps=957000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 957000   |
---------------------------------
Eval num_timesteps=957500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 957500   |
---------------------------------
Eval num_timesteps=958000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 958000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 468      |
|    time_elapsed    | 29301    |
|    total_timesteps | 958464   |
---------------------------------
Eval num_timesteps=958500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 958500      |
| train/                  |             |
|    approx_kl            | 0.003705671 |
|    clip_fraction        | 0.0116      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 1.54        |
|    n_updates            | 946         |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 9.99        |
-----------------------------------------
Eval num_timesteps=959000, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 959000   |
---------------------------------
Eval num_timesteps=959500, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 959500   |
---------------------------------
Eval num_timesteps=960000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=960500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 960500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 469      |
|    time_elapsed    | 29391    |
|    total_timesteps | 960512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=961000, episode_reward=196.49 +/- 1.06
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 961000       |
| train/                  |              |
|    approx_kl            | 0.0050512874 |
|    clip_fraction        | 0.00604      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 5.25         |
|    n_updates            | 948          |
|    policy_gradient_loss | 0.000467     |
|    value_loss           | 29.2         |
------------------------------------------
Eval num_timesteps=961500, episode_reward=196.36 +/- 1.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 961500   |
---------------------------------
Eval num_timesteps=962000, episode_reward=198.58 +/- 13.97
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 962000   |
---------------------------------
Eval num_timesteps=962500, episode_reward=197.85 +/- 13.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 962500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 470      |
|    time_elapsed    | 29462    |
|    total_timesteps | 962560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=963000, episode_reward=195.73 +/- 1.38
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 963000       |
| train/                  |              |
|    approx_kl            | 0.0087865265 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.822        |
|    learning_rate        | 0.001        |
|    loss                 | 94.9         |
|    n_updates            | 949          |
|    policy_gradient_loss | 0.0755       |
|    value_loss           | 58.6         |
------------------------------------------
Eval num_timesteps=963500, episode_reward=195.59 +/- 1.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 963500   |
---------------------------------
Eval num_timesteps=964000, episode_reward=195.68 +/- 1.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 964000   |
---------------------------------
Eval num_timesteps=964500, episode_reward=195.74 +/- 1.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 964500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 471      |
|    time_elapsed    | 29532    |
|    total_timesteps | 964608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=965000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 965000       |
| train/                  |              |
|    approx_kl            | 0.0125291655 |
|    clip_fraction        | 0.00639      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.801        |
|    learning_rate        | 0.001        |
|    loss                 | 72.8         |
|    n_updates            | 950          |
|    policy_gradient_loss | 0.00374      |
|    value_loss           | 58.6         |
------------------------------------------
Eval num_timesteps=965500, episode_reward=195.72 +/- 15.62
Episode length: 515.66 +/- 65.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 965500   |
---------------------------------
Eval num_timesteps=966000, episode_reward=197.91 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 966000   |
---------------------------------
Eval num_timesteps=966500, episode_reward=195.72 +/- 15.62
Episode length: 515.64 +/- 65.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 966500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 472      |
|    time_elapsed    | 29601    |
|    total_timesteps | 966656   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=967000, episode_reward=197.62 +/- 0.49
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 967000       |
| train/                  |              |
|    approx_kl            | 0.0046102386 |
|    clip_fraction        | 0.00475      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.912        |
|    learning_rate        | 0.001        |
|    loss                 | 7.31         |
|    n_updates            | 955          |
|    policy_gradient_loss | 0.00141      |
|    value_loss           | 25.8         |
------------------------------------------
Eval num_timesteps=967500, episode_reward=197.80 +/- 0.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 967500   |
---------------------------------
Eval num_timesteps=968000, episode_reward=197.59 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 968000   |
---------------------------------
Eval num_timesteps=968500, episode_reward=197.67 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 968500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 473      |
|    time_elapsed    | 29675    |
|    total_timesteps | 968704   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=969000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 969000      |
| train/                  |             |
|    approx_kl            | 0.011607721 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 2.39        |
|    n_updates            | 957         |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 8.62        |
-----------------------------------------
Eval num_timesteps=969500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 969500   |
---------------------------------
Eval num_timesteps=970000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 970000   |
---------------------------------
Eval num_timesteps=970500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 970500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 474      |
|    time_elapsed    | 29748    |
|    total_timesteps | 970752   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=971000, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 971000       |
| train/                  |              |
|    approx_kl            | 0.0033086205 |
|    clip_fraction        | 0.00728      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.001        |
|    loss                 | 22.4         |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 23.8         |
------------------------------------------
Eval num_timesteps=971500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 971500   |
---------------------------------
Eval num_timesteps=972000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 972000   |
---------------------------------
Eval num_timesteps=972500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 972500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 475      |
|    time_elapsed    | 29819    |
|    total_timesteps | 972800   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=973000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 973000      |
| train/                  |             |
|    approx_kl            | 0.009479389 |
|    clip_fraction        | 0.00675     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 17.6        |
|    n_updates            | 962         |
|    policy_gradient_loss | -0.000992   |
|    value_loss           | 15.9        |
-----------------------------------------
Eval num_timesteps=973500, episode_reward=197.92 +/- 0.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 973500   |
---------------------------------
Eval num_timesteps=974000, episode_reward=197.92 +/- 0.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 974000   |
---------------------------------
Eval num_timesteps=974500, episode_reward=197.84 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 974500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 476      |
|    time_elapsed    | 29891    |
|    total_timesteps | 974848   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=975000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.012044621 |
|    clip_fraction        | 0.00424     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.001       |
|    loss                 | 14.6        |
|    n_updates            | 965         |
|    policy_gradient_loss | 0.00227     |
|    value_loss           | 21.7        |
-----------------------------------------
Eval num_timesteps=975500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 975500   |
---------------------------------
Eval num_timesteps=976000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 976000   |
---------------------------------
Eval num_timesteps=976500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 976500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 477      |
|    time_elapsed    | 29962    |
|    total_timesteps | 976896   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.15
Eval num_timesteps=977000, episode_reward=197.41 +/- 0.51
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 977000      |
| train/                  |             |
|    approx_kl            | 0.014977617 |
|    clip_fraction        | 0.00218     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.001       |
|    loss                 | 0.68        |
|    n_updates            | 969         |
|    policy_gradient_loss | 0.002       |
|    value_loss           | 12.2        |
-----------------------------------------
Eval num_timesteps=977500, episode_reward=197.60 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 977500   |
---------------------------------
Eval num_timesteps=978000, episode_reward=197.51 +/- 0.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 978000   |
---------------------------------
Eval num_timesteps=978500, episode_reward=197.52 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 978500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 478      |
|    time_elapsed    | 30034    |
|    total_timesteps | 978944   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=979000, episode_reward=197.91 +/- 0.32
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 979000      |
| train/                  |             |
|    approx_kl            | 0.017374607 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0903     |
|    n_updates            | 971         |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 15.3        |
-----------------------------------------
Eval num_timesteps=979500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 979500   |
---------------------------------
Eval num_timesteps=980000, episode_reward=197.87 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=980500, episode_reward=197.88 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 980500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 479      |
|    time_elapsed    | 30105    |
|    total_timesteps | 980992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=981000, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 981000       |
| train/                  |              |
|    approx_kl            | 0.0027059906 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 45.1         |
|    n_updates            | 972          |
|    policy_gradient_loss | 0.0084       |
|    value_loss           | 28.6         |
------------------------------------------
Eval num_timesteps=981500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 981500   |
---------------------------------
Eval num_timesteps=982000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 982000   |
---------------------------------
Eval num_timesteps=982500, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 982500   |
---------------------------------
Eval num_timesteps=983000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 983000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 480      |
|    time_elapsed    | 30193    |
|    total_timesteps | 983040   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=983500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 983500      |
| train/                  |             |
|    approx_kl            | 0.006398198 |
|    clip_fraction        | 0.00274     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.001       |
|    loss                 | 6.26        |
|    n_updates            | 975         |
|    policy_gradient_loss | 4.48e-05    |
|    value_loss           | 8.41        |
-----------------------------------------
Eval num_timesteps=984000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 984000   |
---------------------------------
Eval num_timesteps=984500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 984500   |
---------------------------------
Eval num_timesteps=985000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 985000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 481      |
|    time_elapsed    | 30265    |
|    total_timesteps | 985088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=985500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 985500      |
| train/                  |             |
|    approx_kl            | 0.004359462 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.001       |
|    loss                 | 36.5        |
|    n_updates            | 976         |
|    policy_gradient_loss | 0.00726     |
|    value_loss           | 33.1        |
-----------------------------------------
Eval num_timesteps=986000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 986000   |
---------------------------------
Eval num_timesteps=986500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 986500   |
---------------------------------
Eval num_timesteps=987000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 987000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 482      |
|    time_elapsed    | 30336    |
|    total_timesteps | 987136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=987500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 987500      |
| train/                  |             |
|    approx_kl            | 0.012610628 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.001       |
|    loss                 | 15.6        |
|    n_updates            | 977         |
|    policy_gradient_loss | 0.00325     |
|    value_loss           | 24.8        |
-----------------------------------------
Eval num_timesteps=988000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 988000   |
---------------------------------
Eval num_timesteps=988500, episode_reward=197.87 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 988500   |
---------------------------------
Eval num_timesteps=989000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 989000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 483      |
|    time_elapsed    | 30407    |
|    total_timesteps | 989184   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=989500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 989500      |
| train/                  |             |
|    approx_kl            | 0.008201121 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.001       |
|    loss                 | 5.83        |
|    n_updates            | 979         |
|    policy_gradient_loss | 1.88e-05    |
|    value_loss           | 59.8        |
-----------------------------------------
Eval num_timesteps=990000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 990000   |
---------------------------------
Eval num_timesteps=990500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 990500   |
---------------------------------
Eval num_timesteps=991000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 991000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 484      |
|    time_elapsed    | 30479    |
|    total_timesteps | 991232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=991500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 991500      |
| train/                  |             |
|    approx_kl            | 0.008393239 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0921      |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 6.98        |
-----------------------------------------
Eval num_timesteps=992000, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 992000   |
---------------------------------
Eval num_timesteps=992500, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 992500   |
---------------------------------
Eval num_timesteps=993000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 993000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 485      |
|    time_elapsed    | 30550    |
|    total_timesteps | 993280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=993500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 993500       |
| train/                  |              |
|    approx_kl            | 0.0056870473 |
|    clip_fraction        | 0.0826       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.001        |
|    loss                 | 5.08         |
|    n_updates            | 981          |
|    policy_gradient_loss | -0.00152     |
|    value_loss           | 6.48         |
------------------------------------------
Eval num_timesteps=994000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 994000   |
---------------------------------
Eval num_timesteps=994500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 994500   |
---------------------------------
Eval num_timesteps=995000, episode_reward=197.93 +/- 0.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 995000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 486      |
|    time_elapsed    | 30621    |
|    total_timesteps | 995328   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=995500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 995500      |
| train/                  |             |
|    approx_kl            | 0.015711531 |
|    clip_fraction        | 0.0438      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 3.98        |
|    n_updates            | 985         |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 24.6        |
-----------------------------------------
Eval num_timesteps=996000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=996500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 996500   |
---------------------------------
Eval num_timesteps=997000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 997000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 487      |
|    time_elapsed    | 30693    |
|    total_timesteps | 997376   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=997500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 997500     |
| train/                  |            |
|    approx_kl            | 0.00913468 |
|    clip_fraction        | 0.0271     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.001      |
|    loss                 | 7.6        |
|    n_updates            | 987        |
|    policy_gradient_loss | -0.00105   |
|    value_loss           | 16.1       |
----------------------------------------
Eval num_timesteps=998000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 998000   |
---------------------------------
Eval num_timesteps=998500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 998500   |
---------------------------------
Eval num_timesteps=999000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 999000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 488      |
|    time_elapsed    | 30764    |
|    total_timesteps | 999424   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=999500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 999500      |
| train/                  |             |
|    approx_kl            | 0.011079796 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 5.73        |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 10.2        |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1000500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 1000500  |
---------------------------------
Eval num_timesteps=1001000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 1001000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 32       |
|    iterations      | 489      |
|    time_elapsed    | 30835    |
|    total_timesteps | 1001472  |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
