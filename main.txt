/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-1.00 +/- 0.00
Episode length: 72.34 +/- 9.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-1.00 +/- 0.00
Episode length: 73.80 +/- 11.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.8     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-1.00 +/- 0.00
Episode length: 72.14 +/- 9.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.1     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-1.00 +/- 0.00
Episode length: 69.88 +/- 9.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.9     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | 0.208    |
| time/              |          |
|    fps             | 184      |
|    iterations      | 1        |
|    time_elapsed    | 11       |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=0.74 +/- 0.69
Episode length: 98.68 +/- 13.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.7        |
|    mean_reward          | 0.74        |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.009530687 |
|    clip_fraction        | 0.0701      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.0173      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0325      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00602    |
|    value_loss           | 0.123       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=0.48 +/- 0.81
Episode length: 92.02 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92       |
|    mean_reward     | 0.48     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=0.68 +/- 0.84
Episode length: 98.42 +/- 13.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 0.68     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=0.50 +/- 0.90
Episode length: 92.96 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93       |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.6     |
|    ep_rew_mean     | 0.306    |
| time/              |          |
|    fps             | 156      |
|    iterations      | 2        |
|    time_elapsed    | 26       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=0.42 +/- 0.83
Episode length: 93.02 +/- 14.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93          |
|    mean_reward          | 0.42        |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.013154777 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.022       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00999    |
|    value_loss           | 0.116       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=0.54 +/- 0.64
Episode length: 94.56 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.6     |
|    mean_reward     | 0.54     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=0.28 +/- 0.83
Episode length: 89.94 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.9     |
|    mean_reward     | 0.28     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=0.38 +/- 0.85
Episode length: 93.18 +/- 14.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 0.38     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.8     |
|    ep_rew_mean     | 0.691    |
| time/              |          |
|    fps             | 151      |
|    iterations      | 3        |
|    time_elapsed    | 40       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=-0.74 +/- 0.52
Episode length: 73.98 +/- 13.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74          |
|    mean_reward          | -0.74       |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.010963909 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0414      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.149       |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=-0.64 +/- 0.52
Episode length: 74.48 +/- 10.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | -0.64    |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-0.62 +/- 0.52
Episode length: 74.56 +/- 12.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | -0.62    |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-0.80 +/- 0.40
Episode length: 73.24 +/- 10.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.2     |
|    mean_reward     | -0.8     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 0.955    |
| time/              |          |
|    fps             | 155      |
|    iterations      | 4        |
|    time_elapsed    | 52       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=-0.50 +/- 0.57
Episode length: 76.92 +/- 12.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.9        |
|    mean_reward          | -0.5        |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.007977176 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.468       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0442      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.196       |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=-0.72 +/- 0.53
Episode length: 76.32 +/- 13.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | -0.72    |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-0.58 +/- 0.67
Episode length: 74.48 +/- 14.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | -0.58    |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.68 +/- 0.58
Episode length: 75.28 +/- 12.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.3     |
|    mean_reward     | -0.68    |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.5     |
|    ep_rew_mean     | 1.19     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 5        |
|    time_elapsed    | 64       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=1.58 +/- 0.70
Episode length: 108.70 +/- 8.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 109          |
|    mean_reward          | 1.58         |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0054389145 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.523        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0607       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00662     |
|    value_loss           | 0.258        |
------------------------------------------
New best mean reward!
Eval num_timesteps=11000, episode_reward=1.56 +/- 0.70
Episode length: 106.82 +/- 8.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=1.60 +/- 0.75
Episode length: 107.86 +/- 10.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
New best mean reward!
Eval num_timesteps=12000, episode_reward=1.42 +/- 1.13
Episode length: 106.62 +/- 18.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.9     |
|    ep_rew_mean     | 1.47     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 6        |
|    time_elapsed    | 81       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=1.54 +/- 0.83
Episode length: 107.64 +/- 8.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 1.54        |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.009533826 |
|    clip_fraction        | 0.0858      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.136       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00722    |
|    value_loss           | 0.251       |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=1.58 +/- 0.70
Episode length: 106.20 +/- 7.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=1.36 +/- 0.62
Episode length: 107.90 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 1.36     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=1.42 +/- 0.64
Episode length: 105.28 +/- 8.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 1.88     |
| time/              |          |
|    fps             | 146      |
|    iterations      | 7        |
|    time_elapsed    | 97       |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=1.92 +/- 1.09
Episode length: 117.78 +/- 58.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 118          |
|    mean_reward          | 1.92         |
| time/                   |              |
|    total_timesteps      | 14500        |
| train/                  |              |
|    approx_kl            | 0.0077778026 |
|    clip_fraction        | 0.0496       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.515        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.13         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00487     |
|    value_loss           | 0.241        |
------------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=2.04 +/- 1.15
Episode length: 111.66 +/- 15.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
Eval num_timesteps=15500, episode_reward=2.20 +/- 1.31
Episode length: 113.88 +/- 19.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
New best mean reward!
Eval num_timesteps=16000, episode_reward=2.20 +/- 1.43
Episode length: 112.90 +/- 24.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.04     |
| time/              |          |
|    fps             | 142      |
|    iterations      | 8        |
|    time_elapsed    | 114      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=3.86 +/- 1.69
Episode length: 137.14 +/- 37.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 137         |
|    mean_reward          | 3.86        |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.008965573 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0865      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.226       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=17000, episode_reward=3.64 +/- 1.96
Episode length: 135.22 +/- 42.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 135      |
|    mean_reward     | 3.64     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=4.12 +/- 1.87
Episode length: 140.22 +/- 40.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | 4.12     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
New best mean reward!
Eval num_timesteps=18000, episode_reward=3.96 +/- 1.97
Episode length: 142.22 +/- 44.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 3.96     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.18     |
| time/              |          |
|    fps             | 135      |
|    iterations      | 9        |
|    time_elapsed    | 135      |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=6.04 +/- 3.33
Episode length: 177.50 +/- 70.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 178          |
|    mean_reward          | 6.04         |
| time/                   |              |
|    total_timesteps      | 18500        |
| train/                  |              |
|    approx_kl            | 0.0072319964 |
|    clip_fraction        | 0.0895       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.571        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0598       |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00764     |
|    value_loss           | 0.214        |
------------------------------------------
New best mean reward!
Eval num_timesteps=19000, episode_reward=5.66 +/- 2.53
Episode length: 167.26 +/- 56.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 5.66     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=6.14 +/- 3.23
Episode length: 180.38 +/- 71.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 6.14     |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=5.22 +/- 2.82
Episode length: 163.52 +/- 60.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 5.22     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 10       |
|    time_elapsed    | 160      |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=6.58 +/- 3.26
Episode length: 190.60 +/- 68.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 191          |
|    mean_reward          | 6.58         |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0046516834 |
|    clip_fraction        | 0.0503       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.619        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0425       |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00608     |
|    value_loss           | 0.215        |
------------------------------------------
New best mean reward!
Eval num_timesteps=21000, episode_reward=5.64 +/- 3.02
Episode length: 167.70 +/- 64.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 5.64     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=5.46 +/- 2.75
Episode length: 166.66 +/- 58.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 5.46     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=5.52 +/- 2.53
Episode length: 168.00 +/- 56.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 5.52     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=6.50 +/- 3.11
Episode length: 187.90 +/- 69.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 188      |
|    mean_reward     | 6.5      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 11       |
|    time_elapsed    | 191      |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=7.28 +/- 3.29
Episode length: 190.04 +/- 67.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 190         |
|    mean_reward          | 7.28        |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.006202899 |
|    clip_fraction        | 0.0785      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0481      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 0.22        |
-----------------------------------------
New best mean reward!
Eval num_timesteps=23500, episode_reward=8.44 +/- 3.66
Episode length: 219.22 +/- 76.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 8.44     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
New best mean reward!
Eval num_timesteps=24000, episode_reward=8.46 +/- 3.77
Episode length: 216.20 +/- 74.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 8.46     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
New best mean reward!
Eval num_timesteps=24500, episode_reward=8.22 +/- 4.00
Episode length: 219.76 +/- 82.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 8.22     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 110      |
|    iterations      | 12       |
|    time_elapsed    | 221      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=9.24 +/- 4.77
Episode length: 231.98 +/- 92.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 232          |
|    mean_reward          | 9.24         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0071818773 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0.621        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0166       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00872     |
|    value_loss           | 0.215        |
------------------------------------------
New best mean reward!
Eval num_timesteps=25500, episode_reward=10.40 +/- 5.22
Episode length: 250.48 +/- 102.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
New best mean reward!
Eval num_timesteps=26000, episode_reward=9.88 +/- 4.97
Episode length: 243.28 +/- 94.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 9.88     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=10.30 +/- 4.57
Episode length: 242.24 +/- 84.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 13       |
|    time_elapsed    | 255      |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=12.92 +/- 3.85
Episode length: 270.80 +/- 70.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 12.9        |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.009282881 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0.645       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.131       |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=27500, episode_reward=12.26 +/- 3.79
Episode length: 259.64 +/- 66.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=11.54 +/- 4.26
Episode length: 246.22 +/- 78.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=11.44 +/- 4.12
Episode length: 245.68 +/- 79.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 11.4     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 14       |
|    time_elapsed    | 292      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=10.80 +/- 3.30
Episode length: 226.54 +/- 61.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 227          |
|    mean_reward          | 10.8         |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0069594416 |
|    clip_fraction        | 0.0651       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0.637        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0589       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 0.223        |
------------------------------------------
Eval num_timesteps=29500, episode_reward=10.00 +/- 4.25
Episode length: 215.12 +/- 72.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 10       |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=10.64 +/- 3.47
Episode length: 221.08 +/- 59.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=9.94 +/- 3.25
Episode length: 208.08 +/- 60.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | 9.94     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 15       |
|    time_elapsed    | 323      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=8.12 +/- 1.90
Episode length: 186.30 +/- 41.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 186         |
|    mean_reward          | 8.12        |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.008091073 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.959      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0367      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.217       |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=8.52 +/- 2.17
Episode length: 192.16 +/- 41.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 8.52     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=8.24 +/- 2.04
Episode length: 182.94 +/- 39.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 8.24     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=8.16 +/- 2.03
Episode length: 189.58 +/- 38.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | 8.16     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.3      |
| time/              |          |
|    fps             | 93       |
|    iterations      | 16       |
|    time_elapsed    | 350      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=10.84 +/- 2.70
Episode length: 223.82 +/- 47.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 10.8       |
| time/                   |            |
|    total_timesteps      | 33000      |
| train/                  |            |
|    approx_kl            | 0.01154221 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.912     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0691     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 0.257      |
----------------------------------------
Eval num_timesteps=33500, episode_reward=10.24 +/- 2.81
Episode length: 211.16 +/- 54.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=10.30 +/- 3.43
Episode length: 210.40 +/- 63.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=10.40 +/- 2.93
Episode length: 218.36 +/- 60.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 3.67     |
| time/              |          |
|    fps             | 91       |
|    iterations      | 17       |
|    time_elapsed    | 380      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=12.46 +/- 3.71
Episode length: 255.66 +/- 66.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 12.5        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.010097943 |
|    clip_fraction        | 0.0988      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.884      |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.041       |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.252       |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=13.22 +/- 3.36
Episode length: 273.08 +/- 65.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
New best mean reward!
Eval num_timesteps=36000, episode_reward=12.52 +/- 3.60
Episode length: 258.96 +/- 67.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=12.66 +/- 3.08
Episode length: 255.40 +/- 55.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 4.05     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 18       |
|    time_elapsed    | 416      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=15.12 +/- 3.43
Episode length: 294.78 +/- 63.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 295         |
|    mean_reward          | 15.1        |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.009358723 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.102       |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.277       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=37500, episode_reward=14.90 +/- 4.12
Episode length: 286.10 +/- 73.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=15.90 +/- 3.59
Episode length: 303.88 +/- 61.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
New best mean reward!
Eval num_timesteps=38500, episode_reward=14.66 +/- 3.61
Episode length: 276.34 +/- 66.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 4.42     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 19       |
|    time_elapsed    | 456      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=13.70 +/- 3.69
Episode length: 264.06 +/- 70.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 264         |
|    mean_reward          | 13.7        |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.008704695 |
|    clip_fraction        | 0.0901      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0847      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.205       |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=13.84 +/- 3.41
Episode length: 266.00 +/- 52.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=13.28 +/- 3.76
Episode length: 255.44 +/- 62.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=13.86 +/- 3.72
Episode length: 261.10 +/- 65.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 13.9     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 4.83     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 20       |
|    time_elapsed    | 494      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=13.78 +/- 2.78
Episode length: 266.84 +/- 52.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 267         |
|    mean_reward          | 13.8        |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.008933564 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0442      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.226       |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=13.48 +/- 3.21
Episode length: 265.82 +/- 56.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=13.94 +/- 3.20
Episode length: 268.16 +/- 57.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 13.9     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=14.20 +/- 3.25
Episode length: 273.24 +/- 56.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=13.92 +/- 3.17
Episode length: 270.26 +/- 58.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 13.9     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 5.22     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 21       |
|    time_elapsed    | 541      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=14.32 +/- 2.73
Episode length: 271.42 +/- 45.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 14.3        |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.009853509 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.114       |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.293       |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=14.26 +/- 2.97
Episode length: 269.92 +/- 53.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=14.38 +/- 3.22
Episode length: 271.72 +/- 56.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=14.34 +/- 2.62
Episode length: 275.92 +/- 49.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 5.64     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 22       |
|    time_elapsed    | 579      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=14.08 +/- 3.23
Episode length: 274.04 +/- 59.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 14.1        |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.009092808 |
|    clip_fraction        | 0.094       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0785      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.321       |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=14.88 +/- 2.93
Episode length: 288.98 +/- 56.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=12.78 +/- 3.63
Episode length: 244.92 +/- 69.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=13.60 +/- 3.36
Episode length: 261.68 +/- 59.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 6.07     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 23       |
|    time_elapsed    | 616      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=13.00 +/- 3.61
Episode length: 253.08 +/- 64.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | 13          |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.011374796 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.732      |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.217       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.32        |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=12.94 +/- 3.87
Episode length: 245.98 +/- 67.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=13.58 +/- 3.61
Episode length: 254.84 +/- 64.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=13.54 +/- 3.45
Episode length: 263.30 +/- 55.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 6.53     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 24       |
|    time_elapsed    | 652      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=13.92 +/- 3.43
Episode length: 262.18 +/- 63.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 262         |
|    mean_reward          | 13.9        |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.008141274 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.129       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.336       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=13.44 +/- 3.33
Episode length: 249.36 +/- 55.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 13.4     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=14.24 +/- 3.61
Episode length: 264.36 +/- 64.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=14.60 +/- 3.10
Episode length: 274.58 +/- 54.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 6.8      |
| time/              |          |
|    fps             | 74       |
|    iterations      | 25       |
|    time_elapsed    | 688      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=15.10 +/- 3.23
Episode length: 278.62 +/- 54.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 279         |
|    mean_reward          | 15.1        |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.011411574 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.687      |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.122       |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.302       |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=14.16 +/- 3.25
Episode length: 256.30 +/- 56.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=15.10 +/- 2.92
Episode length: 278.44 +/- 52.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=14.36 +/- 3.55
Episode length: 268.84 +/- 61.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 7.32     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 26       |
|    time_elapsed    | 726      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=14.94 +/- 3.27
Episode length: 270.02 +/- 57.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | 14.9        |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.008247733 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.114       |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.306       |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=14.66 +/- 3.40
Episode length: 276.04 +/- 60.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=14.60 +/- 3.03
Episode length: 269.46 +/- 51.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=14.10 +/- 3.34
Episode length: 266.86 +/- 54.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 7.49     |
| time/              |          |
|    fps             | 72       |
|    iterations      | 27       |
|    time_elapsed    | 763      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=14.92 +/- 3.12
Episode length: 273.94 +/- 54.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 14.9        |
| time/                   |             |
|    total_timesteps      | 55500       |
| train/                  |             |
|    approx_kl            | 0.007997858 |
|    clip_fraction        | 0.0818      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0952      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00968    |
|    value_loss           | 0.351       |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=14.32 +/- 3.55
Episode length: 266.08 +/- 60.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=14.46 +/- 3.28
Episode length: 265.68 +/- 56.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=15.02 +/- 3.64
Episode length: 279.42 +/- 63.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 7.65     |
| time/              |          |
|    fps             | 71       |
|    iterations      | 28       |
|    time_elapsed    | 801      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=12.92 +/- 2.60
Episode length: 250.68 +/- 49.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | 12.9        |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.010373978 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.129       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.341       |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=13.42 +/- 2.74
Episode length: 259.44 +/- 44.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 13.4     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=13.56 +/- 2.50
Episode length: 258.92 +/- 47.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=12.98 +/- 2.69
Episode length: 252.30 +/- 46.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 7.88     |
| time/              |          |
|    fps             | 70       |
|    iterations      | 29       |
|    time_elapsed    | 836      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=12.56 +/- 2.15
Episode length: 240.92 +/- 38.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 12.6        |
| time/                   |             |
|    total_timesteps      | 59500       |
| train/                  |             |
|    approx_kl            | 0.010269677 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.58       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0931      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.28        |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=13.10 +/- 2.51
Episode length: 256.72 +/- 44.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=12.52 +/- 2.59
Episode length: 241.18 +/- 47.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=12.26 +/- 2.63
Episode length: 238.60 +/- 46.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 8.09     |
| time/              |          |
|    fps             | 70       |
|    iterations      | 30       |
|    time_elapsed    | 870      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=13.30 +/- 2.29
Episode length: 255.30 +/- 43.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 13.3        |
| time/                   |             |
|    total_timesteps      | 61500       |
| train/                  |             |
|    approx_kl            | 0.011569519 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.14        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.323       |
-----------------------------------------
Eval num_timesteps=62000, episode_reward=12.34 +/- 2.67
Episode length: 238.68 +/- 45.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=12.88 +/- 2.84
Episode length: 245.84 +/- 46.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=12.48 +/- 3.03
Episode length: 239.76 +/- 54.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 8.15     |
| time/              |          |
|    fps             | 70       |
|    iterations      | 31       |
|    time_elapsed    | 905      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=12.60 +/- 2.22
Episode length: 240.48 +/- 41.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 240        |
|    mean_reward          | 12.6       |
| time/                   |            |
|    total_timesteps      | 63500      |
| train/                  |            |
|    approx_kl            | 0.00992121 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.556     |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0342     |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 0.247      |
----------------------------------------
Eval num_timesteps=64000, episode_reward=12.80 +/- 2.73
Episode length: 241.16 +/- 46.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=12.36 +/- 2.88
Episode length: 234.90 +/- 48.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=12.10 +/- 2.35
Episode length: 235.10 +/- 44.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=11.90 +/- 2.78
Episode length: 231.50 +/- 48.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 8.28     |
| time/              |          |
|    fps             | 69       |
|    iterations      | 32       |
|    time_elapsed    | 946      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=13.82 +/- 2.25
Episode length: 262.58 +/- 38.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 263         |
|    mean_reward          | 13.8        |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.010492287 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.558      |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0656      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.284       |
-----------------------------------------
Eval num_timesteps=66500, episode_reward=13.62 +/- 2.73
Episode length: 263.10 +/- 50.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=13.78 +/- 2.80
Episode length: 262.86 +/- 51.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=12.92 +/- 2.57
Episode length: 250.76 +/- 47.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 8.5      |
| time/              |          |
|    fps             | 68       |
|    iterations      | 33       |
|    time_elapsed    | 983      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=13.30 +/- 2.76
Episode length: 251.74 +/- 54.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 252         |
|    mean_reward          | 13.3        |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.013259353 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0917      |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.298       |
-----------------------------------------
Eval num_timesteps=68500, episode_reward=13.06 +/- 2.50
Episode length: 244.54 +/- 41.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=13.26 +/- 2.14
Episode length: 253.20 +/- 36.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=13.40 +/- 2.56
Episode length: 250.28 +/- 45.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 13.4     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 8.75     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 34       |
|    time_elapsed    | 1018     |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=12.26 +/- 1.92
Episode length: 233.36 +/- 34.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 233         |
|    mean_reward          | 12.3        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.013273194 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0883      |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.286       |
-----------------------------------------
Eval num_timesteps=70500, episode_reward=12.40 +/- 2.16
Episode length: 241.88 +/- 35.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=12.42 +/- 2.37
Episode length: 236.84 +/- 41.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=11.98 +/- 2.63
Episode length: 231.26 +/- 45.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 8.73     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 35       |
|    time_elapsed    | 1051     |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=13.22 +/- 2.68
Episode length: 246.46 +/- 45.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 246        |
|    mean_reward          | 13.2       |
| time/                   |            |
|    total_timesteps      | 72000      |
| train/                  |            |
|    approx_kl            | 0.01184351 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.538     |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.108      |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 0.339      |
----------------------------------------
Eval num_timesteps=72500, episode_reward=13.46 +/- 2.60
Episode length: 251.76 +/- 44.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=12.98 +/- 2.95
Episode length: 244.98 +/- 47.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 13       |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=13.18 +/- 2.81
Episode length: 251.84 +/- 47.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 8.88     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 36       |
|    time_elapsed    | 1086     |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=16.28 +/- 2.41
Episode length: 311.20 +/- 43.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 311        |
|    mean_reward          | 16.3       |
| time/                   |            |
|    total_timesteps      | 74000      |
| train/                  |            |
|    approx_kl            | 0.00904936 |
|    clip_fraction        | 0.0926     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.503     |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0565     |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0148    |
|    value_loss           | 0.315      |
----------------------------------------
New best mean reward!
Eval num_timesteps=74500, episode_reward=15.54 +/- 3.90
Episode length: 294.96 +/- 66.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=16.38 +/- 2.07
Episode length: 306.14 +/- 37.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
New best mean reward!
Eval num_timesteps=75500, episode_reward=15.18 +/- 3.24
Episode length: 288.74 +/- 59.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 9.27     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 37       |
|    time_elapsed    | 1127     |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=17.70 +/- 2.22
Episode length: 321.04 +/- 40.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 321         |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.010509315 |
|    clip_fraction        | 0.0897      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.441      |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.132       |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.319       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=76500, episode_reward=18.30 +/- 2.40
Episode length: 331.10 +/- 43.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 331      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
New best mean reward!
Eval num_timesteps=77000, episode_reward=18.72 +/- 2.28
Episode length: 333.78 +/- 39.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 334      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
New best mean reward!
Eval num_timesteps=77500, episode_reward=18.34 +/- 2.31
Episode length: 329.84 +/- 39.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 9.43     |
| time/              |          |
|    fps             | 66       |
|    iterations      | 38       |
|    time_elapsed    | 1173     |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=16.60 +/- 2.86
Episode length: 303.02 +/- 50.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 303         |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.011505641 |
|    clip_fraction        | 0.083       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.4        |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.228       |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00839    |
|    value_loss           | 0.362       |
-----------------------------------------
Eval num_timesteps=78500, episode_reward=15.62 +/- 2.71
Episode length: 283.14 +/- 47.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=15.98 +/- 2.18
Episode length: 292.86 +/- 39.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=16.02 +/- 2.38
Episode length: 294.04 +/- 43.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 9.64     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 39       |
|    time_elapsed    | 1214     |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=13.68 +/- 2.38
Episode length: 253.80 +/- 39.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 254        |
|    mean_reward          | 13.7       |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.00988248 |
|    clip_fraction        | 0.0939     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0808     |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.263      |
----------------------------------------
Eval num_timesteps=80500, episode_reward=14.20 +/- 2.37
Episode length: 267.80 +/- 46.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=13.24 +/- 2.72
Episode length: 250.14 +/- 49.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=13.96 +/- 1.89
Episode length: 262.30 +/- 42.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 9.87     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 40       |
|    time_elapsed    | 1250     |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=15.98 +/- 3.39
Episode length: 287.04 +/- 57.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 287         |
|    mean_reward          | 16          |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.008411538 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.448      |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0905      |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.291       |
-----------------------------------------
Eval num_timesteps=82500, episode_reward=16.24 +/- 3.04
Episode length: 300.90 +/- 52.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 301      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=17.26 +/- 3.25
Episode length: 304.64 +/- 54.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 305      |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=16.48 +/- 2.81
Episode length: 294.36 +/- 45.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 210      |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 41       |
|    time_elapsed    | 1291     |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=12.30 +/- 4.78
Episode length: 235.16 +/- 75.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 235         |
|    mean_reward          | 12.3        |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.009035423 |
|    clip_fraction        | 0.0764      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.395      |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.117       |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.33        |
-----------------------------------------
Eval num_timesteps=84500, episode_reward=12.44 +/- 4.36
Episode length: 243.10 +/- 78.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=13.26 +/- 4.49
Episode length: 261.60 +/- 76.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=14.02 +/- 3.29
Episode length: 268.14 +/- 62.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=13.18 +/- 4.39
Episode length: 251.22 +/- 71.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 64       |
|    iterations      | 42       |
|    time_elapsed    | 1334     |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=13.58 +/- 2.14
Episode length: 254.80 +/- 37.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 13.6        |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.007824628 |
|    clip_fraction        | 0.0792      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.43       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0923      |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.27        |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=13.74 +/- 2.59
Episode length: 252.46 +/- 45.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 13.7     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=13.78 +/- 2.04
Episode length: 257.06 +/- 33.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=14.16 +/- 2.17
Episode length: 258.54 +/- 36.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    fps             | 64       |
|    iterations      | 43       |
|    time_elapsed    | 1370     |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=14.34 +/- 1.94
Episode length: 264.94 +/- 30.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 265         |
|    mean_reward          | 14.3        |
| time/                   |             |
|    total_timesteps      | 88500       |
| train/                  |             |
|    approx_kl            | 0.009734301 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.419      |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0948      |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.273       |
-----------------------------------------
Eval num_timesteps=89000, episode_reward=14.54 +/- 2.09
Episode length: 271.34 +/- 38.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=14.16 +/- 2.10
Episode length: 263.26 +/- 37.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=14.16 +/- 2.02
Episode length: 265.42 +/- 38.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 64       |
|    iterations      | 44       |
|    time_elapsed    | 1407     |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=13.86 +/- 1.83
Episode length: 255.42 +/- 30.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | 13.9         |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0131677585 |
|    clip_fraction        | 0.092        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.401       |
|    explained_variance   | 0.886        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.132        |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.011       |
|    value_loss           | 0.292        |
------------------------------------------
Eval num_timesteps=91000, episode_reward=13.86 +/- 2.16
Episode length: 257.66 +/- 32.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 13.9     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=13.98 +/- 2.74
Episode length: 256.22 +/- 46.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=13.70 +/- 2.52
Episode length: 253.88 +/- 41.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 13.7     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 63       |
|    iterations      | 45       |
|    time_elapsed    | 1443     |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=15.20 +/- 2.26
Episode length: 273.34 +/- 37.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 273         |
|    mean_reward          | 15.2        |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.011968891 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0626      |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.264       |
-----------------------------------------
Eval num_timesteps=93000, episode_reward=14.16 +/- 2.96
Episode length: 264.92 +/- 48.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=14.66 +/- 2.24
Episode length: 267.62 +/- 40.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=14.46 +/- 2.43
Episode length: 266.18 +/- 38.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 11.4     |
| time/              |          |
|    fps             | 63       |
|    iterations      | 46       |
|    time_elapsed    | 1480     |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=14.90 +/- 2.33
Episode length: 268.82 +/- 38.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 269         |
|    mean_reward          | 14.9        |
| time/                   |             |
|    total_timesteps      | 94500       |
| train/                  |             |
|    approx_kl            | 0.010778594 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.39       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0442      |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.245       |
-----------------------------------------
Eval num_timesteps=95000, episode_reward=15.00 +/- 1.88
Episode length: 270.40 +/- 35.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=14.76 +/- 2.05
Episode length: 271.60 +/- 38.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=14.50 +/- 2.38
Episode length: 262.68 +/- 39.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 11.7     |
| time/              |          |
|    fps             | 63       |
|    iterations      | 47       |
|    time_elapsed    | 1517     |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=14.80 +/- 1.72
Episode length: 269.34 +/- 26.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 269         |
|    mean_reward          | 14.8        |
| time/                   |             |
|    total_timesteps      | 96500       |
| train/                  |             |
|    approx_kl            | 0.010346879 |
|    clip_fraction        | 0.087       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.342      |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.133       |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.325       |
-----------------------------------------
Eval num_timesteps=97000, episode_reward=14.30 +/- 1.77
Episode length: 261.96 +/- 23.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=14.24 +/- 1.67
Episode length: 265.32 +/- 30.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=14.26 +/- 2.01
Episode length: 255.96 +/- 29.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    fps             | 63       |
|    iterations      | 48       |
|    time_elapsed    | 1554     |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=14.00 +/- 2.37
Episode length: 258.50 +/- 38.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 258         |
|    mean_reward          | 14          |
| time/                   |             |
|    total_timesteps      | 98500       |
| train/                  |             |
|    approx_kl            | 0.013660412 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.345      |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0701      |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.247       |
-----------------------------------------
Eval num_timesteps=99000, episode_reward=14.58 +/- 2.15
Episode length: 267.66 +/- 36.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=13.84 +/- 2.00
Episode length: 253.90 +/- 36.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=14.34 +/- 2.09
Episode length: 262.50 +/- 27.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 231      |
|    ep_rew_mean     | 12       |
| time/              |          |
|    fps             | 63       |
|    iterations      | 49       |
|    time_elapsed    | 1591     |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=14.46 +/- 2.35
Episode length: 260.34 +/- 40.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 260         |
|    mean_reward          | 14.5        |
| time/                   |             |
|    total_timesteps      | 100500      |
| train/                  |             |
|    approx_kl            | 0.011976948 |
|    clip_fraction        | 0.0786      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.346      |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0445      |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00918    |
|    value_loss           | 0.268       |
-----------------------------------------
Eval num_timesteps=101000, episode_reward=14.40 +/- 2.25
Episode length: 263.16 +/- 31.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=13.98 +/- 2.23
Episode length: 252.14 +/- 34.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=14.10 +/- 1.92
Episode length: 259.62 +/- 30.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 12.1     |
| time/              |          |
|    fps             | 62       |
|    iterations      | 50       |
|    time_elapsed    | 1627     |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=13.82 +/- 2.02
Episode length: 251.10 +/- 33.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | 13.8        |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.011042641 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.343      |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0502      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.236       |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=14.20 +/- 2.06
Episode length: 260.90 +/- 30.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=14.34 +/- 2.60
Episode length: 261.16 +/- 43.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=14.00 +/- 2.36
Episode length: 256.86 +/- 42.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 237      |
|    ep_rew_mean     | 12.3     |
| time/              |          |
|    fps             | 62       |
|    iterations      | 51       |
|    time_elapsed    | 1663     |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=13.74 +/- 2.33
Episode length: 256.14 +/- 37.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 13.7        |
| time/                   |             |
|    total_timesteps      | 104500      |
| train/                  |             |
|    approx_kl            | 0.013955757 |
|    clip_fraction        | 0.0966      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.364      |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0428      |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.206       |
-----------------------------------------
Eval num_timesteps=105000, episode_reward=13.80 +/- 2.05
Episode length: 253.86 +/- 32.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=14.32 +/- 2.77
Episode length: 265.36 +/- 44.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=14.36 +/- 2.13
Episode length: 270.96 +/- 48.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 62       |
|    iterations      | 52       |
|    time_elapsed    | 1700     |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=14.26 +/- 1.60
Episode length: 263.36 +/- 30.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 263         |
|    mean_reward          | 14.3        |
| time/                   |             |
|    total_timesteps      | 106500      |
| train/                  |             |
|    approx_kl            | 0.011520172 |
|    clip_fraction        | 0.0952      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.369      |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0433      |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.195       |
-----------------------------------------
Eval num_timesteps=107000, episode_reward=14.88 +/- 2.02
Episode length: 268.56 +/- 33.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=14.50 +/- 2.39
Episode length: 263.74 +/- 39.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=14.74 +/- 2.30
Episode length: 259.84 +/- 40.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=15.22 +/- 2.03
Episode length: 270.60 +/- 29.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 12.6     |
| time/              |          |
|    fps             | 62       |
|    iterations      | 53       |
|    time_elapsed    | 1745     |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=15.22 +/- 2.41
Episode length: 271.66 +/- 40.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | 15.2        |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.012348324 |
|    clip_fraction        | 0.0966      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.391      |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0733      |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.235       |
-----------------------------------------
Eval num_timesteps=109500, episode_reward=14.74 +/- 2.16
Episode length: 262.70 +/- 32.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=15.60 +/- 2.01
Episode length: 284.52 +/- 33.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=14.76 +/- 1.64
Episode length: 265.78 +/- 26.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 237      |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 62       |
|    iterations      | 54       |
|    time_elapsed    | 1783     |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=14.90 +/- 2.85
Episode length: 272.10 +/- 51.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 272        |
|    mean_reward          | 14.9       |
| time/                   |            |
|    total_timesteps      | 111000     |
| train/                  |            |
|    approx_kl            | 0.01521491 |
|    clip_fraction        | 0.0769     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.325     |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.121      |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 0.29       |
----------------------------------------
Eval num_timesteps=111500, episode_reward=15.94 +/- 2.24
Episode length: 282.14 +/- 37.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=15.66 +/- 2.23
Episode length: 280.72 +/- 39.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=15.34 +/- 2.46
Episode length: 278.66 +/- 39.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 243      |
|    ep_rew_mean     | 12.8     |
| time/              |          |
|    fps             | 61       |
|    iterations      | 55       |
|    time_elapsed    | 1821     |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=14.72 +/- 2.70
Episode length: 270.86 +/- 46.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 14.7        |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.014482746 |
|    clip_fraction        | 0.0775      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.327      |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0275      |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.199       |
-----------------------------------------
Eval num_timesteps=113500, episode_reward=15.96 +/- 2.18
Episode length: 283.46 +/- 36.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=15.14 +/- 2.76
Episode length: 269.78 +/- 44.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=14.52 +/- 2.32
Episode length: 263.42 +/- 42.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 244      |
|    ep_rew_mean     | 12.9     |
| time/              |          |
|    fps             | 61       |
|    iterations      | 56       |
|    time_elapsed    | 1859     |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=15.32 +/- 2.35
Episode length: 280.46 +/- 36.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 280        |
|    mean_reward          | 15.3       |
| time/                   |            |
|    total_timesteps      | 115000     |
| train/                  |            |
|    approx_kl            | 0.01216846 |
|    clip_fraction        | 0.0869     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.318     |
|    explained_variance   | 0.921      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0537     |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 0.247      |
----------------------------------------
Eval num_timesteps=115500, episode_reward=15.68 +/- 2.20
Episode length: 277.60 +/- 34.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=15.46 +/- 2.41
Episode length: 278.84 +/- 39.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=15.34 +/- 2.05
Episode length: 278.24 +/- 33.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 61       |
|    iterations      | 57       |
|    time_elapsed    | 1898     |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=15.64 +/- 1.80
Episode length: 277.36 +/- 34.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 15.6        |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.012952179 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0525      |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.228       |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=15.04 +/- 2.32
Episode length: 267.98 +/- 36.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=15.20 +/- 2.66
Episode length: 271.08 +/- 44.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=15.68 +/- 2.84
Episode length: 277.62 +/- 45.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 61       |
|    iterations      | 58       |
|    time_elapsed    | 1936     |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=15.46 +/- 2.48
Episode length: 279.66 +/- 39.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.012361625 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0679      |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.202       |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=15.38 +/- 2.18
Episode length: 280.02 +/- 37.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=15.20 +/- 2.10
Episode length: 272.64 +/- 37.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=14.72 +/- 2.58
Episode length: 268.06 +/- 43.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 61       |
|    iterations      | 59       |
|    time_elapsed    | 1974     |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=16.24 +/- 1.97
Episode length: 289.04 +/- 32.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 289         |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.013164421 |
|    clip_fraction        | 0.0901      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.395      |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0488      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.167       |
-----------------------------------------
Eval num_timesteps=121500, episode_reward=15.24 +/- 2.75
Episode length: 276.02 +/- 44.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=16.12 +/- 1.99
Episode length: 289.66 +/- 34.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=16.16 +/- 2.72
Episode length: 292.68 +/- 46.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 61       |
|    iterations      | 60       |
|    time_elapsed    | 2014     |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=16.62 +/- 2.23
Episode length: 291.94 +/- 37.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 292         |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.008879574 |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.309      |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0703      |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00778    |
|    value_loss           | 0.187       |
-----------------------------------------
Eval num_timesteps=123500, episode_reward=16.20 +/- 2.65
Episode length: 287.42 +/- 43.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=15.94 +/- 2.91
Episode length: 282.78 +/- 43.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=16.08 +/- 2.66
Episode length: 286.16 +/- 47.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 253      |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 60       |
|    iterations      | 61       |
|    time_elapsed    | 2053     |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=16.96 +/- 1.80
Episode length: 299.66 +/- 36.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | 17          |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.011118868 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.3        |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0308      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.165       |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=16.50 +/- 2.17
Episode length: 293.92 +/- 37.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=16.72 +/- 2.48
Episode length: 296.24 +/- 38.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 296      |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=16.24 +/- 2.27
Episode length: 301.16 +/- 40.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 301      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 252      |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 60       |
|    iterations      | 62       |
|    time_elapsed    | 2095     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=16.32 +/- 2.53
Episode length: 299.24 +/- 40.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 299         |
|    mean_reward          | 16.3        |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.010726813 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.313      |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.127       |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.307       |
-----------------------------------------
Eval num_timesteps=127500, episode_reward=16.82 +/- 2.24
Episode length: 299.98 +/- 41.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=16.16 +/- 2.15
Episode length: 296.96 +/- 39.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=16.56 +/- 1.99
Episode length: 305.40 +/- 40.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 305      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=16.48 +/- 2.10
Episode length: 297.20 +/- 34.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 253      |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 60       |
|    iterations      | 63       |
|    time_elapsed    | 2146     |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=16.60 +/- 2.43
Episode length: 295.42 +/- 39.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 295         |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.010949022 |
|    clip_fraction        | 0.0865      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.31       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.032       |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.188       |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=16.36 +/- 2.27
Episode length: 289.48 +/- 38.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=16.50 +/- 2.05
Episode length: 297.56 +/- 36.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=16.30 +/- 2.06
Episode length: 294.56 +/- 36.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 253      |
|    ep_rew_mean     | 13.6     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 64       |
|    time_elapsed    | 2186     |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=16.04 +/- 2.53
Episode length: 283.40 +/- 47.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 283         |
|    mean_reward          | 16          |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.011419455 |
|    clip_fraction        | 0.081       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.294      |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.113       |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.224       |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=15.60 +/- 2.23
Episode length: 282.12 +/- 43.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=15.66 +/- 2.58
Episode length: 281.18 +/- 39.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=16.04 +/- 2.37
Episode length: 291.48 +/- 41.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 291      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 255      |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 65       |
|    time_elapsed    | 2226     |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=16.92 +/- 1.85
Episode length: 305.16 +/- 38.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 305         |
|    mean_reward          | 16.9        |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.012780385 |
|    clip_fraction        | 0.0843      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.287      |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.083       |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.213       |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=16.08 +/- 2.49
Episode length: 288.62 +/- 41.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=16.50 +/- 2.76
Episode length: 297.58 +/- 48.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=16.60 +/- 2.51
Episode length: 296.60 +/- 44.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 260      |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 66       |
|    time_elapsed    | 2267     |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=16.06 +/- 3.07
Episode length: 285.00 +/- 56.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | 16.1         |
| time/                   |              |
|    total_timesteps      | 135500       |
| train/                  |              |
|    approx_kl            | 0.0124308765 |
|    clip_fraction        | 0.0798       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.27        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0619       |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.0133      |
|    value_loss           | 0.16         |
------------------------------------------
Eval num_timesteps=136000, episode_reward=16.96 +/- 2.33
Episode length: 304.46 +/- 40.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=16.60 +/- 2.47
Episode length: 299.16 +/- 43.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 299      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=16.54 +/- 2.82
Episode length: 293.90 +/- 47.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 263      |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 67       |
|    time_elapsed    | 2308     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=16.84 +/- 2.19
Episode length: 307.70 +/- 46.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 308         |
|    mean_reward          | 16.8        |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.008308726 |
|    clip_fraction        | 0.0709      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.261      |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0368      |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.178       |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=16.96 +/- 2.61
Episode length: 304.22 +/- 46.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=16.62 +/- 2.83
Episode length: 300.66 +/- 49.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 301      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=17.06 +/- 2.71
Episode length: 306.98 +/- 46.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 264      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 68       |
|    time_elapsed    | 2350     |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=18.54 +/- 3.01
Episode length: 327.10 +/- 50.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 327         |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 139500      |
| train/                  |             |
|    approx_kl            | 0.009955811 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.077       |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.175       |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=18.80 +/- 3.90
Episode length: 326.46 +/- 64.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 326      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
New best mean reward!
Eval num_timesteps=140500, episode_reward=18.96 +/- 2.89
Episode length: 332.92 +/- 48.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 333      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
New best mean reward!
Eval num_timesteps=141000, episode_reward=19.28 +/- 2.88
Episode length: 341.80 +/- 44.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 342      |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 270      |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 58       |
|    iterations      | 69       |
|    time_elapsed    | 2396     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=16.60 +/- 2.16
Episode length: 295.18 +/- 44.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 295         |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.008927064 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.236      |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0254      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.173       |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=16.42 +/- 2.32
Episode length: 300.38 +/- 42.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=16.48 +/- 2.67
Episode length: 293.86 +/- 50.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=17.10 +/- 2.26
Episode length: 306.12 +/- 42.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 58       |
|    iterations      | 70       |
|    time_elapsed    | 2438     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=18.58 +/- 3.40
Episode length: 325.58 +/- 60.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 326         |
|    mean_reward          | 18.6        |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.012906442 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.273      |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.101       |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.225       |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=19.18 +/- 2.79
Episode length: 336.30 +/- 52.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 336      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=19.40 +/- 2.99
Episode length: 334.78 +/- 52.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 335      |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
New best mean reward!
Eval num_timesteps=145000, episode_reward=18.84 +/- 2.99
Episode length: 326.70 +/- 55.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 327      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 58       |
|    iterations      | 71       |
|    time_elapsed    | 2483     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=19.24 +/- 3.06
Episode length: 333.10 +/- 52.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 333         |
|    mean_reward          | 19.2        |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.011339647 |
|    clip_fraction        | 0.0727      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.26       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.031       |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.152       |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=18.78 +/- 3.43
Episode length: 322.98 +/- 53.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 323      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=19.00 +/- 3.32
Episode length: 332.46 +/- 56.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 332      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=18.46 +/- 3.07
Episode length: 323.00 +/- 52.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 323      |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 276      |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 58       |
|    iterations      | 72       |
|    time_elapsed    | 2528     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=18.30 +/- 3.67
Episode length: 319.86 +/- 63.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 320        |
|    mean_reward          | 18.3       |
| time/                   |            |
|    total_timesteps      | 147500     |
| train/                  |            |
|    approx_kl            | 0.01590341 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.299     |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0592     |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.186      |
----------------------------------------
Eval num_timesteps=148000, episode_reward=19.08 +/- 2.52
Episode length: 330.84 +/- 46.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 331      |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=18.86 +/- 2.98
Episode length: 326.02 +/- 46.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 326      |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=18.64 +/- 3.02
Episode length: 330.34 +/- 54.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=18.88 +/- 2.22
Episode length: 330.00 +/- 43.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 278      |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 57       |
|    iterations      | 73       |
|    time_elapsed    | 2584     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=19.22 +/- 2.82
Episode length: 332.80 +/- 48.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 333         |
|    mean_reward          | 19.2        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.012713185 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.285      |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.35e-05    |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.154       |
-----------------------------------------
Eval num_timesteps=150500, episode_reward=19.38 +/- 2.39
Episode length: 331.30 +/- 41.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 331      |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=19.26 +/- 2.71
Episode length: 330.60 +/- 45.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 331      |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=19.80 +/- 2.47
Episode length: 337.86 +/- 41.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 338      |
|    mean_reward     | 19.8     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 283      |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 57       |
|    iterations      | 74       |
|    time_elapsed    | 2630     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=19.52 +/- 2.53
Episode length: 332.62 +/- 42.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 333         |
|    mean_reward          | 19.5        |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.012111137 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.289      |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0465      |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.185       |
-----------------------------------------
Eval num_timesteps=152500, episode_reward=18.82 +/- 3.36
Episode length: 328.42 +/- 61.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 328      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=19.58 +/- 2.30
Episode length: 343.28 +/- 38.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 343      |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=19.62 +/- 2.43
Episode length: 338.86 +/- 35.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 339      |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 57       |
|    iterations      | 75       |
|    time_elapsed    | 2676     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=18.92 +/- 3.11
Episode length: 329.82 +/- 53.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 330         |
|    mean_reward          | 18.9        |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.011784408 |
|    clip_fraction        | 0.0882      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.309      |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0288      |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.167       |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=18.72 +/- 3.29
Episode length: 322.66 +/- 56.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 323      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=19.86 +/- 2.70
Episode length: 345.76 +/- 47.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 346      |
|    mean_reward     | 19.9     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
New best mean reward!
Eval num_timesteps=155500, episode_reward=19.62 +/- 2.95
Episode length: 334.96 +/- 51.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 335      |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 57       |
|    iterations      | 76       |
|    time_elapsed    | 2721     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=20.46 +/- 3.13
Episode length: 352.12 +/- 50.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 352         |
|    mean_reward          | 20.5        |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.010202006 |
|    clip_fraction        | 0.0667      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.271      |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0304      |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.155       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=156500, episode_reward=19.68 +/- 3.04
Episode length: 343.14 +/- 52.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 343      |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=20.50 +/- 2.50
Episode length: 352.04 +/- 41.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 352      |
|    mean_reward     | 20.5     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
New best mean reward!
Eval num_timesteps=157500, episode_reward=20.30 +/- 2.61
Episode length: 349.78 +/- 42.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 350      |
|    mean_reward     | 20.3     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 56       |
|    iterations      | 77       |
|    time_elapsed    | 2769     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=20.48 +/- 3.22
Episode length: 360.04 +/- 55.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 360         |
|    mean_reward          | 20.5        |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.013115449 |
|    clip_fraction        | 0.0728      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.259      |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0277      |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.164       |
-----------------------------------------
Eval num_timesteps=158500, episode_reward=20.02 +/- 3.65
Episode length: 356.78 +/- 64.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 357      |
|    mean_reward     | 20       |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=19.52 +/- 3.69
Episode length: 345.36 +/- 67.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 345      |
|    mean_reward     | 19.5     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=19.48 +/- 3.56
Episode length: 348.28 +/- 65.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 348      |
|    mean_reward     | 19.5     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 56       |
|    iterations      | 78       |
|    time_elapsed    | 2819     |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=19.68 +/- 3.65
Episode length: 354.70 +/- 66.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 19.7        |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.012946812 |
|    clip_fraction        | 0.0695      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0478      |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.154       |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=19.46 +/- 4.05
Episode length: 343.30 +/- 71.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 343      |
|    mean_reward     | 19.5     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=20.62 +/- 2.92
Episode length: 364.08 +/- 56.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 364      |
|    mean_reward     | 20.6     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
New best mean reward!
Eval num_timesteps=161500, episode_reward=19.82 +/- 4.13
Episode length: 346.90 +/- 66.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 347      |
|    mean_reward     | 19.8     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 56       |
|    iterations      | 79       |
|    time_elapsed    | 2867     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=20.80 +/- 2.47
Episode length: 365.46 +/- 44.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 365         |
|    mean_reward          | 20.8        |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.008518377 |
|    clip_fraction        | 0.0656      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.23       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0466      |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.187       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=162500, episode_reward=19.46 +/- 4.00
Episode length: 344.36 +/- 64.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 344      |
|    mean_reward     | 19.5     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=20.84 +/- 2.56
Episode length: 365.24 +/- 47.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 365      |
|    mean_reward     | 20.8     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
New best mean reward!
Eval num_timesteps=163500, episode_reward=19.80 +/- 3.55
Episode length: 352.70 +/- 59.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 353      |
|    mean_reward     | 19.8     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 56       |
|    iterations      | 80       |
|    time_elapsed    | 2916     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=18.66 +/- 3.72
Episode length: 332.12 +/- 64.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 332         |
|    mean_reward          | 18.7        |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.014950639 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.223      |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0883      |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.201       |
-----------------------------------------
Eval num_timesteps=164500, episode_reward=19.82 +/- 3.06
Episode length: 349.76 +/- 56.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 350      |
|    mean_reward     | 19.8     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=20.28 +/- 2.52
Episode length: 356.18 +/- 44.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 356      |
|    mean_reward     | 20.3     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=19.60 +/- 3.49
Episode length: 343.32 +/- 55.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 343      |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 55       |
|    iterations      | 81       |
|    time_elapsed    | 2963     |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=18.30 +/- 4.08
Episode length: 323.74 +/- 71.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 324         |
|    mean_reward          | 18.3        |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.017432362 |
|    clip_fraction        | 0.0722      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.239      |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0724      |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.192       |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=18.88 +/- 3.92
Episode length: 332.84 +/- 69.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 333      |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=18.44 +/- 3.90
Episode length: 325.98 +/- 68.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 326      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=19.40 +/- 3.31
Episode length: 338.28 +/- 62.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 338      |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 55       |
|    iterations      | 82       |
|    time_elapsed    | 3008     |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=18.88 +/- 4.04
Episode length: 330.86 +/- 73.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 331         |
|    mean_reward          | 18.9        |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.012141293 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.249      |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0382      |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.196       |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=19.22 +/- 3.73
Episode length: 336.58 +/- 66.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=17.60 +/- 4.24
Episode length: 310.52 +/- 73.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 311      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=17.90 +/- 4.27
Episode length: 316.60 +/- 71.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 55       |
|    iterations      | 83       |
|    time_elapsed    | 3053     |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=18.40 +/- 4.06
Episode length: 322.24 +/- 69.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 322         |
|    mean_reward          | 18.4        |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.016778966 |
|    clip_fraction        | 0.0725      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.255      |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0284      |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.151       |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=18.80 +/- 3.49
Episode length: 333.30 +/- 55.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 333      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=18.60 +/- 3.94
Episode length: 320.28 +/- 67.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 320      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=18.74 +/- 3.42
Episode length: 330.00 +/- 58.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=18.92 +/- 2.83
Episode length: 332.68 +/- 45.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 333      |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 315      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 55       |
|    iterations      | 84       |
|    time_elapsed    | 3108     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=19.38 +/- 2.49
Episode length: 334.56 +/- 40.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 335         |
|    mean_reward          | 19.4        |
| time/                   |             |
|    total_timesteps      | 172500      |
| train/                  |             |
|    approx_kl            | 0.017526355 |
|    clip_fraction        | 0.0941      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.268      |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0369      |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.158       |
-----------------------------------------
Eval num_timesteps=173000, episode_reward=18.02 +/- 3.36
Episode length: 312.20 +/- 55.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 312      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=18.70 +/- 3.16
Episode length: 330.62 +/- 55.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 331      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=19.20 +/- 2.65
Episode length: 337.38 +/- 43.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 316      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 55       |
|    iterations      | 85       |
|    time_elapsed    | 3153     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=18.34 +/- 3.22
Episode length: 325.08 +/- 54.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 325         |
|    mean_reward          | 18.3        |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.022035537 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.28       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0504      |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.142       |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=18.60 +/- 3.23
Episode length: 320.04 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 320      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=17.96 +/- 3.12
Episode length: 315.42 +/- 53.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 315      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=19.06 +/- 3.05
Episode length: 330.34 +/- 48.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 321      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 55       |
|    iterations      | 86       |
|    time_elapsed    | 3198     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=18.68 +/- 2.66
Episode length: 322.88 +/- 45.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | 18.7        |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.014227533 |
|    clip_fraction        | 0.0929      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0348      |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.144       |
-----------------------------------------
Eval num_timesteps=177000, episode_reward=18.72 +/- 3.13
Episode length: 320.26 +/- 53.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 320      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=18.72 +/- 2.73
Episode length: 324.08 +/- 40.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 324      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=17.94 +/- 3.37
Episode length: 305.70 +/- 51.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 321      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 87       |
|    time_elapsed    | 3242     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=18.16 +/- 2.39
Episode length: 315.80 +/- 42.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 316         |
|    mean_reward          | 18.2        |
| time/                   |             |
|    total_timesteps      | 178500      |
| train/                  |             |
|    approx_kl            | 0.017150225 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.347      |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0718      |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.173       |
-----------------------------------------
Eval num_timesteps=179000, episode_reward=18.56 +/- 2.31
Episode length: 317.08 +/- 41.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=17.56 +/- 2.88
Episode length: 302.70 +/- 48.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=18.46 +/- 1.92
Episode length: 316.10 +/- 34.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 323      |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 88       |
|    time_elapsed    | 3285     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=16.82 +/- 3.30
Episode length: 286.26 +/- 58.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | 16.8        |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.014663184 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.419      |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0498      |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.15        |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=17.32 +/- 3.27
Episode length: 300.02 +/- 57.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=18.00 +/- 2.03
Episode length: 311.32 +/- 34.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 311      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=18.06 +/- 2.56
Episode length: 312.62 +/- 45.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 313      |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 322      |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 89       |
|    time_elapsed    | 3327     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=16.62 +/- 3.23
Episode length: 287.54 +/- 55.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.013069875 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00767     |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.129       |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=17.18 +/- 2.54
Episode length: 292.06 +/- 45.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 292      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=16.80 +/- 3.22
Episode length: 288.28 +/- 49.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=17.32 +/- 2.56
Episode length: 295.84 +/- 45.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 296      |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 314      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 90       |
|    time_elapsed    | 3368     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=17.60 +/- 2.68
Episode length: 301.34 +/- 36.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 301        |
|    mean_reward          | 17.6       |
| time/                   |            |
|    total_timesteps      | 184500     |
| train/                  |            |
|    approx_kl            | 0.01565628 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.425     |
|    explained_variance   | 0.931      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0444     |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.211      |
----------------------------------------
Eval num_timesteps=185000, episode_reward=16.94 +/- 3.34
Episode length: 293.92 +/- 58.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=17.60 +/- 3.05
Episode length: 302.04 +/- 50.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 302      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=17.20 +/- 2.56
Episode length: 295.30 +/- 42.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 91       |
|    time_elapsed    | 3410     |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=16.56 +/- 2.90
Episode length: 285.62 +/- 48.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 286        |
|    mean_reward          | 16.6       |
| time/                   |            |
|    total_timesteps      | 186500     |
| train/                  |            |
|    approx_kl            | 0.02111629 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.424     |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0208     |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 0.235      |
----------------------------------------
Eval num_timesteps=187000, episode_reward=16.34 +/- 3.01
Episode length: 280.34 +/- 48.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=16.16 +/- 2.94
Episode length: 282.10 +/- 47.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=16.40 +/- 3.16
Episode length: 281.98 +/- 50.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 92       |
|    time_elapsed    | 3449     |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=16.00 +/- 2.85
Episode length: 278.06 +/- 49.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 278         |
|    mean_reward          | 16          |
| time/                   |             |
|    total_timesteps      | 188500      |
| train/                  |             |
|    approx_kl            | 0.024981797 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.568      |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0398      |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.178       |
-----------------------------------------
Eval num_timesteps=189000, episode_reward=16.84 +/- 2.80
Episode length: 287.04 +/- 46.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=16.60 +/- 2.04
Episode length: 281.04 +/- 32.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=16.66 +/- 2.55
Episode length: 289.68 +/- 41.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 93       |
|    time_elapsed    | 3488     |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=16.44 +/- 2.54
Episode length: 284.60 +/- 44.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 285         |
|    mean_reward          | 16.4        |
| time/                   |             |
|    total_timesteps      | 190500      |
| train/                  |             |
|    approx_kl            | 0.017462878 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.53       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0642      |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.164       |
-----------------------------------------
Eval num_timesteps=191000, episode_reward=16.22 +/- 2.89
Episode length: 280.60 +/- 47.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=16.64 +/- 3.02
Episode length: 291.24 +/- 52.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 291      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=15.96 +/- 2.55
Episode length: 274.56 +/- 42.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=16.16 +/- 3.41
Episode length: 276.68 +/- 58.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 94       |
|    time_elapsed    | 3538     |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=16.18 +/- 4.12
Episode length: 279.36 +/- 68.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 279         |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.018110119 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.558      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0292      |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.155       |
-----------------------------------------
Eval num_timesteps=193500, episode_reward=16.50 +/- 4.23
Episode length: 286.72 +/- 70.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=16.62 +/- 4.29
Episode length: 285.74 +/- 69.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=15.74 +/- 4.88
Episode length: 270.00 +/- 76.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 289      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 95       |
|    time_elapsed    | 3578     |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=17.78 +/- 3.36
Episode length: 302.22 +/- 49.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 302         |
|    mean_reward          | 17.8        |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.022855436 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.55       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00971     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.158       |
-----------------------------------------
Eval num_timesteps=195500, episode_reward=17.34 +/- 3.60
Episode length: 294.80 +/- 55.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=17.44 +/- 3.19
Episode length: 294.84 +/- 51.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=17.72 +/- 3.03
Episode length: 298.30 +/- 47.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 96       |
|    time_elapsed    | 3619     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=18.14 +/- 3.44
Episode length: 309.04 +/- 58.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 309         |
|    mean_reward          | 18.1        |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.015524734 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.02        |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.128       |
-----------------------------------------
Eval num_timesteps=197500, episode_reward=18.40 +/- 3.13
Episode length: 315.04 +/- 52.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 315      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=18.00 +/- 3.58
Episode length: 308.90 +/- 56.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=17.60 +/- 3.06
Episode length: 295.40 +/- 54.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 278      |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 97       |
|    time_elapsed    | 3661     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=17.74 +/- 3.67
Episode length: 302.92 +/- 58.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 303         |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.012640778 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0485      |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.306       |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=18.36 +/- 2.77
Episode length: 312.62 +/- 45.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 313      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=18.02 +/- 3.28
Episode length: 310.78 +/- 54.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 311      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=18.02 +/- 2.96
Episode length: 305.22 +/- 48.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 305      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 98       |
|    time_elapsed    | 3704     |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=17.68 +/- 3.74
Episode length: 297.40 +/- 60.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 297         |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.020544045 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.495      |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0507      |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.224       |
-----------------------------------------
Eval num_timesteps=201500, episode_reward=17.46 +/- 3.79
Episode length: 303.80 +/- 68.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=17.66 +/- 3.00
Episode length: 302.94 +/- 50.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=16.34 +/- 4.36
Episode length: 287.52 +/- 72.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 267      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 99       |
|    time_elapsed    | 3745     |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=16.78 +/- 4.74
Episode length: 287.16 +/- 78.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 287         |
|    mean_reward          | 16.8        |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.024770685 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0349      |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.148       |
-----------------------------------------
Eval num_timesteps=203500, episode_reward=16.92 +/- 4.48
Episode length: 293.24 +/- 74.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=15.52 +/- 4.24
Episode length: 274.88 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=16.24 +/- 5.24
Episode length: 292.26 +/- 91.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 292      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 267      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 100      |
|    time_elapsed    | 3785     |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=16.96 +/- 3.30
Episode length: 295.92 +/- 49.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 296         |
|    mean_reward          | 17          |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.021572948 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0195      |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.181       |
-----------------------------------------
Eval num_timesteps=205500, episode_reward=16.82 +/- 3.33
Episode length: 290.42 +/- 54.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=16.88 +/- 3.39
Episode length: 294.58 +/- 47.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=17.22 +/- 3.37
Episode length: 293.30 +/- 58.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 262      |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 101      |
|    time_elapsed    | 3825     |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=17.82 +/- 3.90
Episode length: 306.88 +/- 71.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 307        |
|    mean_reward          | 17.8       |
| time/                   |            |
|    total_timesteps      | 207000     |
| train/                  |            |
|    approx_kl            | 0.01894388 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.499     |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.025      |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0196    |
|    value_loss           | 0.218      |
----------------------------------------
Eval num_timesteps=207500, episode_reward=17.60 +/- 3.12
Episode length: 298.00 +/- 52.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=17.80 +/- 3.22
Episode length: 304.02 +/- 52.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=18.26 +/- 3.26
Episode length: 311.24 +/- 56.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 311      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 261      |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 54       |
|    iterations      | 102      |
|    time_elapsed    | 3867     |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=18.24 +/- 3.16
Episode length: 304.58 +/- 52.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 305        |
|    mean_reward          | 18.2       |
| time/                   |            |
|    total_timesteps      | 209000     |
| train/                  |            |
|    approx_kl            | 0.04031513 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.488     |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0515     |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.196      |
----------------------------------------
Eval num_timesteps=209500, episode_reward=17.98 +/- 3.31
Episode length: 303.22 +/- 50.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=17.90 +/- 3.45
Episode length: 304.12 +/- 63.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=18.34 +/- 3.30
Episode length: 309.56 +/- 57.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 310      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 263      |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 103      |
|    time_elapsed    | 3910     |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=17.34 +/- 2.78
Episode length: 305.12 +/- 49.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 305         |
|    mean_reward          | 17.3        |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.021417715 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.455      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0495      |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.156       |
-----------------------------------------
Eval num_timesteps=211500, episode_reward=16.74 +/- 3.51
Episode length: 293.72 +/- 55.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=16.96 +/- 3.30
Episode length: 287.14 +/- 49.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=16.98 +/- 2.87
Episode length: 295.90 +/- 45.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 296      |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 265      |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 104      |
|    time_elapsed    | 3950     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=17.50 +/- 3.67
Episode length: 300.32 +/- 58.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | 17.5        |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.022305222 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.486      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0402      |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.184       |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=17.48 +/- 3.74
Episode length: 290.64 +/- 61.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 291      |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=17.86 +/- 3.73
Episode length: 302.70 +/- 62.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=18.44 +/- 3.01
Episode length: 311.18 +/- 54.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 311      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=17.38 +/- 4.20
Episode length: 302.34 +/- 73.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 302      |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 266      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 105      |
|    time_elapsed    | 4002     |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=17.94 +/- 3.13
Episode length: 304.46 +/- 55.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 304         |
|    mean_reward          | 17.9        |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.020889673 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.477      |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0242      |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.188       |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=18.28 +/- 2.59
Episode length: 309.42 +/- 43.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=17.64 +/- 3.02
Episode length: 302.82 +/- 51.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=19.10 +/- 2.64
Episode length: 331.28 +/- 37.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 331      |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 106      |
|    time_elapsed    | 4045     |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=17.32 +/- 3.41
Episode length: 294.50 +/- 57.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 294        |
|    mean_reward          | 17.3       |
| time/                   |            |
|    total_timesteps      | 217500     |
| train/                  |            |
|    approx_kl            | 0.01942243 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.487     |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0374     |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0181    |
|    value_loss           | 0.123      |
----------------------------------------
Eval num_timesteps=218000, episode_reward=17.90 +/- 2.56
Episode length: 302.04 +/- 45.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 302      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=17.28 +/- 4.03
Episode length: 300.24 +/- 64.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=18.10 +/- 3.14
Episode length: 308.46 +/- 51.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 308      |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 107      |
|    time_elapsed    | 4086     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=17.10 +/- 4.63
Episode length: 287.08 +/- 71.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 287         |
|    mean_reward          | 17.1        |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.017289879 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.453      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.037       |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.135       |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=17.34 +/- 4.40
Episode length: 287.48 +/- 73.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=18.50 +/- 4.19
Episode length: 308.90 +/- 68.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=18.28 +/- 3.73
Episode length: 300.04 +/- 63.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 272      |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 108      |
|    time_elapsed    | 4127     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=16.90 +/- 5.10
Episode length: 286.12 +/- 80.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | 16.9        |
| time/                   |             |
|    total_timesteps      | 221500      |
| train/                  |             |
|    approx_kl            | 0.019413728 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0494      |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.165       |
-----------------------------------------
Eval num_timesteps=222000, episode_reward=16.44 +/- 4.87
Episode length: 277.82 +/- 80.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=17.02 +/- 4.68
Episode length: 287.68 +/- 77.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=15.82 +/- 5.43
Episode length: 270.64 +/- 83.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 270      |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 109      |
|    time_elapsed    | 4166     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=16.84 +/- 4.39
Episode length: 286.46 +/- 68.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 286        |
|    mean_reward          | 16.8       |
| time/                   |            |
|    total_timesteps      | 223500     |
| train/                  |            |
|    approx_kl            | 0.02019092 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.51      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0362     |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 0.169      |
----------------------------------------
Eval num_timesteps=224000, episode_reward=17.32 +/- 4.48
Episode length: 287.98 +/- 72.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=17.54 +/- 4.99
Episode length: 292.04 +/- 79.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 292      |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=18.38 +/- 3.80
Episode length: 302.02 +/- 64.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 302      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 110      |
|    time_elapsed    | 4206     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=17.44 +/- 5.24
Episode length: 295.16 +/- 84.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 295         |
|    mean_reward          | 17.4        |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.023471035 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.494      |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.01       |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.129       |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=18.18 +/- 4.20
Episode length: 307.34 +/- 72.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=16.58 +/- 5.18
Episode length: 287.74 +/- 85.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=19.02 +/- 3.67
Episode length: 323.44 +/- 63.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 323      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 274      |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 111      |
|    time_elapsed    | 4248     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=16.02 +/- 5.07
Episode length: 271.94 +/- 86.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | 16          |
| time/                   |             |
|    total_timesteps      | 227500      |
| train/                  |             |
|    approx_kl            | 0.017258381 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.46       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0371      |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.243       |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=15.02 +/- 4.97
Episode length: 254.48 +/- 77.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=16.76 +/- 5.05
Episode length: 288.82 +/- 85.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=17.74 +/- 4.27
Episode length: 295.44 +/- 73.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 112      |
|    time_elapsed    | 4287     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=19.54 +/- 3.92
Episode length: 327.88 +/- 60.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 328         |
|    mean_reward          | 19.5        |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.019217242 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.483      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0357      |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.13        |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=17.92 +/- 4.13
Episode length: 297.58 +/- 70.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=18.28 +/- 3.91
Episode length: 299.56 +/- 68.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=18.48 +/- 4.26
Episode length: 308.24 +/- 70.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 308      |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 273      |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 113      |
|    time_elapsed    | 4330     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=18.18 +/- 3.79
Episode length: 305.42 +/- 62.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 305         |
|    mean_reward          | 18.2        |
| time/                   |             |
|    total_timesteps      | 231500      |
| train/                  |             |
|    approx_kl            | 0.017399695 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0344      |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.166       |
-----------------------------------------
Eval num_timesteps=232000, episode_reward=18.84 +/- 4.02
Episode length: 309.40 +/- 61.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=18.10 +/- 4.21
Episode length: 304.84 +/- 73.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 305      |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=18.08 +/- 5.16
Episode length: 299.34 +/- 79.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 299      |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 53       |
|    iterations      | 114      |
|    time_elapsed    | 4372     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=18.50 +/- 3.56
Episode length: 314.90 +/- 61.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 315         |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 233500      |
| train/                  |             |
|    approx_kl            | 0.019919833 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.515      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0342      |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.132       |
-----------------------------------------
Eval num_timesteps=234000, episode_reward=19.46 +/- 2.91
Episode length: 329.58 +/- 54.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | 19.5     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=19.12 +/- 3.81
Episode length: 324.40 +/- 66.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 324      |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=18.96 +/- 3.64
Episode length: 316.76 +/- 65.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=19.62 +/- 3.88
Episode length: 332.40 +/- 68.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 332      |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 281      |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 115      |
|    time_elapsed    | 4427     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=17.14 +/- 3.45
Episode length: 296.48 +/- 59.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 296         |
|    mean_reward          | 17.1        |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.017563239 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0136      |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.111       |
-----------------------------------------
Eval num_timesteps=236500, episode_reward=17.86 +/- 3.55
Episode length: 307.90 +/- 52.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 308      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=18.68 +/- 3.18
Episode length: 315.74 +/- 48.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=17.72 +/- 3.01
Episode length: 307.26 +/- 52.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 284      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 116      |
|    time_elapsed    | 4469     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=19.02 +/- 3.46
Episode length: 323.24 +/- 55.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | 19          |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.020370413 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.486      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0197      |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.116       |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=18.10 +/- 4.30
Episode length: 305.72 +/- 65.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=19.44 +/- 3.08
Episode length: 329.26 +/- 49.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 329      |
|    mean_reward     | 19.4     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=19.68 +/- 3.13
Episode length: 328.58 +/- 51.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 329      |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 284      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 117      |
|    time_elapsed    | 4514     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=18.48 +/- 2.93
Episode length: 305.78 +/- 53.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 306         |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.018621048 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.442      |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0701      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.198       |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=19.94 +/- 2.61
Episode length: 329.00 +/- 47.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 329      |
|    mean_reward     | 19.9     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=19.24 +/- 3.57
Episode length: 312.94 +/- 60.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 313      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=18.98 +/- 3.44
Episode length: 309.90 +/- 58.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 310      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 282      |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 118      |
|    time_elapsed    | 4557     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=19.72 +/- 3.50
Episode length: 323.62 +/- 59.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 324         |
|    mean_reward          | 19.7        |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.023460364 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.459      |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0423      |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.214       |
-----------------------------------------
Eval num_timesteps=242500, episode_reward=19.56 +/- 3.57
Episode length: 324.46 +/- 60.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 324      |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=18.98 +/- 3.21
Episode length: 314.00 +/- 56.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 314      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=20.16 +/- 2.63
Episode length: 333.84 +/- 42.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 334      |
|    mean_reward     | 20.2     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 283      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 119      |
|    time_elapsed    | 4602     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=19.22 +/- 3.52
Episode length: 326.32 +/- 60.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 326         |
|    mean_reward          | 19.2        |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.014301766 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.421      |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.02        |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.144       |
-----------------------------------------
Eval num_timesteps=244500, episode_reward=19.86 +/- 3.69
Episode length: 326.34 +/- 58.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 326      |
|    mean_reward     | 19.9     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=19.68 +/- 3.04
Episode length: 330.80 +/- 55.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 331      |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=19.76 +/- 3.61
Episode length: 323.56 +/- 53.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 324      |
|    mean_reward     | 19.8     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 284      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 120      |
|    time_elapsed    | 4648     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=18.74 +/- 2.21
Episode length: 315.12 +/- 43.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 315         |
|    mean_reward          | 18.7        |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.018847913 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.398      |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0221      |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.134       |
-----------------------------------------
Eval num_timesteps=246500, episode_reward=18.64 +/- 2.15
Episode length: 320.52 +/- 39.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 321      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=18.36 +/- 2.76
Episode length: 319.22 +/- 44.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 319      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=18.64 +/- 2.62
Episode length: 318.02 +/- 44.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 318      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 287      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 121      |
|    time_elapsed    | 4694     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=18.60 +/- 2.77
Episode length: 315.74 +/- 49.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 316         |
|    mean_reward          | 18.6        |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.016528104 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.035       |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.128       |
-----------------------------------------
Eval num_timesteps=248500, episode_reward=18.40 +/- 2.99
Episode length: 314.98 +/- 50.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 315      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=18.80 +/- 2.49
Episode length: 322.24 +/- 39.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 322      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=18.04 +/- 3.09
Episode length: 307.32 +/- 52.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 122      |
|    time_elapsed    | 4739     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=18.50 +/- 2.24
Episode length: 315.06 +/- 38.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 315         |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.025230289 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.422      |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00877     |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.102       |
-----------------------------------------
Eval num_timesteps=250500, episode_reward=18.76 +/- 2.76
Episode length: 314.26 +/- 38.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 314      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=18.94 +/- 2.14
Episode length: 313.06 +/- 37.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 313      |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=19.34 +/- 2.56
Episode length: 324.00 +/- 44.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 324      |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 123      |
|    time_elapsed    | 4784     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=18.78 +/- 3.68
Episode length: 307.94 +/- 56.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 308         |
|    mean_reward          | 18.8        |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.016954456 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0349      |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.145       |
-----------------------------------------
Eval num_timesteps=252500, episode_reward=19.70 +/- 3.26
Episode length: 319.72 +/- 51.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 320      |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=19.20 +/- 3.85
Episode length: 311.62 +/- 54.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 312      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=19.18 +/- 3.56
Episode length: 321.80 +/- 61.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 322      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 124      |
|    time_elapsed    | 4828     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=19.94 +/- 2.81
Episode length: 328.42 +/- 47.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 328         |
|    mean_reward          | 19.9        |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.023029337 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00598     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.106       |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=21.22 +/- 2.09
Episode length: 343.40 +/- 40.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 343      |
|    mean_reward     | 21.2     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
New best mean reward!
Eval num_timesteps=255000, episode_reward=19.70 +/- 3.16
Episode length: 320.80 +/- 52.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 321      |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=19.22 +/- 3.38
Episode length: 318.36 +/- 55.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 318      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=19.92 +/- 2.88
Episode length: 328.56 +/- 51.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 329      |
|    mean_reward     | 19.9     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 125      |
|    time_elapsed    | 4884     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=18.70 +/- 3.11
Episode length: 317.52 +/- 49.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 318        |
|    mean_reward          | 18.7       |
| time/                   |            |
|    total_timesteps      | 256500     |
| train/                  |            |
|    approx_kl            | 0.02080824 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.363     |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0472     |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 0.164      |
----------------------------------------
Eval num_timesteps=257000, episode_reward=18.20 +/- 3.60
Episode length: 308.66 +/- 59.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=19.18 +/- 2.30
Episode length: 317.40 +/- 41.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 317      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=18.88 +/- 2.62
Episode length: 319.46 +/- 43.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 319      |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 126      |
|    time_elapsed    | 4928     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=18.00 +/- 2.27
Episode length: 306.30 +/- 46.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 306         |
|    mean_reward          | 18          |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.020694287 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0203      |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.112       |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=17.18 +/- 2.65
Episode length: 293.80 +/- 41.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=17.84 +/- 2.78
Episode length: 299.98 +/- 43.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=17.70 +/- 2.80
Episode length: 299.60 +/- 54.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 127      |
|    time_elapsed    | 4970     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=18.00 +/- 2.77
Episode length: 300.32 +/- 46.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 18        |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0202307 |
|    clip_fraction        | 0.149     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.432    |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0104    |
|    n_updates            | 1270      |
|    policy_gradient_loss | -0.018    |
|    value_loss           | 0.115     |
---------------------------------------
Eval num_timesteps=261000, episode_reward=17.70 +/- 3.09
Episode length: 297.52 +/- 51.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=17.92 +/- 3.09
Episode length: 295.98 +/- 44.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 296      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=16.98 +/- 3.59
Episode length: 285.10 +/- 61.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 128      |
|    time_elapsed    | 5011     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=16.80 +/- 3.14
Episode length: 285.30 +/- 49.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 285         |
|    mean_reward          | 16.8        |
| time/                   |             |
|    total_timesteps      | 262500      |
| train/                  |             |
|    approx_kl            | 0.019031785 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.43       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0134      |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.131       |
-----------------------------------------
Eval num_timesteps=263000, episode_reward=17.14 +/- 2.26
Episode length: 286.78 +/- 35.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=16.00 +/- 3.35
Episode length: 267.34 +/- 51.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=16.82 +/- 2.71
Episode length: 279.82 +/- 39.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 129      |
|    time_elapsed    | 5049     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=16.44 +/- 3.34
Episode length: 284.58 +/- 53.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 285         |
|    mean_reward          | 16.4        |
| time/                   |             |
|    total_timesteps      | 264500      |
| train/                  |             |
|    approx_kl            | 0.023323193 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.483      |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0454      |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.216       |
-----------------------------------------
Eval num_timesteps=265000, episode_reward=15.98 +/- 3.65
Episode length: 268.76 +/- 61.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=16.50 +/- 3.36
Episode length: 281.62 +/- 56.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=16.82 +/- 3.00
Episode length: 286.40 +/- 49.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 130      |
|    time_elapsed    | 5088     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=17.16 +/- 3.37
Episode length: 291.92 +/- 48.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 292         |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 0.021944309 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.46       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0207      |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.164       |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=17.90 +/- 3.03
Episode length: 294.56 +/- 50.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=17.80 +/- 3.03
Episode length: 297.88 +/- 45.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=16.62 +/- 3.61
Episode length: 284.56 +/- 56.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 131      |
|    time_elapsed    | 5129     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=17.32 +/- 3.16
Episode length: 287.88 +/- 42.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 17.3        |
| time/                   |             |
|    total_timesteps      | 268500      |
| train/                  |             |
|    approx_kl            | 0.021178734 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.458      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.109       |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.189       |
-----------------------------------------
Eval num_timesteps=269000, episode_reward=16.92 +/- 3.44
Episode length: 285.84 +/- 53.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=16.96 +/- 3.89
Episode length: 289.24 +/- 62.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=16.46 +/- 3.58
Episode length: 278.02 +/- 50.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 132      |
|    time_elapsed    | 5169     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=17.22 +/- 3.22
Episode length: 288.30 +/- 48.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.022590801 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.459      |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0192      |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.141       |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=17.68 +/- 3.18
Episode length: 293.60 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=16.70 +/- 3.68
Episode length: 282.72 +/- 58.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=16.62 +/- 3.67
Episode length: 285.50 +/- 60.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 133      |
|    time_elapsed    | 5209     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=14.86 +/- 5.10
Episode length: 259.82 +/- 67.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 260         |
|    mean_reward          | 14.9        |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.020874882 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.492      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0262      |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.156       |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=14.32 +/- 5.62
Episode length: 255.26 +/- 80.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=15.68 +/- 5.69
Episode length: 270.58 +/- 80.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=15.78 +/- 5.66
Episode length: 274.50 +/- 76.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 134      |
|    time_elapsed    | 5246     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=16.98 +/- 4.10
Episode length: 287.70 +/- 62.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 17         |
| time/                   |            |
|    total_timesteps      | 274500     |
| train/                  |            |
|    approx_kl            | 0.02021922 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.505     |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0489     |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.154      |
----------------------------------------
Eval num_timesteps=275000, episode_reward=18.46 +/- 2.53
Episode length: 303.14 +/- 39.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=16.86 +/- 3.57
Episode length: 277.82 +/- 53.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=16.82 +/- 3.58
Episode length: 280.44 +/- 54.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 135      |
|    time_elapsed    | 5286     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=13.74 +/- 4.37
Episode length: 249.72 +/- 65.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 250         |
|    mean_reward          | 13.7        |
| time/                   |             |
|    total_timesteps      | 276500      |
| train/                  |             |
|    approx_kl            | 0.025010768 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.467      |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0236     |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.164       |
-----------------------------------------
Eval num_timesteps=277000, episode_reward=15.68 +/- 4.54
Episode length: 267.38 +/- 62.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=14.08 +/- 4.54
Episode length: 244.36 +/- 70.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=14.68 +/- 4.95
Episode length: 253.32 +/- 70.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=14.50 +/- 4.34
Episode length: 246.48 +/- 63.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 52       |
|    iterations      | 136      |
|    time_elapsed    | 5330     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=15.42 +/- 4.86
Episode length: 263.06 +/- 80.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 263         |
|    mean_reward          | 15.4        |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.023336992 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.47       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.106       |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.275       |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=15.82 +/- 3.56
Episode length: 272.26 +/- 53.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=15.46 +/- 3.95
Episode length: 262.56 +/- 55.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=16.40 +/- 3.92
Episode length: 273.54 +/- 64.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 137      |
|    time_elapsed    | 5367     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=15.32 +/- 4.47
Episode length: 268.14 +/- 73.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | 15.3        |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.022991292 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.459      |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0395      |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.15        |
-----------------------------------------
Eval num_timesteps=281500, episode_reward=16.42 +/- 4.14
Episode length: 282.42 +/- 60.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=14.74 +/- 4.58
Episode length: 251.42 +/- 67.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=15.50 +/- 4.46
Episode length: 261.36 +/- 65.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 138      |
|    time_elapsed    | 5404     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=15.86 +/- 3.75
Episode length: 265.44 +/- 53.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 265         |
|    mean_reward          | 15.9        |
| time/                   |             |
|    total_timesteps      | 283000      |
| train/                  |             |
|    approx_kl            | 0.016639978 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.515      |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0435      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.172       |
-----------------------------------------
Eval num_timesteps=283500, episode_reward=15.50 +/- 3.92
Episode length: 259.58 +/- 54.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=15.06 +/- 3.41
Episode length: 251.98 +/- 54.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=15.32 +/- 4.32
Episode length: 259.38 +/- 61.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 274      |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 139      |
|    time_elapsed    | 5440     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=17.62 +/- 3.03
Episode length: 291.64 +/- 50.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 292        |
|    mean_reward          | 17.6       |
| time/                   |            |
|    total_timesteps      | 285000     |
| train/                  |            |
|    approx_kl            | 0.02004388 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.487     |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0562     |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.162      |
----------------------------------------
Eval num_timesteps=285500, episode_reward=15.88 +/- 4.62
Episode length: 267.64 +/- 69.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=16.44 +/- 4.31
Episode length: 276.34 +/- 69.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=15.92 +/- 4.63
Episode length: 266.94 +/- 67.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 272      |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 140      |
|    time_elapsed    | 5479     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=15.78 +/- 3.67
Episode length: 263.90 +/- 58.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 264         |
|    mean_reward          | 15.8        |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.022924304 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0328      |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.158       |
-----------------------------------------
Eval num_timesteps=287500, episode_reward=15.96 +/- 4.10
Episode length: 273.12 +/- 68.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=16.08 +/- 3.96
Episode length: 272.50 +/- 68.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=16.70 +/- 3.51
Episode length: 280.90 +/- 57.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 269      |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 141      |
|    time_elapsed    | 5517     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=13.66 +/- 5.92
Episode length: 250.00 +/- 84.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 250         |
|    mean_reward          | 13.7        |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.026021056 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.477      |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0721      |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.215       |
-----------------------------------------
Eval num_timesteps=289500, episode_reward=13.32 +/- 6.23
Episode length: 242.62 +/- 94.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=14.52 +/- 5.64
Episode length: 260.98 +/- 79.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=12.60 +/- 5.23
Episode length: 228.72 +/- 77.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 12.6     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 273      |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 142      |
|    time_elapsed    | 5551     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=13.70 +/- 6.31
Episode length: 244.36 +/- 88.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 244         |
|    mean_reward          | 13.7        |
| time/                   |             |
|    total_timesteps      | 291000      |
| train/                  |             |
|    approx_kl            | 0.020044617 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.436      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00385     |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.107       |
-----------------------------------------
Eval num_timesteps=291500, episode_reward=13.46 +/- 5.82
Episode length: 247.18 +/- 88.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=14.08 +/- 5.75
Episode length: 253.94 +/- 83.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=12.34 +/- 5.93
Episode length: 231.88 +/- 90.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 276      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 143      |
|    time_elapsed    | 5585     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=8.14 +/- 6.22
Episode length: 170.48 +/- 93.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 170         |
|    mean_reward          | 8.14        |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.025822593 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.461      |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0175      |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.125       |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=8.04 +/- 5.82
Episode length: 172.70 +/- 87.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 8.04     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=7.62 +/- 5.16
Episode length: 166.84 +/- 78.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 7.62     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=7.86 +/- 5.64
Episode length: 168.28 +/- 85.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 7.86     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 276      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 144      |
|    time_elapsed    | 5610     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=10.14 +/- 6.48
Episode length: 197.52 +/- 98.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 198         |
|    mean_reward          | 10.1        |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.028149102 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.47       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0227      |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.168       |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=9.24 +/- 5.72
Episode length: 189.48 +/- 90.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 9.24     |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=9.84 +/- 5.93
Episode length: 194.76 +/- 91.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 9.84     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=8.40 +/- 5.98
Episode length: 171.26 +/- 88.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 8.4      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 279      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 145      |
|    time_elapsed    | 5637     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=18.40 +/- 3.30
Episode length: 302.66 +/- 53.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 303         |
|    mean_reward          | 18.4        |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.017445564 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.417      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00519    |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.122       |
-----------------------------------------
Eval num_timesteps=297500, episode_reward=17.78 +/- 3.85
Episode length: 290.14 +/- 64.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=17.82 +/- 3.41
Episode length: 290.32 +/- 55.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=18.30 +/- 3.41
Episode length: 297.96 +/- 53.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=17.58 +/- 3.76
Episode length: 285.24 +/- 56.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 277      |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 146      |
|    time_elapsed    | 5687     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=18.58 +/- 3.41
Episode length: 312.98 +/- 52.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 313         |
|    mean_reward          | 18.6        |
| time/                   |             |
|    total_timesteps      | 299500      |
| train/                  |             |
|    approx_kl            | 0.022789624 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.394      |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0285      |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.17        |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=18.80 +/- 3.68
Episode length: 308.68 +/- 58.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=18.56 +/- 3.47
Episode length: 309.26 +/- 57.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=18.62 +/- 4.56
Episode length: 313.62 +/- 67.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 314      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 277      |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 147      |
|    time_elapsed    | 5730     |
|    total_timesteps | 301056   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/defend-center/ppo-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0001, 'gamma': 0.92, 'gae_lambda': 0.95}
Training steps: 300000
Frame skip: 4
Using cuda device
Eval num_timesteps=500, episode_reward=3.44 +/- 2.65
Episode length: 105.88 +/- 38.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 3.44     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=2.66 +/- 2.08
Episode length: 91.20 +/- 32.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.2     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=3.40 +/- 2.28
Episode length: 98.44 +/- 34.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 3.4      |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=2.80 +/- 2.22
Episode length: 94.98 +/- 35.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95       |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 9.23     |
| time/              |          |
|    fps             | 138      |
|    iterations      | 1        |
|    time_elapsed    | 14       |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=10.16 +/- 3.31
Episode length: 135.88 +/- 42.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 136         |
|    mean_reward          | 10.2        |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.004795531 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0519     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.158       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00721    |
|    value_loss           | 0.487       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=10.02 +/- 3.38
Episode length: 132.74 +/- 46.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 10       |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=9.94 +/- 2.58
Episode length: 132.34 +/- 35.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 9.94     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=9.64 +/- 3.21
Episode length: 127.72 +/- 41.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 128      |
|    mean_reward     | 9.64     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 8.74     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 2        |
|    time_elapsed    | 35       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=10.14 +/- 2.72
Episode length: 134.62 +/- 39.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 135         |
|    mean_reward          | 10.1        |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.014412661 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.255       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.626       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=10.18 +/- 3.62
Episode length: 138.24 +/- 47.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
Eval num_timesteps=5500, episode_reward=10.36 +/- 2.83
Episode length: 138.68 +/- 38.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
New best mean reward!
Eval num_timesteps=6000, episode_reward=9.70 +/- 2.98
Episode length: 132.74 +/- 41.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 9.7      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 8.6      |
| time/              |          |
|    fps             | 108      |
|    iterations      | 3        |
|    time_elapsed    | 56       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=9.14 +/- 2.78
Episode length: 123.08 +/- 38.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 123         |
|    mean_reward          | 9.14        |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.010864354 |
|    clip_fraction        | 0.079       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.149       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00793    |
|    value_loss           | 0.438       |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=9.36 +/- 3.10
Episode length: 125.86 +/- 43.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 126      |
|    mean_reward     | 9.36     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=9.78 +/- 3.37
Episode length: 132.54 +/- 45.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 9.78     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=10.22 +/- 2.92
Episode length: 137.90 +/- 40.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 10.2     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 8.7      |
| time/              |          |
|    fps             | 107      |
|    iterations      | 4        |
|    time_elapsed    | 76       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=10.44 +/- 3.66
Episode length: 136.82 +/- 42.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 137         |
|    mean_reward          | 10.4        |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.008970819 |
|    clip_fraction        | 0.0736      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.279       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.735       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=9000, episode_reward=10.14 +/- 4.26
Episode length: 130.60 +/- 49.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=11.18 +/- 4.11
Episode length: 141.10 +/- 46.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 141      |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
New best mean reward!
Eval num_timesteps=10000, episode_reward=11.24 +/- 3.93
Episode length: 142.30 +/- 48.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 8.89     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 5        |
|    time_elapsed    | 97       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=9.60 +/- 2.61
Episode length: 127.52 +/- 34.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 9.6         |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.010063014 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.278       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.682       |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=9.12 +/- 2.83
Episode length: 123.74 +/- 43.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 9.12     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=9.22 +/- 2.15
Episode length: 122.06 +/- 27.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 9.22     |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=9.68 +/- 2.90
Episode length: 131.44 +/- 40.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 9.68     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 9.39     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 6        |
|    time_elapsed    | 117      |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=13.30 +/- 4.18
Episode length: 155.20 +/- 44.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 155        |
|    mean_reward          | 13.3       |
| time/                   |            |
|    total_timesteps      | 12500      |
| train/                  |            |
|    approx_kl            | 0.00625176 |
|    clip_fraction        | 0.0555     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.26       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00676   |
|    value_loss           | 0.65       |
----------------------------------------
New best mean reward!
Eval num_timesteps=13000, episode_reward=12.90 +/- 4.63
Episode length: 156.60 +/- 50.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 157      |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=12.76 +/- 4.38
Episode length: 155.38 +/- 52.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=12.68 +/- 4.30
Episode length: 150.48 +/- 52.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 101      |
|    iterations      | 7        |
|    time_elapsed    | 140      |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=13.60 +/- 5.25
Episode length: 160.12 +/- 57.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 160          |
|    mean_reward          | 13.6         |
| time/                   |              |
|    total_timesteps      | 14500        |
| train/                  |              |
|    approx_kl            | 0.0053974823 |
|    clip_fraction        | 0.0709       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.673        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.212        |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00916     |
|    value_loss           | 0.741        |
------------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=12.40 +/- 4.17
Episode length: 140.50 +/- 43.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=13.46 +/- 3.68
Episode length: 154.36 +/- 39.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 154      |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=14.00 +/- 5.64
Episode length: 168.00 +/- 63.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 8        |
|    time_elapsed    | 163      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=14.60 +/- 5.13
Episode length: 168.82 +/- 57.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.010428924 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.308       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.773       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=17000, episode_reward=13.24 +/- 4.00
Episode length: 158.16 +/- 41.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 158      |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=14.24 +/- 6.56
Episode length: 168.82 +/- 74.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=13.54 +/- 5.37
Episode length: 156.18 +/- 60.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 156      |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 9        |
|    time_elapsed    | 187      |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=13.76 +/- 4.87
Episode length: 160.32 +/- 54.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 160         |
|    mean_reward          | 13.8        |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.009765541 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.232       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 0.697       |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=15.04 +/- 5.01
Episode length: 171.22 +/- 61.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
New best mean reward!
Eval num_timesteps=19500, episode_reward=14.50 +/- 4.84
Episode length: 167.08 +/- 56.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=13.46 +/- 4.60
Episode length: 159.00 +/- 51.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 10       |
|    time_elapsed    | 212      |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=12.30 +/- 3.54
Episode length: 144.10 +/- 37.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 144          |
|    mean_reward          | 12.3         |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0065810783 |
|    clip_fraction        | 0.0659       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.791        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.27         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00852     |
|    value_loss           | 0.657        |
------------------------------------------
Eval num_timesteps=21000, episode_reward=12.86 +/- 4.44
Episode length: 154.88 +/- 51.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=13.28 +/- 4.36
Episode length: 152.48 +/- 48.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 152      |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=13.10 +/- 3.93
Episode length: 153.56 +/- 42.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 154      |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=11.76 +/- 3.72
Episode length: 145.64 +/- 45.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 146      |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 94       |
|    iterations      | 11       |
|    time_elapsed    | 239      |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=13.42 +/- 4.17
Episode length: 154.58 +/- 43.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 155         |
|    mean_reward          | 13.4        |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.011939706 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.276       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.786       |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=14.00 +/- 5.44
Episode length: 160.72 +/- 59.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 161      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=13.26 +/- 3.87
Episode length: 151.34 +/- 47.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 151      |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=13.42 +/- 4.61
Episode length: 148.90 +/- 46.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 13.4     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 12       |
|    time_elapsed    | 262      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=13.12 +/- 4.81
Episode length: 149.40 +/- 50.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 149          |
|    mean_reward          | 13.1         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0055346424 |
|    clip_fraction        | 0.059        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.778        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.378        |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00596     |
|    value_loss           | 0.846        |
------------------------------------------
Eval num_timesteps=25500, episode_reward=12.46 +/- 4.00
Episode length: 146.24 +/- 43.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 146      |
|    mean_reward     | 12.5     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=12.84 +/- 4.21
Episode length: 145.86 +/- 45.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 146      |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=13.12 +/- 4.50
Episode length: 150.64 +/- 52.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 151      |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 13       |
|    time_elapsed    | 284      |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=11.60 +/- 4.85
Episode length: 141.18 +/- 57.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 141          |
|    mean_reward          | 11.6         |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0085731875 |
|    clip_fraction        | 0.0711       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.849        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.16         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 0.546        |
------------------------------------------
Eval num_timesteps=27500, episode_reward=12.82 +/- 3.84
Episode length: 149.82 +/- 41.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=11.96 +/- 4.10
Episode length: 144.38 +/- 48.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 144      |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=11.92 +/- 3.99
Episode length: 141.78 +/- 49.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 14       |
|    time_elapsed    | 305      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=12.54 +/- 4.35
Episode length: 152.16 +/- 47.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 152         |
|    mean_reward          | 12.5        |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.006752949 |
|    clip_fraction        | 0.0909      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.229       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 0.671       |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=12.34 +/- 3.78
Episode length: 147.08 +/- 47.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 147      |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=11.68 +/- 4.04
Episode length: 143.60 +/- 47.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 144      |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=12.06 +/- 4.20
Episode length: 144.66 +/- 47.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 145      |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 15       |
|    time_elapsed    | 327      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=12.50 +/- 3.45
Episode length: 154.62 +/- 41.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 155          |
|    mean_reward          | 12.5         |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0064323293 |
|    clip_fraction        | 0.0571       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.844        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.194        |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00965     |
|    value_loss           | 0.692        |
------------------------------------------
Eval num_timesteps=31500, episode_reward=11.72 +/- 3.18
Episode length: 145.68 +/- 37.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 146      |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=13.06 +/- 3.63
Episode length: 157.88 +/- 42.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 158      |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=13.52 +/- 4.49
Episode length: 161.58 +/- 50.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 162      |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 16       |
|    time_elapsed    | 350      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=12.94 +/- 4.60
Episode length: 151.74 +/- 55.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 152          |
|    mean_reward          | 12.9         |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0064312858 |
|    clip_fraction        | 0.0982       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.971       |
|    explained_variance   | 0.829        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.186        |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00986     |
|    value_loss           | 0.607        |
------------------------------------------
Eval num_timesteps=33500, episode_reward=14.50 +/- 4.46
Episode length: 167.74 +/- 49.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=13.60 +/- 5.00
Episode length: 159.48 +/- 56.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=13.30 +/- 4.61
Episode length: 152.58 +/- 49.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 153      |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 17       |
|    time_elapsed    | 374      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=14.16 +/- 4.52
Episode length: 159.10 +/- 50.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 159         |
|    mean_reward          | 14.2        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.009550713 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.313       |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.758       |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=14.46 +/- 3.94
Episode length: 160.80 +/- 46.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 161      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=14.74 +/- 5.17
Episode length: 166.20 +/- 58.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=13.80 +/- 4.46
Episode length: 157.38 +/- 49.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 157      |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 18       |
|    time_elapsed    | 398      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=13.46 +/- 4.60
Episode length: 154.00 +/- 51.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 154          |
|    mean_reward          | 13.5         |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0073379516 |
|    clip_fraction        | 0.0822       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.95        |
|    explained_variance   | 0.805        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.14         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 0.754        |
------------------------------------------
Eval num_timesteps=37500, episode_reward=15.18 +/- 4.16
Episode length: 170.84 +/- 48.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
New best mean reward!
Eval num_timesteps=38000, episode_reward=13.80 +/- 4.52
Episode length: 154.32 +/- 45.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 154      |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=14.56 +/- 3.84
Episode length: 162.26 +/- 42.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 162      |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 19       |
|    time_elapsed    | 421      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=13.54 +/- 5.01
Episode length: 157.42 +/- 56.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 157          |
|    mean_reward          | 13.5         |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0065255724 |
|    clip_fraction        | 0.0684       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.965       |
|    explained_variance   | 0.823        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.31         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00801     |
|    value_loss           | 0.702        |
------------------------------------------
Eval num_timesteps=39500, episode_reward=14.04 +/- 4.78
Episode length: 157.98 +/- 52.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 158      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=14.52 +/- 4.84
Episode length: 168.84 +/- 51.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=14.64 +/- 4.79
Episode length: 168.32 +/- 51.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 91       |
|    iterations      | 20       |
|    time_elapsed    | 445      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=14.62 +/- 5.09
Episode length: 163.60 +/- 57.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 164         |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.006999787 |
|    clip_fraction        | 0.061       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.974      |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.2         |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00903    |
|    value_loss           | 0.618       |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=14.96 +/- 4.07
Episode length: 167.92 +/- 43.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=14.32 +/- 5.71
Episode length: 161.10 +/- 63.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 161      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=13.54 +/- 4.80
Episode length: 153.18 +/- 50.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 153      |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=14.86 +/- 5.61
Episode length: 169.12 +/- 64.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 21       |
|    time_elapsed    | 475      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=15.60 +/- 4.75
Episode length: 178.92 +/- 54.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 179          |
|    mean_reward          | 15.6         |
| time/                   |              |
|    total_timesteps      | 43500        |
| train/                  |              |
|    approx_kl            | 0.0063388525 |
|    clip_fraction        | 0.0767       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.87        |
|    explained_variance   | 0.834        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.254        |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00849     |
|    value_loss           | 0.754        |
------------------------------------------
New best mean reward!
Eval num_timesteps=44000, episode_reward=14.86 +/- 4.92
Episode length: 170.50 +/- 56.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=16.00 +/- 5.22
Episode length: 184.62 +/- 62.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
New best mean reward!
Eval num_timesteps=45000, episode_reward=14.42 +/- 4.82
Episode length: 159.98 +/- 50.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 22       |
|    time_elapsed    | 500      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=14.72 +/- 4.24
Episode length: 167.02 +/- 53.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 167         |
|    mean_reward          | 14.7        |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.008244364 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.9        |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.189       |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.625       |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=15.52 +/- 4.68
Episode length: 174.02 +/- 54.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=14.86 +/- 4.37
Episode length: 167.82 +/- 48.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=14.00 +/- 4.36
Episode length: 155.00 +/- 47.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 23       |
|    time_elapsed    | 524      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=13.90 +/- 5.06
Episode length: 158.52 +/- 59.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 159         |
|    mean_reward          | 13.9        |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.007053867 |
|    clip_fraction        | 0.0731      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.858      |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.218       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00955    |
|    value_loss           | 0.631       |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=13.68 +/- 4.74
Episode length: 158.20 +/- 56.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 158      |
|    mean_reward     | 13.7     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=14.80 +/- 4.14
Episode length: 167.16 +/- 48.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=14.14 +/- 4.36
Episode length: 159.44 +/- 49.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 24       |
|    time_elapsed    | 548      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=14.06 +/- 5.32
Episode length: 160.80 +/- 53.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 161         |
|    mean_reward          | 14.1        |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.009432672 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.255       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 0.597       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=13.58 +/- 4.24
Episode length: 157.22 +/- 56.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 157      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=12.44 +/- 4.19
Episode length: 144.48 +/- 48.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 144      |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=13.64 +/- 4.71
Episode length: 155.20 +/- 55.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 25       |
|    time_elapsed    | 571      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=13.52 +/- 4.35
Episode length: 155.10 +/- 50.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 155          |
|    mean_reward          | 13.5         |
| time/                   |              |
|    total_timesteps      | 51500        |
| train/                  |              |
|    approx_kl            | 0.0071848044 |
|    clip_fraction        | 0.0762       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.709       |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.145        |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00991     |
|    value_loss           | 0.502        |
------------------------------------------
Eval num_timesteps=52000, episode_reward=13.60 +/- 4.63
Episode length: 159.46 +/- 51.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=13.28 +/- 4.88
Episode length: 153.62 +/- 53.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 154      |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=14.18 +/- 4.65
Episode length: 161.22 +/- 47.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 161      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 26       |
|    time_elapsed    | 594      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=13.32 +/- 3.99
Episode length: 155.94 +/- 49.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 13.3        |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.004741749 |
|    clip_fraction        | 0.0639      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.706      |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.213       |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00865    |
|    value_loss           | 0.641       |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=12.78 +/- 4.28
Episode length: 150.74 +/- 47.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 151      |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=13.24 +/- 3.95
Episode length: 150.70 +/- 47.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 151      |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=13.60 +/- 4.49
Episode length: 156.44 +/- 48.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 156      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 27       |
|    time_elapsed    | 617      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=13.62 +/- 4.56
Episode length: 155.12 +/- 51.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 155         |
|    mean_reward          | 13.6        |
| time/                   |             |
|    total_timesteps      | 55500       |
| train/                  |             |
|    approx_kl            | 0.006418809 |
|    clip_fraction        | 0.091       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.262       |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.588       |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=14.40 +/- 4.97
Episode length: 165.36 +/- 58.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=13.76 +/- 4.60
Episode length: 159.68 +/- 55.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=15.00 +/- 5.18
Episode length: 171.44 +/- 58.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 28       |
|    time_elapsed    | 641      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=12.96 +/- 4.19
Episode length: 150.68 +/- 46.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 151         |
|    mean_reward          | 13          |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.008315083 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.652      |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.174       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 0.617       |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=14.52 +/- 4.87
Episode length: 166.28 +/- 60.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=14.08 +/- 4.62
Episode length: 165.30 +/- 54.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=14.20 +/- 4.85
Episode length: 159.86 +/- 55.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 29       |
|    time_elapsed    | 665      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=13.74 +/- 4.67
Episode length: 157.62 +/- 50.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 158          |
|    mean_reward          | 13.7         |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0072293337 |
|    clip_fraction        | 0.0943       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.736       |
|    explained_variance   | 0.852        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.165        |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.0115      |
|    value_loss           | 0.646        |
------------------------------------------
Eval num_timesteps=60000, episode_reward=14.96 +/- 5.23
Episode length: 176.52 +/- 59.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=14.44 +/- 4.33
Episode length: 164.64 +/- 55.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=13.24 +/- 4.39
Episode length: 147.66 +/- 50.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 148      |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 30       |
|    time_elapsed    | 689      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=15.04 +/- 4.76
Episode length: 166.94 +/- 55.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 167          |
|    mean_reward          | 15           |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | 0.0060841255 |
|    clip_fraction        | 0.0787       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.758       |
|    explained_variance   | 0.875        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.465        |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.011       |
|    value_loss           | 0.616        |
------------------------------------------
Eval num_timesteps=62000, episode_reward=15.48 +/- 4.25
Episode length: 167.38 +/- 49.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=13.84 +/- 4.59
Episode length: 156.04 +/- 54.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 156      |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=15.22 +/- 5.33
Episode length: 168.20 +/- 57.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 31       |
|    time_elapsed    | 713      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=13.76 +/- 3.88
Episode length: 152.90 +/- 45.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 153         |
|    mean_reward          | 13.8        |
| time/                   |             |
|    total_timesteps      | 63500       |
| train/                  |             |
|    approx_kl            | 0.008690912 |
|    clip_fraction        | 0.0816      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.294       |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.732       |
-----------------------------------------
Eval num_timesteps=64000, episode_reward=13.50 +/- 4.06
Episode length: 146.66 +/- 43.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 147      |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=14.78 +/- 4.02
Episode length: 163.86 +/- 45.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 14.8     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=13.34 +/- 3.92
Episode length: 149.62 +/- 43.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=13.60 +/- 4.13
Episode length: 152.92 +/- 45.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 153      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 32       |
|    time_elapsed    | 741      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=13.28 +/- 4.64
Episode length: 145.86 +/- 47.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 146         |
|    mean_reward          | 13.3        |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.007727725 |
|    clip_fraction        | 0.0795      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.26        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.648       |
-----------------------------------------
Eval num_timesteps=66500, episode_reward=14.72 +/- 4.32
Episode length: 164.44 +/- 47.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=13.62 +/- 4.21
Episode length: 149.14 +/- 46.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=15.36 +/- 3.53
Episode length: 168.54 +/- 40.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 33       |
|    time_elapsed    | 765      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=13.80 +/- 4.33
Episode length: 151.46 +/- 50.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 151         |
|    mean_reward          | 13.8        |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.005316369 |
|    clip_fraction        | 0.0593      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.731      |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.138       |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.551       |
-----------------------------------------
Eval num_timesteps=68500, episode_reward=14.48 +/- 4.10
Episode length: 158.42 +/- 46.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 158      |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=15.18 +/- 4.07
Episode length: 165.48 +/- 43.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=15.92 +/- 5.31
Episode length: 175.16 +/- 59.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 34       |
|    time_elapsed    | 789      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=14.32 +/- 3.75
Episode length: 155.06 +/- 39.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 155          |
|    mean_reward          | 14.3         |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 0.0054853414 |
|    clip_fraction        | 0.0684       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.752       |
|    explained_variance   | 0.856        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.162        |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 0.589        |
------------------------------------------
Eval num_timesteps=70500, episode_reward=15.10 +/- 4.22
Episode length: 168.12 +/- 47.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=15.56 +/- 5.26
Episode length: 171.50 +/- 59.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 172      |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=15.30 +/- 5.10
Episode length: 163.20 +/- 62.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 163      |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 35       |
|    time_elapsed    | 813      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=14.68 +/- 4.56
Episode length: 158.74 +/- 54.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 159         |
|    mean_reward          | 14.7        |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.009226941 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.256       |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.564       |
-----------------------------------------
Eval num_timesteps=72500, episode_reward=15.02 +/- 3.83
Episode length: 162.04 +/- 44.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 162      |
|    mean_reward     | 15       |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=14.94 +/- 4.88
Episode length: 163.12 +/- 53.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 163      |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=15.26 +/- 4.77
Episode length: 169.94 +/- 53.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 36       |
|    time_elapsed    | 837      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=15.18 +/- 4.55
Episode length: 166.28 +/- 52.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 166          |
|    mean_reward          | 15.2         |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0070063015 |
|    clip_fraction        | 0.0889       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.692       |
|    explained_variance   | 0.854        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.176        |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.0129      |
|    value_loss           | 0.581        |
------------------------------------------
Eval num_timesteps=74500, episode_reward=15.46 +/- 5.25
Episode length: 170.56 +/- 61.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=14.90 +/- 4.94
Episode length: 166.92 +/- 58.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=15.68 +/- 5.00
Episode length: 178.04 +/- 59.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 37       |
|    time_elapsed    | 862      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=16.26 +/- 4.67
Episode length: 180.48 +/- 52.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 180         |
|    mean_reward          | 16.3        |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.007244638 |
|    clip_fraction        | 0.0899      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.672      |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.149       |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 0.532       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=76500, episode_reward=15.58 +/- 5.56
Episode length: 175.92 +/- 56.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=15.72 +/- 5.08
Episode length: 177.34 +/- 54.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=16.28 +/- 3.89
Episode length: 181.70 +/- 45.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 38       |
|    time_elapsed    | 888      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=15.50 +/- 4.36
Episode length: 165.14 +/- 50.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 165          |
|    mean_reward          | 15.5         |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0074920016 |
|    clip_fraction        | 0.0863       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.642       |
|    explained_variance   | 0.898        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.19         |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.0106      |
|    value_loss           | 0.545        |
------------------------------------------
Eval num_timesteps=78500, episode_reward=16.28 +/- 4.61
Episode length: 179.30 +/- 51.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 16.3     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=16.52 +/- 4.32
Episode length: 174.70 +/- 49.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
New best mean reward!
Eval num_timesteps=79500, episode_reward=17.92 +/- 4.42
Episode length: 194.88 +/- 49.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 39       |
|    time_elapsed    | 915      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=16.04 +/- 4.19
Episode length: 175.88 +/- 47.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 176          |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0058391364 |
|    clip_fraction        | 0.072        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.729       |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.174        |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 0.553        |
------------------------------------------
Eval num_timesteps=80500, episode_reward=15.80 +/- 4.58
Episode length: 173.32 +/- 52.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=16.08 +/- 4.51
Episode length: 175.12 +/- 46.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=15.88 +/- 4.56
Episode length: 172.94 +/- 55.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 40       |
|    time_elapsed    | 940      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=17.84 +/- 4.10
Episode length: 186.00 +/- 44.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 186        |
|    mean_reward          | 17.8       |
| time/                   |            |
|    total_timesteps      | 82000      |
| train/                  |            |
|    approx_kl            | 0.00820093 |
|    clip_fraction        | 0.0705     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.902      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.119      |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.489      |
----------------------------------------
Eval num_timesteps=82500, episode_reward=16.76 +/- 4.07
Episode length: 173.14 +/- 48.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=16.62 +/- 3.95
Episode length: 171.38 +/- 42.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=17.66 +/- 4.86
Episode length: 185.10 +/- 54.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 17.7     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 41       |
|    time_elapsed    | 966      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=17.50 +/- 4.30
Episode length: 186.72 +/- 52.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 187          |
|    mean_reward          | 17.5         |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0077481586 |
|    clip_fraction        | 0.0824       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.715       |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.168        |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.0103      |
|    value_loss           | 0.584        |
------------------------------------------
Eval num_timesteps=84500, episode_reward=16.68 +/- 4.05
Episode length: 175.16 +/- 47.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=18.18 +/- 5.60
Episode length: 195.20 +/- 66.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
New best mean reward!
Eval num_timesteps=85500, episode_reward=16.84 +/- 4.21
Episode length: 174.20 +/- 46.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=17.88 +/- 4.10
Episode length: 189.68 +/- 45.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 42       |
|    time_elapsed    | 999      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=18.02 +/- 4.09
Episode length: 183.66 +/- 44.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 184         |
|    mean_reward          | 18          |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.010258086 |
|    clip_fraction        | 0.0909      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.73       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.209       |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.588       |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=17.18 +/- 4.68
Episode length: 177.70 +/- 50.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=17.64 +/- 4.42
Episode length: 181.76 +/- 52.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=17.94 +/- 4.85
Episode length: 186.70 +/- 52.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 43       |
|    time_elapsed    | 1026     |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=17.20 +/- 5.74
Episode length: 184.64 +/- 67.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 185         |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 88500       |
| train/                  |             |
|    approx_kl            | 0.008808253 |
|    clip_fraction        | 0.093       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.194       |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.51        |
-----------------------------------------
Eval num_timesteps=89000, episode_reward=16.74 +/- 4.78
Episode length: 175.64 +/- 57.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=16.04 +/- 3.86
Episode length: 166.48 +/- 44.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=16.94 +/- 4.44
Episode length: 176.68 +/- 48.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 44       |
|    time_elapsed    | 1051     |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=16.90 +/- 4.93
Episode length: 175.82 +/- 54.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 176         |
|    mean_reward          | 16.9        |
| time/                   |             |
|    total_timesteps      | 90500       |
| train/                  |             |
|    approx_kl            | 0.007828742 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.203       |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.494       |
-----------------------------------------
Eval num_timesteps=91000, episode_reward=17.40 +/- 5.15
Episode length: 182.52 +/- 59.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=16.68 +/- 3.65
Episode length: 175.26 +/- 40.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=16.94 +/- 4.08
Episode length: 174.92 +/- 49.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 45       |
|    time_elapsed    | 1078     |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=19.00 +/- 5.19
Episode length: 203.94 +/- 60.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 204         |
|    mean_reward          | 19          |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.008312285 |
|    clip_fraction        | 0.0978      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.158       |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.462       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=93000, episode_reward=17.54 +/- 5.18
Episode length: 181.44 +/- 55.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=17.38 +/- 4.56
Episode length: 184.70 +/- 54.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=16.60 +/- 5.00
Episode length: 175.74 +/- 57.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 46       |
|    time_elapsed    | 1107     |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=16.44 +/- 4.49
Episode length: 171.86 +/- 52.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 172         |
|    mean_reward          | 16.4        |
| time/                   |             |
|    total_timesteps      | 94500       |
| train/                  |             |
|    approx_kl            | 0.008910118 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.255       |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.602       |
-----------------------------------------
Eval num_timesteps=95000, episode_reward=18.04 +/- 5.05
Episode length: 192.76 +/- 59.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=17.04 +/- 4.42
Episode length: 181.08 +/- 49.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=15.82 +/- 4.32
Episode length: 165.16 +/- 47.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 15.8     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 47       |
|    time_elapsed    | 1133     |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=16.28 +/- 4.35
Episode length: 173.68 +/- 53.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 174         |
|    mean_reward          | 16.3        |
| time/                   |             |
|    total_timesteps      | 96500       |
| train/                  |             |
|    approx_kl            | 0.008635614 |
|    clip_fraction        | 0.0824      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.24        |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.562       |
-----------------------------------------
Eval num_timesteps=97000, episode_reward=16.58 +/- 5.10
Episode length: 180.06 +/- 59.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=16.08 +/- 4.87
Episode length: 172.32 +/- 50.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 172      |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=16.18 +/- 4.72
Episode length: 169.92 +/- 51.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 48       |
|    time_elapsed    | 1158     |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=17.18 +/- 4.35
Episode length: 178.20 +/- 47.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 178         |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 98500       |
| train/                  |             |
|    approx_kl            | 0.010257199 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.805      |
|    explained_variance   | 0.858       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.196       |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.531       |
-----------------------------------------
Eval num_timesteps=99000, episode_reward=16.94 +/- 4.87
Episode length: 175.28 +/- 54.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=17.24 +/- 5.55
Episode length: 179.72 +/- 63.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=17.38 +/- 4.26
Episode length: 186.02 +/- 50.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 49       |
|    time_elapsed    | 1185     |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=17.02 +/- 5.66
Episode length: 177.06 +/- 62.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 177         |
|    mean_reward          | 17          |
| time/                   |             |
|    total_timesteps      | 100500      |
| train/                  |             |
|    approx_kl            | 0.009370992 |
|    clip_fraction        | 0.0906      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.737      |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.201       |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.549       |
-----------------------------------------
Eval num_timesteps=101000, episode_reward=17.26 +/- 4.50
Episode length: 177.84 +/- 48.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=17.18 +/- 5.18
Episode length: 181.26 +/- 55.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=16.64 +/- 4.60
Episode length: 175.56 +/- 55.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 50       |
|    time_elapsed    | 1211     |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=17.16 +/- 4.16
Episode length: 179.68 +/- 44.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 180         |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.009914331 |
|    clip_fraction        | 0.0861      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.779      |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.182       |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.511       |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=16.88 +/- 4.04
Episode length: 176.52 +/- 44.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=16.92 +/- 3.77
Episode length: 175.16 +/- 44.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=16.86 +/- 4.59
Episode length: 174.08 +/- 49.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 51       |
|    time_elapsed    | 1237     |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=18.74 +/- 4.92
Episode length: 194.90 +/- 56.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 195         |
|    mean_reward          | 18.7        |
| time/                   |             |
|    total_timesteps      | 104500      |
| train/                  |             |
|    approx_kl            | 0.009302827 |
|    clip_fraction        | 0.0942      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.235       |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.624       |
-----------------------------------------
Eval num_timesteps=105000, episode_reward=16.02 +/- 4.95
Episode length: 167.46 +/- 56.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=16.82 +/- 5.09
Episode length: 174.36 +/- 56.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=18.80 +/- 4.95
Episode length: 193.08 +/- 59.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 52       |
|    time_elapsed    | 1264     |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=17.66 +/- 5.26
Episode length: 184.62 +/- 59.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 185         |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 106500      |
| train/                  |             |
|    approx_kl            | 0.007509353 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.73       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.165       |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.583       |
-----------------------------------------
Eval num_timesteps=107000, episode_reward=17.36 +/- 4.36
Episode length: 178.62 +/- 49.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=17.12 +/- 4.91
Episode length: 174.96 +/- 59.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=15.44 +/- 4.59
Episode length: 160.02 +/- 51.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 15.4     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=17.58 +/- 4.62
Episode length: 180.70 +/- 50.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 83       |
|    iterations      | 53       |
|    time_elapsed    | 1295     |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=18.20 +/- 4.18
Episode length: 188.56 +/- 46.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 189          |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0076422845 |
|    clip_fraction        | 0.0948       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.717       |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.219        |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.536        |
------------------------------------------
Eval num_timesteps=109500, episode_reward=17.58 +/- 3.95
Episode length: 176.14 +/- 43.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=17.18 +/- 4.00
Episode length: 174.40 +/- 45.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=18.00 +/- 4.89
Episode length: 186.46 +/- 55.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 54       |
|    time_elapsed    | 1322     |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=17.48 +/- 4.41
Episode length: 177.50 +/- 52.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 178         |
|    mean_reward          | 17.5        |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.010277456 |
|    clip_fraction        | 0.0974      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.102       |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.489       |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=16.22 +/- 4.31
Episode length: 168.10 +/- 49.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=16.82 +/- 4.12
Episode length: 173.70 +/- 49.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=17.18 +/- 4.77
Episode length: 176.98 +/- 54.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 55       |
|    time_elapsed    | 1347     |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=16.14 +/- 4.40
Episode length: 165.98 +/- 48.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 16.1        |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.012693841 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.261       |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.688       |
-----------------------------------------
Eval num_timesteps=113500, episode_reward=16.40 +/- 4.16
Episode length: 169.32 +/- 45.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=17.36 +/- 3.35
Episode length: 177.12 +/- 38.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=15.62 +/- 4.56
Episode length: 164.44 +/- 51.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 56       |
|    time_elapsed    | 1372     |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=17.26 +/- 4.54
Episode length: 179.34 +/- 53.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 179         |
|    mean_reward          | 17.3        |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.008913716 |
|    clip_fraction        | 0.0843      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.224       |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.579       |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=17.64 +/- 4.73
Episode length: 183.22 +/- 53.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=17.52 +/- 4.33
Episode length: 180.04 +/- 50.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=17.06 +/- 4.39
Episode length: 175.92 +/- 48.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 57       |
|    time_elapsed    | 1399     |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=17.98 +/- 4.87
Episode length: 187.04 +/- 57.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 187         |
|    mean_reward          | 18          |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.011428975 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.679      |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.204       |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.519       |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=16.52 +/- 4.04
Episode length: 167.54 +/- 41.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=17.06 +/- 4.61
Episode length: 177.56 +/- 47.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=16.48 +/- 4.16
Episode length: 167.62 +/- 45.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 58       |
|    time_elapsed    | 1424     |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=17.56 +/- 3.79
Episode length: 182.12 +/- 46.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 182         |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.012741344 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.617      |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.181       |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.626       |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=17.96 +/- 4.36
Episode length: 186.28 +/- 48.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=17.64 +/- 4.81
Episode length: 179.76 +/- 56.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=16.66 +/- 4.39
Episode length: 168.96 +/- 44.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 59       |
|    time_elapsed    | 1450     |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=17.16 +/- 4.77
Episode length: 173.82 +/- 55.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 174         |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.010430024 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.625      |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0898      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.482       |
-----------------------------------------
Eval num_timesteps=121500, episode_reward=17.24 +/- 4.68
Episode length: 176.18 +/- 53.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=17.24 +/- 5.44
Episode length: 174.96 +/- 58.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=15.70 +/- 4.64
Episode length: 159.18 +/- 53.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 60       |
|    time_elapsed    | 1475     |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=17.26 +/- 4.58
Episode length: 171.38 +/- 46.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 171         |
|    mean_reward          | 17.3        |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.010865514 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.351       |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.687       |
-----------------------------------------
Eval num_timesteps=123500, episode_reward=16.76 +/- 3.98
Episode length: 170.30 +/- 46.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | 16.8     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=16.22 +/- 4.41
Episode length: 165.78 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=17.46 +/- 5.63
Episode length: 176.84 +/- 63.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 17.5     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 61       |
|    time_elapsed    | 1500     |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=17.84 +/- 4.42
Episode length: 177.42 +/- 52.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 177         |
|    mean_reward          | 17.8        |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.007869468 |
|    clip_fraction        | 0.0914      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.613      |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.235       |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.695       |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=17.64 +/- 4.39
Episode length: 176.78 +/- 47.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=17.76 +/- 4.04
Episode length: 181.14 +/- 47.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=19.02 +/- 4.30
Episode length: 190.32 +/- 45.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 62       |
|    time_elapsed    | 1527     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=17.26 +/- 4.72
Episode length: 173.62 +/- 54.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 174         |
|    mean_reward          | 17.3        |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.010309756 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.127       |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.572       |
-----------------------------------------
Eval num_timesteps=127500, episode_reward=17.62 +/- 5.61
Episode length: 179.26 +/- 65.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=17.42 +/- 5.52
Episode length: 172.58 +/- 61.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=18.08 +/- 4.55
Episode length: 182.82 +/- 52.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=18.88 +/- 4.07
Episode length: 190.66 +/- 43.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 63       |
|    time_elapsed    | 1559     |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=17.60 +/- 5.02
Episode length: 174.08 +/- 55.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 174         |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.009624681 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.644      |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.161       |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.55        |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=17.22 +/- 4.73
Episode length: 173.96 +/- 50.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=17.36 +/- 4.10
Episode length: 175.66 +/- 45.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 17.4     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=17.24 +/- 4.72
Episode length: 175.60 +/- 49.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 64       |
|    time_elapsed    | 1585     |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=18.54 +/- 3.99
Episode length: 185.46 +/- 38.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 185         |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.010602314 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.14        |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.555       |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=19.34 +/- 5.52
Episode length: 192.66 +/- 65.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
New best mean reward!
Eval num_timesteps=132500, episode_reward=18.32 +/- 5.65
Episode length: 181.76 +/- 62.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=18.20 +/- 4.45
Episode length: 182.66 +/- 47.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 65       |
|    time_elapsed    | 1612     |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=19.98 +/- 5.07
Episode length: 204.72 +/- 58.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 205         |
|    mean_reward          | 20          |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.009915102 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.177       |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.622       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=134000, episode_reward=17.34 +/- 4.67
Episode length: 175.16 +/- 52.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=18.18 +/- 4.93
Episode length: 182.06 +/- 53.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=18.62 +/- 5.03
Episode length: 185.60 +/- 53.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 66       |
|    time_elapsed    | 1639     |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=18.32 +/- 4.68
Episode length: 183.40 +/- 50.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | 18.3        |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.012856001 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.249       |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.717       |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=18.84 +/- 4.45
Episode length: 188.88 +/- 50.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=18.82 +/- 4.21
Episode length: 184.16 +/- 43.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=18.72 +/- 4.88
Episode length: 188.74 +/- 52.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 67       |
|    time_elapsed    | 1666     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=19.42 +/- 4.40
Episode length: 193.66 +/- 51.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | 19.4        |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.013709212 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.275       |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.681       |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=17.28 +/- 4.00
Episode length: 173.54 +/- 43.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=19.22 +/- 5.72
Episode length: 198.96 +/- 66.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=18.44 +/- 4.69
Episode length: 183.78 +/- 52.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 82       |
|    iterations      | 68       |
|    time_elapsed    | 1693     |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=18.48 +/- 5.01
Episode length: 188.42 +/- 56.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 188       |
|    mean_reward          | 18.5      |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0093991 |
|    clip_fraction        | 0.123     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.625    |
|    explained_variance   | 0.887     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.163     |
|    n_updates            | 680       |
|    policy_gradient_loss | -0.0173   |
|    value_loss           | 0.484     |
---------------------------------------
Eval num_timesteps=140000, episode_reward=17.88 +/- 4.54
Episode length: 177.92 +/- 50.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=18.56 +/- 4.66
Episode length: 182.68 +/- 53.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=19.00 +/- 5.09
Episode length: 190.70 +/- 59.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 69       |
|    time_elapsed    | 1720     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=17.92 +/- 4.33
Episode length: 179.40 +/- 48.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 179         |
|    mean_reward          | 17.9        |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.009040565 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.556      |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.176       |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.624       |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=18.54 +/- 4.37
Episode length: 190.94 +/- 48.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=20.20 +/- 4.76
Episode length: 206.38 +/- 53.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 20.2     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
New best mean reward!
Eval num_timesteps=143000, episode_reward=18.42 +/- 4.89
Episode length: 183.50 +/- 52.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 70       |
|    time_elapsed    | 1748     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=19.76 +/- 5.42
Episode length: 201.46 +/- 66.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 201        |
|    mean_reward          | 19.8       |
| time/                   |            |
|    total_timesteps      | 143500     |
| train/                  |            |
|    approx_kl            | 0.01213975 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.635     |
|    explained_variance   | 0.876      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.161      |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.019     |
|    value_loss           | 0.528      |
----------------------------------------
Eval num_timesteps=144000, episode_reward=17.16 +/- 3.61
Episode length: 171.76 +/- 39.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 172      |
|    mean_reward     | 17.2     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=19.16 +/- 4.62
Episode length: 192.74 +/- 49.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=17.86 +/- 4.10
Episode length: 179.52 +/- 43.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 17.9     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 71       |
|    time_elapsed    | 1775     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=19.82 +/- 5.05
Episode length: 202.34 +/- 56.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 19.8        |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.010273531 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.163       |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.552       |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=18.98 +/- 5.29
Episode length: 194.18 +/- 63.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=19.02 +/- 4.54
Episode length: 192.16 +/- 53.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=18.56 +/- 4.27
Episode length: 185.64 +/- 48.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 72       |
|    time_elapsed    | 1803     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=18.36 +/- 5.13
Episode length: 188.60 +/- 57.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 189         |
|    mean_reward          | 18.4        |
| time/                   |             |
|    total_timesteps      | 147500      |
| train/                  |             |
|    approx_kl            | 0.012588185 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.136       |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.477       |
-----------------------------------------
Eval num_timesteps=148000, episode_reward=18.74 +/- 4.26
Episode length: 186.28 +/- 45.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=17.62 +/- 5.44
Episode length: 178.94 +/- 59.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=18.28 +/- 5.09
Episode length: 184.98 +/- 55.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=18.52 +/- 5.19
Episode length: 189.20 +/- 58.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 81       |
|    iterations      | 73       |
|    time_elapsed    | 1837     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=17.76 +/- 4.76
Episode length: 182.06 +/- 52.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 182         |
|    mean_reward          | 17.8        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.012778175 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0847      |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.568       |
-----------------------------------------
Eval num_timesteps=150500, episode_reward=19.64 +/- 5.12
Episode length: 200.16 +/- 58.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=18.36 +/- 4.87
Episode length: 187.08 +/- 54.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=18.74 +/- 5.02
Episode length: 190.96 +/- 55.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 19.2     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 74       |
|    time_elapsed    | 1864     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=18.62 +/- 5.07
Episode length: 183.02 +/- 56.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 183        |
|    mean_reward          | 18.6       |
| time/                   |            |
|    total_timesteps      | 152000     |
| train/                  |            |
|    approx_kl            | 0.00968487 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.129      |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 0.482      |
----------------------------------------
Eval num_timesteps=152500, episode_reward=19.02 +/- 4.05
Episode length: 188.70 +/- 44.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=18.02 +/- 4.02
Episode length: 182.42 +/- 44.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | 18       |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=18.50 +/- 5.06
Episode length: 186.64 +/- 58.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | 18.5     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 19.3     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 75       |
|    time_elapsed    | 1892     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=17.94 +/- 4.33
Episode length: 180.06 +/- 47.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 180         |
|    mean_reward          | 17.9        |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.010020833 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.167       |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.557       |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=19.80 +/- 4.36
Episode length: 194.80 +/- 47.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 19.8     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=19.02 +/- 4.64
Episode length: 191.96 +/- 49.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=19.06 +/- 5.35
Episode length: 195.54 +/- 61.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 196      |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 19.3     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 76       |
|    time_elapsed    | 1920     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=19.26 +/- 5.16
Episode length: 196.56 +/- 62.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 197         |
|    mean_reward          | 19.3        |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.015328319 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.678      |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.144       |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.515       |
-----------------------------------------
Eval num_timesteps=156500, episode_reward=18.34 +/- 5.27
Episode length: 184.78 +/- 58.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=19.24 +/- 5.17
Episode length: 197.74 +/- 59.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | 19.2     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=18.74 +/- 4.06
Episode length: 187.28 +/- 40.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 77       |
|    time_elapsed    | 1948     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=18.92 +/- 5.12
Episode length: 187.78 +/- 54.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 188         |
|    mean_reward          | 18.9        |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.014455752 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.669      |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.182       |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.631       |
-----------------------------------------
Eval num_timesteps=158500, episode_reward=17.78 +/- 4.92
Episode length: 181.10 +/- 55.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=19.28 +/- 5.28
Episode length: 194.02 +/- 63.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=20.00 +/- 4.98
Episode length: 203.48 +/- 57.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | 20       |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 78       |
|    time_elapsed    | 1975     |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=19.70 +/- 4.82
Episode length: 200.54 +/- 54.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 201         |
|    mean_reward          | 19.7        |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.013809293 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.653      |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.111       |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.525       |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=19.04 +/- 4.09
Episode length: 191.84 +/- 48.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=19.14 +/- 4.76
Episode length: 193.16 +/- 56.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=19.12 +/- 4.63
Episode length: 198.16 +/- 53.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 20       |
| time/              |          |
|    fps             | 80       |
|    iterations      | 79       |
|    time_elapsed    | 2004     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=18.38 +/- 4.79
Episode length: 188.48 +/- 53.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 188         |
|    mean_reward          | 18.4        |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.009126453 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.617      |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.165       |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.513       |
-----------------------------------------
Eval num_timesteps=162500, episode_reward=18.24 +/- 3.79
Episode length: 185.06 +/- 43.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=20.50 +/- 5.07
Episode length: 211.78 +/- 60.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 20.5     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
New best mean reward!
Eval num_timesteps=163500, episode_reward=19.04 +/- 4.66
Episode length: 193.72 +/- 51.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 80       |
|    time_elapsed    | 2032     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=18.58 +/- 4.70
Episode length: 189.36 +/- 51.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 189         |
|    mean_reward          | 18.6        |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.013722036 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.617      |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.235       |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.618       |
-----------------------------------------
Eval num_timesteps=164500, episode_reward=17.84 +/- 5.12
Episode length: 178.64 +/- 50.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=17.60 +/- 3.77
Episode length: 173.78 +/- 45.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 17.6     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=18.96 +/- 5.87
Episode length: 193.50 +/- 65.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 20       |
| time/              |          |
|    fps             | 80       |
|    iterations      | 81       |
|    time_elapsed    | 2059     |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=18.90 +/- 4.96
Episode length: 189.08 +/- 54.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 189         |
|    mean_reward          | 18.9        |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.013477447 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.64       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.247       |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.709       |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=18.24 +/- 5.42
Episode length: 185.60 +/- 59.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=18.34 +/- 4.59
Episode length: 183.40 +/- 50.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=17.06 +/- 3.59
Episode length: 169.04 +/- 35.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 20       |
| time/              |          |
|    fps             | 80       |
|    iterations      | 82       |
|    time_elapsed    | 2085     |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=18.74 +/- 5.04
Episode length: 189.14 +/- 55.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 189         |
|    mean_reward          | 18.7        |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.011001773 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.253       |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.641       |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=18.08 +/- 4.21
Episode length: 184.82 +/- 47.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=19.84 +/- 4.70
Episode length: 204.56 +/- 50.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 19.8     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=18.62 +/- 5.32
Episode length: 192.64 +/- 57.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 19.7     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 83       |
|    time_elapsed    | 2113     |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=18.06 +/- 4.64
Episode length: 183.14 +/- 52.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | 18.1        |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.013632633 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.628      |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.202       |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.55        |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=19.04 +/- 4.27
Episode length: 189.46 +/- 49.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=18.98 +/- 4.60
Episode length: 188.80 +/- 47.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=18.38 +/- 4.75
Episode length: 185.28 +/- 48.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=20.06 +/- 3.75
Episode length: 198.62 +/- 44.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | 20.1     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 84       |
|    time_elapsed    | 2147     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=19.72 +/- 4.94
Episode length: 201.40 +/- 53.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 201         |
|    mean_reward          | 19.7        |
| time/                   |             |
|    total_timesteps      | 172500      |
| train/                  |             |
|    approx_kl            | 0.014408199 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.667      |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.168       |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 0.454       |
-----------------------------------------
Eval num_timesteps=173000, episode_reward=20.00 +/- 4.66
Episode length: 200.92 +/- 53.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 20       |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=18.40 +/- 5.01
Episode length: 186.30 +/- 58.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=19.08 +/- 4.54
Episode length: 191.60 +/- 49.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 19.1     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 19.3     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 85       |
|    time_elapsed    | 2176     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=19.22 +/- 4.57
Episode length: 186.58 +/- 51.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 187        |
|    mean_reward          | 19.2       |
| time/                   |            |
|    total_timesteps      | 174500     |
| train/                  |            |
|    approx_kl            | 0.01591924 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.662     |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.125      |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.0231    |
|    value_loss           | 0.448      |
----------------------------------------
Eval num_timesteps=175000, episode_reward=18.12 +/- 4.97
Episode length: 181.70 +/- 55.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | 18.1     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=20.78 +/- 5.49
Episode length: 207.16 +/- 63.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | 20.8     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
New best mean reward!
Eval num_timesteps=176000, episode_reward=18.88 +/- 4.38
Episode length: 188.06 +/- 47.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 188      |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 86       |
|    time_elapsed    | 2203     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=19.48 +/- 4.52
Episode length: 194.48 +/- 55.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | 19.5        |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.012982756 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0807      |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 0.508       |
-----------------------------------------
Eval num_timesteps=177000, episode_reward=18.38 +/- 4.72
Episode length: 181.62 +/- 48.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=19.32 +/- 4.81
Episode length: 191.54 +/- 52.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=18.64 +/- 5.02
Episode length: 186.64 +/- 57.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 87       |
|    time_elapsed    | 2231     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=18.30 +/- 4.30
Episode length: 182.82 +/- 47.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | 18.3        |
| time/                   |             |
|    total_timesteps      | 178500      |
| train/                  |             |
|    approx_kl            | 0.015614993 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.128       |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.445       |
-----------------------------------------
Eval num_timesteps=179000, episode_reward=19.72 +/- 5.13
Episode length: 197.52 +/- 52.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=19.78 +/- 5.18
Episode length: 195.00 +/- 55.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 19.8     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=19.62 +/- 5.15
Episode length: 195.38 +/- 59.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 88       |
|    time_elapsed    | 2259     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=21.40 +/- 4.72
Episode length: 213.80 +/- 51.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 214         |
|    mean_reward          | 21.4        |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.014121425 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.293       |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.554       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=181000, episode_reward=19.86 +/- 4.89
Episode length: 193.22 +/- 51.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 19.9     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=20.26 +/- 4.52
Episode length: 200.06 +/- 50.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 20.3     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=20.82 +/- 5.89
Episode length: 207.60 +/- 66.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | 20.8     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 89       |
|    time_elapsed    | 2289     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=19.08 +/- 4.75
Episode length: 189.16 +/- 53.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 189         |
|    mean_reward          | 19.1        |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.018571436 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.612      |
|    explained_variance   | 0.868       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.157       |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.519       |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=19.64 +/- 5.02
Episode length: 197.10 +/- 56.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=20.00 +/- 4.41
Episode length: 198.70 +/- 44.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | 20       |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=20.22 +/- 5.83
Episode length: 205.06 +/- 66.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 20.2     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 90       |
|    time_elapsed    | 2318     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=20.02 +/- 4.99
Episode length: 198.28 +/- 50.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 198         |
|    mean_reward          | 20          |
| time/                   |             |
|    total_timesteps      | 184500      |
| train/                  |             |
|    approx_kl            | 0.013772981 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.173       |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.567       |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=21.42 +/- 5.11
Episode length: 214.04 +/- 57.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
New best mean reward!
Eval num_timesteps=185500, episode_reward=19.26 +/- 4.81
Episode length: 192.26 +/- 54.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=20.68 +/- 5.22
Episode length: 204.42 +/- 56.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | 20.7     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 91       |
|    time_elapsed    | 2347     |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=19.46 +/- 4.46
Episode length: 189.18 +/- 49.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 189         |
|    mean_reward          | 19.5        |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.015383294 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.679      |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.136       |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.554       |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=20.26 +/- 4.90
Episode length: 199.40 +/- 52.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | 20.3     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=19.72 +/- 4.94
Episode length: 194.84 +/- 54.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=20.94 +/- 3.74
Episode length: 200.98 +/- 44.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 20.9     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 92       |
|    time_elapsed    | 2375     |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=19.94 +/- 4.83
Episode length: 196.44 +/- 48.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 196          |
|    mean_reward          | 19.9         |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 0.0139995925 |
|    clip_fraction        | 0.13         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.653       |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.264        |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.0177      |
|    value_loss           | 0.665        |
------------------------------------------
Eval num_timesteps=189000, episode_reward=20.26 +/- 5.18
Episode length: 197.22 +/- 55.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 20.3     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=20.54 +/- 4.78
Episode length: 200.38 +/- 50.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 20.5     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=18.94 +/- 5.25
Episode length: 185.96 +/- 53.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 18.9     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 20       |
| time/              |          |
|    fps             | 79       |
|    iterations      | 93       |
|    time_elapsed    | 2404     |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=19.28 +/- 4.63
Episode length: 188.28 +/- 52.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 188        |
|    mean_reward          | 19.3       |
| time/                   |            |
|    total_timesteps      | 190500     |
| train/                  |            |
|    approx_kl            | 0.01670688 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.715     |
|    explained_variance   | 0.885      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.22       |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0256    |
|    value_loss           | 0.611      |
----------------------------------------
Eval num_timesteps=191000, episode_reward=20.78 +/- 5.53
Episode length: 207.46 +/- 60.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | 20.8     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=21.06 +/- 5.68
Episode length: 206.48 +/- 64.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 21.1     |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=20.22 +/- 4.05
Episode length: 197.60 +/- 40.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | 20.2     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=21.66 +/- 4.31
Episode length: 208.90 +/- 48.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 20.2     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 94       |
|    time_elapsed    | 2440     |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=21.18 +/- 5.66
Episode length: 207.88 +/- 59.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 208          |
|    mean_reward          | 21.2         |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0141446125 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.676       |
|    explained_variance   | 0.869        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.203        |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.0211      |
|    value_loss           | 0.746        |
------------------------------------------
Eval num_timesteps=193500, episode_reward=22.02 +/- 5.46
Episode length: 220.50 +/- 62.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
New best mean reward!
Eval num_timesteps=194000, episode_reward=20.66 +/- 4.52
Episode length: 200.28 +/- 46.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 20.7     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=20.18 +/- 5.16
Episode length: 197.70 +/- 51.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | 20.2     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 20.2     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 95       |
|    time_elapsed    | 2470     |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=20.50 +/- 5.51
Episode length: 198.90 +/- 59.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 20.5        |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.013965251 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.686      |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.214       |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.695       |
-----------------------------------------
Eval num_timesteps=195500, episode_reward=20.40 +/- 5.56
Episode length: 199.78 +/- 59.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 20.4     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=21.62 +/- 6.42
Episode length: 214.96 +/- 68.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 21.6     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=23.74 +/- 5.36
Episode length: 230.62 +/- 59.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 96       |
|    time_elapsed    | 2500     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=21.80 +/- 5.15
Episode length: 218.66 +/- 57.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 219        |
|    mean_reward          | 21.8       |
| time/                   |            |
|    total_timesteps      | 197000     |
| train/                  |            |
|    approx_kl            | 0.01662368 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.668     |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.142      |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0226    |
|    value_loss           | 0.77       |
----------------------------------------
Eval num_timesteps=197500, episode_reward=19.58 +/- 5.90
Episode length: 191.50 +/- 57.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 19.6     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=21.80 +/- 5.26
Episode length: 217.00 +/- 56.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=20.92 +/- 4.60
Episode length: 205.60 +/- 51.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 20.9     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 20       |
| time/              |          |
|    fps             | 78       |
|    iterations      | 97       |
|    time_elapsed    | 2530     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=21.12 +/- 6.01
Episode length: 206.68 +/- 62.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | 21.1        |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.014895746 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.724      |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.209       |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.6         |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=20.24 +/- 4.96
Episode length: 200.50 +/- 53.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 20.2     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=21.18 +/- 5.42
Episode length: 204.04 +/- 55.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | 21.2     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=19.66 +/- 5.21
Episode length: 191.14 +/- 48.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 19.7     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 19.8     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 98       |
|    time_elapsed    | 2559     |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=21.02 +/- 4.32
Episode length: 203.20 +/- 50.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 21          |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.016174639 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.225       |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.0224     |
|    value_loss           | 0.624       |
-----------------------------------------
Eval num_timesteps=201500, episode_reward=21.26 +/- 4.42
Episode length: 207.94 +/- 47.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | 21.3     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=20.94 +/- 5.94
Episode length: 205.06 +/- 61.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 20.9     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=21.24 +/- 6.08
Episode length: 211.96 +/- 63.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 21.2     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 99       |
|    time_elapsed    | 2589     |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=22.06 +/- 5.80
Episode length: 213.44 +/- 61.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 213         |
|    mean_reward          | 22.1        |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.015602359 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.14        |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.479       |
-----------------------------------------
Eval num_timesteps=203500, episode_reward=21.38 +/- 4.76
Episode length: 203.68 +/- 45.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=21.74 +/- 4.37
Episode length: 210.06 +/- 45.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=22.54 +/- 5.43
Episode length: 217.34 +/- 57.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 19.7     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 100      |
|    time_elapsed    | 2620     |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=21.54 +/- 5.15
Episode length: 203.72 +/- 55.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 204        |
|    mean_reward          | 21.5       |
| time/                   |            |
|    total_timesteps      | 205000     |
| train/                  |            |
|    approx_kl            | 0.01670457 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.703     |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.172      |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0243    |
|    value_loss           | 0.635      |
----------------------------------------
Eval num_timesteps=205500, episode_reward=20.62 +/- 5.16
Episode length: 199.34 +/- 50.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | 20.6     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=22.74 +/- 5.96
Episode length: 220.56 +/- 64.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=22.20 +/- 5.00
Episode length: 215.38 +/- 53.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 101      |
|    time_elapsed    | 2650     |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=21.74 +/- 4.85
Episode length: 211.28 +/- 48.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | 21.7        |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.014812324 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.645      |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.194       |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.704       |
-----------------------------------------
Eval num_timesteps=207500, episode_reward=22.12 +/- 3.97
Episode length: 214.76 +/- 46.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=20.44 +/- 5.08
Episode length: 194.60 +/- 56.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 20.4     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=22.32 +/- 5.79
Episode length: 215.96 +/- 68.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 102      |
|    time_elapsed    | 2680     |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=22.92 +/- 6.21
Episode length: 222.68 +/- 68.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | 22.9        |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.013948348 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.131       |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.524       |
-----------------------------------------
Eval num_timesteps=209500, episode_reward=21.72 +/- 4.67
Episode length: 209.18 +/- 52.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 21.7     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=20.16 +/- 4.86
Episode length: 194.24 +/- 54.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 20.2     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=23.04 +/- 5.05
Episode length: 220.20 +/- 54.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 19.8     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 103      |
|    time_elapsed    | 2710     |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=20.58 +/- 5.00
Episode length: 194.22 +/- 53.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | 20.6        |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.015137668 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.646      |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.166       |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.637       |
-----------------------------------------
Eval num_timesteps=211500, episode_reward=22.44 +/- 6.33
Episode length: 215.42 +/- 66.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=21.94 +/- 4.95
Episode length: 211.58 +/- 49.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=22.92 +/- 6.71
Episode length: 219.84 +/- 69.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 19.8     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 104      |
|    time_elapsed    | 2741     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=21.66 +/- 5.19
Episode length: 206.36 +/- 56.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 206        |
|    mean_reward          | 21.7       |
| time/                   |            |
|    total_timesteps      | 213000     |
| train/                  |            |
|    approx_kl            | 0.01770079 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.131      |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0251    |
|    value_loss           | 0.504      |
----------------------------------------
Eval num_timesteps=213500, episode_reward=21.00 +/- 5.87
Episode length: 202.54 +/- 63.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | 21       |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=20.58 +/- 4.72
Episode length: 194.28 +/- 48.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 20.6     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=22.14 +/- 6.20
Episode length: 214.38 +/- 67.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=22.44 +/- 4.91
Episode length: 218.80 +/- 50.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 105      |
|    time_elapsed    | 2777     |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=21.96 +/- 3.84
Episode length: 209.52 +/- 43.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | 22          |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.019907642 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0948      |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.446       |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=21.34 +/- 4.76
Episode length: 200.62 +/- 48.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 21.3     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=20.82 +/- 4.99
Episode length: 202.88 +/- 55.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | 20.8     |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=20.58 +/- 5.28
Episode length: 198.14 +/- 54.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | 20.6     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 106      |
|    time_elapsed    | 2807     |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=21.58 +/- 5.49
Episode length: 208.18 +/- 62.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 208         |
|    mean_reward          | 21.6        |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.019010007 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.123       |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 0.594       |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=22.28 +/- 5.49
Episode length: 214.60 +/- 62.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=20.74 +/- 4.57
Episode length: 197.12 +/- 44.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 20.7     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=20.90 +/- 4.73
Episode length: 199.26 +/- 52.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | 20.9     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 107      |
|    time_elapsed    | 2836     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=22.74 +/- 5.63
Episode length: 218.76 +/- 64.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 219         |
|    mean_reward          | 22.7        |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.017820634 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.626      |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.114       |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.0274     |
|    value_loss           | 0.607       |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=22.00 +/- 5.36
Episode length: 211.36 +/- 59.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=22.00 +/- 6.39
Episode length: 209.72 +/- 67.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=20.68 +/- 4.99
Episode length: 200.14 +/- 53.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 20.7     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 20.5     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 108      |
|    time_elapsed    | 2866     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=21.26 +/- 4.96
Episode length: 202.94 +/- 52.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 21.3        |
| time/                   |             |
|    total_timesteps      | 221500      |
| train/                  |             |
|    approx_kl            | 0.021507803 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.627      |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.115       |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 0.545       |
-----------------------------------------
Eval num_timesteps=222000, episode_reward=23.16 +/- 6.02
Episode length: 222.30 +/- 63.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=23.10 +/- 4.78
Episode length: 218.00 +/- 52.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=22.40 +/- 6.52
Episode length: 215.32 +/- 69.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 20.7     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 109      |
|    time_elapsed    | 2897     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=21.78 +/- 5.46
Episode length: 209.12 +/- 59.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 209        |
|    mean_reward          | 21.8       |
| time/                   |            |
|    total_timesteps      | 223500     |
| train/                  |            |
|    approx_kl            | 0.01706927 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.652     |
|    explained_variance   | 0.879      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.156      |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0226    |
|    value_loss           | 0.483      |
----------------------------------------
Eval num_timesteps=224000, episode_reward=22.36 +/- 5.77
Episode length: 215.22 +/- 61.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=22.12 +/- 5.31
Episode length: 212.44 +/- 56.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=21.48 +/- 5.99
Episode length: 203.52 +/- 63.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 110      |
|    time_elapsed    | 2928     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=23.80 +/- 6.68
Episode length: 228.10 +/- 72.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 228         |
|    mean_reward          | 23.8        |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.024030143 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.111       |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.0287     |
|    value_loss           | 0.546       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=226000, episode_reward=22.28 +/- 4.56
Episode length: 212.14 +/- 47.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=22.16 +/- 5.22
Episode length: 214.08 +/- 55.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=23.30 +/- 5.79
Episode length: 219.96 +/- 60.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | 21       |
| time/              |          |
|    fps             | 76       |
|    iterations      | 111      |
|    time_elapsed    | 2959     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=21.56 +/- 4.13
Episode length: 201.06 +/- 45.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 201        |
|    mean_reward          | 21.6       |
| time/                   |            |
|    total_timesteps      | 227500     |
| train/                  |            |
|    approx_kl            | 0.01724834 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.104      |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0251    |
|    value_loss           | 0.537      |
----------------------------------------
Eval num_timesteps=228000, episode_reward=22.30 +/- 4.71
Episode length: 216.20 +/- 52.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=21.32 +/- 4.73
Episode length: 200.38 +/- 54.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 21.3     |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=22.34 +/- 5.06
Episode length: 213.16 +/- 57.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 112      |
|    time_elapsed    | 2989     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=21.86 +/- 6.06
Episode length: 209.92 +/- 64.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 21.9       |
| time/                   |            |
|    total_timesteps      | 229500     |
| train/                  |            |
|    approx_kl            | 0.02188463 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.644     |
|    explained_variance   | 0.894      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.156      |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0245    |
|    value_loss           | 0.589      |
----------------------------------------
Eval num_timesteps=230000, episode_reward=21.86 +/- 6.45
Episode length: 209.44 +/- 63.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=22.72 +/- 5.56
Episode length: 214.40 +/- 59.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=22.58 +/- 6.00
Episode length: 213.10 +/- 64.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 207      |
|    ep_rew_mean     | 21       |
| time/              |          |
|    fps             | 76       |
|    iterations      | 113      |
|    time_elapsed    | 3020     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=23.08 +/- 4.99
Episode length: 218.40 +/- 56.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 218         |
|    mean_reward          | 23.1        |
| time/                   |             |
|    total_timesteps      | 231500      |
| train/                  |             |
|    approx_kl            | 0.020658214 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.619      |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.175       |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 0.616       |
-----------------------------------------
Eval num_timesteps=232000, episode_reward=22.66 +/- 5.01
Episode length: 216.46 +/- 54.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=23.42 +/- 5.98
Episode length: 222.64 +/- 62.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=23.94 +/- 5.89
Episode length: 227.14 +/- 63.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | 21.1     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 114      |
|    time_elapsed    | 3051     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=23.04 +/- 5.41
Episode length: 223.26 +/- 55.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 223       |
|    mean_reward          | 23        |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.0201453 |
|    clip_fraction        | 0.171     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.639    |
|    explained_variance   | 0.887     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.194     |
|    n_updates            | 1140      |
|    policy_gradient_loss | -0.0237   |
|    value_loss           | 0.608     |
---------------------------------------
Eval num_timesteps=234000, episode_reward=22.36 +/- 5.57
Episode length: 214.62 +/- 59.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=23.00 +/- 4.76
Episode length: 218.12 +/- 47.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=23.14 +/- 5.47
Episode length: 221.84 +/- 53.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=22.62 +/- 6.09
Episode length: 215.28 +/- 64.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 21       |
| time/              |          |
|    fps             | 76       |
|    iterations      | 115      |
|    time_elapsed    | 3090     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=21.90 +/- 5.00
Episode length: 204.64 +/- 49.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 205         |
|    mean_reward          | 21.9        |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.021173138 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.239       |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 0.653       |
-----------------------------------------
Eval num_timesteps=236500, episode_reward=22.52 +/- 6.83
Episode length: 217.86 +/- 73.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=24.06 +/- 5.98
Episode length: 232.98 +/- 64.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
New best mean reward!
Eval num_timesteps=237500, episode_reward=22.46 +/- 5.81
Episode length: 216.20 +/- 62.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 210      |
|    ep_rew_mean     | 21.4     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 116      |
|    time_elapsed    | 3122     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=22.16 +/- 5.61
Episode length: 211.50 +/- 58.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 212         |
|    mean_reward          | 22.2        |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.021402119 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.172       |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 0.535       |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=22.38 +/- 7.32
Episode length: 211.76 +/- 75.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=22.06 +/- 6.78
Episode length: 210.32 +/- 69.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | 22.1     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=21.92 +/- 5.86
Episode length: 204.54 +/- 56.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 21.9     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 117      |
|    time_elapsed    | 3152     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=21.80 +/- 5.20
Episode length: 207.28 +/- 55.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | 21.8        |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.022917297 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0001      |
|    loss                 | 0.123       |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.519       |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=22.48 +/- 5.48
Episode length: 212.06 +/- 55.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=23.00 +/- 5.02
Episode length: 215.68 +/- 51.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=23.98 +/- 5.11
Episode length: 229.46 +/- 52.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 118      |
|    time_elapsed    | 3183     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=23.28 +/- 5.34
Episode length: 220.22 +/- 57.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 220         |
|    mean_reward          | 23.3        |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.024187503 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.238       |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.722       |
-----------------------------------------
Eval num_timesteps=242500, episode_reward=23.76 +/- 5.65
Episode length: 223.70 +/- 58.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=22.72 +/- 5.87
Episode length: 217.18 +/- 61.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=21.82 +/- 4.77
Episode length: 204.04 +/- 48.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 21.7     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 119      |
|    time_elapsed    | 3214     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=21.80 +/- 5.61
Episode length: 205.88 +/- 57.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 206        |
|    mean_reward          | 21.8       |
| time/                   |            |
|    total_timesteps      | 244000     |
| train/                  |            |
|    approx_kl            | 0.02156781 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.623     |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.242      |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0197    |
|    value_loss           | 0.73       |
----------------------------------------
Eval num_timesteps=244500, episode_reward=24.26 +/- 4.89
Episode length: 229.26 +/- 54.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
New best mean reward!
Eval num_timesteps=245000, episode_reward=22.24 +/- 5.00
Episode length: 211.24 +/- 52.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 22.2     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=22.70 +/- 4.96
Episode length: 215.66 +/- 55.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 21.7     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 120      |
|    time_elapsed    | 3245     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=23.48 +/- 5.43
Episode length: 222.14 +/- 55.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 222         |
|    mean_reward          | 23.5        |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.023773534 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.633      |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.154       |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 0.609       |
-----------------------------------------
Eval num_timesteps=246500, episode_reward=22.66 +/- 5.05
Episode length: 213.60 +/- 49.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=23.62 +/- 5.93
Episode length: 222.10 +/- 63.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=23.42 +/- 5.46
Episode length: 222.76 +/- 56.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 21.5     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 121      |
|    time_elapsed    | 3277     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=23.22 +/- 5.18
Episode length: 219.56 +/- 49.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 220         |
|    mean_reward          | 23.2        |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.021880647 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.126       |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.539       |
-----------------------------------------
Eval num_timesteps=248500, episode_reward=23.44 +/- 5.76
Episode length: 224.54 +/- 56.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=22.86 +/- 6.27
Episode length: 214.62 +/- 65.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=23.14 +/- 6.25
Episode length: 222.14 +/- 66.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 122      |
|    time_elapsed    | 3309     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=23.82 +/- 5.26
Episode length: 227.00 +/- 56.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 227         |
|    mean_reward          | 23.8        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.018672807 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.232       |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0272     |
|    value_loss           | 0.644       |
-----------------------------------------
Eval num_timesteps=250500, episode_reward=24.78 +/- 6.65
Episode length: 234.50 +/- 69.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | 24.8     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
New best mean reward!
Eval num_timesteps=251000, episode_reward=22.80 +/- 5.62
Episode length: 216.06 +/- 55.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 22.8     |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=24.88 +/- 5.43
Episode length: 237.46 +/- 58.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 24.9     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 22       |
| time/              |          |
|    fps             | 75       |
|    iterations      | 123      |
|    time_elapsed    | 3342     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=22.20 +/- 5.36
Episode length: 208.84 +/- 52.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 22.2        |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.020398077 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.581      |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.158       |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.647       |
-----------------------------------------
Eval num_timesteps=252500, episode_reward=24.14 +/- 4.56
Episode length: 228.48 +/- 50.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=22.42 +/- 5.62
Episode length: 211.60 +/- 57.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 22.4     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=23.12 +/- 5.87
Episode length: 218.16 +/- 61.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 21.9     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 124      |
|    time_elapsed    | 3373     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=24.42 +/- 5.88
Episode length: 229.78 +/- 63.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 230         |
|    mean_reward          | 24.4        |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.024942037 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.24        |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.555       |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=21.82 +/- 5.90
Episode length: 202.46 +/- 62.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | 21.8     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=23.16 +/- 5.95
Episode length: 216.86 +/- 56.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=23.14 +/- 5.29
Episode length: 217.28 +/- 52.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=23.46 +/- 5.55
Episode length: 217.44 +/- 57.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 125      |
|    time_elapsed    | 3411     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=22.52 +/- 4.73
Episode length: 213.70 +/- 52.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 214         |
|    mean_reward          | 22.5        |
| time/                   |             |
|    total_timesteps      | 256500      |
| train/                  |             |
|    approx_kl            | 0.021584935 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.162       |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.589       |
-----------------------------------------
Eval num_timesteps=257000, episode_reward=23.34 +/- 5.90
Episode length: 220.60 +/- 62.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=24.26 +/- 5.96
Episode length: 230.92 +/- 61.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=24.48 +/- 6.01
Episode length: 232.88 +/- 63.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 22.5     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 126      |
|    time_elapsed    | 3444     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=22.84 +/- 4.88
Episode length: 215.60 +/- 49.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 216         |
|    mean_reward          | 22.8        |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.020486832 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.554      |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.179       |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 0.62        |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=24.18 +/- 5.37
Episode length: 231.60 +/- 55.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=22.90 +/- 4.89
Episode length: 218.34 +/- 50.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=24.46 +/- 5.23
Episode length: 233.04 +/- 58.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 22.5     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 127      |
|    time_elapsed    | 3476     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=22.08 +/- 5.74
Episode length: 207.56 +/- 57.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 208         |
|    mean_reward          | 22.1        |
| time/                   |             |
|    total_timesteps      | 260500      |
| train/                  |             |
|    approx_kl            | 0.018822158 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.22        |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.61        |
-----------------------------------------
Eval num_timesteps=261000, episode_reward=24.00 +/- 5.99
Episode length: 225.64 +/- 63.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=22.54 +/- 5.64
Episode length: 212.12 +/- 54.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 22.5     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=22.02 +/- 5.33
Episode length: 204.82 +/- 51.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 22       |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 22.6     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 128      |
|    time_elapsed    | 3506     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=23.22 +/- 6.16
Episode length: 218.00 +/- 63.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 218         |
|    mean_reward          | 23.2        |
| time/                   |             |
|    total_timesteps      | 262500      |
| train/                  |             |
|    approx_kl            | 0.019168343 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.19        |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.659       |
-----------------------------------------
Eval num_timesteps=263000, episode_reward=24.06 +/- 4.95
Episode length: 223.58 +/- 49.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=24.22 +/- 6.87
Episode length: 230.12 +/- 75.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=23.46 +/- 6.10
Episode length: 221.52 +/- 65.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 22.5     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 129      |
|    time_elapsed    | 3538     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=23.90 +/- 5.89
Episode length: 224.32 +/- 62.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 264500      |
| train/                  |             |
|    approx_kl            | 0.023653654 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.569      |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.209       |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 0.646       |
-----------------------------------------
Eval num_timesteps=265000, episode_reward=23.24 +/- 4.96
Episode length: 219.40 +/- 50.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=23.20 +/- 5.93
Episode length: 219.88 +/- 60.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=23.74 +/- 5.62
Episode length: 224.12 +/- 59.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 22.8     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 130      |
|    time_elapsed    | 3570     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=23.60 +/- 5.85
Episode length: 223.50 +/- 59.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | 23.6        |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 0.020320538 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.163       |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.508       |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=23.30 +/- 5.22
Episode length: 220.64 +/- 59.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=23.92 +/- 6.28
Episode length: 229.30 +/- 67.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=23.20 +/- 5.41
Episode length: 217.04 +/- 55.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 23       |
| time/              |          |
|    fps             | 74       |
|    iterations      | 131      |
|    time_elapsed    | 3602     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=23.90 +/- 5.24
Episode length: 222.20 +/- 57.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 222        |
|    mean_reward          | 23.9       |
| time/                   |            |
|    total_timesteps      | 268500     |
| train/                  |            |
|    approx_kl            | 0.02245614 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.523     |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.187      |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0256    |
|    value_loss           | 0.563      |
----------------------------------------
Eval num_timesteps=269000, episode_reward=23.80 +/- 5.77
Episode length: 223.28 +/- 58.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=22.86 +/- 4.93
Episode length: 217.16 +/- 49.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=23.04 +/- 6.55
Episode length: 221.36 +/- 70.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 132      |
|    time_elapsed    | 3634     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=24.08 +/- 5.55
Episode length: 223.08 +/- 55.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | 24.1        |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.022759274 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0001      |
|    loss                 | 0.173       |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 0.6         |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=23.58 +/- 6.52
Episode length: 224.96 +/- 69.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=24.06 +/- 6.15
Episode length: 225.42 +/- 66.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=23.08 +/- 5.95
Episode length: 213.26 +/- 62.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 22.8     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 133      |
|    time_elapsed    | 3666     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=23.72 +/- 5.73
Episode length: 223.92 +/- 59.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | 23.7        |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.023813548 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.267       |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.653       |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=24.96 +/- 7.35
Episode length: 235.30 +/- 80.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | 25       |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
New best mean reward!
Eval num_timesteps=273500, episode_reward=23.16 +/- 5.87
Episode length: 216.08 +/- 59.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=22.66 +/- 5.80
Episode length: 208.52 +/- 62.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 22.7     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 22.5     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 134      |
|    time_elapsed    | 3697     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=23.90 +/- 5.88
Episode length: 225.48 +/- 63.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.025275659 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.18        |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 0.575       |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=24.30 +/- 5.54
Episode length: 228.44 +/- 58.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 24.3     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=24.90 +/- 4.75
Episode length: 236.62 +/- 52.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 24.9     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=23.70 +/- 6.11
Episode length: 225.44 +/- 64.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 22.6     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 135      |
|    time_elapsed    | 3730     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=23.82 +/- 5.61
Episode length: 221.92 +/- 59.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 222        |
|    mean_reward          | 23.8       |
| time/                   |            |
|    total_timesteps      | 276500     |
| train/                  |            |
|    approx_kl            | 0.02615438 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.536     |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.132      |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.0256    |
|    value_loss           | 0.551      |
----------------------------------------
Eval num_timesteps=277000, episode_reward=23.74 +/- 4.88
Episode length: 223.72 +/- 53.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=24.58 +/- 4.96
Episode length: 228.90 +/- 51.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 24.6     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=24.92 +/- 5.45
Episode length: 237.84 +/- 59.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 24.9     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=24.46 +/- 4.81
Episode length: 230.30 +/- 54.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 136      |
|    time_elapsed    | 3770     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=24.66 +/- 5.64
Episode length: 230.62 +/- 58.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 231         |
|    mean_reward          | 24.7        |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.024686188 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.492      |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.251       |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.69        |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=23.86 +/- 5.54
Episode length: 220.52 +/- 52.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=24.08 +/- 5.07
Episode length: 227.86 +/- 57.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=23.78 +/- 5.81
Episode length: 221.06 +/- 61.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 22.8     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 137      |
|    time_elapsed    | 3803     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=24.32 +/- 4.34
Episode length: 226.44 +/- 45.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 24.3        |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.019503372 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.477      |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.122       |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.531       |
-----------------------------------------
Eval num_timesteps=281500, episode_reward=25.62 +/- 5.79
Episode length: 239.70 +/- 61.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 25.6     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
New best mean reward!
Eval num_timesteps=282000, episode_reward=24.22 +/- 5.07
Episode length: 228.10 +/- 57.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=23.84 +/- 4.62
Episode length: 225.28 +/- 47.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 138      |
|    time_elapsed    | 3835     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=22.08 +/- 5.20
Episode length: 211.92 +/- 55.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 22.1       |
| time/                   |            |
|    total_timesteps      | 283000     |
| train/                  |            |
|    approx_kl            | 0.02711539 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.524     |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.195      |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 0.691      |
----------------------------------------
Eval num_timesteps=283500, episode_reward=23.50 +/- 5.10
Episode length: 221.96 +/- 59.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=23.06 +/- 6.24
Episode length: 216.50 +/- 63.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 23.1     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=23.46 +/- 5.64
Episode length: 220.20 +/- 56.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 139      |
|    time_elapsed    | 3867     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=22.22 +/- 5.00
Episode length: 208.50 +/- 54.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 208         |
|    mean_reward          | 22.2        |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.024189746 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.194       |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.608       |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=22.98 +/- 5.91
Episode length: 218.66 +/- 65.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 23       |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=22.92 +/- 5.82
Episode length: 213.56 +/- 56.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=23.20 +/- 5.95
Episode length: 218.86 +/- 58.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 23.1     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 140      |
|    time_elapsed    | 3897     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=24.38 +/- 5.57
Episode length: 229.94 +/- 65.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 230        |
|    mean_reward          | 24.4       |
| time/                   |            |
|    total_timesteps      | 287000     |
| train/                  |            |
|    approx_kl            | 0.02724787 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.492     |
|    explained_variance   | 0.9        |
|    learning_rate        | 0.0001     |
|    loss                 | 0.236      |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 0.622      |
----------------------------------------
Eval num_timesteps=287500, episode_reward=24.22 +/- 4.58
Episode length: 229.22 +/- 50.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=24.90 +/- 6.40
Episode length: 236.54 +/- 68.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 24.9     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=24.72 +/- 4.66
Episode length: 230.98 +/- 48.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 24.7     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 23.2     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 141      |
|    time_elapsed    | 3930     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=23.26 +/- 6.42
Episode length: 217.54 +/- 65.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 218         |
|    mean_reward          | 23.3        |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.025946455 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.146       |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.5         |
-----------------------------------------
Eval num_timesteps=289500, episode_reward=23.52 +/- 4.75
Episode length: 220.12 +/- 50.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=23.44 +/- 4.80
Episode length: 219.26 +/- 52.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 23.4     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=24.82 +/- 6.42
Episode length: 233.42 +/- 66.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 24.8     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 23.3     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 142      |
|    time_elapsed    | 3962     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=24.70 +/- 6.24
Episode length: 233.00 +/- 65.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 233        |
|    mean_reward          | 24.7       |
| time/                   |            |
|    total_timesteps      | 291000     |
| train/                  |            |
|    approx_kl            | 0.02243908 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.495     |
|    explained_variance   | 0.899      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.204      |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0209    |
|    value_loss           | 0.517      |
----------------------------------------
Eval num_timesteps=291500, episode_reward=23.52 +/- 5.80
Episode length: 220.00 +/- 60.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=23.18 +/- 5.92
Episode length: 218.30 +/- 61.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 23.2     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=25.02 +/- 5.76
Episode length: 237.62 +/- 60.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 25       |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 23.4     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 143      |
|    time_elapsed    | 3995     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=24.46 +/- 5.70
Episode length: 228.14 +/- 55.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 24.5       |
| time/                   |            |
|    total_timesteps      | 293000     |
| train/                  |            |
|    approx_kl            | 0.02152289 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.504     |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.266      |
|    n_updates            | 1430       |
|    policy_gradient_loss | -0.0207    |
|    value_loss           | 0.726      |
----------------------------------------
Eval num_timesteps=293500, episode_reward=23.76 +/- 6.15
Episode length: 224.94 +/- 64.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=24.24 +/- 6.12
Episode length: 230.44 +/- 68.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=23.78 +/- 5.79
Episode length: 221.58 +/- 60.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 23.8     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 23.3     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 144      |
|    time_elapsed    | 4027     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=24.66 +/- 5.50
Episode length: 227.70 +/- 58.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 228         |
|    mean_reward          | 24.7        |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.026827026 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.497      |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.184       |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 0.62        |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=24.54 +/- 5.54
Episode length: 226.08 +/- 53.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | 24.5     |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=24.56 +/- 5.63
Episode length: 227.50 +/- 56.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 24.6     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=23.90 +/- 6.64
Episode length: 226.34 +/- 68.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | 23.9     |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 23.6     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 145      |
|    time_elapsed    | 4060     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=24.58 +/- 5.56
Episode length: 231.08 +/- 58.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 231         |
|    mean_reward          | 24.6        |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.030022126 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.152       |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.627       |
-----------------------------------------
Eval num_timesteps=297500, episode_reward=25.68 +/- 5.74
Episode length: 236.04 +/- 63.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | 25.7     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
New best mean reward!
Eval num_timesteps=298000, episode_reward=24.20 +/- 5.37
Episode length: 223.42 +/- 54.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 24.2     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=25.02 +/- 5.56
Episode length: 227.86 +/- 60.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 25       |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=23.68 +/- 5.89
Episode length: 217.10 +/- 58.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 72       |
|    iterations      | 146      |
|    time_elapsed    | 4100     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=25.86 +/- 6.25
Episode length: 241.28 +/- 63.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 25.9        |
| time/                   |             |
|    total_timesteps      | 299500      |
| train/                  |             |
|    approx_kl            | 0.026830168 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.505      |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.129       |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.531       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=300000, episode_reward=22.90 +/- 5.75
Episode length: 213.20 +/- 58.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 22.9     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=24.42 +/- 5.48
Episode length: 225.52 +/- 54.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | 24.4     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=25.46 +/- 5.95
Episode length: 237.34 +/- 64.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 25.5     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 23.8     |
| time/              |          |
|    fps             | 72       |
|    iterations      | 147      |
|    time_elapsed    | 4133     |
|    total_timesteps | 301056   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/defend-line/ppo-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0001, 'gamma': 0.94, 'gae_lambda': 0.93}
Training steps: 300000
Frame skip: 4
