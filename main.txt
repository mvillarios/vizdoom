/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=1.90 +/- 2.74
Episode length: 93.40 +/- 40.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=1.66 +/- 2.06
Episode length: 97.12 +/- 37.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=2.20 +/- 3.50
Episode length: 92.18 +/- 30.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.2     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=1.26 +/- 2.23
Episode length: 93.68 +/- 35.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.7     |
|    mean_reward     | 1.26     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.6     |
|    ep_rew_mean     | 1.86     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 1        |
|    time_elapsed    | 22       |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=1.12 +/- 1.98
Episode length: 99.90 +/- 35.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 99.9       |
|    mean_reward          | 1.12       |
| time/                   |            |
|    total_timesteps      | 2500       |
| train/                  |            |
|    approx_kl            | 0.00877822 |
|    clip_fraction        | 0.0314     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.94      |
|    explained_variance   | -0.00447   |
|    learning_rate        | 0.0001     |
|    loss                 | 0.206      |
|    n_updates            | 3          |
|    policy_gradient_loss | -0.00413   |
|    value_loss           | 0.219      |
----------------------------------------
Eval num_timesteps=3000, episode_reward=1.76 +/- 2.69
Episode length: 99.82 +/- 34.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=1.22 +/- 2.99
Episode length: 92.80 +/- 36.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.8     |
|    mean_reward     | 1.22     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=1.02 +/- 1.71
Episode length: 97.00 +/- 30.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | 1.02     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.8     |
|    ep_rew_mean     | 1.71     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 2        |
|    time_elapsed    | 42       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=2.34 +/- 3.85
Episode length: 189.58 +/- 108.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 190         |
|    mean_reward          | 2.34        |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.011898878 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -0.298      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.14        |
|    n_updates            | 13          |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 0.27        |
-----------------------------------------
New best mean reward!
Eval num_timesteps=5000, episode_reward=1.88 +/- 3.50
Episode length: 224.48 +/- 134.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=2.34 +/- 3.79
Episode length: 205.34 +/- 133.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=2.90 +/- 4.30
Episode length: 209.30 +/- 126.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 3        |
|    time_elapsed    | 74       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=1.46 +/- 2.82
Episode length: 90.88 +/- 28.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.9        |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.012917719 |
|    clip_fraction        | 0.0626      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -0.0354     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.173       |
|    n_updates            | 18          |
|    policy_gradient_loss | -0.00806    |
|    value_loss           | 0.689       |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=1.40 +/- 4.08
Episode length: 93.30 +/- 46.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.3     |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=2.52 +/- 3.77
Episode length: 98.34 +/- 34.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=1.02 +/- 1.81
Episode length: 90.62 +/- 37.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.6     |
|    mean_reward     | 1.02     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 4        |
|    time_elapsed    | 91       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=8500, episode_reward=1.54 +/- 2.24
Episode length: 99.66 +/- 30.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.7        |
|    mean_reward          | 1.54        |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.006293376 |
|    clip_fraction        | 0.0514      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -0.235      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.533       |
|    n_updates            | 24          |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 0.922       |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=1.98 +/- 3.03
Episode length: 97.96 +/- 39.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98       |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=2.26 +/- 2.95
Episode length: 101.92 +/- 35.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=1.52 +/- 2.11
Episode length: 96.72 +/- 31.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.7     |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 3.45     |
| time/              |          |
|    fps             | 94       |
|    iterations      | 5        |
|    time_elapsed    | 108      |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=1.66 +/- 2.40
Episode length: 93.36 +/- 38.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 93.4         |
|    mean_reward          | 1.66         |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0068795662 |
|    clip_fraction        | 0.0592       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.91        |
|    explained_variance   | -0.0253      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.548        |
|    n_updates            | 34           |
|    policy_gradient_loss | -0.0133      |
|    value_loss           | 0.654        |
------------------------------------------
Eval num_timesteps=11000, episode_reward=1.88 +/- 2.88
Episode length: 93.40 +/- 31.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=1.70 +/- 2.24
Episode length: 88.88 +/- 31.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.9     |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=1.94 +/- 2.77
Episode length: 90.58 +/- 31.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.6     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 3.56     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 6        |
|    time_elapsed    | 125      |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=12500, episode_reward=1.80 +/- 2.89
Episode length: 94.86 +/- 40.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.9         |
|    mean_reward          | 1.8          |
| time/                   |              |
|    total_timesteps      | 12500        |
| train/                  |              |
|    approx_kl            | 0.0037803575 |
|    clip_fraction        | 0.0443       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.9         |
|    explained_variance   | -0.331       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.681        |
|    n_updates            | 35           |
|    policy_gradient_loss | -0.000286    |
|    value_loss           | 0.735        |
------------------------------------------
Eval num_timesteps=13000, episode_reward=2.60 +/- 3.52
Episode length: 100.10 +/- 30.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=1.60 +/- 2.03
Episode length: 95.82 +/- 31.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.8     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=1.48 +/- 2.37
Episode length: 96.58 +/- 29.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.6     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 3.89     |
| time/              |          |
|    fps             | 101      |
|    iterations      | 7        |
|    time_elapsed    | 141      |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
Eval num_timesteps=14500, episode_reward=1.82 +/- 2.73
Episode length: 96.14 +/- 41.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.1        |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.008315042 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -0.117      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.209       |
|    n_updates            | 44          |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.446       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=1.98 +/- 2.68
Episode length: 100.32 +/- 38.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=1.62 +/- 2.10
Episode length: 102.18 +/- 38.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=1.20 +/- 1.93
Episode length: 82.26 +/- 25.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.7     |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 103      |
|    iterations      | 8        |
|    time_elapsed    | 159      |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=16500, episode_reward=1.86 +/- 2.21
Episode length: 94.38 +/- 28.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.4        |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.007878164 |
|    clip_fraction        | 0.0751      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.00297     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.223       |
|    n_updates            | 46          |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 0.334       |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=1.20 +/- 1.84
Episode length: 90.36 +/- 31.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.4     |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=0.86 +/- 1.57
Episode length: 92.02 +/- 32.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92       |
|    mean_reward     | 0.86     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=1.48 +/- 2.74
Episode length: 96.40 +/- 31.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.9     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 9        |
|    time_elapsed    | 175      |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=18500, episode_reward=2.12 +/- 2.89
Episode length: 93.78 +/- 32.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.8        |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.006426793 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.0812     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0593      |
|    n_updates            | 54          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.247       |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=1.54 +/- 2.27
Episode length: 96.42 +/- 36.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=1.60 +/- 2.90
Episode length: 91.60 +/- 36.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=1.50 +/- 2.99
Episode length: 98.36 +/- 39.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.8     |
|    ep_rew_mean     | 2.28     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 10       |
|    time_elapsed    | 192      |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=2.32 +/- 3.66
Episode length: 92.78 +/- 30.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.8        |
|    mean_reward          | 2.32        |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.006225854 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.0217      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.655       |
|    n_updates            | 55          |
|    policy_gradient_loss | -2.65e-06   |
|    value_loss           | 0.762       |
-----------------------------------------
Eval num_timesteps=21000, episode_reward=1.72 +/- 3.05
Episode length: 96.86 +/- 37.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.9     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=1.30 +/- 2.32
Episode length: 95.84 +/- 31.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.8     |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=1.48 +/- 2.80
Episode length: 91.36 +/- 30.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.4     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=1.86 +/- 3.11
Episode length: 102.18 +/- 35.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.8     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 11       |
|    time_elapsed    | 212      |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=23000, episode_reward=1.18 +/- 2.00
Episode length: 89.82 +/- 33.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89.8         |
|    mean_reward          | 1.18         |
| time/                   |              |
|    total_timesteps      | 23000        |
| train/                  |              |
|    approx_kl            | 0.0033704767 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | -0.0276      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.207        |
|    n_updates            | 56           |
|    policy_gradient_loss | 0.00123      |
|    value_loss           | 0.506        |
------------------------------------------
Eval num_timesteps=23500, episode_reward=1.26 +/- 1.88
Episode length: 88.36 +/- 28.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.4     |
|    mean_reward     | 1.26     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=1.44 +/- 2.19
Episode length: 104.84 +/- 41.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=1.68 +/- 2.06
Episode length: 92.94 +/- 28.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.9     |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.3     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 107      |
|    iterations      | 12       |
|    time_elapsed    | 228      |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=25000, episode_reward=2.06 +/- 3.43
Episode length: 89.68 +/- 34.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.7        |
|    mean_reward          | 2.06        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.004104168 |
|    clip_fraction        | 0.0255      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | -0.0167     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.228       |
|    n_updates            | 57          |
|    policy_gradient_loss | 0.00236     |
|    value_loss           | 0.411       |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=1.44 +/- 2.33
Episode length: 95.40 +/- 31.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.4     |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=1.62 +/- 2.61
Episode length: 84.66 +/- 27.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.7     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=1.62 +/- 2.82
Episode length: 93.32 +/- 32.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.3     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.5     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 13       |
|    time_elapsed    | 244      |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=27000, episode_reward=2.16 +/- 3.27
Episode length: 93.88 +/- 35.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.9        |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.004256121 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.0747      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.153       |
|    n_updates            | 58          |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 0.383       |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=1.64 +/- 3.91
Episode length: 93.16 +/- 34.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=1.02 +/- 1.77
Episode length: 86.14 +/- 30.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.1     |
|    mean_reward     | 1.02     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=1.32 +/- 1.84
Episode length: 94.06 +/- 33.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.1     |
|    mean_reward     | 1.32     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.9     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 109      |
|    iterations      | 14       |
|    time_elapsed    | 260      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=29000, episode_reward=1.78 +/- 2.72
Episode length: 91.58 +/- 30.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 91.6         |
|    mean_reward          | 1.78         |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0047798157 |
|    clip_fraction        | 0.0703       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.53        |
|    explained_variance   | -0.18        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.14         |
|    n_updates            | 59           |
|    policy_gradient_loss | 0.00276      |
|    value_loss           | 0.19         |
------------------------------------------
Eval num_timesteps=29500, episode_reward=1.12 +/- 1.88
Episode length: 90.52 +/- 36.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 1.12     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=1.70 +/- 3.68
Episode length: 102.58 +/- 40.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=1.44 +/- 2.29
Episode length: 91.60 +/- 28.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.8     |
|    ep_rew_mean     | 1.85     |
| time/              |          |
|    fps             | 110      |
|    iterations      | 15       |
|    time_elapsed    | 277      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=31000, episode_reward=1.78 +/- 2.99
Episode length: 86.36 +/- 28.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.4        |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.004021277 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | -0.0774     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.121       |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00205     |
|    value_loss           | 0.163       |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=1.34 +/- 2.37
Episode length: 97.42 +/- 42.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.4     |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=1.66 +/- 3.04
Episode length: 95.76 +/- 39.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.8     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=1.80 +/- 2.82
Episode length: 92.66 +/- 38.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.7     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 1.56     |
| time/              |          |
|    fps             | 111      |
|    iterations      | 16       |
|    time_elapsed    | 293      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=33000, episode_reward=2.26 +/- 3.93
Episode length: 101.78 +/- 37.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 2.26        |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.013780084 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | -0.0156     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.141       |
|    n_updates            | 62          |
|    policy_gradient_loss | 0.00163     |
|    value_loss           | 0.446       |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=1.54 +/- 2.77
Episode length: 90.46 +/- 31.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=1.38 +/- 2.83
Episode length: 102.92 +/- 30.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=1.78 +/- 3.85
Episode length: 93.14 +/- 33.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.9     |
|    ep_rew_mean     | 1.5      |
| time/              |          |
|    fps             | 112      |
|    iterations      | 17       |
|    time_elapsed    | 310      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=35000, episode_reward=1.42 +/- 2.33
Episode length: 91.42 +/- 29.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.4        |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.004558197 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.0411      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.169       |
|    n_updates            | 63          |
|    policy_gradient_loss | -0.00336    |
|    value_loss           | 0.542       |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=1.16 +/- 1.67
Episode length: 97.74 +/- 35.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.7     |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=2.18 +/- 3.14
Episode length: 93.72 +/- 40.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.7     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=0.82 +/- 1.88
Episode length: 84.54 +/- 29.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.5     |
|    mean_reward     | 0.82     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.8     |
|    ep_rew_mean     | 1.62     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 18       |
|    time_elapsed    | 326      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=37000, episode_reward=1.96 +/- 2.98
Episode length: 99.86 +/- 34.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.9         |
|    mean_reward          | 1.96         |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0075240615 |
|    clip_fraction        | 0.0488       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | -0.0168      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0522       |
|    n_updates            | 64           |
|    policy_gradient_loss | -0.000164    |
|    value_loss           | 0.448        |
------------------------------------------
Eval num_timesteps=37500, episode_reward=0.92 +/- 1.79
Episode length: 86.00 +/- 33.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86       |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=1.66 +/- 4.14
Episode length: 84.48 +/- 25.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.5     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=1.52 +/- 2.29
Episode length: 97.02 +/- 28.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.9     |
|    ep_rew_mean     | 1.86     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 19       |
|    time_elapsed    | 342      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=39000, episode_reward=1.72 +/- 3.58
Episode length: 89.00 +/- 27.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89           |
|    mean_reward          | 1.72         |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0031166673 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.0362       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0533       |
|    n_updates            | 69           |
|    policy_gradient_loss | -0.000353    |
|    value_loss           | 0.2          |
------------------------------------------
Eval num_timesteps=39500, episode_reward=1.60 +/- 2.10
Episode length: 85.34 +/- 26.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.3     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=1.34 +/- 2.16
Episode length: 95.54 +/- 39.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=1.48 +/- 3.38
Episode length: 94.82 +/- 34.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.8     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.8     |
|    ep_rew_mean     | 1.83     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 20       |
|    time_elapsed    | 358      |
|    total_timesteps | 40960    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=41000, episode_reward=2.08 +/- 2.73
Episode length: 95.62 +/- 34.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.6         |
|    mean_reward          | 2.08         |
| time/                   |              |
|    total_timesteps      | 41000        |
| train/                  |              |
|    approx_kl            | 0.0056923544 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.788       |
|    explained_variance   | -0.0795      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0851       |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00442     |
|    value_loss           | 0.162        |
------------------------------------------
Eval num_timesteps=41500, episode_reward=2.52 +/- 3.44
Episode length: 103.08 +/- 40.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=1.54 +/- 2.44
Episode length: 100.16 +/- 33.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=2.12 +/- 3.14
Episode length: 99.02 +/- 34.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=1.40 +/- 2.43
Episode length: 95.88 +/- 35.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.9     |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.7     |
|    ep_rew_mean     | 1.61     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 21       |
|    time_elapsed    | 380      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=43500, episode_reward=1.98 +/- 2.54
Episode length: 94.50 +/- 31.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.5        |
|    mean_reward          | 1.98        |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.004871653 |
|    clip_fraction        | 0.0741      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.675      |
|    explained_variance   | 0.061       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.169       |
|    n_updates            | 72          |
|    policy_gradient_loss | -0.000521   |
|    value_loss           | 0.336       |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=1.74 +/- 2.34
Episode length: 95.12 +/- 31.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.1     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=1.20 +/- 1.96
Episode length: 94.26 +/- 34.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.3     |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=1.40 +/- 2.27
Episode length: 90.86 +/- 35.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.3     |
|    ep_rew_mean     | 1.8      |
| time/              |          |
|    fps             | 113      |
|    iterations      | 22       |
|    time_elapsed    | 396      |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=45500, episode_reward=1.66 +/- 2.26
Episode length: 95.38 +/- 34.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.4         |
|    mean_reward          | 1.66         |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 0.0054300316 |
|    clip_fraction        | 0.0441       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.513       |
|    explained_variance   | -0.0545      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.273        |
|    n_updates            | 79           |
|    policy_gradient_loss | -0.000941    |
|    value_loss           | 0.607        |
------------------------------------------
Eval num_timesteps=46000, episode_reward=1.38 +/- 2.65
Episode length: 91.58 +/- 26.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=1.74 +/- 2.39
Episode length: 97.90 +/- 35.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.9     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=1.16 +/- 2.04
Episode length: 94.36 +/- 45.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.7     |
|    ep_rew_mean     | 1.52     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 23       |
|    time_elapsed    | 413      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=47500, episode_reward=1.74 +/- 2.30
Episode length: 96.64 +/- 30.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.6        |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.004335186 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.496      |
|    explained_variance   | 0.0807      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.122       |
|    n_updates            | 82          |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 0.215       |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=1.56 +/- 2.52
Episode length: 98.38 +/- 32.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=1.34 +/- 1.87
Episode length: 91.76 +/- 32.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=1.62 +/- 2.54
Episode length: 90.94 +/- 32.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88       |
|    ep_rew_mean     | 1.58     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 24       |
|    time_elapsed    | 429      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=1.54 +/- 2.38
Episode length: 98.56 +/- 36.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.6        |
|    mean_reward          | 1.54        |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.003185404 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.0982      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.134       |
|    n_updates            | 92          |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 0.164       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=2.08 +/- 2.92
Episode length: 105.86 +/- 39.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=2.00 +/- 3.16
Episode length: 95.96 +/- 40.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=1.84 +/- 2.56
Episode length: 91.86 +/- 31.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.9     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.9     |
|    ep_rew_mean     | 1.54     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 25       |
|    time_elapsed    | 447      |
|    total_timesteps | 51200    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=51500, episode_reward=2.00 +/- 2.80
Episode length: 106.36 +/- 38.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 106          |
|    mean_reward          | 2            |
| time/                   |              |
|    total_timesteps      | 51500        |
| train/                  |              |
|    approx_kl            | 0.0074395654 |
|    clip_fraction        | 0.0425       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | 0.0443       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0665       |
|    n_updates            | 94           |
|    policy_gradient_loss | 0.00184      |
|    value_loss           | 0.186        |
------------------------------------------
Eval num_timesteps=52000, episode_reward=1.56 +/- 2.22
Episode length: 99.34 +/- 34.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=1.40 +/- 2.32
Episode length: 99.58 +/- 35.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=1.64 +/- 2.78
Episode length: 92.24 +/- 32.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.2     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.7     |
|    ep_rew_mean     | 1.47     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 26       |
|    time_elapsed    | 464      |
|    total_timesteps | 53248    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=53500, episode_reward=1.98 +/- 2.89
Episode length: 100.66 +/- 36.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 1.98        |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.003582753 |
|    clip_fraction        | 0.024       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | -0.0531     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0996      |
|    n_updates            | 96          |
|    policy_gradient_loss | 0.0023      |
|    value_loss           | 0.25        |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=1.78 +/- 3.05
Episode length: 101.60 +/- 37.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=1.44 +/- 2.12
Episode length: 96.36 +/- 29.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=1.00 +/- 2.00
Episode length: 97.64 +/- 32.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.4     |
|    ep_rew_mean     | 1.58     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 27       |
|    time_elapsed    | 482      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=1.44 +/- 2.20
Episode length: 92.62 +/- 31.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.6         |
|    mean_reward          | 1.44         |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 0.0007919561 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.121       |
|    explained_variance   | -0.0379      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.16         |
|    n_updates            | 106          |
|    policy_gradient_loss | 2.01e-05     |
|    value_loss           | 0.308        |
------------------------------------------
Eval num_timesteps=56000, episode_reward=1.52 +/- 2.16
Episode length: 97.70 +/- 37.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.7     |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=2.10 +/- 2.94
Episode length: 96.04 +/- 35.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=2.18 +/- 3.02
Episode length: 99.04 +/- 36.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.1     |
|    ep_rew_mean     | 1.69     |
| time/              |          |
|    fps             | 114      |
|    iterations      | 28       |
|    time_elapsed    | 499      |
|    total_timesteps | 57344    |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.04
Eval num_timesteps=57500, episode_reward=1.78 +/- 2.23
Episode length: 100.56 +/- 34.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | 1.78         |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 0.0029940114 |
|    clip_fraction        | 0.00915      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0927      |
|    explained_variance   | 0.0111       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0959       |
|    n_updates            | 114          |
|    policy_gradient_loss | 0.000654     |
|    value_loss           | 0.311        |
------------------------------------------
Eval num_timesteps=58000, episode_reward=0.92 +/- 1.44
Episode length: 93.00 +/- 27.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93       |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=1.36 +/- 2.44
Episode length: 92.28 +/- 36.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.3     |
|    mean_reward     | 1.36     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=1.04 +/- 1.36
Episode length: 87.36 +/- 30.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.4     |
|    mean_reward     | 1.04     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.5     |
|    ep_rew_mean     | 1.79     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 29       |
|    time_elapsed    | 515      |
|    total_timesteps | 59392    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=59500, episode_reward=1.70 +/- 3.07
Episode length: 98.34 +/- 36.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.3         |
|    mean_reward          | 1.7          |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0054564034 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | 0.0672       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.135        |
|    n_updates            | 118          |
|    policy_gradient_loss | -0.000193    |
|    value_loss           | 0.194        |
------------------------------------------
Eval num_timesteps=60000, episode_reward=2.12 +/- 3.59
Episode length: 91.34 +/- 26.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.3     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=1.86 +/- 2.71
Episode length: 99.90 +/- 30.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.9     |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=1.76 +/- 2.54
Episode length: 95.54 +/- 32.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | 1.7      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 30       |
|    time_elapsed    | 532      |
|    total_timesteps | 61440    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=61500, episode_reward=1.44 +/- 1.93
Episode length: 90.46 +/- 24.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.5        |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 61500       |
| train/                  |             |
|    approx_kl            | 0.002160589 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.393      |
|    explained_variance   | -0.0438     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0669      |
|    n_updates            | 119         |
|    policy_gradient_loss | 0.00186     |
|    value_loss           | 0.187       |
-----------------------------------------
Eval num_timesteps=62000, episode_reward=1.44 +/- 2.41
Episode length: 87.88 +/- 33.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.9     |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=1.98 +/- 2.63
Episode length: 91.20 +/- 33.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.2     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=1.58 +/- 2.65
Episode length: 92.48 +/- 42.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.5     |
|    ep_rew_mean     | 1.67     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 31       |
|    time_elapsed    | 548      |
|    total_timesteps | 63488    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=63500, episode_reward=1.38 +/- 2.12
Episode length: 92.70 +/- 28.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.7         |
|    mean_reward          | 1.38         |
| time/                   |              |
|    total_timesteps      | 63500        |
| train/                  |              |
|    approx_kl            | 0.0043527316 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.161       |
|    explained_variance   | 0.0701       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.113        |
|    n_updates            | 120          |
|    policy_gradient_loss | 0.00283      |
|    value_loss           | 0.357        |
------------------------------------------
Eval num_timesteps=64000, episode_reward=1.22 +/- 2.38
Episode length: 85.36 +/- 38.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.4     |
|    mean_reward     | 1.22     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=2.02 +/- 2.43
Episode length: 106.40 +/- 42.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=1.44 +/- 2.31
Episode length: 88.26 +/- 26.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.3     |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=1.66 +/- 2.20
Episode length: 90.78 +/- 33.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.8     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.4     |
|    ep_rew_mean     | 1.6      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 32       |
|    time_elapsed    | 568      |
|    total_timesteps | 65536    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=66000, episode_reward=1.74 +/- 2.50
Episode length: 99.72 +/- 36.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.7         |
|    mean_reward          | 1.74         |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0022700434 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0.204        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0459       |
|    n_updates            | 121          |
|    policy_gradient_loss | 0.000216     |
|    value_loss           | 0.0861       |
------------------------------------------
Eval num_timesteps=66500, episode_reward=1.20 +/- 1.93
Episode length: 95.72 +/- 27.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.7     |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=1.36 +/- 3.19
Episode length: 92.84 +/- 39.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.8     |
|    mean_reward     | 1.36     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=1.68 +/- 2.87
Episode length: 97.08 +/- 32.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.6     |
|    ep_rew_mean     | 1.57     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 33       |
|    time_elapsed    | 585      |
|    total_timesteps | 67584    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=68000, episode_reward=1.80 +/- 3.04
Episode length: 97.64 +/- 34.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.6        |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.004479522 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.245      |
|    explained_variance   | -0.0429     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0267      |
|    n_updates            | 125         |
|    policy_gradient_loss | -0.000392   |
|    value_loss           | 0.12        |
-----------------------------------------
Eval num_timesteps=68500, episode_reward=1.54 +/- 2.21
Episode length: 89.56 +/- 34.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.6     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=1.58 +/- 2.72
Episode length: 98.52 +/- 36.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.5     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=1.60 +/- 2.47
Episode length: 91.30 +/- 27.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.3     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91       |
|    ep_rew_mean     | 1.36     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 34       |
|    time_elapsed    | 601      |
|    total_timesteps | 69632    |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=70000, episode_reward=1.94 +/- 3.15
Episode length: 97.70 +/- 32.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.7        |
|    mean_reward          | 1.94        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.005236603 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | -0.0114     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.161       |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.000241   |
|    value_loss           | 0.151       |
-----------------------------------------
Eval num_timesteps=70500, episode_reward=1.04 +/- 1.71
Episode length: 92.34 +/- 32.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.3     |
|    mean_reward     | 1.04     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=1.80 +/- 2.36
Episode length: 96.66 +/- 41.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.7     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=1.74 +/- 2.76
Episode length: 100.42 +/- 34.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 1.43     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 35       |
|    time_elapsed    | 618      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=1.58 +/- 2.39
Episode length: 97.86 +/- 35.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.9          |
|    mean_reward          | 1.58          |
| time/                   |               |
|    total_timesteps      | 72000         |
| train/                  |               |
|    approx_kl            | 0.00043335592 |
|    clip_fraction        | 0.0064        |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.0817       |
|    explained_variance   | 0.0382        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.0269        |
|    n_updates            | 140           |
|    policy_gradient_loss | -0.00112      |
|    value_loss           | 0.179         |
-------------------------------------------
Eval num_timesteps=72500, episode_reward=1.60 +/- 2.63
Episode length: 96.60 +/- 42.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.6     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=1.64 +/- 2.59
Episode length: 104.12 +/- 32.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=2.06 +/- 3.92
Episode length: 99.76 +/- 39.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.8     |
|    ep_rew_mean     | 1.59     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 36       |
|    time_elapsed    | 636      |
|    total_timesteps | 73728    |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.04
Eval num_timesteps=74000, episode_reward=1.82 +/- 2.63
Episode length: 102.46 +/- 40.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | 1.82         |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0025609166 |
|    clip_fraction        | 0.00791      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | 0.0242       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.624        |
|    n_updates            | 148          |
|    policy_gradient_loss | -0.000507    |
|    value_loss           | 0.299        |
------------------------------------------
Eval num_timesteps=74500, episode_reward=1.40 +/- 1.96
Episode length: 91.38 +/- 40.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.4     |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=1.90 +/- 2.50
Episode length: 95.54 +/- 32.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=2.24 +/- 3.27
Episode length: 97.98 +/- 47.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98       |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 1.8      |
| time/              |          |
|    fps             | 115      |
|    iterations      | 37       |
|    time_elapsed    | 653      |
|    total_timesteps | 75776    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=76000, episode_reward=1.94 +/- 3.34
Episode length: 97.30 +/- 36.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.3         |
|    mean_reward          | 1.94         |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 0.0030219213 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.19        |
|    explained_variance   | 0.0826       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.141        |
|    n_updates            | 151          |
|    policy_gradient_loss | 0.000443     |
|    value_loss           | 0.343        |
------------------------------------------
Eval num_timesteps=76500, episode_reward=1.04 +/- 1.77
Episode length: 91.88 +/- 31.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.9     |
|    mean_reward     | 1.04     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=1.98 +/- 2.34
Episode length: 102.66 +/- 40.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=1.46 +/- 2.47
Episode length: 92.60 +/- 32.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.6     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.7     |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 38       |
|    time_elapsed    | 670      |
|    total_timesteps | 77824    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=78000, episode_reward=1.56 +/- 2.12
Episode length: 94.90 +/- 34.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.9        |
|    mean_reward          | 1.56        |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.005273025 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0626     |
|    explained_variance   | 0.0255      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.342       |
|    n_updates            | 152         |
|    policy_gradient_loss | -0.00163    |
|    value_loss           | 0.263       |
-----------------------------------------
Eval num_timesteps=78500, episode_reward=1.16 +/- 1.63
Episode length: 99.40 +/- 36.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=1.40 +/- 2.17
Episode length: 97.62 +/- 34.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=1.38 +/- 2.26
Episode length: 93.04 +/- 26.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93       |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.1     |
|    ep_rew_mean     | 1.95     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 39       |
|    time_elapsed    | 687      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=1.34 +/- 2.10
Episode length: 97.42 +/- 33.76
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 97.4           |
|    mean_reward          | 1.34           |
| time/                   |                |
|    total_timesteps      | 80000          |
| train/                  |                |
|    approx_kl            | 0.000121313235 |
|    clip_fraction        | 0.0021         |
|    clip_range           | 0.2            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -0.0365        |
|    explained_variance   | -0.0406        |
|    learning_rate        | 0.0001         |
|    loss                 | 0.21           |
|    n_updates            | 162            |
|    policy_gradient_loss | -0.000453      |
|    value_loss           | 0.202          |
--------------------------------------------
Eval num_timesteps=80500, episode_reward=1.66 +/- 3.24
Episode length: 90.46 +/- 36.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=1.70 +/- 2.20
Episode length: 99.10 +/- 33.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=1.92 +/- 3.24
Episode length: 105.94 +/- 30.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97       |
|    ep_rew_mean     | 2.11     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 40       |
|    time_elapsed    | 705      |
|    total_timesteps | 81920    |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
Eval num_timesteps=82000, episode_reward=1.44 +/- 2.27
Episode length: 96.28 +/- 37.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.3         |
|    mean_reward          | 1.44         |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0015597924 |
|    clip_fraction        | 0.0025       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0321      |
|    explained_variance   | 0.0563       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.282        |
|    n_updates            | 172          |
|    policy_gradient_loss | -0.000421    |
|    value_loss           | 0.399        |
------------------------------------------
Eval num_timesteps=82500, episode_reward=2.14 +/- 2.88
Episode length: 100.76 +/- 41.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=1.66 +/- 3.15
Episode length: 93.84 +/- 33.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=2.06 +/- 2.90
Episode length: 97.12 +/- 36.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | 2        |
| time/              |          |
|    fps             | 116      |
|    iterations      | 41       |
|    time_elapsed    | 722      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=1.14 +/- 2.42
Episode length: 90.14 +/- 28.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 90.1         |
|    mean_reward          | 1.14         |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0003591187 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0424      |
|    explained_variance   | 0.0229       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0207       |
|    n_updates            | 182          |
|    policy_gradient_loss | -0.000507    |
|    value_loss           | 0.145        |
------------------------------------------
Eval num_timesteps=84500, episode_reward=1.66 +/- 2.22
Episode length: 98.76 +/- 37.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.8     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=1.50 +/- 2.62
Episode length: 99.60 +/- 35.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=2.08 +/- 3.32
Episode length: 94.76 +/- 39.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.8     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=1.24 +/- 2.16
Episode length: 94.50 +/- 41.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.5     |
|    mean_reward     | 1.24     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 1.82     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 42       |
|    time_elapsed    | 743      |
|    total_timesteps | 86016    |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=86500, episode_reward=1.90 +/- 2.99
Episode length: 106.76 +/- 42.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 107         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.002552779 |
|    clip_fraction        | 0.00333     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0391     |
|    explained_variance   | 0.055       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00551    |
|    n_updates            | 189         |
|    policy_gradient_loss | -0.000444   |
|    value_loss           | 0.177       |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=2.02 +/- 2.54
Episode length: 95.36 +/- 28.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.4     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=1.80 +/- 3.09
Episode length: 91.76 +/- 31.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=1.46 +/- 2.97
Episode length: 97.10 +/- 32.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.7     |
|    ep_rew_mean     | 1.74     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 43       |
|    time_elapsed    | 760      |
|    total_timesteps | 88064    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=88500, episode_reward=2.12 +/- 3.57
Episode length: 104.72 +/- 48.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 105        |
|    mean_reward          | 2.12       |
| time/                   |            |
|    total_timesteps      | 88500      |
| train/                  |            |
|    approx_kl            | 0.00546647 |
|    clip_fraction        | 0.00977    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0829    |
|    explained_variance   | 0.018      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.116      |
|    n_updates            | 192        |
|    policy_gradient_loss | 0.00171    |
|    value_loss           | 0.148      |
----------------------------------------
Eval num_timesteps=89000, episode_reward=1.10 +/- 1.90
Episode length: 95.32 +/- 38.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=1.12 +/- 1.95
Episode length: 87.90 +/- 28.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.9     |
|    mean_reward     | 1.12     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=1.66 +/- 2.57
Episode length: 89.46 +/- 34.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.5     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.1     |
|    ep_rew_mean     | 1.53     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 44       |
|    time_elapsed    | 776      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=2.06 +/- 3.17
Episode length: 95.36 +/- 33.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.4         |
|    mean_reward          | 2.06         |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0012587043 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0967      |
|    explained_variance   | -0.0462      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0878       |
|    n_updates            | 202          |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 0.144        |
------------------------------------------
Eval num_timesteps=91000, episode_reward=2.24 +/- 3.31
Episode length: 99.54 +/- 37.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.5     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=1.70 +/- 2.93
Episode length: 91.80 +/- 30.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=1.22 +/- 2.03
Episode length: 85.62 +/- 29.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.6     |
|    mean_reward     | 1.22     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.9     |
|    ep_rew_mean     | 1.43     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 45       |
|    time_elapsed    | 793      |
|    total_timesteps | 92160    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=92500, episode_reward=2.04 +/- 3.01
Episode length: 91.98 +/- 31.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92           |
|    mean_reward          | 2.04         |
| time/                   |              |
|    total_timesteps      | 92500        |
| train/                  |              |
|    approx_kl            | 0.0020934558 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.135       |
|    explained_variance   | 0.0195       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.397        |
|    n_updates            | 204          |
|    policy_gradient_loss | 0.000132     |
|    value_loss           | 0.284        |
------------------------------------------
Eval num_timesteps=93000, episode_reward=1.94 +/- 3.13
Episode length: 94.44 +/- 34.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=2.40 +/- 3.52
Episode length: 105.08 +/- 41.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=1.56 +/- 2.11
Episode length: 91.66 +/- 31.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.7     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95       |
|    ep_rew_mean     | 1.45     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 46       |
|    time_elapsed    | 811      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=2.04 +/- 2.76
Episode length: 95.62 +/- 42.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 95.6          |
|    mean_reward          | 2.04          |
| time/                   |               |
|    total_timesteps      | 94500         |
| train/                  |               |
|    approx_kl            | 0.00031322858 |
|    clip_fraction        | 0.00405       |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.0505       |
|    explained_variance   | 0.116         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.0734        |
|    n_updates            | 214           |
|    policy_gradient_loss | -0.000551     |
|    value_loss           | 0.137         |
-------------------------------------------
Eval num_timesteps=95000, episode_reward=1.72 +/- 2.76
Episode length: 94.42 +/- 38.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=1.82 +/- 2.30
Episode length: 96.44 +/- 39.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=1.56 +/- 3.04
Episode length: 99.82 +/- 39.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.8     |
|    ep_rew_mean     | 1.42     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 47       |
|    time_elapsed    | 829      |
|    total_timesteps | 96256    |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=96500, episode_reward=1.26 +/- 1.76
Episode length: 93.38 +/- 34.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.4        |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total_timesteps      | 96500       |
| train/                  |             |
|    approx_kl            | 0.002455547 |
|    clip_fraction        | 0.00707     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0535     |
|    explained_variance   | 0.0305      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0217      |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.000362    |
|    value_loss           | 0.145       |
-----------------------------------------
Eval num_timesteps=97000, episode_reward=2.00 +/- 2.68
Episode length: 100.22 +/- 32.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=1.78 +/- 2.23
Episode length: 97.78 +/- 33.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=1.72 +/- 2.74
Episode length: 96.48 +/- 34.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.9     |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 48       |
|    time_elapsed    | 846      |
|    total_timesteps | 98304    |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=98500, episode_reward=0.88 +/- 1.69
Episode length: 104.00 +/- 40.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | 0.88         |
| time/                   |              |
|    total_timesteps      | 98500        |
| train/                  |              |
|    approx_kl            | 0.0010987348 |
|    clip_fraction        | 0.0011       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0252      |
|    explained_variance   | 0.0213       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.292        |
|    n_updates            | 228          |
|    policy_gradient_loss | -0.000297    |
|    value_loss           | 0.434        |
------------------------------------------
Eval num_timesteps=99000, episode_reward=1.74 +/- 3.10
Episode length: 107.44 +/- 43.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=1.50 +/- 2.66
Episode length: 90.84 +/- 29.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.8     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=2.10 +/- 2.29
Episode length: 103.38 +/- 32.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | 1.88     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 49       |
|    time_elapsed    | 864      |
|    total_timesteps | 100352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=100500, episode_reward=1.58 +/- 2.70
Episode length: 87.60 +/- 25.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.6         |
|    mean_reward          | 1.58         |
| time/                   |              |
|    total_timesteps      | 100500       |
| train/                  |              |
|    approx_kl            | 0.0018969416 |
|    clip_fraction        | 0.00808      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.115       |
|    explained_variance   | 0.0335       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.298        |
|    n_updates            | 229          |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 0.19         |
------------------------------------------
Eval num_timesteps=101000, episode_reward=1.28 +/- 2.08
Episode length: 98.14 +/- 44.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.1     |
|    mean_reward     | 1.28     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=1.34 +/- 2.19
Episode length: 97.54 +/- 38.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=1.76 +/- 2.70
Episode length: 91.46 +/- 34.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.5     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.2     |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 50       |
|    time_elapsed    | 880      |
|    total_timesteps | 102400   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=102500, episode_reward=2.36 +/- 3.43
Episode length: 94.20 +/- 30.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.2        |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.001469392 |
|    clip_fraction        | 0.00313     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0399     |
|    explained_variance   | 0.12        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.208       |
|    n_updates            | 233         |
|    policy_gradient_loss | 0.000114    |
|    value_loss           | 0.494       |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=1.50 +/- 2.33
Episode length: 98.96 +/- 28.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=1.16 +/- 1.65
Episode length: 98.20 +/- 37.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=1.42 +/- 1.90
Episode length: 85.64 +/- 26.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.6     |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.4     |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 51       |
|    time_elapsed    | 896      |
|    total_timesteps | 104448   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=104500, episode_reward=1.42 +/- 2.21
Episode length: 92.80 +/- 34.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.8         |
|    mean_reward          | 1.42         |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0039641787 |
|    clip_fraction        | 0.00551      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0906      |
|    explained_variance   | -0.0139      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0704       |
|    n_updates            | 239          |
|    policy_gradient_loss | -0.000328    |
|    value_loss           | 0.245        |
------------------------------------------
Eval num_timesteps=105000, episode_reward=1.88 +/- 2.73
Episode length: 95.64 +/- 33.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.6     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=2.02 +/- 3.02
Episode length: 89.92 +/- 33.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.9     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=1.56 +/- 2.31
Episode length: 87.82 +/- 32.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.8     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.3     |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 52       |
|    time_elapsed    | 913      |
|    total_timesteps | 106496   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=106500, episode_reward=1.62 +/- 2.83
Episode length: 96.32 +/- 29.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.3         |
|    mean_reward          | 1.62         |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0022991523 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.241       |
|    explained_variance   | 0.0644       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.191        |
|    n_updates            | 241          |
|    policy_gradient_loss | 0.001        |
|    value_loss           | 0.267        |
------------------------------------------
Eval num_timesteps=107000, episode_reward=2.18 +/- 3.31
Episode length: 97.06 +/- 31.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=2.98 +/- 5.25
Episode length: 97.32 +/- 40.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
New best mean reward!
Eval num_timesteps=108000, episode_reward=1.70 +/- 3.07
Episode length: 92.98 +/- 32.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93       |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=1.12 +/- 1.69
Episode length: 100.36 +/- 31.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.12     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.7     |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 53       |
|    time_elapsed    | 933      |
|    total_timesteps | 108544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=109000, episode_reward=1.54 +/- 2.88
Episode length: 87.36 +/- 30.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.4         |
|    mean_reward          | 1.54         |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0034274238 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | -0.0876      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.103        |
|    n_updates            | 242          |
|    policy_gradient_loss | -0.000277    |
|    value_loss           | 0.304        |
------------------------------------------
Eval num_timesteps=109500, episode_reward=2.02 +/- 3.09
Episode length: 96.16 +/- 32.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.2     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=2.10 +/- 2.83
Episode length: 104.54 +/- 34.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=2.06 +/- 3.01
Episode length: 94.88 +/- 30.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98       |
|    ep_rew_mean     | 2.12     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 54       |
|    time_elapsed    | 950      |
|    total_timesteps | 110592   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=111000, episode_reward=2.00 +/- 2.91
Episode length: 99.56 +/- 41.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.6         |
|    mean_reward          | 2            |
| time/                   |              |
|    total_timesteps      | 111000       |
| train/                  |              |
|    approx_kl            | 0.0020528242 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.108        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0645       |
|    n_updates            | 244          |
|    policy_gradient_loss | 0.000462     |
|    value_loss           | 0.204        |
------------------------------------------
Eval num_timesteps=111500, episode_reward=1.56 +/- 2.08
Episode length: 85.16 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.2     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=1.58 +/- 2.63
Episode length: 88.26 +/- 30.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.3     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=2.36 +/- 4.45
Episode length: 95.92 +/- 35.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.9     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.4     |
|    ep_rew_mean     | 2.01     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 55       |
|    time_elapsed    | 966      |
|    total_timesteps | 112640   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=113000, episode_reward=1.74 +/- 2.68
Episode length: 90.42 +/- 34.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.4        |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.011972913 |
|    clip_fraction        | 0.015       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.139      |
|    explained_variance   | 0.0982      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0782      |
|    n_updates            | 248         |
|    policy_gradient_loss | -0.000335   |
|    value_loss           | 0.234       |
-----------------------------------------
Eval num_timesteps=113500, episode_reward=1.18 +/- 2.35
Episode length: 99.48 +/- 30.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.5     |
|    mean_reward     | 1.18     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=2.00 +/- 2.97
Episode length: 91.86 +/- 30.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.9     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=1.94 +/- 3.09
Episode length: 96.12 +/- 35.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 56       |
|    time_elapsed    | 983      |
|    total_timesteps | 114688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=115000, episode_reward=1.14 +/- 1.84
Episode length: 90.12 +/- 32.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 90.1         |
|    mean_reward          | 1.14         |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 0.0030291963 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.214       |
|    explained_variance   | 0.126        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.449        |
|    n_updates            | 249          |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 0.762        |
------------------------------------------
Eval num_timesteps=115500, episode_reward=1.54 +/- 2.48
Episode length: 99.76 +/- 36.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=0.96 +/- 2.13
Episode length: 89.90 +/- 29.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.9     |
|    mean_reward     | 0.96     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=1.74 +/- 2.89
Episode length: 103.40 +/- 50.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.6     |
|    ep_rew_mean     | 2.25     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 57       |
|    time_elapsed    | 1000     |
|    total_timesteps | 116736   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=117000, episode_reward=1.84 +/- 3.32
Episode length: 100.20 +/- 40.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.009882281 |
|    clip_fraction        | 0.0189      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | -0.0121     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0974      |
|    n_updates            | 254         |
|    policy_gradient_loss | 0.000696    |
|    value_loss           | 0.298       |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=1.26 +/- 2.08
Episode length: 95.08 +/- 30.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.1     |
|    mean_reward     | 1.26     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=2.40 +/- 3.62
Episode length: 102.52 +/- 41.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=1.20 +/- 1.82
Episode length: 90.86 +/- 30.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.9     |
|    ep_rew_mean     | 2.31     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 58       |
|    time_elapsed    | 1017     |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=1.60 +/- 2.73
Episode length: 88.02 +/- 36.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 88           |
|    mean_reward          | 1.6          |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0005603273 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0481      |
|    explained_variance   | 0.126        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.105        |
|    n_updates            | 264          |
|    policy_gradient_loss | -0.000668    |
|    value_loss           | 0.232        |
------------------------------------------
Eval num_timesteps=119500, episode_reward=1.48 +/- 2.75
Episode length: 92.06 +/- 32.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.1     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=1.60 +/- 3.24
Episode length: 97.96 +/- 40.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98       |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=1.88 +/- 3.04
Episode length: 96.30 +/- 35.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.3     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 59       |
|    time_elapsed    | 1034     |
|    total_timesteps | 120832   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=121000, episode_reward=2.34 +/- 3.77
Episode length: 96.52 +/- 39.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.5         |
|    mean_reward          | 2.34         |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 0.0014680806 |
|    clip_fraction        | 0.00746      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0496      |
|    explained_variance   | 0.0593       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.195        |
|    n_updates            | 269          |
|    policy_gradient_loss | -0.000286    |
|    value_loss           | 0.33         |
------------------------------------------
Eval num_timesteps=121500, episode_reward=1.64 +/- 2.40
Episode length: 86.10 +/- 26.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.1     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=1.08 +/- 2.12
Episode length: 91.52 +/- 32.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.5     |
|    mean_reward     | 1.08     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=2.04 +/- 3.36
Episode length: 90.96 +/- 32.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91       |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.9     |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 60       |
|    time_elapsed    | 1050     |
|    total_timesteps | 122880   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=123000, episode_reward=1.70 +/- 2.44
Episode length: 97.60 +/- 28.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.6         |
|    mean_reward          | 1.7          |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0032061804 |
|    clip_fraction        | 0.00603      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0188      |
|    explained_variance   | 0.0688       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.027        |
|    n_updates            | 272          |
|    policy_gradient_loss | 0.00201      |
|    value_loss           | 0.209        |
------------------------------------------
Eval num_timesteps=123500, episode_reward=2.00 +/- 3.32
Episode length: 102.50 +/- 29.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=1.60 +/- 2.50
Episode length: 94.38 +/- 34.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=2.42 +/- 3.58
Episode length: 100.56 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.8     |
|    ep_rew_mean     | 2.02     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 61       |
|    time_elapsed    | 1067     |
|    total_timesteps | 124928   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
Eval num_timesteps=125000, episode_reward=1.40 +/- 2.05
Episode length: 88.36 +/- 27.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 88.4         |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0009040476 |
|    clip_fraction        | 0.000854     |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0119      |
|    explained_variance   | 0.0405       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.173        |
|    n_updates            | 276          |
|    policy_gradient_loss | -8.14e-05    |
|    value_loss           | 0.205        |
------------------------------------------
Eval num_timesteps=125500, episode_reward=1.86 +/- 3.09
Episode length: 98.88 +/- 32.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=1.86 +/- 3.26
Episode length: 97.12 +/- 31.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=1.90 +/- 2.89
Episode length: 102.20 +/- 37.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 2.05     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 62       |
|    time_elapsed    | 1084     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=1.82 +/- 3.04
Episode length: 94.68 +/- 38.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 94.7          |
|    mean_reward          | 1.82          |
| time/                   |               |
|    total_timesteps      | 127000        |
| train/                  |               |
|    approx_kl            | 0.00051980256 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.0181       |
|    explained_variance   | 0.0022        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.0618        |
|    n_updates            | 286           |
|    policy_gradient_loss | -0.000444     |
|    value_loss           | 0.172         |
-------------------------------------------
Eval num_timesteps=127500, episode_reward=1.56 +/- 2.59
Episode length: 100.30 +/- 33.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=1.48 +/- 2.70
Episode length: 95.30 +/- 41.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=1.26 +/- 2.30
Episode length: 96.34 +/- 33.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.3     |
|    mean_reward     | 1.26     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=1.24 +/- 1.85
Episode length: 96.58 +/- 35.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.6     |
|    mean_reward     | 1.24     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 1.99     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 63       |
|    time_elapsed    | 1105     |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=1.54 +/- 3.44
Episode length: 87.42 +/- 32.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.4         |
|    mean_reward          | 1.54         |
| time/                   |              |
|    total_timesteps      | 129500       |
| train/                  |              |
|    approx_kl            | 0.0016649144 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0345      |
|    explained_variance   | -0.0107      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0201       |
|    n_updates            | 296          |
|    policy_gradient_loss | -0.00051     |
|    value_loss           | 0.187        |
------------------------------------------
Eval num_timesteps=130000, episode_reward=0.94 +/- 1.63
Episode length: 85.94 +/- 30.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.9     |
|    mean_reward     | 0.94     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=2.04 +/- 3.30
Episode length: 96.84 +/- 31.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=2.16 +/- 2.74
Episode length: 101.08 +/- 32.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 1.81     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 64       |
|    time_elapsed    | 1122     |
|    total_timesteps | 131072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=131500, episode_reward=1.70 +/- 2.74
Episode length: 89.20 +/- 30.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89.2         |
|    mean_reward          | 1.7          |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 0.0014376108 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.013       |
|    explained_variance   | 0.0518       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0856       |
|    n_updates            | 297          |
|    policy_gradient_loss | -0.00294     |
|    value_loss           | 0.153        |
------------------------------------------
Eval num_timesteps=132000, episode_reward=1.28 +/- 2.13
Episode length: 92.66 +/- 24.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.7     |
|    mean_reward     | 1.28     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=2.00 +/- 3.15
Episode length: 94.68 +/- 28.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.7     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=1.66 +/- 2.13
Episode length: 98.82 +/- 34.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.8     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 1.69     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 65       |
|    time_elapsed    | 1140     |
|    total_timesteps | 133120   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
Eval num_timesteps=133500, episode_reward=2.82 +/- 4.00
Episode length: 102.18 +/- 33.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.015828732 |
|    clip_fraction        | 0.00152     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0212     |
|    explained_variance   | 0.0717      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0114      |
|    n_updates            | 306         |
|    policy_gradient_loss | -0.000389   |
|    value_loss           | 0.109       |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=1.88 +/- 2.53
Episode length: 96.18 +/- 29.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.2     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=2.00 +/- 3.19
Episode length: 97.50 +/- 35.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=2.86 +/- 3.93
Episode length: 98.76 +/- 34.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.8     |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 1.86     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 66       |
|    time_elapsed    | 1157     |
|    total_timesteps | 135168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=135500, episode_reward=1.48 +/- 2.80
Episode length: 90.90 +/- 27.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 90.9         |
|    mean_reward          | 1.48         |
| time/                   |              |
|    total_timesteps      | 135500       |
| train/                  |              |
|    approx_kl            | 0.0019957041 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.412       |
|    explained_variance   | 0.000239     |
|    learning_rate        | 0.0001       |
|    loss                 | 0.168        |
|    n_updates            | 307          |
|    policy_gradient_loss | 0.00327      |
|    value_loss           | 0.482        |
------------------------------------------
Eval num_timesteps=136000, episode_reward=1.58 +/- 2.46
Episode length: 97.34 +/- 33.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=1.22 +/- 2.33
Episode length: 91.48 +/- 30.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.5     |
|    mean_reward     | 1.22     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=1.54 +/- 1.95
Episode length: 95.46 +/- 26.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 2.1      |
| time/              |          |
|    fps             | 116      |
|    iterations      | 67       |
|    time_elapsed    | 1174     |
|    total_timesteps | 137216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=137500, episode_reward=2.08 +/- 4.34
Episode length: 96.48 +/- 36.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.5         |
|    mean_reward          | 2.08         |
| time/                   |              |
|    total_timesteps      | 137500       |
| train/                  |              |
|    approx_kl            | 0.0039612595 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.263       |
|    explained_variance   | 0.0857       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.188        |
|    n_updates            | 308          |
|    policy_gradient_loss | 0.00011      |
|    value_loss           | 0.406        |
------------------------------------------
Eval num_timesteps=138000, episode_reward=1.36 +/- 2.34
Episode length: 86.30 +/- 31.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.3     |
|    mean_reward     | 1.36     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=1.78 +/- 2.75
Episode length: 92.64 +/- 31.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.6     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=1.72 +/- 2.91
Episode length: 90.24 +/- 28.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.2     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 2.04     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 68       |
|    time_elapsed    | 1190     |
|    total_timesteps | 139264   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=139500, episode_reward=1.70 +/- 2.58
Episode length: 91.38 +/- 32.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.4        |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 139500      |
| train/                  |             |
|    approx_kl            | 0.006990414 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.23       |
|    explained_variance   | -0.0375     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.055       |
|    n_updates            | 313         |
|    policy_gradient_loss | -0.000335   |
|    value_loss           | 0.151       |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=1.66 +/- 2.41
Episode length: 91.14 +/- 32.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.1     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=1.44 +/- 2.06
Episode length: 101.78 +/- 35.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=1.12 +/- 2.49
Episode length: 94.56 +/- 35.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.6     |
|    mean_reward     | 1.12     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.8     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 69       |
|    time_elapsed    | 1206     |
|    total_timesteps | 141312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=141500, episode_reward=1.58 +/- 2.49
Episode length: 93.72 +/- 36.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 93.7         |
|    mean_reward          | 1.58         |
| time/                   |              |
|    total_timesteps      | 141500       |
| train/                  |              |
|    approx_kl            | 0.0049652494 |
|    clip_fraction        | 0.0513       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.447       |
|    explained_variance   | 0.0503       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0628       |
|    n_updates            | 314          |
|    policy_gradient_loss | -0.00238     |
|    value_loss           | 0.503        |
------------------------------------------
Eval num_timesteps=142000, episode_reward=1.38 +/- 1.80
Episode length: 98.84 +/- 38.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.8     |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=1.72 +/- 2.60
Episode length: 105.92 +/- 39.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=2.18 +/- 3.07
Episode length: 98.24 +/- 35.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.6     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 70       |
|    time_elapsed    | 1223     |
|    total_timesteps | 143360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=143500, episode_reward=0.96 +/- 1.85
Episode length: 79.80 +/- 28.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.8        |
|    mean_reward          | 0.96        |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.004532103 |
|    clip_fraction        | 0.0402      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.0175      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.125       |
|    n_updates            | 315         |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 0.343       |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=2.12 +/- 4.44
Episode length: 96.42 +/- 31.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=1.58 +/- 2.74
Episode length: 98.16 +/- 32.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=2.24 +/- 3.74
Episode length: 98.30 +/- 28.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.9     |
|    ep_rew_mean     | 2.24     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 71       |
|    time_elapsed    | 1240     |
|    total_timesteps | 145408   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=145500, episode_reward=1.22 +/- 2.11
Episode length: 96.10 +/- 33.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.1         |
|    mean_reward          | 1.22         |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 0.0027051049 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.179       |
|    explained_variance   | 0.0854       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.153        |
|    n_updates            | 318          |
|    policy_gradient_loss | -0.000568    |
|    value_loss           | 0.288        |
------------------------------------------
Eval num_timesteps=146000, episode_reward=1.56 +/- 2.61
Episode length: 91.58 +/- 29.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=2.08 +/- 3.08
Episode length: 92.74 +/- 29.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.7     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=1.56 +/- 1.98
Episode length: 91.48 +/- 34.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.5     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.8     |
|    ep_rew_mean     | 2.09     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 72       |
|    time_elapsed    | 1256     |
|    total_timesteps | 147456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=147500, episode_reward=1.22 +/- 1.87
Episode length: 92.66 +/- 34.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.7         |
|    mean_reward          | 1.22         |
| time/                   |              |
|    total_timesteps      | 147500       |
| train/                  |              |
|    approx_kl            | 0.0039935014 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.33        |
|    explained_variance   | 0.0876       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0729       |
|    n_updates            | 319          |
|    policy_gradient_loss | 0.00279      |
|    value_loss           | 0.183        |
------------------------------------------
Eval num_timesteps=148000, episode_reward=2.08 +/- 3.43
Episode length: 97.78 +/- 30.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=1.92 +/- 2.74
Episode length: 93.12 +/- 30.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=2.10 +/- 3.35
Episode length: 101.62 +/- 34.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=1.38 +/- 2.56
Episode length: 99.68 +/- 35.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.7     |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.8     |
|    ep_rew_mean     | 2.15     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 73       |
|    time_elapsed    | 1276     |
|    total_timesteps | 149504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=150000, episode_reward=2.02 +/- 2.90
Episode length: 102.32 +/- 38.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | 2.02         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0038214182 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.0615       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.341        |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00111     |
|    value_loss           | 0.315        |
------------------------------------------
Eval num_timesteps=150500, episode_reward=1.82 +/- 3.47
Episode length: 94.20 +/- 35.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.2     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=2.82 +/- 4.91
Episode length: 94.10 +/- 35.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.1     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=1.86 +/- 2.88
Episode length: 97.90 +/- 34.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.9     |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.4     |
|    ep_rew_mean     | 1.95     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 74       |
|    time_elapsed    | 1293     |
|    total_timesteps | 151552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=152000, episode_reward=1.10 +/- 2.65
Episode length: 80.46 +/- 29.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 80.5         |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0033651832 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0881      |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.119        |
|    n_updates            | 322          |
|    policy_gradient_loss | 0.0015       |
|    value_loss           | 0.206        |
------------------------------------------
Eval num_timesteps=152500, episode_reward=1.32 +/- 2.20
Episode length: 90.60 +/- 28.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.6     |
|    mean_reward     | 1.32     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=2.72 +/- 3.29
Episode length: 99.32 +/- 30.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=1.60 +/- 3.17
Episode length: 88.34 +/- 31.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.3     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92       |
|    ep_rew_mean     | 1.88     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 75       |
|    time_elapsed    | 1309     |
|    total_timesteps | 153600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=154000, episode_reward=1.56 +/- 2.14
Episode length: 86.86 +/- 29.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 86.9         |
|    mean_reward          | 1.56         |
| time/                   |              |
|    total_timesteps      | 154000       |
| train/                  |              |
|    approx_kl            | 0.0055239885 |
|    clip_fraction        | 0.029        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.142        |
|    n_updates            | 323          |
|    policy_gradient_loss | -0.000713    |
|    value_loss           | 0.213        |
------------------------------------------
Eval num_timesteps=154500, episode_reward=1.58 +/- 2.37
Episode length: 90.36 +/- 32.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.4     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=1.46 +/- 2.74
Episode length: 93.08 +/- 44.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=1.56 +/- 2.73
Episode length: 100.04 +/- 35.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.9     |
|    ep_rew_mean     | 1.79     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 76       |
|    time_elapsed    | 1325     |
|    total_timesteps | 155648   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=156000, episode_reward=1.34 +/- 2.04
Episode length: 86.46 +/- 30.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.5        |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.009551939 |
|    clip_fraction        | 0.00694     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0817     |
|    explained_variance   | 0.112       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0361      |
|    n_updates            | 325         |
|    policy_gradient_loss | 0.000383    |
|    value_loss           | 0.205       |
-----------------------------------------
Eval num_timesteps=156500, episode_reward=1.38 +/- 1.72
Episode length: 95.76 +/- 33.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.8     |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=2.30 +/- 2.39
Episode length: 101.32 +/- 35.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=1.70 +/- 2.85
Episode length: 91.24 +/- 30.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.2     |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.6     |
|    ep_rew_mean     | 1.62     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 77       |
|    time_elapsed    | 1341     |
|    total_timesteps | 157696   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=158000, episode_reward=1.34 +/- 2.12
Episode length: 103.82 +/- 35.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 1.34        |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.008194045 |
|    clip_fraction        | 0.0144      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.0887      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0153      |
|    n_updates            | 327         |
|    policy_gradient_loss | -1.79e-05   |
|    value_loss           | 0.0973      |
-----------------------------------------
Eval num_timesteps=158500, episode_reward=1.66 +/- 2.58
Episode length: 94.32 +/- 34.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.3     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=1.56 +/- 2.65
Episode length: 91.62 +/- 31.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=1.22 +/- 1.54
Episode length: 86.70 +/- 20.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.7     |
|    mean_reward     | 1.22     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.3     |
|    ep_rew_mean     | 1.58     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 78       |
|    time_elapsed    | 1358     |
|    total_timesteps | 159744   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=160000, episode_reward=1.26 +/- 1.56
Episode length: 96.50 +/- 33.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.5         |
|    mean_reward          | 1.26         |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0021755479 |
|    clip_fraction        | 0.00681      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0626      |
|    explained_variance   | 0.0727       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.171        |
|    n_updates            | 330          |
|    policy_gradient_loss | 0.000332     |
|    value_loss           | 0.225        |
------------------------------------------
Eval num_timesteps=160500, episode_reward=1.66 +/- 2.35
Episode length: 85.30 +/- 32.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.3     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=1.56 +/- 2.75
Episode length: 100.50 +/- 29.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=1.06 +/- 1.92
Episode length: 99.28 +/- 36.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | 1.06     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.8     |
|    ep_rew_mean     | 1.65     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 79       |
|    time_elapsed    | 1374     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=1.62 +/- 2.51
Episode length: 101.36 +/- 34.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | 1.62         |
| time/                   |              |
|    total_timesteps      | 162000       |
| train/                  |              |
|    approx_kl            | 0.0014464611 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0637      |
|    explained_variance   | 0.0564       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.306        |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 0.311        |
------------------------------------------
Eval num_timesteps=162500, episode_reward=1.98 +/- 3.89
Episode length: 104.10 +/- 44.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=1.20 +/- 2.23
Episode length: 89.96 +/- 40.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90       |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=1.98 +/- 3.07
Episode length: 99.12 +/- 37.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94       |
|    ep_rew_mean     | 1.67     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 80       |
|    time_elapsed    | 1392     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=1.66 +/- 2.80
Episode length: 96.52 +/- 31.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.5         |
|    mean_reward          | 1.66         |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0002678329 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.036       |
|    explained_variance   | 0.00925      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.258        |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.000264    |
|    value_loss           | 0.279        |
------------------------------------------
Eval num_timesteps=164500, episode_reward=1.46 +/- 2.85
Episode length: 92.56 +/- 33.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.6     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=2.30 +/- 2.91
Episode length: 98.78 +/- 29.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.8     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=1.72 +/- 2.18
Episode length: 100.30 +/- 41.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.5     |
|    ep_rew_mean     | 1.75     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 81       |
|    time_elapsed    | 1410     |
|    total_timesteps | 165888   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=166000, episode_reward=1.58 +/- 2.85
Episode length: 94.72 +/- 32.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.7         |
|    mean_reward          | 1.58         |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0098531945 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0857      |
|    explained_variance   | 0.0559       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0219       |
|    n_updates            | 353          |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 0.095        |
------------------------------------------
Eval num_timesteps=166500, episode_reward=1.74 +/- 2.54
Episode length: 89.72 +/- 32.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.7     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=1.22 +/- 2.40
Episode length: 101.40 +/- 41.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 1.22     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=1.54 +/- 2.91
Episode length: 88.58 +/- 29.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.6     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.2     |
|    ep_rew_mean     | 1.86     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 82       |
|    time_elapsed    | 1426     |
|    total_timesteps | 167936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=168000, episode_reward=1.72 +/- 3.19
Episode length: 97.86 +/- 33.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.9          |
|    mean_reward          | 1.72          |
| time/                   |               |
|    total_timesteps      | 168000        |
| train/                  |               |
|    approx_kl            | 0.00065678614 |
|    clip_fraction        | 0.000579      |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.0242       |
|    explained_variance   | 0.0113        |
|    learning_rate        | 0.0001        |
|    loss                 | 0.0731        |
|    n_updates            | 354           |
|    policy_gradient_loss | -0.000258     |
|    value_loss           | 0.226         |
-------------------------------------------
Eval num_timesteps=168500, episode_reward=2.06 +/- 2.22
Episode length: 97.32 +/- 31.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=1.30 +/- 2.04
Episode length: 90.76 +/- 36.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.8     |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=1.86 +/- 2.07
Episode length: 97.28 +/- 35.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.1     |
|    ep_rew_mean     | 1.62     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 83       |
|    time_elapsed    | 1443     |
|    total_timesteps | 169984   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=170000, episode_reward=1.16 +/- 1.96
Episode length: 85.32 +/- 29.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 85.3         |
|    mean_reward          | 1.16         |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0020156922 |
|    clip_fraction        | 0.00533      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0376      |
|    explained_variance   | -0.0418      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0817       |
|    n_updates            | 356          |
|    policy_gradient_loss | -0.000271    |
|    value_loss           | 0.141        |
------------------------------------------
Eval num_timesteps=170500, episode_reward=1.78 +/- 3.02
Episode length: 93.60 +/- 34.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=2.02 +/- 3.42
Episode length: 98.14 +/- 34.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.1     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=2.06 +/- 2.69
Episode length: 99.36 +/- 28.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=1.60 +/- 2.74
Episode length: 98.70 +/- 34.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.7     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.9     |
|    ep_rew_mean     | 1.62     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 84       |
|    time_elapsed    | 1463     |
|    total_timesteps | 172032   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=172500, episode_reward=2.46 +/- 3.20
Episode length: 102.78 +/- 42.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 103          |
|    mean_reward          | 2.46         |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0025118424 |
|    clip_fraction        | 0.00887      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0528      |
|    explained_variance   | 0.0892       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.208        |
|    n_updates            | 364          |
|    policy_gradient_loss | -0.000427    |
|    value_loss           | 0.162        |
------------------------------------------
Eval num_timesteps=173000, episode_reward=1.54 +/- 2.31
Episode length: 93.56 +/- 35.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=1.54 +/- 2.49
Episode length: 89.62 +/- 35.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.6     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=1.98 +/- 2.83
Episode length: 99.50 +/- 35.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.5     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 1.63     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 85       |
|    time_elapsed    | 1480     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=1.60 +/- 3.03
Episode length: 92.78 +/- 29.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.8         |
|    mean_reward          | 1.6          |
| time/                   |              |
|    total_timesteps      | 174500       |
| train/                  |              |
|    approx_kl            | 0.0005005779 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0376      |
|    explained_variance   | 0.0698       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0744       |
|    n_updates            | 374          |
|    policy_gradient_loss | -0.000429    |
|    value_loss           | 0.207        |
------------------------------------------
Eval num_timesteps=175000, episode_reward=1.40 +/- 2.40
Episode length: 87.04 +/- 28.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87       |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=1.22 +/- 2.02
Episode length: 90.26 +/- 31.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.3     |
|    mean_reward     | 1.22     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=1.04 +/- 2.17
Episode length: 98.50 +/- 39.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.5     |
|    mean_reward     | 1.04     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | 1.95     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 86       |
|    time_elapsed    | 1497     |
|    total_timesteps | 176128   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=176500, episode_reward=1.44 +/- 2.21
Episode length: 99.24 +/- 31.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.2         |
|    mean_reward          | 1.44         |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 0.0013458312 |
|    clip_fraction        | 0.00544      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.027       |
|    explained_variance   | 0.0943       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.1          |
|    n_updates            | 379          |
|    policy_gradient_loss | -0.000268    |
|    value_loss           | 0.348        |
------------------------------------------
Eval num_timesteps=177000, episode_reward=1.32 +/- 2.56
Episode length: 100.84 +/- 29.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 1.32     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=1.52 +/- 1.92
Episode length: 87.00 +/- 33.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87       |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=1.50 +/- 2.17
Episode length: 95.58 +/- 35.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.6     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 1.82     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 87       |
|    time_elapsed    | 1514     |
|    total_timesteps | 178176   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=178500, episode_reward=1.58 +/- 2.65
Episode length: 95.42 +/- 27.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.4        |
|    mean_reward          | 1.58        |
| time/                   |             |
|    total_timesteps      | 178500      |
| train/                  |             |
|    approx_kl            | 0.016093565 |
|    clip_fraction        | 0.00181     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0233     |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0293      |
|    n_updates            | 387         |
|    policy_gradient_loss | 4.67e-05    |
|    value_loss           | 0.14        |
-----------------------------------------
Eval num_timesteps=179000, episode_reward=1.50 +/- 2.35
Episode length: 98.24 +/- 34.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=1.98 +/- 3.40
Episode length: 93.64 +/- 40.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=1.96 +/- 3.91
Episode length: 98.34 +/- 36.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.7     |
|    ep_rew_mean     | 1.82     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 88       |
|    time_elapsed    | 1531     |
|    total_timesteps | 180224   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=180500, episode_reward=1.94 +/- 2.72
Episode length: 90.20 +/- 31.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 90.2         |
|    mean_reward          | 1.94         |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 0.0015188344 |
|    clip_fraction        | 0.00535      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0351      |
|    explained_variance   | 0.0325       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.201        |
|    n_updates            | 390          |
|    policy_gradient_loss | 0.000319     |
|    value_loss           | 0.178        |
------------------------------------------
Eval num_timesteps=181000, episode_reward=2.04 +/- 2.90
Episode length: 92.48 +/- 32.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=2.14 +/- 3.69
Episode length: 92.36 +/- 37.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=2.50 +/- 4.05
Episode length: 100.82 +/- 37.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.8     |
|    ep_rew_mean     | 1.87     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 89       |
|    time_elapsed    | 1548     |
|    total_timesteps | 182272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=182500, episode_reward=1.50 +/- 2.18
Episode length: 92.72 +/- 29.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.7         |
|    mean_reward          | 1.5          |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 0.0019359275 |
|    clip_fraction        | 0.00456      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0896      |
|    explained_variance   | 0.0815       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0611       |
|    n_updates            | 391          |
|    policy_gradient_loss | -0.000971    |
|    value_loss           | 0.297        |
------------------------------------------
Eval num_timesteps=183000, episode_reward=1.50 +/- 3.45
Episode length: 90.54 +/- 30.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=2.08 +/- 2.92
Episode length: 97.60 +/- 38.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=1.92 +/- 2.59
Episode length: 90.70 +/- 34.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.7     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | 1.79     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 90       |
|    time_elapsed    | 1564     |
|    total_timesteps | 184320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=184500, episode_reward=1.86 +/- 2.75
Episode length: 88.30 +/- 38.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 88.3         |
|    mean_reward          | 1.86         |
| time/                   |              |
|    total_timesteps      | 184500       |
| train/                  |              |
|    approx_kl            | 0.0020231274 |
|    clip_fraction        | 0.00862      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.112        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0563       |
|    n_updates            | 392          |
|    policy_gradient_loss | 0.000811     |
|    value_loss           | 0.204        |
------------------------------------------
Eval num_timesteps=185000, episode_reward=1.72 +/- 2.50
Episode length: 98.60 +/- 32.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=1.38 +/- 2.12
Episode length: 90.50 +/- 30.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=1.68 +/- 3.25
Episode length: 86.54 +/- 29.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.5     |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.3     |
|    ep_rew_mean     | 1.61     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 91       |
|    time_elapsed    | 1580     |
|    total_timesteps | 186368   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=186500, episode_reward=2.54 +/- 4.21
Episode length: 96.90 +/- 45.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.9         |
|    mean_reward          | 2.54         |
| time/                   |              |
|    total_timesteps      | 186500       |
| train/                  |              |
|    approx_kl            | 0.0067268047 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.363       |
|    explained_variance   | 0.0538       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0454       |
|    n_updates            | 394          |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 0.148        |
------------------------------------------
Eval num_timesteps=187000, episode_reward=1.56 +/- 2.17
Episode length: 91.12 +/- 28.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.1     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=1.54 +/- 2.56
Episode length: 98.28 +/- 41.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=2.48 +/- 3.01
Episode length: 100.02 +/- 38.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.3     |
|    ep_rew_mean     | 1.55     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 92       |
|    time_elapsed    | 1597     |
|    total_timesteps | 188416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=188500, episode_reward=1.72 +/- 3.16
Episode length: 101.58 +/- 41.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 188500      |
| train/                  |             |
|    approx_kl            | 0.008841167 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.32       |
|    explained_variance   | 0.108       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0268      |
|    n_updates            | 395         |
|    policy_gradient_loss | -0.002      |
|    value_loss           | 0.12        |
-----------------------------------------
Eval num_timesteps=189000, episode_reward=0.94 +/- 1.54
Episode length: 91.52 +/- 30.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.5     |
|    mean_reward     | 0.94     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=1.72 +/- 2.78
Episode length: 98.62 +/- 41.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=2.16 +/- 3.98
Episode length: 94.76 +/- 31.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.8     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.4     |
|    ep_rew_mean     | 1.55     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 93       |
|    time_elapsed    | 1614     |
|    total_timesteps | 190464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=190500, episode_reward=2.20 +/- 2.84
Episode length: 95.92 +/- 36.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.9        |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 190500      |
| train/                  |             |
|    approx_kl            | 0.005102016 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.123       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0899      |
|    n_updates            | 396         |
|    policy_gradient_loss | 0.00587     |
|    value_loss           | 0.193       |
-----------------------------------------
Eval num_timesteps=191000, episode_reward=2.44 +/- 3.85
Episode length: 97.06 +/- 37.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=1.10 +/- 1.88
Episode length: 90.32 +/- 33.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.3     |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=2.04 +/- 3.01
Episode length: 93.38 +/- 33.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=1.08 +/- 1.90
Episode length: 87.86 +/- 30.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.9     |
|    mean_reward     | 1.08     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 1.59     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 94       |
|    time_elapsed    | 1633     |
|    total_timesteps | 192512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=193000, episode_reward=2.06 +/- 3.71
Episode length: 93.98 +/- 24.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94           |
|    mean_reward          | 2.06         |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0030211464 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.129       |
|    explained_variance   | 0.126        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.278        |
|    n_updates            | 398          |
|    policy_gradient_loss | 3.76e-05     |
|    value_loss           | 0.189        |
------------------------------------------
Eval num_timesteps=193500, episode_reward=1.72 +/- 2.33
Episode length: 97.16 +/- 30.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=1.48 +/- 2.35
Episode length: 99.82 +/- 31.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=1.16 +/- 2.11
Episode length: 92.58 +/- 34.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.6     |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.4     |
|    ep_rew_mean     | 1.65     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 95       |
|    time_elapsed    | 1650     |
|    total_timesteps | 194560   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=195000, episode_reward=1.66 +/- 2.43
Episode length: 87.80 +/- 30.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.8        |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.007719409 |
|    clip_fraction        | 0.00692     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0953     |
|    explained_variance   | 0.0454      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.104       |
|    n_updates            | 401         |
|    policy_gradient_loss | -0.000297   |
|    value_loss           | 0.249       |
-----------------------------------------
Eval num_timesteps=195500, episode_reward=2.12 +/- 3.15
Episode length: 96.36 +/- 33.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=1.22 +/- 2.24
Episode length: 95.16 +/- 34.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.2     |
|    mean_reward     | 1.22     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=1.52 +/- 2.53
Episode length: 94.98 +/- 34.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95       |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.7     |
|    ep_rew_mean     | 1.92     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 96       |
|    time_elapsed    | 1667     |
|    total_timesteps | 196608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=197000, episode_reward=1.54 +/- 2.39
Episode length: 91.06 +/- 29.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 91.1         |
|    mean_reward          | 1.54         |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 0.0034053545 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.146       |
|    explained_variance   | 0.0824       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.181        |
|    n_updates            | 402          |
|    policy_gradient_loss | 0.00125      |
|    value_loss           | 0.354        |
------------------------------------------
Eval num_timesteps=197500, episode_reward=1.28 +/- 2.23
Episode length: 97.52 +/- 29.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 1.28     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=1.26 +/- 2.18
Episode length: 84.30 +/- 28.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.3     |
|    mean_reward     | 1.26     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=1.44 +/- 1.72
Episode length: 96.30 +/- 29.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.3     |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.4     |
|    ep_rew_mean     | 2.02     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 97       |
|    time_elapsed    | 1683     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=1.26 +/- 2.17
Episode length: 88.98 +/- 30.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89           |
|    mean_reward          | 1.26         |
| time/                   |              |
|    total_timesteps      | 199000       |
| train/                  |              |
|    approx_kl            | 0.0015803941 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0559      |
|    explained_variance   | 0.0371       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0624       |
|    n_updates            | 412          |
|    policy_gradient_loss | -0.000902    |
|    value_loss           | 0.177        |
------------------------------------------
Eval num_timesteps=199500, episode_reward=1.34 +/- 2.37
Episode length: 101.72 +/- 34.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=1.32 +/- 2.07
Episode length: 90.68 +/- 33.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.7     |
|    mean_reward     | 1.32     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=1.46 +/- 2.06
Episode length: 88.06 +/- 28.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.1     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.7     |
|    ep_rew_mean     | 1.9      |
| time/              |          |
|    fps             | 118      |
|    iterations      | 98       |
|    time_elapsed    | 1699     |
|    total_timesteps | 200704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=201000, episode_reward=1.68 +/- 2.95
Episode length: 96.46 +/- 30.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.5         |
|    mean_reward          | 1.68         |
| time/                   |              |
|    total_timesteps      | 201000       |
| train/                  |              |
|    approx_kl            | 0.0029854027 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.0665       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.226        |
|    n_updates            | 413          |
|    policy_gradient_loss | 0.00204      |
|    value_loss           | 0.183        |
------------------------------------------
Eval num_timesteps=201500, episode_reward=1.54 +/- 3.06
Episode length: 87.34 +/- 38.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.3     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=1.36 +/- 1.72
Episode length: 94.40 +/- 29.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 1.36     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=1.62 +/- 3.95
Episode length: 97.14 +/- 38.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89       |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 99       |
|    time_elapsed    | 1715     |
|    total_timesteps | 202752   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=203000, episode_reward=1.76 +/- 3.85
Episode length: 91.06 +/- 35.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.1        |
|    mean_reward          | 1.76        |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.004667782 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.168       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.189       |
|    n_updates            | 416         |
|    policy_gradient_loss | 0.000377    |
|    value_loss           | 0.216       |
-----------------------------------------
Eval num_timesteps=203500, episode_reward=1.48 +/- 2.61
Episode length: 94.50 +/- 26.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.5     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=1.34 +/- 2.06
Episode length: 91.60 +/- 28.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=2.02 +/- 3.10
Episode length: 96.68 +/- 32.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.7     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.6     |
|    ep_rew_mean     | 1.61     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 100      |
|    time_elapsed    | 1732     |
|    total_timesteps | 204800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=205000, episode_reward=1.46 +/- 2.00
Episode length: 95.06 +/- 40.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.1         |
|    mean_reward          | 1.46         |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0036319352 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.254       |
|    explained_variance   | 0.0184       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.163        |
|    n_updates            | 417          |
|    policy_gradient_loss | 0.00279      |
|    value_loss           | 0.195        |
------------------------------------------
Eval num_timesteps=205500, episode_reward=1.54 +/- 1.79
Episode length: 93.90 +/- 31.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.9     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=1.62 +/- 2.75
Episode length: 92.74 +/- 30.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.7     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=2.32 +/- 3.96
Episode length: 95.06 +/- 32.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.1     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.5     |
|    ep_rew_mean     | 1.76     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 101      |
|    time_elapsed    | 1748     |
|    total_timesteps | 206848   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=207000, episode_reward=2.06 +/- 2.93
Episode length: 94.36 +/- 35.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.4         |
|    mean_reward          | 2.06         |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0025190813 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.164       |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.1          |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00159     |
|    value_loss           | 0.343        |
------------------------------------------
Eval num_timesteps=207500, episode_reward=1.86 +/- 2.97
Episode length: 96.54 +/- 41.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=1.84 +/- 2.80
Episode length: 100.76 +/- 38.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=1.32 +/- 1.92
Episode length: 92.40 +/- 34.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | 1.32     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.6     |
|    ep_rew_mean     | 1.82     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 102      |
|    time_elapsed    | 1765     |
|    total_timesteps | 208896   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=209000, episode_reward=1.42 +/- 2.88
Episode length: 86.54 +/- 33.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 86.5         |
|    mean_reward          | 1.42         |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0062081236 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.206       |
|    explained_variance   | 0.0887       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.105        |
|    n_updates            | 425          |
|    policy_gradient_loss | -0.000763    |
|    value_loss           | 0.235        |
------------------------------------------
Eval num_timesteps=209500, episode_reward=1.88 +/- 3.20
Episode length: 96.08 +/- 30.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=1.64 +/- 2.57
Episode length: 93.62 +/- 29.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=2.02 +/- 2.35
Episode length: 100.32 +/- 31.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.8     |
|    ep_rew_mean     | 1.8      |
| time/              |          |
|    fps             | 118      |
|    iterations      | 103      |
|    time_elapsed    | 1781     |
|    total_timesteps | 210944   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=211000, episode_reward=1.98 +/- 3.07
Episode length: 96.36 +/- 42.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.4         |
|    mean_reward          | 1.98         |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 0.0052394215 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.234       |
|    explained_variance   | -0.000903    |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0631       |
|    n_updates            | 429          |
|    policy_gradient_loss | 0.00025      |
|    value_loss           | 0.128        |
------------------------------------------
Eval num_timesteps=211500, episode_reward=1.82 +/- 3.21
Episode length: 96.68 +/- 32.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.7     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=1.10 +/- 1.78
Episode length: 87.96 +/- 28.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88       |
|    mean_reward     | 1.1      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=2.34 +/- 2.72
Episode length: 95.42 +/- 37.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.4     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 1.69     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 104      |
|    time_elapsed    | 1798     |
|    total_timesteps | 212992   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=213000, episode_reward=1.34 +/- 2.38
Episode length: 89.26 +/- 27.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89.3         |
|    mean_reward          | 1.34         |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 0.0048034913 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.448       |
|    explained_variance   | 0.0443       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.109        |
|    n_updates            | 431          |
|    policy_gradient_loss | -1.1e-05     |
|    value_loss           | 0.112        |
------------------------------------------
Eval num_timesteps=213500, episode_reward=1.46 +/- 2.16
Episode length: 93.24 +/- 37.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=1.60 +/- 2.05
Episode length: 99.20 +/- 39.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=1.54 +/- 2.37
Episode length: 99.64 +/- 35.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=1.60 +/- 2.20
Episode length: 93.06 +/- 27.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.7     |
|    ep_rew_mean     | 1.66     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 105      |
|    time_elapsed    | 1818     |
|    total_timesteps | 215040   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=215500, episode_reward=1.38 +/- 2.12
Episode length: 95.94 +/- 31.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.9         |
|    mean_reward          | 1.38         |
| time/                   |              |
|    total_timesteps      | 215500       |
| train/                  |              |
|    approx_kl            | 0.0027378816 |
|    clip_fraction        | 0.0269       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.309       |
|    explained_variance   | 0.103        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.114        |
|    n_updates            | 434          |
|    policy_gradient_loss | -1.2e-06     |
|    value_loss           | 0.289        |
------------------------------------------
Eval num_timesteps=216000, episode_reward=1.24 +/- 1.96
Episode length: 97.52 +/- 32.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 1.24     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=1.30 +/- 2.49
Episode length: 93.60 +/- 30.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=1.32 +/- 2.19
Episode length: 92.08 +/- 31.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.1     |
|    mean_reward     | 1.32     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89       |
|    ep_rew_mean     | 1.68     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 106      |
|    time_elapsed    | 1835     |
|    total_timesteps | 217088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=217500, episode_reward=2.08 +/- 3.20
Episode length: 93.20 +/- 30.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.2        |
|    mean_reward          | 2.08        |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.005254981 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.5        |
|    explained_variance   | 0.0448      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0742      |
|    n_updates            | 435         |
|    policy_gradient_loss | 0.00218     |
|    value_loss           | 0.433       |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=1.38 +/- 2.44
Episode length: 89.68 +/- 32.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.7     |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=1.44 +/- 1.82
Episode length: 88.54 +/- 32.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.5     |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=1.30 +/- 2.15
Episode length: 93.58 +/- 29.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.8     |
|    ep_rew_mean     | 2.04     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 107      |
|    time_elapsed    | 1850     |
|    total_timesteps | 219136   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=219500, episode_reward=0.90 +/- 1.72
Episode length: 90.32 +/- 32.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.3        |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.007053852 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.466      |
|    explained_variance   | 0.0721      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.239       |
|    n_updates            | 438         |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 0.284       |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=1.56 +/- 3.05
Episode length: 92.48 +/- 34.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=1.94 +/- 2.74
Episode length: 98.20 +/- 33.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=1.28 +/- 2.12
Episode length: 96.40 +/- 42.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 1.28     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 108      |
|    time_elapsed    | 1867     |
|    total_timesteps | 221184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=221500, episode_reward=2.72 +/- 3.89
Episode length: 94.64 +/- 34.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.6         |
|    mean_reward          | 2.72         |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 0.0061324765 |
|    clip_fraction        | 0.0699       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.695       |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.125        |
|    n_updates            | 439          |
|    policy_gradient_loss | 0.00217      |
|    value_loss           | 0.419        |
------------------------------------------
Eval num_timesteps=222000, episode_reward=2.58 +/- 4.70
Episode length: 102.14 +/- 43.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=1.96 +/- 2.36
Episode length: 95.18 +/- 37.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.2     |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=1.80 +/- 2.70
Episode length: 90.62 +/- 29.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.6     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.5     |
|    ep_rew_mean     | 2.18     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 109      |
|    time_elapsed    | 1884     |
|    total_timesteps | 223232   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=223500, episode_reward=2.10 +/- 2.44
Episode length: 93.04 +/- 28.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93          |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 223500      |
| train/                  |             |
|    approx_kl            | 0.004218044 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.457      |
|    explained_variance   | 0.0827      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.102       |
|    n_updates            | 442         |
|    policy_gradient_loss | -0.00318    |
|    value_loss           | 0.196       |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=2.08 +/- 3.78
Episode length: 92.62 +/- 40.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.6     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=1.48 +/- 1.91
Episode length: 102.22 +/- 37.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=1.70 +/- 2.56
Episode length: 95.04 +/- 33.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95       |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.6     |
|    ep_rew_mean     | 1.87     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 110      |
|    time_elapsed    | 1900     |
|    total_timesteps | 225280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=225500, episode_reward=1.72 +/- 2.87
Episode length: 99.98 +/- 41.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 1.72         |
| time/                   |              |
|    total_timesteps      | 225500       |
| train/                  |              |
|    approx_kl            | 0.0024356602 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.565       |
|    explained_variance   | 0.162        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0672       |
|    n_updates            | 443          |
|    policy_gradient_loss | -0.000392    |
|    value_loss           | 0.147        |
------------------------------------------
Eval num_timesteps=226000, episode_reward=2.26 +/- 3.32
Episode length: 93.24 +/- 32.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=2.06 +/- 3.59
Episode length: 100.14 +/- 36.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=0.96 +/- 1.96
Episode length: 98.94 +/- 30.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | 0.96     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | 1.85     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 111      |
|    time_elapsed    | 1917     |
|    total_timesteps | 227328   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=227500, episode_reward=1.46 +/- 2.48
Episode length: 87.42 +/- 30.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.4        |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 227500      |
| train/                  |             |
|    approx_kl            | 0.004773325 |
|    clip_fraction        | 0.0361      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.212       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.07        |
|    n_updates            | 446         |
|    policy_gradient_loss | -0.00157    |
|    value_loss           | 0.207       |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=1.44 +/- 2.46
Episode length: 96.36 +/- 36.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=1.42 +/- 2.59
Episode length: 94.52 +/- 36.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.5     |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=1.28 +/- 1.94
Episode length: 93.76 +/- 27.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 1.28     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.8     |
|    ep_rew_mean     | 1.41     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 112      |
|    time_elapsed    | 1934     |
|    total_timesteps | 229376   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=229500, episode_reward=1.88 +/- 2.93
Episode length: 97.76 +/- 31.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.8         |
|    mean_reward          | 1.88         |
| time/                   |              |
|    total_timesteps      | 229500       |
| train/                  |              |
|    approx_kl            | 0.0063431603 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.454       |
|    explained_variance   | 0.134        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.197        |
|    n_updates            | 448          |
|    policy_gradient_loss | 0.00077      |
|    value_loss           | 0.11         |
------------------------------------------
Eval num_timesteps=230000, episode_reward=1.70 +/- 3.03
Episode length: 89.90 +/- 29.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.9     |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=1.78 +/- 2.82
Episode length: 98.32 +/- 40.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=1.60 +/- 2.84
Episode length: 88.12 +/- 34.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.1     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.4     |
|    ep_rew_mean     | 1.53     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 113      |
|    time_elapsed    | 1950     |
|    total_timesteps | 231424   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=231500, episode_reward=1.50 +/- 2.26
Episode length: 97.70 +/- 33.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.7        |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 231500      |
| train/                  |             |
|    approx_kl            | 0.007469088 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.457      |
|    explained_variance   | 0.16        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0194      |
|    n_updates            | 451         |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 0.404       |
-----------------------------------------
Eval num_timesteps=232000, episode_reward=1.68 +/- 2.90
Episode length: 90.20 +/- 30.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.2     |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=1.94 +/- 2.63
Episode length: 98.32 +/- 35.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=1.14 +/- 1.90
Episode length: 89.84 +/- 28.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.8     |
|    mean_reward     | 1.14     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.7     |
|    ep_rew_mean     | 1.86     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 114      |
|    time_elapsed    | 1966     |
|    total_timesteps | 233472   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=233500, episode_reward=1.06 +/- 1.73
Episode length: 92.04 +/- 34.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92          |
|    mean_reward          | 1.06        |
| time/                   |             |
|    total_timesteps      | 233500      |
| train/                  |             |
|    approx_kl            | 0.008402724 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | 0.146       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0767      |
|    n_updates            | 453         |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 0.239       |
-----------------------------------------
Eval num_timesteps=234000, episode_reward=1.50 +/- 3.19
Episode length: 89.46 +/- 33.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.5     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=2.10 +/- 2.35
Episode length: 94.78 +/- 37.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.8     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=2.34 +/- 3.97
Episode length: 108.88 +/- 48.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=1.14 +/- 2.02
Episode length: 97.66 +/- 38.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.7     |
|    mean_reward     | 1.14     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.8     |
|    ep_rew_mean     | 1.64     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 115      |
|    time_elapsed    | 1987     |
|    total_timesteps | 235520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=236000, episode_reward=1.74 +/- 3.75
Episode length: 93.44 +/- 34.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 93.4         |
|    mean_reward          | 1.74         |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0054299645 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.533       |
|    explained_variance   | 0.0928       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0419       |
|    n_updates            | 454          |
|    policy_gradient_loss | -0.00706     |
|    value_loss           | 0.223        |
------------------------------------------
Eval num_timesteps=236500, episode_reward=1.54 +/- 2.25
Episode length: 97.00 +/- 35.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=1.90 +/- 3.13
Episode length: 98.94 +/- 25.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=0.92 +/- 1.53
Episode length: 90.82 +/- 32.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.8     |
|    mean_reward     | 0.92     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.9     |
|    ep_rew_mean     | 1.92     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 116      |
|    time_elapsed    | 2003     |
|    total_timesteps | 237568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=238000, episode_reward=1.68 +/- 2.54
Episode length: 92.74 +/- 30.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.7        |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.006111893 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.304      |
|    explained_variance   | 0.128       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.107       |
|    n_updates            | 455         |
|    policy_gradient_loss | 0.00624     |
|    value_loss           | 0.275       |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=1.58 +/- 2.28
Episode length: 84.90 +/- 33.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.9     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=1.46 +/- 2.39
Episode length: 88.82 +/- 36.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.8     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=2.08 +/- 3.51
Episode length: 108.60 +/- 42.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.5     |
|    ep_rew_mean     | 1.77     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 117      |
|    time_elapsed    | 2020     |
|    total_timesteps | 239616   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=240000, episode_reward=1.24 +/- 2.27
Episode length: 92.58 +/- 30.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.6        |
|    mean_reward          | 1.24        |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.012698715 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.346      |
|    explained_variance   | 0.0438      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0969      |
|    n_updates            | 458         |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 0.2         |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=2.70 +/- 3.70
Episode length: 94.14 +/- 28.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.1     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=1.70 +/- 3.38
Episode length: 106.12 +/- 42.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=2.10 +/- 3.11
Episode length: 97.58 +/- 29.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.1     |
|    ep_rew_mean     | 1.98     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 118      |
|    time_elapsed    | 2037     |
|    total_timesteps | 241664   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=242000, episode_reward=1.44 +/- 1.90
Episode length: 89.50 +/- 29.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.5        |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.005966617 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.142       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.692       |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 0.387       |
-----------------------------------------
Eval num_timesteps=242500, episode_reward=1.96 +/- 2.89
Episode length: 96.08 +/- 31.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=1.64 +/- 2.83
Episode length: 98.18 +/- 34.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=1.92 +/- 2.76
Episode length: 93.78 +/- 33.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.7     |
|    ep_rew_mean     | 2.26     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 119      |
|    time_elapsed    | 2053     |
|    total_timesteps | 243712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=244000, episode_reward=1.64 +/- 3.63
Episode length: 99.52 +/- 35.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.5        |
|    mean_reward          | 1.64        |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.004619224 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.175       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.22        |
|    n_updates            | 461         |
|    policy_gradient_loss | 0.00132     |
|    value_loss           | 0.256       |
-----------------------------------------
Eval num_timesteps=244500, episode_reward=1.76 +/- 2.88
Episode length: 96.20 +/- 35.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.2     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=1.32 +/- 3.44
Episode length: 95.64 +/- 36.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.6     |
|    mean_reward     | 1.32     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=0.70 +/- 1.06
Episode length: 90.88 +/- 30.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.3     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 120      |
|    time_elapsed    | 2070     |
|    total_timesteps | 245760   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=246000, episode_reward=1.36 +/- 1.94
Episode length: 92.10 +/- 29.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.1         |
|    mean_reward          | 1.36         |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0064568124 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.349       |
|    explained_variance   | 0.22         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0854       |
|    n_updates            | 467          |
|    policy_gradient_loss | -0.00528     |
|    value_loss           | 0.368        |
------------------------------------------
Eval num_timesteps=246500, episode_reward=1.34 +/- 1.94
Episode length: 94.48 +/- 24.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.5     |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=1.62 +/- 2.61
Episode length: 94.24 +/- 34.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.2     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=1.74 +/- 2.65
Episode length: 95.46 +/- 31.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.4     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 121      |
|    time_elapsed    | 2087     |
|    total_timesteps | 247808   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=248000, episode_reward=1.84 +/- 2.66
Episode length: 105.24 +/- 36.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 105         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.008708689 |
|    clip_fraction        | 0.0282      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.287      |
|    explained_variance   | 0.167       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0853      |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00096    |
|    value_loss           | 0.197       |
-----------------------------------------
Eval num_timesteps=248500, episode_reward=1.72 +/- 2.62
Episode length: 97.80 +/- 42.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=1.24 +/- 2.41
Episode length: 90.44 +/- 32.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.4     |
|    mean_reward     | 1.24     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=2.46 +/- 3.23
Episode length: 94.02 +/- 31.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94       |
|    mean_reward     | 2.46     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.7     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 122      |
|    time_elapsed    | 2103     |
|    total_timesteps | 249856   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=250000, episode_reward=1.64 +/- 2.38
Episode length: 95.24 +/- 28.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.2        |
|    mean_reward          | 1.64        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.007446412 |
|    clip_fraction        | 0.0296      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.313      |
|    explained_variance   | 0.129       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0258      |
|    n_updates            | 475         |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 0.53        |
-----------------------------------------
Eval num_timesteps=250500, episode_reward=1.40 +/- 2.32
Episode length: 95.28 +/- 33.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=1.56 +/- 2.71
Episode length: 94.52 +/- 35.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.5     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=2.26 +/- 3.99
Episode length: 102.04 +/- 40.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.9     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 123      |
|    time_elapsed    | 2120     |
|    total_timesteps | 251904   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=252000, episode_reward=2.02 +/- 3.11
Episode length: 89.08 +/- 34.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.1        |
|    mean_reward          | 2.02        |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.015030502 |
|    clip_fraction        | 0.0326      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.399      |
|    explained_variance   | 0.115       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0415      |
|    n_updates            | 478         |
|    policy_gradient_loss | -0.000755   |
|    value_loss           | 0.155       |
-----------------------------------------
Eval num_timesteps=252500, episode_reward=1.22 +/- 2.29
Episode length: 91.38 +/- 34.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.4     |
|    mean_reward     | 1.22     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=1.84 +/- 3.11
Episode length: 98.36 +/- 34.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=1.54 +/- 2.57
Episode length: 92.64 +/- 29.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.6     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.1     |
|    ep_rew_mean     | 1.87     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 124      |
|    time_elapsed    | 2137     |
|    total_timesteps | 253952   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=254000, episode_reward=1.76 +/- 2.27
Episode length: 89.54 +/- 32.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.5        |
|    mean_reward          | 1.76        |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.006373403 |
|    clip_fraction        | 0.0438      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.404      |
|    explained_variance   | 0.0622      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0514      |
|    n_updates            | 482         |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 0.128       |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=1.04 +/- 1.65
Episode length: 97.76 +/- 36.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | 1.04     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=2.62 +/- 2.96
Episode length: 99.06 +/- 36.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=2.32 +/- 4.09
Episode length: 95.74 +/- 45.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.7     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=1.80 +/- 2.48
Episode length: 95.22 +/- 25.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.2     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.8     |
|    ep_rew_mean     | 2.11     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 125      |
|    time_elapsed    | 2157     |
|    total_timesteps | 256000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=256500, episode_reward=2.46 +/- 3.06
Episode length: 99.72 +/- 35.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.7        |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 256500      |
| train/                  |             |
|    approx_kl            | 0.005962019 |
|    clip_fraction        | 0.0529      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.385      |
|    explained_variance   | 0.0997      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0159      |
|    n_updates            | 483         |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 0.317       |
-----------------------------------------
Eval num_timesteps=257000, episode_reward=1.88 +/- 2.60
Episode length: 98.68 +/- 30.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.7     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=1.18 +/- 2.35
Episode length: 96.18 +/- 32.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.2     |
|    mean_reward     | 1.18     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=1.88 +/- 3.40
Episode length: 93.40 +/- 31.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89       |
|    ep_rew_mean     | 1.64     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 126      |
|    time_elapsed    | 2174     |
|    total_timesteps | 258048   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=258500, episode_reward=1.42 +/- 2.50
Episode length: 96.10 +/- 37.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.1        |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.012417129 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.473      |
|    explained_variance   | 0.08        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.128       |
|    n_updates            | 485         |
|    policy_gradient_loss | -6.13e-07   |
|    value_loss           | 0.269       |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=1.42 +/- 1.89
Episode length: 94.38 +/- 28.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=2.16 +/- 4.63
Episode length: 101.64 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=1.68 +/- 2.53
Episode length: 95.74 +/- 30.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.7     |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.7     |
|    ep_rew_mean     | 1.62     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 127      |
|    time_elapsed    | 2191     |
|    total_timesteps | 260096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=260500, episode_reward=1.52 +/- 2.05
Episode length: 93.98 +/- 36.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94          |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 260500      |
| train/                  |             |
|    approx_kl            | 0.005230759 |
|    clip_fraction        | 0.0604      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.123       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.033       |
|    n_updates            | 486         |
|    policy_gradient_loss | -0.00227    |
|    value_loss           | 0.191       |
-----------------------------------------
Eval num_timesteps=261000, episode_reward=1.58 +/- 2.27
Episode length: 93.38 +/- 33.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=1.60 +/- 2.32
Episode length: 95.30 +/- 39.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=1.34 +/- 2.21
Episode length: 92.42 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.5     |
|    ep_rew_mean     | 1.93     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 128      |
|    time_elapsed    | 2208     |
|    total_timesteps | 262144   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=262500, episode_reward=1.74 +/- 2.54
Episode length: 94.52 +/- 34.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.5         |
|    mean_reward          | 1.74         |
| time/                   |              |
|    total_timesteps      | 262500       |
| train/                  |              |
|    approx_kl            | 0.0058081523 |
|    clip_fraction        | 0.0381       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.424       |
|    explained_variance   | 0.104        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.106        |
|    n_updates            | 488          |
|    policy_gradient_loss | 0.00272      |
|    value_loss           | 0.358        |
------------------------------------------
Eval num_timesteps=263000, episode_reward=1.92 +/- 3.58
Episode length: 99.06 +/- 33.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=2.62 +/- 3.59
Episode length: 100.58 +/- 32.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=3.06 +/- 4.56
Episode length: 100.94 +/- 40.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.6     |
|    ep_rew_mean     | 1.71     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 129      |
|    time_elapsed    | 2225     |
|    total_timesteps | 264192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=264500, episode_reward=1.56 +/- 2.24
Episode length: 92.78 +/- 34.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.8         |
|    mean_reward          | 1.56         |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 0.0017150084 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.26        |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0753       |
|    n_updates            | 489          |
|    policy_gradient_loss | -9.89e-05    |
|    value_loss           | 0.216        |
------------------------------------------
Eval num_timesteps=265000, episode_reward=1.00 +/- 2.24
Episode length: 91.16 +/- 28.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.2     |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=1.92 +/- 3.01
Episode length: 98.20 +/- 29.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=1.26 +/- 1.91
Episode length: 90.06 +/- 30.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.1     |
|    mean_reward     | 1.26     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87       |
|    ep_rew_mean     | 1.75     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 130      |
|    time_elapsed    | 2242     |
|    total_timesteps | 266240   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=266500, episode_reward=1.08 +/- 1.90
Episode length: 90.06 +/- 38.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.1        |
|    mean_reward          | 1.08        |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 0.012184543 |
|    clip_fraction        | 0.0299      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.5        |
|    explained_variance   | 0.146       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.147       |
|    n_updates            | 491         |
|    policy_gradient_loss | -0.000646   |
|    value_loss           | 0.196       |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=1.50 +/- 2.19
Episode length: 102.32 +/- 39.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=2.24 +/- 3.26
Episode length: 92.20 +/- 33.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.2     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=1.16 +/- 2.36
Episode length: 92.20 +/- 33.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.2     |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.2     |
|    ep_rew_mean     | 1.77     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 131      |
|    time_elapsed    | 2258     |
|    total_timesteps | 268288   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=268500, episode_reward=1.56 +/- 2.58
Episode length: 95.26 +/- 34.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.3         |
|    mean_reward          | 1.56         |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 0.0033466225 |
|    clip_fraction        | 0.0262       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.298       |
|    explained_variance   | 0.129        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0128       |
|    n_updates            | 496          |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 0.184        |
------------------------------------------
Eval num_timesteps=269000, episode_reward=2.14 +/- 3.76
Episode length: 97.80 +/- 34.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=1.12 +/- 1.46
Episode length: 97.24 +/- 30.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 1.12     |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=2.24 +/- 3.47
Episode length: 95.36 +/- 32.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.4     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90       |
|    ep_rew_mean     | 1.88     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 132      |
|    time_elapsed    | 2275     |
|    total_timesteps | 270336   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=270500, episode_reward=1.08 +/- 2.06
Episode length: 101.32 +/- 40.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 1.08        |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.003987798 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.405      |
|    explained_variance   | 0.0749      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.202       |
|    n_updates            | 498         |
|    policy_gradient_loss | -7.07e-05   |
|    value_loss           | 0.278       |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=1.58 +/- 2.93
Episode length: 90.14 +/- 27.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.1     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=2.32 +/- 3.06
Episode length: 99.90 +/- 37.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.9     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=1.00 +/- 1.71
Episode length: 87.36 +/- 28.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.4     |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86       |
|    ep_rew_mean     | 1.56     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 133      |
|    time_elapsed    | 2292     |
|    total_timesteps | 272384   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=272500, episode_reward=1.52 +/- 2.79
Episode length: 91.18 +/- 37.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.2        |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.004341255 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.547      |
|    explained_variance   | 0.0506      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0259      |
|    n_updates            | 501         |
|    policy_gradient_loss | -0.00257    |
|    value_loss           | 0.183       |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=1.48 +/- 2.26
Episode length: 98.50 +/- 38.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.5     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=1.54 +/- 2.22
Episode length: 89.68 +/- 27.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.7     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=1.04 +/- 1.99
Episode length: 90.86 +/- 29.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 1.04     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | 1.58     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 134      |
|    time_elapsed    | 2308     |
|    total_timesteps | 274432   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=274500, episode_reward=1.56 +/- 2.16
Episode length: 95.42 +/- 33.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.4        |
|    mean_reward          | 1.56        |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.009549314 |
|    clip_fraction        | 0.0428      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.495      |
|    explained_variance   | 0.0806      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.198       |
|    n_updates            | 504         |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 0.207       |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=1.92 +/- 2.98
Episode length: 86.60 +/- 30.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.6     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=1.52 +/- 2.39
Episode length: 90.48 +/- 31.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=1.42 +/- 2.17
Episode length: 97.34 +/- 35.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.4     |
|    ep_rew_mean     | 1.49     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 135      |
|    time_elapsed    | 2324     |
|    total_timesteps | 276480   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=276500, episode_reward=0.92 +/- 1.75
Episode length: 88.86 +/- 30.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.9        |
|    mean_reward          | 0.92        |
| time/                   |             |
|    total_timesteps      | 276500      |
| train/                  |             |
|    approx_kl            | 0.008867766 |
|    clip_fraction        | 0.0497      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.678      |
|    explained_variance   | 0.15        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0423      |
|    n_updates            | 509         |
|    policy_gradient_loss | -0.00516    |
|    value_loss           | 0.157       |
-----------------------------------------
Eval num_timesteps=277000, episode_reward=1.82 +/- 2.53
Episode length: 92.00 +/- 28.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92       |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=2.12 +/- 3.08
Episode length: 100.10 +/- 37.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=1.34 +/- 2.08
Episode length: 92.72 +/- 36.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.7     |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=1.30 +/- 1.82
Episode length: 91.54 +/- 28.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.5     |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84       |
|    ep_rew_mean     | 1.58     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 136      |
|    time_elapsed    | 2344     |
|    total_timesteps | 278528   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=279000, episode_reward=2.20 +/- 3.29
Episode length: 92.20 +/- 29.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.2        |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.019631414 |
|    clip_fraction        | 0.0518      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.694      |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.14        |
|    n_updates            | 514         |
|    policy_gradient_loss | -0.00494    |
|    value_loss           | 0.23        |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=1.78 +/- 2.84
Episode length: 93.78 +/- 38.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=0.98 +/- 1.84
Episode length: 88.48 +/- 32.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.5     |
|    mean_reward     | 0.98     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=2.06 +/- 2.63
Episode length: 95.24 +/- 28.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.2     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.9     |
|    ep_rew_mean     | 1.94     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 137      |
|    time_elapsed    | 2360     |
|    total_timesteps | 280576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=281000, episode_reward=2.02 +/- 2.72
Episode length: 102.48 +/- 35.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | 2.02         |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0058918274 |
|    clip_fraction        | 0.0631       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.862       |
|    explained_variance   | 0.143        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.552        |
|    n_updates            | 515          |
|    policy_gradient_loss | 0.00338      |
|    value_loss           | 0.44         |
------------------------------------------
Eval num_timesteps=281500, episode_reward=1.98 +/- 3.43
Episode length: 103.54 +/- 49.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=1.74 +/- 2.62
Episode length: 96.70 +/- 28.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.7     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=1.78 +/- 3.14
Episode length: 97.52 +/- 37.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.3     |
|    ep_rew_mean     | 1.72     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 138      |
|    time_elapsed    | 2378     |
|    total_timesteps | 282624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=283000, episode_reward=1.54 +/- 1.97
Episode length: 86.50 +/- 29.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 86.5         |
|    mean_reward          | 1.54         |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 0.0037964908 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.897       |
|    explained_variance   | -0.034       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0151       |
|    n_updates            | 516          |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 0.124        |
------------------------------------------
Eval num_timesteps=283500, episode_reward=1.64 +/- 2.22
Episode length: 90.06 +/- 26.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.1     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=1.92 +/- 2.75
Episode length: 91.18 +/- 38.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.2     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=1.64 +/- 2.42
Episode length: 100.36 +/- 37.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89       |
|    ep_rew_mean     | 1.96     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 139      |
|    time_elapsed    | 2394     |
|    total_timesteps | 284672   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=285000, episode_reward=1.38 +/- 1.87
Episode length: 94.06 +/- 29.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.1        |
|    mean_reward          | 1.38        |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.012152985 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.695      |
|    explained_variance   | 0.0527      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0609      |
|    n_updates            | 519         |
|    policy_gradient_loss | 0.000761    |
|    value_loss           | 0.24        |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=2.14 +/- 3.06
Episode length: 93.26 +/- 31.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.3     |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=1.32 +/- 1.88
Episode length: 101.10 +/- 37.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 1.32     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=2.18 +/- 2.96
Episode length: 91.68 +/- 31.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.7     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.6     |
|    ep_rew_mean     | 2.04     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 140      |
|    time_elapsed    | 2410     |
|    total_timesteps | 286720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=287000, episode_reward=1.46 +/- 2.73
Episode length: 94.78 +/- 36.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.8         |
|    mean_reward          | 1.46         |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 0.0055622905 |
|    clip_fraction        | 0.0716       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.846       |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.16         |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 0.506        |
------------------------------------------
Eval num_timesteps=287500, episode_reward=1.18 +/- 1.99
Episode length: 91.08 +/- 35.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.1     |
|    mean_reward     | 1.18     |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=2.42 +/- 3.01
Episode length: 96.52 +/- 32.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=2.24 +/- 3.64
Episode length: 94.60 +/- 30.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.6     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.2     |
|    ep_rew_mean     | 1.94     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 141      |
|    time_elapsed    | 2427     |
|    total_timesteps | 288768   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=289000, episode_reward=2.04 +/- 2.70
Episode length: 95.78 +/- 30.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.8        |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.006323294 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.864      |
|    explained_variance   | 0.102       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.214       |
|    n_updates            | 522         |
|    policy_gradient_loss | 0.000987    |
|    value_loss           | 0.339       |
-----------------------------------------
Eval num_timesteps=289500, episode_reward=1.70 +/- 3.21
Episode length: 93.38 +/- 30.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=1.52 +/- 2.66
Episode length: 97.38 +/- 39.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.4     |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=1.76 +/- 3.32
Episode length: 101.10 +/- 37.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.4     |
|    ep_rew_mean     | 2.18     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 142      |
|    time_elapsed    | 2444     |
|    total_timesteps | 290816   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=291000, episode_reward=0.94 +/- 1.83
Episode length: 86.56 +/- 29.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.6        |
|    mean_reward          | 0.94        |
| time/                   |             |
|    total_timesteps      | 291000      |
| train/                  |             |
|    approx_kl            | 0.010394619 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.48       |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0845      |
|    n_updates            | 525         |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 0.293       |
-----------------------------------------
Eval num_timesteps=291500, episode_reward=2.20 +/- 3.15
Episode length: 98.40 +/- 31.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=1.00 +/- 2.04
Episode length: 87.06 +/- 36.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.1     |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=1.46 +/- 2.17
Episode length: 88.76 +/- 27.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.8     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.3     |
|    ep_rew_mean     | 2.05     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 143      |
|    time_elapsed    | 2460     |
|    total_timesteps | 292864   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=293000, episode_reward=1.78 +/- 4.23
Episode length: 95.64 +/- 32.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.6         |
|    mean_reward          | 1.78         |
| time/                   |              |
|    total_timesteps      | 293000       |
| train/                  |              |
|    approx_kl            | 0.0058396854 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.665       |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0439       |
|    n_updates            | 527          |
|    policy_gradient_loss | -0.00203     |
|    value_loss           | 0.12         |
------------------------------------------
Eval num_timesteps=293500, episode_reward=1.68 +/- 2.88
Episode length: 92.66 +/- 34.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.7     |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=1.72 +/- 3.05
Episode length: 96.12 +/- 37.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=1.18 +/- 2.14
Episode length: 84.98 +/- 31.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85       |
|    mean_reward     | 1.18     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.5     |
|    ep_rew_mean     | 1.86     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 144      |
|    time_elapsed    | 2476     |
|    total_timesteps | 294912   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=295000, episode_reward=1.88 +/- 2.39
Episode length: 85.88 +/- 26.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85.9        |
|    mean_reward          | 1.88        |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.010654309 |
|    clip_fraction        | 0.0467      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.561      |
|    explained_variance   | 0.129       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.337       |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 0.347       |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=1.20 +/- 2.18
Episode length: 88.82 +/- 32.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.8     |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=1.22 +/- 2.52
Episode length: 95.80 +/- 34.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.8     |
|    mean_reward     | 1.22     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=2.08 +/- 3.24
Episode length: 103.20 +/- 37.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.6     |
|    ep_rew_mean     | 1.71     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 145      |
|    time_elapsed    | 2492     |
|    total_timesteps | 296960   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=297000, episode_reward=1.28 +/- 1.81
Episode length: 88.68 +/- 24.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.7        |
|    mean_reward          | 1.28        |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.007010876 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.065       |
|    n_updates            | 532         |
|    policy_gradient_loss | -0.000492   |
|    value_loss           | 0.25        |
-----------------------------------------
Eval num_timesteps=297500, episode_reward=1.40 +/- 2.13
Episode length: 93.12 +/- 38.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=1.86 +/- 2.62
Episode length: 96.10 +/- 39.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=1.62 +/- 3.09
Episode length: 93.28 +/- 32.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.3     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=2.76 +/- 4.08
Episode length: 99.14 +/- 32.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | 1.55     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 146      |
|    time_elapsed    | 2512     |
|    total_timesteps | 299008   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=299500, episode_reward=2.36 +/- 3.78
Episode length: 91.44 +/- 31.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.4        |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 299500      |
| train/                  |             |
|    approx_kl            | 0.007065599 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.79       |
|    explained_variance   | 0.105       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.131       |
|    n_updates            | 534         |
|    policy_gradient_loss | 3.34e-05    |
|    value_loss           | 0.152       |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=1.50 +/- 2.24
Episode length: 101.06 +/- 39.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=1.48 +/- 2.65
Episode length: 93.36 +/- 38.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=1.82 +/- 2.28
Episode length: 102.56 +/- 32.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | 1.7      |
| time/              |          |
|    fps             | 118      |
|    iterations      | 147      |
|    time_elapsed    | 2529     |
|    total_timesteps | 301056   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=301500, episode_reward=1.46 +/- 2.83
Episode length: 94.10 +/- 36.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.1        |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 301500      |
| train/                  |             |
|    approx_kl            | 0.006057486 |
|    clip_fraction        | 0.0301      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.94       |
|    explained_variance   | 0.0677      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.175       |
|    n_updates            | 536         |
|    policy_gradient_loss | -9.31e-05   |
|    value_loss           | 0.212       |
-----------------------------------------
Eval num_timesteps=302000, episode_reward=1.28 +/- 1.72
Episode length: 96.92 +/- 40.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.9     |
|    mean_reward     | 1.28     |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=1.68 +/- 2.59
Episode length: 93.88 +/- 35.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.9     |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=1.98 +/- 3.51
Episode length: 103.18 +/- 39.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.6     |
|    ep_rew_mean     | 1.85     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 148      |
|    time_elapsed    | 2547     |
|    total_timesteps | 303104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=303500, episode_reward=1.56 +/- 2.33
Episode length: 103.72 +/- 41.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | 1.56         |
| time/                   |              |
|    total_timesteps      | 303500       |
| train/                  |              |
|    approx_kl            | 0.0029387316 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.842       |
|    explained_variance   | 0.105        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.177        |
|    n_updates            | 537          |
|    policy_gradient_loss | -0.00278     |
|    value_loss           | 0.283        |
------------------------------------------
Eval num_timesteps=304000, episode_reward=1.66 +/- 2.98
Episode length: 99.78 +/- 34.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=1.40 +/- 2.15
Episode length: 96.54 +/- 40.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=1.98 +/- 2.82
Episode length: 96.84 +/- 36.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.5     |
|    ep_rew_mean     | 1.87     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 149      |
|    time_elapsed    | 2565     |
|    total_timesteps | 305152   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=305500, episode_reward=1.82 +/- 2.30
Episode length: 90.04 +/- 31.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90          |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 305500      |
| train/                  |             |
|    approx_kl            | 0.007708718 |
|    clip_fraction        | 0.0508      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.706      |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.307       |
|    n_updates            | 541         |
|    policy_gradient_loss | -0.00319    |
|    value_loss           | 0.284       |
-----------------------------------------
Eval num_timesteps=306000, episode_reward=1.60 +/- 2.35
Episode length: 101.88 +/- 34.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=1.14 +/- 1.73
Episode length: 91.22 +/- 29.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.2     |
|    mean_reward     | 1.14     |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=2.28 +/- 3.01
Episode length: 103.66 +/- 37.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.3     |
|    ep_rew_mean     | 2.06     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 150      |
|    time_elapsed    | 2582     |
|    total_timesteps | 307200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=307500, episode_reward=2.42 +/- 3.09
Episode length: 95.40 +/- 25.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.4         |
|    mean_reward          | 2.42         |
| time/                   |              |
|    total_timesteps      | 307500       |
| train/                  |              |
|    approx_kl            | 0.0060468786 |
|    clip_fraction        | 0.0649       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.645       |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0914       |
|    n_updates            | 542          |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 0.347        |
------------------------------------------
Eval num_timesteps=308000, episode_reward=2.06 +/- 2.67
Episode length: 106.58 +/- 28.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=1.88 +/- 2.68
Episode length: 96.56 +/- 37.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.6     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=1.74 +/- 3.06
Episode length: 94.88 +/- 38.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.3     |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 151      |
|    time_elapsed    | 2599     |
|    total_timesteps | 309248   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=309500, episode_reward=1.36 +/- 2.12
Episode length: 82.36 +/- 26.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.4        |
|    mean_reward          | 1.36        |
| time/                   |             |
|    total_timesteps      | 309500      |
| train/                  |             |
|    approx_kl            | 0.007250116 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.58       |
|    explained_variance   | 0.107       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0705      |
|    n_updates            | 546         |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 0.234       |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=1.82 +/- 3.08
Episode length: 96.60 +/- 34.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.6     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=1.76 +/- 2.45
Episode length: 96.88 +/- 35.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.9     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=1.32 +/- 2.03
Episode length: 97.14 +/- 34.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 1.32     |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.6     |
|    ep_rew_mean     | 1.82     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 152      |
|    time_elapsed    | 2616     |
|    total_timesteps | 311296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=311500, episode_reward=1.18 +/- 1.84
Episode length: 95.34 +/- 35.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.3        |
|    mean_reward          | 1.18        |
| time/                   |             |
|    total_timesteps      | 311500      |
| train/                  |             |
|    approx_kl            | 0.004377386 |
|    clip_fraction        | 0.0448      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.855      |
|    explained_variance   | 0.108       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0736      |
|    n_updates            | 547         |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 0.189       |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=2.56 +/- 3.61
Episode length: 102.72 +/- 36.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=1.36 +/- 1.88
Episode length: 85.42 +/- 31.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.4     |
|    mean_reward     | 1.36     |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=1.38 +/- 2.41
Episode length: 96.46 +/- 40.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.3     |
|    ep_rew_mean     | 1.85     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 153      |
|    time_elapsed    | 2632     |
|    total_timesteps | 313344   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=313500, episode_reward=2.00 +/- 2.97
Episode length: 91.06 +/- 27.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.1        |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 313500      |
| train/                  |             |
|    approx_kl            | 0.017234007 |
|    clip_fraction        | 0.0466      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.735      |
|    explained_variance   | 0.109       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.136       |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 0.274       |
-----------------------------------------
Eval num_timesteps=314000, episode_reward=1.74 +/- 2.64
Episode length: 93.80 +/- 36.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=2.10 +/- 3.28
Episode length: 92.62 +/- 29.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.6     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=1.78 +/- 2.72
Episode length: 87.56 +/- 30.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.6     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.7     |
|    ep_rew_mean     | 1.79     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 154      |
|    time_elapsed    | 2648     |
|    total_timesteps | 315392   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=315500, episode_reward=2.02 +/- 3.65
Episode length: 97.12 +/- 35.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.1        |
|    mean_reward          | 2.02        |
| time/                   |             |
|    total_timesteps      | 315500      |
| train/                  |             |
|    approx_kl            | 0.004605649 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.911      |
|    explained_variance   | 0.175       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.217       |
|    n_updates            | 552         |
|    policy_gradient_loss | 0.000248    |
|    value_loss           | 0.211       |
-----------------------------------------
Eval num_timesteps=316000, episode_reward=1.40 +/- 2.46
Episode length: 97.24 +/- 30.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=1.48 +/- 2.27
Episode length: 95.66 +/- 31.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.7     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=1.52 +/- 2.20
Episode length: 98.28 +/- 38.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 1.54     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 155      |
|    time_elapsed    | 2665     |
|    total_timesteps | 317440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=317500, episode_reward=1.38 +/- 2.52
Episode length: 97.52 +/- 38.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.5         |
|    mean_reward          | 1.38         |
| time/                   |              |
|    total_timesteps      | 317500       |
| train/                  |              |
|    approx_kl            | 0.0027979272 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.804       |
|    explained_variance   | 0.0592       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.136        |
|    n_updates            | 553          |
|    policy_gradient_loss | 0.00472      |
|    value_loss           | 0.244        |
------------------------------------------
Eval num_timesteps=318000, episode_reward=1.94 +/- 2.15
Episode length: 94.88 +/- 31.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=2.04 +/- 3.08
Episode length: 91.78 +/- 27.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=1.38 +/- 2.12
Episode length: 87.90 +/- 25.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.9     |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 1.6      |
| time/              |          |
|    fps             | 119      |
|    iterations      | 156      |
|    time_elapsed    | 2681     |
|    total_timesteps | 319488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=319500, episode_reward=2.26 +/- 3.54
Episode length: 101.62 +/- 33.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | 2.26         |
| time/                   |              |
|    total_timesteps      | 319500       |
| train/                  |              |
|    approx_kl            | 0.0036774368 |
|    clip_fraction        | 0.0373       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.888       |
|    explained_variance   | 0.137        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0546       |
|    n_updates            | 554          |
|    policy_gradient_loss | 0.000285     |
|    value_loss           | 0.159        |
------------------------------------------
Eval num_timesteps=320000, episode_reward=1.46 +/- 2.17
Episode length: 96.76 +/- 31.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=1.82 +/- 2.59
Episode length: 99.58 +/- 36.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=2.42 +/- 3.36
Episode length: 105.18 +/- 40.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=1.74 +/- 2.97
Episode length: 100.24 +/- 37.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80       |
|    ep_rew_mean     | 1.51     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 157      |
|    time_elapsed    | 2703     |
|    total_timesteps | 321536   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=322000, episode_reward=1.82 +/- 2.47
Episode length: 99.04 +/- 35.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99          |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 322000      |
| train/                  |             |
|    approx_kl            | 0.016999418 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.152       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.048       |
|    n_updates            | 558         |
|    policy_gradient_loss | -0.000169   |
|    value_loss           | 0.316       |
-----------------------------------------
Eval num_timesteps=322500, episode_reward=1.26 +/- 2.03
Episode length: 94.46 +/- 28.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.5     |
|    mean_reward     | 1.26     |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=1.64 +/- 2.84
Episode length: 98.40 +/- 35.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=1.70 +/- 2.33
Episode length: 96.14 +/- 30.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.8     |
|    ep_rew_mean     | 1.36     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 158      |
|    time_elapsed    | 2720     |
|    total_timesteps | 323584   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=324000, episode_reward=1.26 +/- 2.11
Episode length: 92.68 +/- 36.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.7        |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.008269782 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.129       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0889      |
|    n_updates            | 564         |
|    policy_gradient_loss | -0.00652    |
|    value_loss           | 0.0674      |
-----------------------------------------
Eval num_timesteps=324500, episode_reward=1.82 +/- 2.73
Episode length: 91.16 +/- 32.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.2     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=1.50 +/- 2.39
Episode length: 92.58 +/- 39.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.6     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=1.96 +/- 2.48
Episode length: 100.00 +/- 39.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.4     |
|    ep_rew_mean     | 1.4      |
| time/              |          |
|    fps             | 118      |
|    iterations      | 159      |
|    time_elapsed    | 2736     |
|    total_timesteps | 325632   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=326000, episode_reward=1.44 +/- 2.69
Episode length: 96.90 +/- 33.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.9        |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 326000      |
| train/                  |             |
|    approx_kl            | 0.011001194 |
|    clip_fraction        | 0.0359      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.078       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.319       |
|    n_updates            | 566         |
|    policy_gradient_loss | -0.000297   |
|    value_loss           | 0.224       |
-----------------------------------------
Eval num_timesteps=326500, episode_reward=1.72 +/- 2.74
Episode length: 92.80 +/- 32.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.8     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=1.12 +/- 2.04
Episode length: 86.84 +/- 26.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.8     |
|    mean_reward     | 1.12     |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=1.78 +/- 2.77
Episode length: 90.10 +/- 29.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.1     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83       |
|    ep_rew_mean     | 1.81     |
| time/              |          |
|    fps             | 118      |
|    iterations      | 160      |
|    time_elapsed    | 2753     |
|    total_timesteps | 327680   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=328000, episode_reward=1.64 +/- 2.62
Episode length: 86.46 +/- 25.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.5        |
|    mean_reward          | 1.64        |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.007869044 |
|    clip_fraction        | 0.0651      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.796      |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.365       |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 0.411       |
-----------------------------------------
Eval num_timesteps=328500, episode_reward=2.06 +/- 3.12
Episode length: 91.44 +/- 23.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.4     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=1.42 +/- 2.31
Episode length: 90.04 +/- 33.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90       |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=1.46 +/- 2.30
Episode length: 86.60 +/- 32.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.6     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82       |
|    ep_rew_mean     | 1.76     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 161      |
|    time_elapsed    | 2769     |
|    total_timesteps | 329728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=330000, episode_reward=2.24 +/- 3.63
Episode length: 92.36 +/- 34.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.4         |
|    mean_reward          | 2.24         |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0034726532 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.789       |
|    explained_variance   | 0.0943       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.192        |
|    n_updates            | 571          |
|    policy_gradient_loss | 0.00144      |
|    value_loss           | 0.219        |
------------------------------------------
Eval num_timesteps=330500, episode_reward=1.20 +/- 2.09
Episode length: 88.78 +/- 31.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.8     |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=1.42 +/- 2.91
Episode length: 86.64 +/- 34.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.6     |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=2.00 +/- 2.58
Episode length: 91.30 +/- 37.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.3     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.9     |
|    ep_rew_mean     | 1.99     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 162      |
|    time_elapsed    | 2785     |
|    total_timesteps | 331776   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=332000, episode_reward=1.66 +/- 2.87
Episode length: 87.84 +/- 30.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.8         |
|    mean_reward          | 1.66         |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0122910505 |
|    clip_fraction        | 0.0443       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.944       |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.03         |
|    n_updates            | 574          |
|    policy_gradient_loss | -0.00197     |
|    value_loss           | 0.261        |
------------------------------------------
Eval num_timesteps=332500, episode_reward=1.62 +/- 2.40
Episode length: 88.68 +/- 25.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.7     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=1.60 +/- 2.65
Episode length: 90.70 +/- 27.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.7     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=1.64 +/- 2.80
Episode length: 92.42 +/- 36.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.9     |
|    ep_rew_mean     | 1.78     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 163      |
|    time_elapsed    | 2801     |
|    total_timesteps | 333824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=334000, episode_reward=1.74 +/- 2.50
Episode length: 96.26 +/- 30.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.3        |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 334000      |
| train/                  |             |
|    approx_kl            | 0.003688987 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.926      |
|    explained_variance   | 0.0966      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.022       |
|    n_updates            | 575         |
|    policy_gradient_loss | 0.0022      |
|    value_loss           | 0.189       |
-----------------------------------------
Eval num_timesteps=334500, episode_reward=1.50 +/- 1.84
Episode length: 88.60 +/- 30.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.6     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=1.90 +/- 3.73
Episode length: 96.24 +/- 27.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.2     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=1.26 +/- 1.85
Episode length: 93.54 +/- 34.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.5     |
|    mean_reward     | 1.26     |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.2     |
|    ep_rew_mean     | 1.64     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 164      |
|    time_elapsed    | 2817     |
|    total_timesteps | 335872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=336000, episode_reward=0.70 +/- 1.37
Episode length: 96.30 +/- 36.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.3        |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 336000      |
| train/                  |             |
|    approx_kl            | 0.005432229 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.0935      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.092       |
|    n_updates            | 576         |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 0.171       |
-----------------------------------------
Eval num_timesteps=336500, episode_reward=1.80 +/- 2.75
Episode length: 95.42 +/- 41.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.4     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=2.02 +/- 3.69
Episode length: 92.26 +/- 36.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.3     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=2.52 +/- 3.91
Episode length: 96.48 +/- 32.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.2     |
|    ep_rew_mean     | 1.77     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 165      |
|    time_elapsed    | 2833     |
|    total_timesteps | 337920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=338000, episode_reward=1.42 +/- 2.43
Episode length: 88.14 +/- 29.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 88.1         |
|    mean_reward          | 1.42         |
| time/                   |              |
|    total_timesteps      | 338000       |
| train/                  |              |
|    approx_kl            | 0.0033539387 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.7         |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0612       |
|    n_updates            | 577          |
|    policy_gradient_loss | -0.0013      |
|    value_loss           | 0.375        |
------------------------------------------
Eval num_timesteps=338500, episode_reward=2.34 +/- 3.44
Episode length: 99.70 +/- 37.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.7     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=1.36 +/- 2.06
Episode length: 90.96 +/- 23.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91       |
|    mean_reward     | 1.36     |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=1.62 +/- 3.07
Episode length: 95.58 +/- 37.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.6     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.5     |
|    ep_rew_mean     | 1.89     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 166      |
|    time_elapsed    | 2850     |
|    total_timesteps | 339968   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=340000, episode_reward=0.98 +/- 2.00
Episode length: 84.80 +/- 27.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.8        |
|    mean_reward          | 0.98        |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.004288491 |
|    clip_fraction        | 0.0324      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.584      |
|    explained_variance   | 0.11        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.157       |
|    n_updates            | 579         |
|    policy_gradient_loss | 0.00075     |
|    value_loss           | 0.291       |
-----------------------------------------
Eval num_timesteps=340500, episode_reward=1.20 +/- 2.17
Episode length: 89.40 +/- 34.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.4     |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=1.44 +/- 2.28
Episode length: 100.38 +/- 36.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=2.00 +/- 3.09
Episode length: 98.94 +/- 33.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=1.38 +/- 2.24
Episode length: 96.88 +/- 36.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.9     |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.7     |
|    ep_rew_mean     | 1.92     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 167      |
|    time_elapsed    | 2870     |
|    total_timesteps | 342016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=342500, episode_reward=1.46 +/- 1.98
Episode length: 84.16 +/- 25.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.2         |
|    mean_reward          | 1.46         |
| time/                   |              |
|    total_timesteps      | 342500       |
| train/                  |              |
|    approx_kl            | 0.0030236708 |
|    clip_fraction        | 0.0308       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.581       |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0377       |
|    n_updates            | 580          |
|    policy_gradient_loss | 3.95e-05     |
|    value_loss           | 0.191        |
------------------------------------------
Eval num_timesteps=343000, episode_reward=1.88 +/- 3.09
Episode length: 106.00 +/- 38.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=0.84 +/- 1.68
Episode length: 98.94 +/- 34.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | 0.84     |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=1.16 +/- 2.36
Episode length: 92.14 +/- 34.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.1     |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81       |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 168      |
|    time_elapsed    | 2886     |
|    total_timesteps | 344064   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=344500, episode_reward=1.52 +/- 2.81
Episode length: 92.02 +/- 31.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92          |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 344500      |
| train/                  |             |
|    approx_kl            | 0.006306914 |
|    clip_fraction        | 0.0635      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.153       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0468      |
|    n_updates            | 583         |
|    policy_gradient_loss | -0.00087    |
|    value_loss           | 0.17        |
-----------------------------------------
Eval num_timesteps=345000, episode_reward=1.36 +/- 2.56
Episode length: 90.20 +/- 33.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.2     |
|    mean_reward     | 1.36     |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=1.16 +/- 2.16
Episode length: 87.12 +/- 28.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.1     |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=2.08 +/- 3.00
Episode length: 97.38 +/- 36.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.4     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.7     |
|    ep_rew_mean     | 1.43     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 169      |
|    time_elapsed    | 2903     |
|    total_timesteps | 346112   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=346500, episode_reward=1.48 +/- 2.62
Episode length: 86.56 +/- 29.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 86.6         |
|    mean_reward          | 1.48         |
| time/                   |              |
|    total_timesteps      | 346500       |
| train/                  |              |
|    approx_kl            | 0.0048518535 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.783       |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0724       |
|    n_updates            | 585          |
|    policy_gradient_loss | -0.000879    |
|    value_loss           | 0.105        |
------------------------------------------
Eval num_timesteps=347000, episode_reward=2.14 +/- 3.35
Episode length: 97.24 +/- 40.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=1.96 +/- 4.07
Episode length: 93.14 +/- 35.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=1.76 +/- 2.67
Episode length: 88.86 +/- 33.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.9     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 1.32     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 170      |
|    time_elapsed    | 2919     |
|    total_timesteps | 348160   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=348500, episode_reward=1.52 +/- 1.93
Episode length: 95.10 +/- 34.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.1        |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 348500      |
| train/                  |             |
|    approx_kl            | 0.016180795 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.121       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.233       |
|    n_updates            | 588         |
|    policy_gradient_loss | -0.000454   |
|    value_loss           | 0.241       |
-----------------------------------------
Eval num_timesteps=349000, episode_reward=1.56 +/- 3.14
Episode length: 91.82 +/- 32.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=1.12 +/- 2.09
Episode length: 95.52 +/- 41.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | 1.12     |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=1.58 +/- 2.42
Episode length: 89.62 +/- 37.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.6     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.7     |
|    ep_rew_mean     | 1.14     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 171      |
|    time_elapsed    | 2936     |
|    total_timesteps | 350208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=350500, episode_reward=1.74 +/- 2.93
Episode length: 90.72 +/- 36.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.7        |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 350500      |
| train/                  |             |
|    approx_kl            | 0.006284107 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.628      |
|    explained_variance   | 0.0129      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0912      |
|    n_updates            | 589         |
|    policy_gradient_loss | 0.001       |
|    value_loss           | 0.151       |
-----------------------------------------
Eval num_timesteps=351000, episode_reward=1.82 +/- 3.17
Episode length: 92.80 +/- 34.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.8     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=1.84 +/- 2.87
Episode length: 94.34 +/- 33.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.3     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=2.08 +/- 2.67
Episode length: 97.00 +/- 29.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.2     |
|    ep_rew_mean     | 1.38     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 172      |
|    time_elapsed    | 2952     |
|    total_timesteps | 352256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=352500, episode_reward=2.60 +/- 3.64
Episode length: 112.46 +/- 48.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 2.6         |
| time/                   |             |
|    total_timesteps      | 352500      |
| train/                  |             |
|    approx_kl            | 0.004312684 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.142       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0849      |
|    n_updates            | 590         |
|    policy_gradient_loss | 0.00146     |
|    value_loss           | 0.268       |
-----------------------------------------
Eval num_timesteps=353000, episode_reward=1.50 +/- 1.90
Episode length: 95.36 +/- 34.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.4     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=1.58 +/- 2.36
Episode length: 86.54 +/- 28.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.5     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=0.88 +/- 1.41
Episode length: 89.72 +/- 31.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.7     |
|    mean_reward     | 0.88     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.4     |
|    ep_rew_mean     | 1.49     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 173      |
|    time_elapsed    | 2969     |
|    total_timesteps | 354304   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=354500, episode_reward=1.84 +/- 3.02
Episode length: 93.04 +/- 29.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93          |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 354500      |
| train/                  |             |
|    approx_kl            | 0.007944437 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.173       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0215      |
|    n_updates            | 592         |
|    policy_gradient_loss | -0.00157    |
|    value_loss           | 0.143       |
-----------------------------------------
Eval num_timesteps=355000, episode_reward=2.36 +/- 3.67
Episode length: 90.48 +/- 31.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=1.46 +/- 2.27
Episode length: 95.10 +/- 32.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.1     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=1.82 +/- 3.20
Episode length: 93.88 +/- 36.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.9     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.9     |
|    ep_rew_mean     | 1.61     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 174      |
|    time_elapsed    | 2985     |
|    total_timesteps | 356352   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=356500, episode_reward=1.46 +/- 1.88
Episode length: 84.68 +/- 31.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.7        |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 356500      |
| train/                  |             |
|    approx_kl            | 0.007407201 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.078       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.332       |
|    n_updates            | 595         |
|    policy_gradient_loss | -1.78e-06   |
|    value_loss           | 0.38        |
-----------------------------------------
Eval num_timesteps=357000, episode_reward=2.12 +/- 3.32
Episode length: 97.16 +/- 41.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=1.80 +/- 3.25
Episode length: 96.06 +/- 36.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=3.00 +/- 4.35
Episode length: 99.08 +/- 41.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.8     |
|    ep_rew_mean     | 1.96     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 175      |
|    time_elapsed    | 3002     |
|    total_timesteps | 358400   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=358500, episode_reward=2.12 +/- 3.17
Episode length: 97.54 +/- 37.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.5       |
|    mean_reward          | 2.12       |
| time/                   |            |
|    total_timesteps      | 358500     |
| train/                  |            |
|    approx_kl            | 0.01493841 |
|    clip_fraction        | 0.0528     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.878     |
|    explained_variance   | 0.0898     |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0459     |
|    n_updates            | 597        |
|    policy_gradient_loss | 0.00277    |
|    value_loss           | 0.31       |
----------------------------------------
Eval num_timesteps=359000, episode_reward=1.52 +/- 1.94
Episode length: 91.76 +/- 31.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=2.08 +/- 3.60
Episode length: 91.34 +/- 24.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.3     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=1.98 +/- 2.49
Episode length: 92.98 +/- 38.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93       |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.4     |
|    ep_rew_mean     | 2.06     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 176      |
|    time_elapsed    | 3018     |
|    total_timesteps | 360448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=360500, episode_reward=2.26 +/- 2.57
Episode length: 95.42 +/- 31.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.4         |
|    mean_reward          | 2.26         |
| time/                   |              |
|    total_timesteps      | 360500       |
| train/                  |              |
|    approx_kl            | 0.0036735882 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.787       |
|    explained_variance   | 0.0887       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0895       |
|    n_updates            | 598          |
|    policy_gradient_loss | -0.000177    |
|    value_loss           | 0.328        |
------------------------------------------
Eval num_timesteps=361000, episode_reward=1.82 +/- 3.06
Episode length: 90.30 +/- 34.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.3     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=1.12 +/- 2.39
Episode length: 90.40 +/- 27.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.4     |
|    mean_reward     | 1.12     |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=1.02 +/- 1.85
Episode length: 88.26 +/- 27.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.3     |
|    mean_reward     | 1.02     |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83       |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 177      |
|    time_elapsed    | 3034     |
|    total_timesteps | 362496   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=362500, episode_reward=2.32 +/- 3.58
Episode length: 87.90 +/- 34.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.9         |
|    mean_reward          | 2.32         |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 0.0065492424 |
|    clip_fraction        | 0.048        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.917       |
|    explained_variance   | 0.157        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.14         |
|    n_updates            | 600          |
|    policy_gradient_loss | 0.000335     |
|    value_loss           | 0.426        |
------------------------------------------
Eval num_timesteps=363000, episode_reward=1.34 +/- 2.53
Episode length: 86.80 +/- 28.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.8     |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=1.88 +/- 2.62
Episode length: 94.92 +/- 36.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=1.72 +/- 2.45
Episode length: 89.36 +/- 34.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.4     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=1.74 +/- 2.39
Episode length: 99.86 +/- 35.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.9     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.3     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 178      |
|    time_elapsed    | 3054     |
|    total_timesteps | 364544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=365000, episode_reward=2.30 +/- 3.32
Episode length: 87.40 +/- 30.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.4         |
|    mean_reward          | 2.3          |
| time/                   |              |
|    total_timesteps      | 365000       |
| train/                  |              |
|    approx_kl            | 0.0031836003 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.949       |
|    explained_variance   | 0.246        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.224        |
|    n_updates            | 601          |
|    policy_gradient_loss | 0.00255      |
|    value_loss           | 0.325        |
------------------------------------------
Eval num_timesteps=365500, episode_reward=1.62 +/- 2.12
Episode length: 83.36 +/- 26.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.4     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=1.78 +/- 2.68
Episode length: 88.92 +/- 28.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.9     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=1.84 +/- 3.07
Episode length: 95.04 +/- 41.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95       |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 179      |
|    time_elapsed    | 3069     |
|    total_timesteps | 366592   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=367000, episode_reward=1.98 +/- 2.98
Episode length: 92.88 +/- 26.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.9        |
|    mean_reward          | 1.98        |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.008735063 |
|    clip_fraction        | 0.0605      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.208       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0964      |
|    n_updates            | 605         |
|    policy_gradient_loss | -0.00643    |
|    value_loss           | 0.211       |
-----------------------------------------
Eval num_timesteps=367500, episode_reward=2.68 +/- 3.53
Episode length: 94.72 +/- 37.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.7     |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=1.60 +/- 2.88
Episode length: 82.28 +/- 29.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=2.36 +/- 3.05
Episode length: 90.30 +/- 40.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.3     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.9     |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 180      |
|    time_elapsed    | 3085     |
|    total_timesteps | 368640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=369000, episode_reward=1.62 +/- 2.67
Episode length: 81.20 +/- 25.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81.2         |
|    mean_reward          | 1.62         |
| time/                   |              |
|    total_timesteps      | 369000       |
| train/                  |              |
|    approx_kl            | 0.0050709313 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0491       |
|    n_updates            | 606          |
|    policy_gradient_loss | -0.00294     |
|    value_loss           | 0.276        |
------------------------------------------
Eval num_timesteps=369500, episode_reward=2.12 +/- 2.75
Episode length: 87.28 +/- 28.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.3     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=1.00 +/- 1.66
Episode length: 75.72 +/- 26.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | 1        |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=1.58 +/- 2.22
Episode length: 88.82 +/- 28.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.8     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82       |
|    ep_rew_mean     | 1.96     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 181      |
|    time_elapsed    | 3101     |
|    total_timesteps | 370688   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=371000, episode_reward=1.48 +/- 2.59
Episode length: 87.18 +/- 36.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.2        |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.009479583 |
|    clip_fraction        | 0.0673      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.198       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.129       |
|    n_updates            | 609         |
|    policy_gradient_loss | -0.00457    |
|    value_loss           | 0.148       |
-----------------------------------------
Eval num_timesteps=371500, episode_reward=1.78 +/- 2.50
Episode length: 86.40 +/- 30.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.4     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=1.34 +/- 2.07
Episode length: 84.16 +/- 30.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.2     |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=1.42 +/- 2.20
Episode length: 86.46 +/- 30.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.5     |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.3     |
|    ep_rew_mean     | 1.89     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 182      |
|    time_elapsed    | 3116     |
|    total_timesteps | 372736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=373000, episode_reward=1.26 +/- 2.17
Episode length: 85.76 +/- 35.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 85.8       |
|    mean_reward          | 1.26       |
| time/                   |            |
|    total_timesteps      | 373000     |
| train/                  |            |
|    approx_kl            | 0.00391609 |
|    clip_fraction        | 0.0515     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.879     |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.137      |
|    n_updates            | 610        |
|    policy_gradient_loss | 0.00224    |
|    value_loss           | 0.371      |
----------------------------------------
Eval num_timesteps=373500, episode_reward=1.42 +/- 2.62
Episode length: 85.58 +/- 30.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.6     |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=1.56 +/- 2.77
Episode length: 83.68 +/- 25.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.7     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=1.60 +/- 1.83
Episode length: 89.66 +/- 30.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.7     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 1.88     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 183      |
|    time_elapsed    | 3131     |
|    total_timesteps | 374784   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=375000, episode_reward=2.02 +/- 2.82
Episode length: 84.34 +/- 33.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.3        |
|    mean_reward          | 2.02        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.004291683 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.215       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00952     |
|    n_updates            | 612         |
|    policy_gradient_loss | -0.00171    |
|    value_loss           | 0.267       |
-----------------------------------------
Eval num_timesteps=375500, episode_reward=2.46 +/- 4.37
Episode length: 87.50 +/- 35.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.5     |
|    mean_reward     | 2.46     |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=3.18 +/- 4.55
Episode length: 88.54 +/- 32.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.5     |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
New best mean reward!
Eval num_timesteps=376500, episode_reward=1.96 +/- 3.22
Episode length: 84.04 +/- 28.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84       |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.5     |
|    ep_rew_mean     | 1.71     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 184      |
|    time_elapsed    | 3147     |
|    total_timesteps | 376832   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=377000, episode_reward=2.16 +/- 3.16
Episode length: 84.60 +/- 30.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.6        |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.006649896 |
|    clip_fraction        | 0.0613      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.194       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0183      |
|    n_updates            | 615         |
|    policy_gradient_loss | -0.0056     |
|    value_loss           | 0.148       |
-----------------------------------------
Eval num_timesteps=377500, episode_reward=1.28 +/- 1.88
Episode length: 82.98 +/- 33.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83       |
|    mean_reward     | 1.28     |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=2.00 +/- 2.90
Episode length: 83.96 +/- 34.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84       |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=1.66 +/- 2.45
Episode length: 80.22 +/- 25.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82       |
|    ep_rew_mean     | 1.85     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 185      |
|    time_elapsed    | 3162     |
|    total_timesteps | 378880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=379000, episode_reward=2.14 +/- 3.09
Episode length: 81.16 +/- 23.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81.2         |
|    mean_reward          | 2.14         |
| time/                   |              |
|    total_timesteps      | 379000       |
| train/                  |              |
|    approx_kl            | 0.0053554964 |
|    clip_fraction        | 0.0457       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.174        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.397        |
|    n_updates            | 616          |
|    policy_gradient_loss | 0.00496      |
|    value_loss           | 0.269        |
------------------------------------------
Eval num_timesteps=379500, episode_reward=1.72 +/- 2.13
Episode length: 83.86 +/- 26.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.9     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=1.52 +/- 2.20
Episode length: 80.34 +/- 23.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=1.18 +/- 2.76
Episode length: 81.28 +/- 28.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.3     |
|    mean_reward     | 1.18     |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.7     |
|    ep_rew_mean     | 1.73     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 186      |
|    time_elapsed    | 3176     |
|    total_timesteps | 380928   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=381000, episode_reward=1.42 +/- 2.35
Episode length: 77.72 +/- 25.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.7        |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.005045561 |
|    clip_fraction        | 0.0615      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.914      |
|    explained_variance   | 0.211       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0712      |
|    n_updates            | 619         |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 0.188       |
-----------------------------------------
Eval num_timesteps=381500, episode_reward=1.06 +/- 2.18
Episode length: 75.82 +/- 28.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.8     |
|    mean_reward     | 1.06     |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=1.90 +/- 2.09
Episode length: 89.78 +/- 31.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.8     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=1.56 +/- 2.57
Episode length: 78.96 +/- 26.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79       |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.3     |
|    ep_rew_mean     | 2.15     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 187      |
|    time_elapsed    | 3191     |
|    total_timesteps | 382976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=383000, episode_reward=1.78 +/- 2.19
Episode length: 85.26 +/- 24.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 85.3         |
|    mean_reward          | 1.78         |
| time/                   |              |
|    total_timesteps      | 383000       |
| train/                  |              |
|    approx_kl            | 0.0049428525 |
|    clip_fraction        | 0.0608       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.239        |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 0.613        |
------------------------------------------
Eval num_timesteps=383500, episode_reward=1.52 +/- 2.63
Episode length: 74.48 +/- 24.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=2.54 +/- 3.13
Episode length: 86.94 +/- 28.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.9     |
|    mean_reward     | 2.54     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=2.20 +/- 3.14
Episode length: 88.16 +/- 35.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.2     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=2.12 +/- 2.90
Episode length: 86.46 +/- 31.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.5     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.8     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 188      |
|    time_elapsed    | 3209     |
|    total_timesteps | 385024   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=385500, episode_reward=1.24 +/- 2.75
Episode length: 74.40 +/- 24.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.4        |
|    mean_reward          | 1.24        |
| time/                   |             |
|    total_timesteps      | 385500      |
| train/                  |             |
|    approx_kl            | 0.010984678 |
|    clip_fraction        | 0.083       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.137       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0921      |
|    n_updates            | 622         |
|    policy_gradient_loss | 0.000637    |
|    value_loss           | 0.678       |
-----------------------------------------
Eval num_timesteps=386000, episode_reward=2.16 +/- 2.77
Episode length: 82.20 +/- 29.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.2     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=2.78 +/- 4.19
Episode length: 84.34 +/- 24.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.3     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=1.80 +/- 2.25
Episode length: 83.56 +/- 26.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.6     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.4     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 189      |
|    time_elapsed    | 3224     |
|    total_timesteps | 387072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=387500, episode_reward=1.52 +/- 2.71
Episode length: 87.94 +/- 33.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.9         |
|    mean_reward          | 1.52         |
| time/                   |              |
|    total_timesteps      | 387500       |
| train/                  |              |
|    approx_kl            | 0.0037511147 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.00695      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0629       |
|    n_updates            | 623          |
|    policy_gradient_loss | -0.000734    |
|    value_loss           | 0.154        |
------------------------------------------
Eval num_timesteps=388000, episode_reward=1.76 +/- 2.57
Episode length: 76.68 +/- 23.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.7     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=1.46 +/- 2.09
Episode length: 82.30 +/- 27.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=1.66 +/- 2.42
Episode length: 90.86 +/- 31.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87       |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 190      |
|    time_elapsed    | 3239     |
|    total_timesteps | 389120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=389500, episode_reward=2.50 +/- 3.92
Episode length: 84.06 +/- 37.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.1        |
|    mean_reward          | 2.5         |
| time/                   |             |
|    total_timesteps      | 389500      |
| train/                  |             |
|    approx_kl            | 0.005034277 |
|    clip_fraction        | 0.0504      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.179       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.203       |
|    n_updates            | 624         |
|    policy_gradient_loss | -0.000754   |
|    value_loss           | 0.434       |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=1.64 +/- 3.29
Episode length: 79.76 +/- 26.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.8     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=1.54 +/- 2.57
Episode length: 84.16 +/- 28.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.2     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=1.42 +/- 2.41
Episode length: 82.08 +/- 34.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.2     |
|    ep_rew_mean     | 2.24     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 191      |
|    time_elapsed    | 3254     |
|    total_timesteps | 391168   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=391500, episode_reward=1.30 +/- 2.52
Episode length: 82.46 +/- 52.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 82.5         |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 391500       |
| train/                  |              |
|    approx_kl            | 0.0062564733 |
|    clip_fraction        | 0.0418       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | -0.0383      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.03         |
|    n_updates            | 626          |
|    policy_gradient_loss | -0.00693     |
|    value_loss           | 0.118        |
------------------------------------------
Eval num_timesteps=392000, episode_reward=1.28 +/- 2.14
Episode length: 83.46 +/- 29.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.5     |
|    mean_reward     | 1.28     |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=2.38 +/- 3.35
Episode length: 83.42 +/- 29.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.4     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=1.88 +/- 2.49
Episode length: 87.34 +/- 26.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.3     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.8     |
|    ep_rew_mean     | 1.6      |
| time/              |          |
|    fps             | 120      |
|    iterations      | 192      |
|    time_elapsed    | 3269     |
|    total_timesteps | 393216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=393500, episode_reward=1.30 +/- 2.22
Episode length: 77.38 +/- 27.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.4         |
|    mean_reward          | 1.3          |
| time/                   |              |
|    total_timesteps      | 393500       |
| train/                  |              |
|    approx_kl            | 0.0026318643 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0673       |
|    n_updates            | 627          |
|    policy_gradient_loss | -0.000567    |
|    value_loss           | 0.164        |
------------------------------------------
Eval num_timesteps=394000, episode_reward=1.98 +/- 2.99
Episode length: 83.72 +/- 30.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.7     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=1.94 +/- 3.19
Episode length: 80.52 +/- 25.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.5     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=2.30 +/- 2.89
Episode length: 91.36 +/- 26.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.4     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.7     |
|    ep_rew_mean     | 1.73     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 193      |
|    time_elapsed    | 3284     |
|    total_timesteps | 395264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=395500, episode_reward=1.60 +/- 2.37
Episode length: 76.20 +/- 30.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.2        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 395500      |
| train/                  |             |
|    approx_kl            | 0.005148378 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.249       |
|    n_updates            | 628         |
|    policy_gradient_loss | 0.00376     |
|    value_loss           | 0.203       |
-----------------------------------------
Eval num_timesteps=396000, episode_reward=1.94 +/- 3.01
Episode length: 89.46 +/- 36.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.5     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=2.52 +/- 3.48
Episode length: 89.92 +/- 29.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.9     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=2.22 +/- 3.07
Episode length: 79.72 +/- 25.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.9     |
|    ep_rew_mean     | 1.24     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 194      |
|    time_elapsed    | 3299     |
|    total_timesteps | 397312   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=397500, episode_reward=1.92 +/- 3.05
Episode length: 80.72 +/- 29.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.7        |
|    mean_reward          | 1.92        |
| time/                   |             |
|    total_timesteps      | 397500      |
| train/                  |             |
|    approx_kl            | 0.007957867 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.036       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0306      |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.000176   |
|    value_loss           | 0.183       |
-----------------------------------------
Eval num_timesteps=398000, episode_reward=1.62 +/- 2.57
Episode length: 86.26 +/- 31.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.3     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=1.74 +/- 2.36
Episode length: 80.32 +/- 27.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=1.48 +/- 1.92
Episode length: 85.88 +/- 26.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.9     |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 1.66     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 195      |
|    time_elapsed    | 3314     |
|    total_timesteps | 399360   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=399500, episode_reward=2.38 +/- 4.01
Episode length: 82.18 +/- 27.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.2        |
|    mean_reward          | 2.38        |
| time/                   |             |
|    total_timesteps      | 399500      |
| train/                  |             |
|    approx_kl            | 0.009529264 |
|    clip_fraction        | 0.0683      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.0679      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.47        |
|    n_updates            | 632         |
|    policy_gradient_loss | -0.000855   |
|    value_loss           | 0.426       |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=1.12 +/- 2.13
Episode length: 77.66 +/- 23.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 1.12     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=2.14 +/- 3.06
Episode length: 84.98 +/- 29.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85       |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=1.36 +/- 2.39
Episode length: 73.56 +/- 21.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.6     |
|    mean_reward     | 1.36     |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81       |
|    ep_rew_mean     | 2.08     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 196      |
|    time_elapsed    | 3328     |
|    total_timesteps | 401408   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=401500, episode_reward=2.60 +/- 2.74
Episode length: 83.38 +/- 25.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.4        |
|    mean_reward          | 2.6         |
| time/                   |             |
|    total_timesteps      | 401500      |
| train/                  |             |
|    approx_kl            | 0.009323696 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.116       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.189       |
|    n_updates            | 636         |
|    policy_gradient_loss | -0.00821    |
|    value_loss           | 0.521       |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=1.66 +/- 2.90
Episode length: 79.44 +/- 25.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=2.48 +/- 2.71
Episode length: 84.92 +/- 26.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.9     |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=1.50 +/- 2.37
Episode length: 77.16 +/- 26.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 2.24     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 197      |
|    time_elapsed    | 3343     |
|    total_timesteps | 403456   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=403500, episode_reward=1.84 +/- 2.96
Episode length: 76.16 +/- 25.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.2        |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 403500      |
| train/                  |             |
|    approx_kl            | 0.008495372 |
|    clip_fraction        | 0.0588      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.109       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.161       |
|    n_updates            | 639         |
|    policy_gradient_loss | -0.00302    |
|    value_loss           | 0.271       |
-----------------------------------------
Eval num_timesteps=404000, episode_reward=2.80 +/- 3.10
Episode length: 87.46 +/- 28.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.5     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=1.44 +/- 2.79
Episode length: 72.74 +/- 24.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=2.16 +/- 3.60
Episode length: 79.96 +/- 26.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=2.52 +/- 4.41
Episode length: 77.32 +/- 28.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.4     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 198      |
|    time_elapsed    | 3360     |
|    total_timesteps | 405504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=406000, episode_reward=1.48 +/- 2.66
Episode length: 72.78 +/- 28.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72.8         |
|    mean_reward          | 1.48         |
| time/                   |              |
|    total_timesteps      | 406000       |
| train/                  |              |
|    approx_kl            | 0.0031063317 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.12         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.252        |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.000307    |
|    value_loss           | 0.573        |
------------------------------------------
Eval num_timesteps=406500, episode_reward=2.94 +/- 4.48
Episode length: 85.18 +/- 35.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.2     |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=2.52 +/- 3.87
Episode length: 81.86 +/- 24.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.9     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=2.38 +/- 3.22
Episode length: 81.60 +/- 29.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.6     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.6     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 199      |
|    time_elapsed    | 3375     |
|    total_timesteps | 407552   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=408000, episode_reward=1.80 +/- 2.62
Episode length: 76.32 +/- 24.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.3        |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.005115919 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.0941      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.149       |
|    n_updates            | 644         |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 0.465       |
-----------------------------------------
Eval num_timesteps=408500, episode_reward=1.94 +/- 2.63
Episode length: 77.24 +/- 28.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=2.36 +/- 2.22
Episode length: 78.82 +/- 23.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.8     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=1.66 +/- 2.38
Episode length: 78.58 +/- 29.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.2     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 200      |
|    time_elapsed    | 3389     |
|    total_timesteps | 409600   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=410000, episode_reward=2.32 +/- 3.54
Episode length: 79.18 +/- 28.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.2        |
|    mean_reward          | 2.32        |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.009601183 |
|    clip_fraction        | 0.0494      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.125       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0444      |
|    n_updates            | 648         |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 0.191       |
-----------------------------------------
Eval num_timesteps=410500, episode_reward=1.90 +/- 2.59
Episode length: 79.88 +/- 27.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=2.44 +/- 2.79
Episode length: 74.90 +/- 23.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=3.10 +/- 3.92
Episode length: 81.44 +/- 25.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.4     |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.3     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 201      |
|    time_elapsed    | 3403     |
|    total_timesteps | 411648   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=412000, episode_reward=3.14 +/- 4.12
Episode length: 82.98 +/- 27.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83          |
|    mean_reward          | 3.14        |
| time/                   |             |
|    total_timesteps      | 412000      |
| train/                  |             |
|    approx_kl            | 0.007866373 |
|    clip_fraction        | 0.0584      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.084       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.106       |
|    n_updates            | 651         |
|    policy_gradient_loss | -0.00382    |
|    value_loss           | 0.379       |
-----------------------------------------
Eval num_timesteps=412500, episode_reward=1.64 +/- 2.35
Episode length: 76.88 +/- 28.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=2.18 +/- 2.29
Episode length: 82.00 +/- 26.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82       |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=2.30 +/- 3.61
Episode length: 78.18 +/- 24.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.2     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 202      |
|    time_elapsed    | 3419     |
|    total_timesteps | 413696   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=414000, episode_reward=1.52 +/- 2.13
Episode length: 74.94 +/- 23.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.9        |
|    mean_reward          | 1.52        |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.013420448 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.13        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0942      |
|    n_updates            | 655         |
|    policy_gradient_loss | -0.00677    |
|    value_loss           | 0.387       |
-----------------------------------------
Eval num_timesteps=414500, episode_reward=2.16 +/- 2.74
Episode length: 81.48 +/- 30.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.5     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=1.76 +/- 2.77
Episode length: 68.22 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.2     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=2.06 +/- 3.36
Episode length: 74.60 +/- 24.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.5     |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 203      |
|    time_elapsed    | 3432     |
|    total_timesteps | 415744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=416000, episode_reward=1.80 +/- 2.39
Episode length: 74.62 +/- 21.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 74.6         |
|    mean_reward          | 1.8          |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0036425644 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.0412       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.261        |
|    n_updates            | 656          |
|    policy_gradient_loss | 0.00151      |
|    value_loss           | 0.503        |
------------------------------------------
Eval num_timesteps=416500, episode_reward=2.04 +/- 3.05
Episode length: 77.28 +/- 22.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=2.50 +/- 3.91
Episode length: 76.76 +/- 29.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=1.78 +/- 2.93
Episode length: 73.96 +/- 25.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74       |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.9     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 204      |
|    time_elapsed    | 3446     |
|    total_timesteps | 417792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=418000, episode_reward=1.80 +/- 2.51
Episode length: 73.84 +/- 26.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73.8         |
|    mean_reward          | 1.8          |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0067725047 |
|    clip_fraction        | 0.0873       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.186        |
|    n_updates            | 657          |
|    policy_gradient_loss | -0.00315     |
|    value_loss           | 0.393        |
------------------------------------------
Eval num_timesteps=418500, episode_reward=2.60 +/- 3.22
Episode length: 81.30 +/- 25.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.3     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=2.52 +/- 4.09
Episode length: 77.92 +/- 24.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=1.80 +/- 2.65
Episode length: 73.94 +/- 25.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.8     |
|    ep_rew_mean     | 2.14     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 205      |
|    time_elapsed    | 3461     |
|    total_timesteps | 419840   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=420000, episode_reward=2.70 +/- 3.69
Episode length: 75.98 +/- 26.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 76         |
|    mean_reward          | 2.7        |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.00759884 |
|    clip_fraction        | 0.0531     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | 0.0827     |
|    learning_rate        | 0.0001     |
|    loss                 | 0.12       |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.00145   |
|    value_loss           | 0.23       |
----------------------------------------
Eval num_timesteps=420500, episode_reward=2.34 +/- 3.05
Episode length: 75.66 +/- 22.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=2.40 +/- 3.29
Episode length: 80.06 +/- 29.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=1.74 +/- 2.56
Episode length: 78.54 +/- 24.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.6     |
|    ep_rew_mean     | 1.81     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 206      |
|    time_elapsed    | 3476     |
|    total_timesteps | 421888   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=422000, episode_reward=1.84 +/- 2.77
Episode length: 74.28 +/- 27.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.3        |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.008943103 |
|    clip_fraction        | 0.0679      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.0712      |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00162    |
|    n_updates            | 665         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0915      |
-----------------------------------------
Eval num_timesteps=422500, episode_reward=3.04 +/- 3.89
Episode length: 82.16 +/- 30.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.2     |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=2.94 +/- 4.60
Episode length: 79.54 +/- 27.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=2.78 +/- 3.59
Episode length: 85.14 +/- 23.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.1     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.8     |
|    ep_rew_mean     | 1.39     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 207      |
|    time_elapsed    | 3491     |
|    total_timesteps | 423936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=424000, episode_reward=1.60 +/- 2.52
Episode length: 74.98 +/- 21.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.003808583 |
|    clip_fraction        | 0.03        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | -0.137      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.242       |
|    n_updates            | 666         |
|    policy_gradient_loss | 0.00298     |
|    value_loss           | 0.264       |
-----------------------------------------
Eval num_timesteps=424500, episode_reward=2.52 +/- 3.93
Episode length: 83.32 +/- 29.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.3     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=2.72 +/- 4.42
Episode length: 78.74 +/- 28.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.7     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=2.62 +/- 3.71
Episode length: 79.46 +/- 31.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 1.34     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 208      |
|    time_elapsed    | 3505     |
|    total_timesteps | 425984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=426000, episode_reward=1.56 +/- 2.54
Episode length: 79.62 +/- 30.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.6        |
|    mean_reward          | 1.56        |
| time/                   |             |
|    total_timesteps      | 426000      |
| train/                  |             |
|    approx_kl            | 0.004781031 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.116       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.169       |
|    n_updates            | 667         |
|    policy_gradient_loss | 0.00293     |
|    value_loss           | 0.265       |
-----------------------------------------
Eval num_timesteps=426500, episode_reward=2.40 +/- 4.05
Episode length: 85.70 +/- 27.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.7     |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=1.96 +/- 2.93
Episode length: 82.48 +/- 31.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.5     |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=1.26 +/- 1.92
Episode length: 75.62 +/- 22.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.6     |
|    mean_reward     | 1.26     |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=2.30 +/- 3.16
Episode length: 80.22 +/- 24.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.9     |
|    ep_rew_mean     | 1.47     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 209      |
|    time_elapsed    | 3523     |
|    total_timesteps | 428032   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=428500, episode_reward=1.82 +/- 2.88
Episode length: 69.90 +/- 20.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 69.9         |
|    mean_reward          | 1.82         |
| time/                   |              |
|    total_timesteps      | 428500       |
| train/                  |              |
|    approx_kl            | 0.0077094706 |
|    clip_fraction        | 0.0509       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.168        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0731       |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00515     |
|    value_loss           | 0.253        |
------------------------------------------
Eval num_timesteps=429000, episode_reward=1.94 +/- 2.52
Episode length: 81.22 +/- 23.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.2     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=1.92 +/- 3.36
Episode length: 79.36 +/- 33.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=2.04 +/- 2.65
Episode length: 75.52 +/- 23.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.5     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.2     |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 210      |
|    time_elapsed    | 3537     |
|    total_timesteps | 430080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=430500, episode_reward=2.78 +/- 3.21
Episode length: 76.98 +/- 24.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77          |
|    mean_reward          | 2.78        |
| time/                   |             |
|    total_timesteps      | 430500      |
| train/                  |             |
|    approx_kl            | 0.002831794 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.147       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.349       |
|    n_updates            | 671         |
|    policy_gradient_loss | 0.000319    |
|    value_loss           | 0.374       |
-----------------------------------------
Eval num_timesteps=431000, episode_reward=2.24 +/- 3.26
Episode length: 79.08 +/- 23.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.1     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=2.28 +/- 3.09
Episode length: 78.06 +/- 19.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=1.84 +/- 3.58
Episode length: 72.40 +/- 24.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.4     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83       |
|    ep_rew_mean     | 2.02     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 211      |
|    time_elapsed    | 3551     |
|    total_timesteps | 432128   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=432500, episode_reward=2.24 +/- 2.53
Episode length: 75.80 +/- 24.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.8        |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 432500      |
| train/                  |             |
|    approx_kl            | 0.013458636 |
|    clip_fraction        | 0.0784      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.15        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.329       |
|    n_updates            | 674         |
|    policy_gradient_loss | -0.00584    |
|    value_loss           | 0.347       |
-----------------------------------------
Eval num_timesteps=433000, episode_reward=2.84 +/- 3.76
Episode length: 77.90 +/- 30.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=1.58 +/- 2.86
Episode length: 79.44 +/- 29.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=1.94 +/- 2.52
Episode length: 74.52 +/- 24.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 2.22     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 212      |
|    time_elapsed    | 3565     |
|    total_timesteps | 434176   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=434500, episode_reward=2.42 +/- 3.15
Episode length: 78.64 +/- 25.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.6        |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 434500      |
| train/                  |             |
|    approx_kl            | 0.012183505 |
|    clip_fraction        | 0.0772      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.0668      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.14        |
|    n_updates            | 676         |
|    policy_gradient_loss | 0.00274     |
|    value_loss           | 0.483       |
-----------------------------------------
Eval num_timesteps=435000, episode_reward=3.46 +/- 3.99
Episode length: 77.08 +/- 28.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 3.46     |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
New best mean reward!
Eval num_timesteps=435500, episode_reward=2.50 +/- 2.66
Episode length: 76.94 +/- 20.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=2.66 +/- 3.20
Episode length: 78.22 +/- 24.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 2.03     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 213      |
|    time_elapsed    | 3579     |
|    total_timesteps | 436224   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=436500, episode_reward=2.46 +/- 3.58
Episode length: 81.94 +/- 27.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81.9         |
|    mean_reward          | 2.46         |
| time/                   |              |
|    total_timesteps      | 436500       |
| train/                  |              |
|    approx_kl            | 0.0060646413 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0727       |
|    n_updates            | 678          |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 0.137        |
------------------------------------------
Eval num_timesteps=437000, episode_reward=2.10 +/- 3.38
Episode length: 78.42 +/- 26.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=2.58 +/- 3.05
Episode length: 81.30 +/- 32.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.3     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=2.48 +/- 3.48
Episode length: 80.24 +/- 29.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.2     |
|    ep_rew_mean     | 1.91     |
| time/              |          |
|    fps             | 121      |
|    iterations      | 214      |
|    time_elapsed    | 3593     |
|    total_timesteps | 438272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=438500, episode_reward=1.88 +/- 2.92
Episode length: 76.60 +/- 23.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.6         |
|    mean_reward          | 1.88         |
| time/                   |              |
|    total_timesteps      | 438500       |
| train/                  |              |
|    approx_kl            | 0.0027099545 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.083        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0486       |
|    n_updates            | 679          |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 0.287        |
------------------------------------------
Eval num_timesteps=439000, episode_reward=2.40 +/- 2.71
Episode length: 79.80 +/- 25.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.8     |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=2.08 +/- 2.87
Episode length: 68.52 +/- 25.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.5     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=2.82 +/- 3.34
Episode length: 84.88 +/- 30.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.9     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78       |
|    ep_rew_mean     | 1.59     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 215      |
|    time_elapsed    | 3607     |
|    total_timesteps | 440320   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=440500, episode_reward=2.46 +/- 3.15
Episode length: 78.82 +/- 25.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.8        |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 440500      |
| train/                  |             |
|    approx_kl            | 0.008684873 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.152       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0138      |
|    n_updates            | 684         |
|    policy_gradient_loss | -0.00998    |
|    value_loss           | 0.152       |
-----------------------------------------
Eval num_timesteps=441000, episode_reward=3.00 +/- 3.77
Episode length: 82.26 +/- 29.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=2.12 +/- 2.68
Episode length: 76.02 +/- 26.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=2.28 +/- 3.16
Episode length: 77.66 +/- 23.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.9     |
|    ep_rew_mean     | 1.61     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 216      |
|    time_elapsed    | 3622     |
|    total_timesteps | 442368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=442500, episode_reward=2.42 +/- 3.97
Episode length: 84.52 +/- 31.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.5         |
|    mean_reward          | 2.42         |
| time/                   |              |
|    total_timesteps      | 442500       |
| train/                  |              |
|    approx_kl            | 0.0070277997 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.0876       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0384       |
|    n_updates            | 685          |
|    policy_gradient_loss | -8.6e-06     |
|    value_loss           | 0.25         |
------------------------------------------
Eval num_timesteps=443000, episode_reward=1.70 +/- 2.54
Episode length: 78.08 +/- 25.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=2.12 +/- 2.52
Episode length: 78.26 +/- 29.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=2.02 +/- 2.48
Episode length: 72.24 +/- 19.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.2     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78       |
|    ep_rew_mean     | 1.7      |
| time/              |          |
|    fps             | 122      |
|    iterations      | 217      |
|    time_elapsed    | 3636     |
|    total_timesteps | 444416   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=444500, episode_reward=3.02 +/- 3.30
Episode length: 84.08 +/- 31.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.1        |
|    mean_reward          | 3.02        |
| time/                   |             |
|    total_timesteps      | 444500      |
| train/                  |             |
|    approx_kl            | 0.016839419 |
|    clip_fraction        | 0.0923      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.976      |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00129     |
|    n_updates            | 688         |
|    policy_gradient_loss | -0.0069     |
|    value_loss           | 0.182       |
-----------------------------------------
Eval num_timesteps=445000, episode_reward=2.16 +/- 3.94
Episode length: 71.82 +/- 25.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=3.54 +/- 4.68
Episode length: 78.64 +/- 29.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 3.54     |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
New best mean reward!
Eval num_timesteps=446000, episode_reward=2.30 +/- 3.13
Episode length: 75.04 +/- 24.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.1     |
|    ep_rew_mean     | 1.57     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 218      |
|    time_elapsed    | 3650     |
|    total_timesteps | 446464   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=446500, episode_reward=2.22 +/- 3.04
Episode length: 72.54 +/- 23.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72.5         |
|    mean_reward          | 2.22         |
| time/                   |              |
|    total_timesteps      | 446500       |
| train/                  |              |
|    approx_kl            | 0.0071657095 |
|    clip_fraction        | 0.0485       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.863       |
|    explained_variance   | 0.157        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.302        |
|    n_updates            | 691          |
|    policy_gradient_loss | -0.00318     |
|    value_loss           | 0.228        |
------------------------------------------
Eval num_timesteps=447000, episode_reward=1.62 +/- 2.70
Episode length: 73.74 +/- 23.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=2.02 +/- 2.99
Episode length: 76.34 +/- 25.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=1.98 +/- 3.18
Episode length: 69.70 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.7     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=1.78 +/- 2.44
Episode length: 77.98 +/- 21.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 1.86     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 219      |
|    time_elapsed    | 3667     |
|    total_timesteps | 448512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=449000, episode_reward=3.00 +/- 3.33
Episode length: 81.08 +/- 28.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.1        |
|    mean_reward          | 3           |
| time/                   |             |
|    total_timesteps      | 449000      |
| train/                  |             |
|    approx_kl            | 0.004770362 |
|    clip_fraction        | 0.061       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.851      |
|    explained_variance   | 0.098       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.224       |
|    n_updates            | 692         |
|    policy_gradient_loss | -0.000378   |
|    value_loss           | 0.411       |
-----------------------------------------
Eval num_timesteps=449500, episode_reward=1.28 +/- 2.11
Episode length: 67.54 +/- 23.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.5     |
|    mean_reward     | 1.28     |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=2.62 +/- 3.86
Episode length: 77.40 +/- 28.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=2.78 +/- 3.74
Episode length: 73.68 +/- 26.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.3     |
|    ep_rew_mean     | 2.33     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 220      |
|    time_elapsed    | 3680     |
|    total_timesteps | 450560   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=451000, episode_reward=1.82 +/- 2.80
Episode length: 72.48 +/- 26.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.5        |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 451000      |
| train/                  |             |
|    approx_kl            | 0.005942576 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.769      |
|    explained_variance   | 0.0921      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.226       |
|    n_updates            | 697         |
|    policy_gradient_loss | -0.00685    |
|    value_loss           | 0.469       |
-----------------------------------------
Eval num_timesteps=451500, episode_reward=2.32 +/- 2.69
Episode length: 81.24 +/- 26.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.2     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=3.06 +/- 4.28
Episode length: 77.00 +/- 27.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=1.82 +/- 2.74
Episode length: 72.84 +/- 24.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.7     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 221      |
|    time_elapsed    | 3695     |
|    total_timesteps | 452608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=453000, episode_reward=2.16 +/- 2.44
Episode length: 74.06 +/- 20.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 74.1       |
|    mean_reward          | 2.16       |
| time/                   |            |
|    total_timesteps      | 453000     |
| train/                  |            |
|    approx_kl            | 0.00410887 |
|    clip_fraction        | 0.05       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.667     |
|    explained_variance   | 0.0893     |
|    learning_rate        | 0.0001     |
|    loss                 | 0.104      |
|    n_updates            | 698        |
|    policy_gradient_loss | -0.00297   |
|    value_loss           | 0.23       |
----------------------------------------
Eval num_timesteps=453500, episode_reward=2.22 +/- 2.59
Episode length: 78.10 +/- 23.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=2.44 +/- 3.15
Episode length: 74.22 +/- 24.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.2     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=2.56 +/- 3.88
Episode length: 81.34 +/- 27.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.3     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.5     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 222      |
|    time_elapsed    | 3710     |
|    total_timesteps | 454656   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=455000, episode_reward=1.64 +/- 2.39
Episode length: 71.90 +/- 21.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 71.9         |
|    mean_reward          | 1.64         |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0047488413 |
|    clip_fraction        | 0.062        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.454       |
|    explained_variance   | 0.204        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.104        |
|    n_updates            | 702          |
|    policy_gradient_loss | -0.00562     |
|    value_loss           | 0.285        |
------------------------------------------
Eval num_timesteps=455500, episode_reward=2.12 +/- 3.40
Episode length: 75.00 +/- 22.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=2.10 +/- 3.03
Episode length: 78.10 +/- 24.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=1.82 +/- 2.47
Episode length: 73.70 +/- 21.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.2     |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 223      |
|    time_elapsed    | 3724     |
|    total_timesteps | 456704   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=457000, episode_reward=2.92 +/- 3.35
Episode length: 77.68 +/- 21.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.7        |
|    mean_reward          | 2.92        |
| time/                   |             |
|    total_timesteps      | 457000      |
| train/                  |             |
|    approx_kl            | 0.004885771 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.409      |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0927      |
|    n_updates            | 705         |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 0.311       |
-----------------------------------------
Eval num_timesteps=457500, episode_reward=1.98 +/- 2.92
Episode length: 75.32 +/- 30.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.3     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=2.96 +/- 3.46
Episode length: 81.40 +/- 25.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.4     |
|    mean_reward     | 2.96     |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=1.94 +/- 2.28
Episode length: 72.52 +/- 25.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.5     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.2     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 122      |
|    iterations      | 224      |
|    time_elapsed    | 3739     |
|    total_timesteps | 458752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=459000, episode_reward=1.88 +/- 2.34
Episode length: 75.38 +/- 27.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 75.4       |
|    mean_reward          | 1.88       |
| time/                   |            |
|    total_timesteps      | 459000     |
| train/                  |            |
|    approx_kl            | 0.00512821 |
|    clip_fraction        | 0.0559     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.472     |
|    explained_variance   | 0.104      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.31       |
|    n_updates            | 706        |
|    policy_gradient_loss | -0.000123  |
|    value_loss           | 0.464      |
----------------------------------------
Eval num_timesteps=459500, episode_reward=2.38 +/- 3.87
Episode length: 72.82 +/- 27.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=2.54 +/- 3.77
Episode length: 74.94 +/- 26.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | 2.54     |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=1.70 +/- 2.65
Episode length: 71.04 +/- 23.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71       |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80       |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 225      |
|    time_elapsed    | 3752     |
|    total_timesteps | 460800   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=461000, episode_reward=3.28 +/- 4.33
Episode length: 79.10 +/- 24.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79.1         |
|    mean_reward          | 3.28         |
| time/                   |              |
|    total_timesteps      | 461000       |
| train/                  |              |
|    approx_kl            | 0.0091375355 |
|    clip_fraction        | 0.0381       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.419       |
|    explained_variance   | 0.152        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0811       |
|    n_updates            | 714          |
|    policy_gradient_loss | -0.00723     |
|    value_loss           | 0.222        |
------------------------------------------
Eval num_timesteps=461500, episode_reward=2.32 +/- 2.71
Episode length: 74.58 +/- 23.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=2.36 +/- 3.08
Episode length: 75.22 +/- 25.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.2     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=2.62 +/- 3.26
Episode length: 72.20 +/- 22.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.2     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.8     |
|    ep_rew_mean     | 1.89     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 226      |
|    time_elapsed    | 3766     |
|    total_timesteps | 462848   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=463000, episode_reward=2.62 +/- 2.95
Episode length: 81.04 +/- 24.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81           |
|    mean_reward          | 2.62         |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 0.0049466854 |
|    clip_fraction        | 0.0563       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.495       |
|    explained_variance   | -0.072       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.139        |
|    n_updates            | 717          |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 0.201        |
------------------------------------------
Eval num_timesteps=463500, episode_reward=2.66 +/- 3.64
Episode length: 80.38 +/- 25.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.4     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=2.26 +/- 3.50
Episode length: 80.96 +/- 21.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81       |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=2.42 +/- 3.14
Episode length: 76.66 +/- 22.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.7     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78       |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 227      |
|    time_elapsed    | 3781     |
|    total_timesteps | 464896   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
Eval num_timesteps=465000, episode_reward=2.80 +/- 3.42
Episode length: 85.00 +/- 36.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85          |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.015202184 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.342      |
|    explained_variance   | 0.12        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.147       |
|    n_updates            | 727         |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 0.464       |
-----------------------------------------
Eval num_timesteps=465500, episode_reward=1.62 +/- 2.66
Episode length: 65.94 +/- 21.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.9     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=2.58 +/- 3.67
Episode length: 76.46 +/- 28.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.5     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=2.10 +/- 2.68
Episode length: 81.82 +/- 28.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.8     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.5     |
|    ep_rew_mean     | 2.12     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 228      |
|    time_elapsed    | 3796     |
|    total_timesteps | 466944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=467000, episode_reward=2.84 +/- 3.53
Episode length: 82.58 +/- 24.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 82.6         |
|    mean_reward          | 2.84         |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0030731813 |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.497       |
|    explained_variance   | 0.0879       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.215        |
|    n_updates            | 728          |
|    policy_gradient_loss | 0.000263     |
|    value_loss           | 0.306        |
------------------------------------------
Eval num_timesteps=467500, episode_reward=2.80 +/- 3.38
Episode length: 80.82 +/- 24.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.8     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=1.78 +/- 2.26
Episode length: 72.92 +/- 20.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=1.76 +/- 3.61
Episode length: 72.96 +/- 23.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73       |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.7     |
|    ep_rew_mean     | 2.08     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 229      |
|    time_elapsed    | 3810     |
|    total_timesteps | 468992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=469000, episode_reward=2.98 +/- 3.48
Episode length: 75.62 +/- 23.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.6        |
|    mean_reward          | 2.98        |
| time/                   |             |
|    total_timesteps      | 469000      |
| train/                  |             |
|    approx_kl            | 0.005081642 |
|    clip_fraction        | 0.0481      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.476      |
|    explained_variance   | 0.0299      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0973      |
|    n_updates            | 729         |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 0.243       |
-----------------------------------------
Eval num_timesteps=469500, episode_reward=2.64 +/- 3.11
Episode length: 79.94 +/- 28.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 2.64     |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=1.80 +/- 2.37
Episode length: 76.82 +/- 32.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=2.16 +/- 2.64
Episode length: 72.92 +/- 22.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=2.22 +/- 3.13
Episode length: 77.32 +/- 27.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.8     |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 230      |
|    time_elapsed    | 3827     |
|    total_timesteps | 471040   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=471500, episode_reward=2.96 +/- 4.28
Episode length: 79.36 +/- 29.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.4        |
|    mean_reward          | 2.96        |
| time/                   |             |
|    total_timesteps      | 471500      |
| train/                  |             |
|    approx_kl            | 0.008940338 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.392      |
|    explained_variance   | 0.0973      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.494       |
|    n_updates            | 733         |
|    policy_gradient_loss | -0.00315    |
|    value_loss           | 0.688       |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=2.04 +/- 2.82
Episode length: 71.60 +/- 21.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.6     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=2.44 +/- 3.40
Episode length: 77.52 +/- 31.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.5     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=2.56 +/- 2.56
Episode length: 78.76 +/- 24.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.8     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.8     |
|    ep_rew_mean     | 2.04     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 231      |
|    time_elapsed    | 3841     |
|    total_timesteps | 473088   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=473500, episode_reward=1.86 +/- 2.43
Episode length: 81.18 +/- 24.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.2        |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 473500      |
| train/                  |             |
|    approx_kl            | 0.005916651 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.291      |
|    explained_variance   | 0.129       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0822      |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00258    |
|    value_loss           | 0.158       |
-----------------------------------------
Eval num_timesteps=474000, episode_reward=3.06 +/- 4.18
Episode length: 76.60 +/- 24.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=2.52 +/- 3.23
Episode length: 84.52 +/- 23.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.5     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=1.68 +/- 2.53
Episode length: 69.96 +/- 26.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70       |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.6     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 123      |
|    iterations      | 232      |
|    time_elapsed    | 3855     |
|    total_timesteps | 475136   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=475500, episode_reward=3.06 +/- 4.16
Episode length: 83.46 +/- 27.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.5        |
|    mean_reward          | 3.06        |
| time/                   |             |
|    total_timesteps      | 475500      |
| train/                  |             |
|    approx_kl            | 0.014408982 |
|    clip_fraction        | 0.0483      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0.0858      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.117       |
|    n_updates            | 742         |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 0.466       |
-----------------------------------------
Eval num_timesteps=476000, episode_reward=2.02 +/- 3.54
Episode length: 78.46 +/- 24.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=2.42 +/- 2.74
Episode length: 81.04 +/- 21.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81       |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=2.36 +/- 3.20
Episode length: 74.86 +/- 23.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.2     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 233      |
|    time_elapsed    | 3870     |
|    total_timesteps | 477184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=477500, episode_reward=2.36 +/- 3.20
Episode length: 78.50 +/- 26.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.5         |
|    mean_reward          | 2.36         |
| time/                   |              |
|    total_timesteps      | 477500       |
| train/                  |              |
|    approx_kl            | 0.0051220586 |
|    clip_fraction        | 0.0488       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.516       |
|    explained_variance   | 0.11         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.327        |
|    n_updates            | 743          |
|    policy_gradient_loss | 0.00431      |
|    value_loss           | 0.344        |
------------------------------------------
Eval num_timesteps=478000, episode_reward=2.50 +/- 3.21
Episode length: 75.54 +/- 23.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.5     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=1.96 +/- 2.86
Episode length: 70.16 +/- 21.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=2.04 +/- 2.31
Episode length: 77.56 +/- 27.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.9     |
|    ep_rew_mean     | 2.09     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 234      |
|    time_elapsed    | 3884     |
|    total_timesteps | 479232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=479500, episode_reward=2.36 +/- 2.76
Episode length: 76.76 +/- 22.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.8        |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 479500      |
| train/                  |             |
|    approx_kl            | 0.005292823 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.329      |
|    explained_variance   | 0.122       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.188       |
|    n_updates            | 744         |
|    policy_gradient_loss | 0.00115     |
|    value_loss           | 0.212       |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=1.94 +/- 2.94
Episode length: 73.76 +/- 23.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.8     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=2.00 +/- 3.30
Episode length: 73.36 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.4     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=2.06 +/- 2.74
Episode length: 74.56 +/- 20.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.4     |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 235      |
|    time_elapsed    | 3898     |
|    total_timesteps | 481280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=481500, episode_reward=2.18 +/- 2.61
Episode length: 75.42 +/- 22.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.4         |
|    mean_reward          | 2.18         |
| time/                   |              |
|    total_timesteps      | 481500       |
| train/                  |              |
|    approx_kl            | 0.0043975515 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.268        |
|    n_updates            | 745          |
|    policy_gradient_loss | 0.0014       |
|    value_loss           | 0.222        |
------------------------------------------
Eval num_timesteps=482000, episode_reward=2.06 +/- 2.78
Episode length: 74.20 +/- 22.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.2     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=2.12 +/- 3.41
Episode length: 80.58 +/- 30.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.6     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=1.74 +/- 2.40
Episode length: 75.54 +/- 23.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.5     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.1     |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 236      |
|    time_elapsed    | 3912     |
|    total_timesteps | 483328   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=483500, episode_reward=2.62 +/- 4.21
Episode length: 73.38 +/- 21.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73.4         |
|    mean_reward          | 2.62         |
| time/                   |              |
|    total_timesteps      | 483500       |
| train/                  |              |
|    approx_kl            | 0.0053197746 |
|    clip_fraction        | 0.0416       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.487       |
|    explained_variance   | 0.125        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.186        |
|    n_updates            | 749          |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 0.502        |
------------------------------------------
Eval num_timesteps=484000, episode_reward=1.92 +/- 3.22
Episode length: 73.68 +/- 23.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=3.18 +/- 4.15
Episode length: 77.92 +/- 26.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=2.86 +/- 3.32
Episode length: 76.68 +/- 24.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.7     |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.2     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 237      |
|    time_elapsed    | 3926     |
|    total_timesteps | 485376   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=485500, episode_reward=2.68 +/- 3.61
Episode length: 83.44 +/- 30.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.4        |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 485500      |
| train/                  |             |
|    approx_kl            | 0.004275273 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.39       |
|    explained_variance   | 0.218       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0831      |
|    n_updates            | 754         |
|    policy_gradient_loss | -0.0033     |
|    value_loss           | 0.351       |
-----------------------------------------
Eval num_timesteps=486000, episode_reward=1.48 +/- 2.60
Episode length: 68.96 +/- 26.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69       |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=2.84 +/- 2.55
Episode length: 84.76 +/- 26.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.8     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=2.62 +/- 3.47
Episode length: 77.76 +/- 26.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 238      |
|    time_elapsed    | 3940     |
|    total_timesteps | 487424   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=487500, episode_reward=2.08 +/- 2.79
Episode length: 76.10 +/- 19.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.1        |
|    mean_reward          | 2.08        |
| time/                   |             |
|    total_timesteps      | 487500      |
| train/                  |             |
|    approx_kl            | 0.006053146 |
|    clip_fraction        | 0.0466      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.463      |
|    explained_variance   | 0.1         |
|    learning_rate        | 0.0001      |
|    loss                 | 0.541       |
|    n_updates            | 758         |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 0.463       |
-----------------------------------------
Eval num_timesteps=488000, episode_reward=2.12 +/- 2.86
Episode length: 76.22 +/- 25.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=2.12 +/- 2.80
Episode length: 75.90 +/- 26.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=2.88 +/- 3.04
Episode length: 75.22 +/- 24.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.2     |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.6     |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 239      |
|    time_elapsed    | 3954     |
|    total_timesteps | 489472   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=489500, episode_reward=1.68 +/- 2.48
Episode length: 73.36 +/- 25.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73.4         |
|    mean_reward          | 1.68         |
| time/                   |              |
|    total_timesteps      | 489500       |
| train/                  |              |
|    approx_kl            | 0.0062803114 |
|    clip_fraction        | 0.0499       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.443       |
|    explained_variance   | 0.108        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0735       |
|    n_updates            | 761          |
|    policy_gradient_loss | -0.00405     |
|    value_loss           | 0.191        |
------------------------------------------
Eval num_timesteps=490000, episode_reward=1.72 +/- 2.42
Episode length: 77.04 +/- 25.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=1.98 +/- 2.61
Episode length: 77.26 +/- 27.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=3.00 +/- 3.87
Episode length: 81.60 +/- 26.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.6     |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=2.00 +/- 2.55
Episode length: 73.64 +/- 25.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.6     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.6     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 240      |
|    time_elapsed    | 3971     |
|    total_timesteps | 491520   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=492000, episode_reward=2.58 +/- 3.44
Episode length: 70.42 +/- 20.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.4         |
|    mean_reward          | 2.58         |
| time/                   |              |
|    total_timesteps      | 492000       |
| train/                  |              |
|    approx_kl            | 0.0071986513 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.31        |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0562       |
|    n_updates            | 765          |
|    policy_gradient_loss | -0.000552    |
|    value_loss           | 0.248        |
------------------------------------------
Eval num_timesteps=492500, episode_reward=2.16 +/- 2.52
Episode length: 82.24 +/- 26.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.2     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=2.68 +/- 4.10
Episode length: 77.52 +/- 25.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.5     |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=2.26 +/- 3.00
Episode length: 75.40 +/- 23.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 2.14     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 241      |
|    time_elapsed    | 3985     |
|    total_timesteps | 493568   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=494000, episode_reward=2.32 +/- 3.17
Episode length: 76.26 +/- 21.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.3         |
|    mean_reward          | 2.32         |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0053039407 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.411       |
|    explained_variance   | 0.17         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0206       |
|    n_updates            | 768          |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 0.228        |
------------------------------------------
Eval num_timesteps=494500, episode_reward=1.90 +/- 4.11
Episode length: 74.18 +/- 22.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.2     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=3.06 +/- 4.20
Episode length: 81.30 +/- 27.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.3     |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=2.18 +/- 2.96
Episode length: 75.86 +/- 24.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.7     |
|    ep_rew_mean     | 1.89     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 242      |
|    time_elapsed    | 4000     |
|    total_timesteps | 495616   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=496000, episode_reward=2.76 +/- 4.00
Episode length: 77.58 +/- 25.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.6         |
|    mean_reward          | 2.76         |
| time/                   |              |
|    total_timesteps      | 496000       |
| train/                  |              |
|    approx_kl            | 0.0026838584 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.255       |
|    explained_variance   | 0.108        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0472       |
|    n_updates            | 771          |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 0.213        |
------------------------------------------
Eval num_timesteps=496500, episode_reward=2.80 +/- 3.26
Episode length: 77.00 +/- 26.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=2.74 +/- 3.59
Episode length: 77.88 +/- 28.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=2.10 +/- 2.89
Episode length: 76.68 +/- 23.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.7     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.2     |
|    ep_rew_mean     | 1.92     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 243      |
|    time_elapsed    | 4014     |
|    total_timesteps | 497664   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=498000, episode_reward=2.72 +/- 4.13
Episode length: 79.28 +/- 27.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.3        |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.008174582 |
|    clip_fraction        | 0.0403      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.0986      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.251       |
|    n_updates            | 773         |
|    policy_gradient_loss | 0.00123     |
|    value_loss           | 0.345       |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=2.28 +/- 2.91
Episode length: 74.56 +/- 23.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=1.68 +/- 2.13
Episode length: 75.08 +/- 24.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=3.16 +/- 3.11
Episode length: 78.34 +/- 24.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.7     |
|    ep_rew_mean     | 2.13     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 244      |
|    time_elapsed    | 4028     |
|    total_timesteps | 499712   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=500000, episode_reward=2.46 +/- 2.78
Episode length: 74.12 +/- 20.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 74.1       |
|    mean_reward          | 2.46       |
| time/                   |            |
|    total_timesteps      | 500000     |
| train/                  |            |
|    approx_kl            | 0.00449272 |
|    clip_fraction        | 0.0152     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.185     |
|    explained_variance   | 0.16       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.309      |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.00169   |
|    value_loss           | 0.321      |
----------------------------------------
Eval num_timesteps=500500, episode_reward=3.00 +/- 3.52
Episode length: 76.80 +/- 28.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=2.06 +/- 2.99
Episode length: 77.82 +/- 27.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=1.76 +/- 2.68
Episode length: 75.58 +/- 25.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.6     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78       |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 245      |
|    time_elapsed    | 4042     |
|    total_timesteps | 501760   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=502000, episode_reward=2.20 +/- 2.54
Episode length: 78.86 +/- 26.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.9         |
|    mean_reward          | 2.2          |
| time/                   |              |
|    total_timesteps      | 502000       |
| train/                  |              |
|    approx_kl            | 0.0049782805 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.203       |
|    explained_variance   | 0.051        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.138        |
|    n_updates            | 783          |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 0.489        |
------------------------------------------
Eval num_timesteps=502500, episode_reward=2.08 +/- 3.24
Episode length: 76.48 +/- 22.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.5     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
Eval num_timesteps=503000, episode_reward=2.58 +/- 3.38
Episode length: 83.88 +/- 27.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.9     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=503500, episode_reward=2.42 +/- 2.84
Episode length: 81.26 +/- 28.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.3     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.3     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 246      |
|    time_elapsed    | 4057     |
|    total_timesteps | 503808   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=504000, episode_reward=2.76 +/- 3.23
Episode length: 74.42 +/- 23.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.4        |
|    mean_reward          | 2.76        |
| time/                   |             |
|    total_timesteps      | 504000      |
| train/                  |             |
|    approx_kl            | 0.008891861 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.301      |
|    explained_variance   | 0.0842      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0989      |
|    n_updates            | 791         |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 0.292       |
-----------------------------------------
Eval num_timesteps=504500, episode_reward=2.66 +/- 4.14
Episode length: 73.46 +/- 23.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
Eval num_timesteps=505000, episode_reward=2.00 +/- 3.29
Episode length: 74.90 +/- 23.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=505500, episode_reward=1.76 +/- 2.79
Episode length: 73.74 +/- 21.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75       |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 124      |
|    iterations      | 247      |
|    time_elapsed    | 4071     |
|    total_timesteps | 505856   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=506000, episode_reward=2.48 +/- 3.44
Episode length: 84.22 +/- 29.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.2        |
|    mean_reward          | 2.48        |
| time/                   |             |
|    total_timesteps      | 506000      |
| train/                  |             |
|    approx_kl            | 0.009506686 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.0411      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.208       |
|    n_updates            | 793         |
|    policy_gradient_loss | -0.000129   |
|    value_loss           | 0.316       |
-----------------------------------------
Eval num_timesteps=506500, episode_reward=2.92 +/- 2.90
Episode length: 79.36 +/- 25.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 2.92     |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
Eval num_timesteps=507000, episode_reward=2.26 +/- 2.58
Episode length: 79.26 +/- 21.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.3     |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=507500, episode_reward=1.58 +/- 2.11
Episode length: 68.78 +/- 17.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.8     |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.5     |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 248      |
|    time_elapsed    | 4085     |
|    total_timesteps | 507904   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=508000, episode_reward=2.62 +/- 3.08
Episode length: 72.38 +/- 22.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.4        |
|    mean_reward          | 2.62        |
| time/                   |             |
|    total_timesteps      | 508000      |
| train/                  |             |
|    approx_kl            | 0.008715014 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.384      |
|    explained_variance   | 0.0551      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.103       |
|    n_updates            | 796         |
|    policy_gradient_loss | -0.00265    |
|    value_loss           | 0.27        |
-----------------------------------------
Eval num_timesteps=508500, episode_reward=1.94 +/- 2.89
Episode length: 74.92 +/- 23.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
Eval num_timesteps=509000, episode_reward=2.04 +/- 2.99
Episode length: 73.14 +/- 20.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=509500, episode_reward=2.74 +/- 3.32
Episode length: 77.74 +/- 24.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.6     |
|    ep_rew_mean     | 1.98     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 249      |
|    time_elapsed    | 4099     |
|    total_timesteps | 509952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=510000, episode_reward=2.70 +/- 4.31
Episode length: 77.10 +/- 25.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.1        |
|    mean_reward          | 2.7         |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.004436001 |
|    clip_fraction        | 0.0359      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.275      |
|    explained_variance   | 0.0578      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0499      |
|    n_updates            | 797         |
|    policy_gradient_loss | -0.00175    |
|    value_loss           | 0.282       |
-----------------------------------------
Eval num_timesteps=510500, episode_reward=2.24 +/- 2.58
Episode length: 74.08 +/- 25.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.1     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
Eval num_timesteps=511000, episode_reward=2.44 +/- 2.92
Episode length: 73.10 +/- 22.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=511500, episode_reward=2.76 +/- 3.76
Episode length: 83.54 +/- 24.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.5     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=3.54 +/- 3.70
Episode length: 86.32 +/- 25.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.3     |
|    mean_reward     | 3.54     |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.6     |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 250      |
|    time_elapsed    | 4116     |
|    total_timesteps | 512000   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=512500, episode_reward=2.72 +/- 2.78
Episode length: 77.92 +/- 22.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 77.9       |
|    mean_reward          | 2.72       |
| time/                   |            |
|    total_timesteps      | 512500     |
| train/                  |            |
|    approx_kl            | 0.00514915 |
|    clip_fraction        | 0.0296     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.285     |
|    explained_variance   | 0.0289     |
|    learning_rate        | 0.0001     |
|    loss                 | 0.152      |
|    n_updates            | 802        |
|    policy_gradient_loss | -0.00283   |
|    value_loss           | 0.403      |
----------------------------------------
Eval num_timesteps=513000, episode_reward=2.00 +/- 3.27
Episode length: 73.32 +/- 26.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.3     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
Eval num_timesteps=513500, episode_reward=2.74 +/- 3.67
Episode length: 80.36 +/- 31.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.4     |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 513500   |
---------------------------------
Eval num_timesteps=514000, episode_reward=2.66 +/- 3.79
Episode length: 80.50 +/- 28.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.5     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77       |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 251      |
|    time_elapsed    | 4131     |
|    total_timesteps | 514048   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=514500, episode_reward=1.46 +/- 2.01
Episode length: 77.96 +/- 23.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78          |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 514500      |
| train/                  |             |
|    approx_kl            | 0.008168946 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.211      |
|    explained_variance   | 0.0493      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.333       |
|    n_updates            | 805         |
|    policy_gradient_loss | 0.000657    |
|    value_loss           | 0.623       |
-----------------------------------------
Eval num_timesteps=515000, episode_reward=2.04 +/- 2.99
Episode length: 73.38 +/- 25.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.4     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
Eval num_timesteps=515500, episode_reward=2.24 +/- 3.73
Episode length: 73.82 +/- 27.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.8     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 515500   |
---------------------------------
Eval num_timesteps=516000, episode_reward=2.52 +/- 3.48
Episode length: 78.40 +/- 23.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.7     |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 252      |
|    time_elapsed    | 4145     |
|    total_timesteps | 516096   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=516500, episode_reward=1.62 +/- 2.14
Episode length: 72.98 +/- 18.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73          |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 516500      |
| train/                  |             |
|    approx_kl            | 0.004088194 |
|    clip_fraction        | 0.00951     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.113       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.13        |
|    n_updates            | 808         |
|    policy_gradient_loss | -0.000662   |
|    value_loss           | 0.309       |
-----------------------------------------
Eval num_timesteps=517000, episode_reward=2.88 +/- 4.11
Episode length: 81.88 +/- 29.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.9     |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
Eval num_timesteps=517500, episode_reward=2.08 +/- 2.43
Episode length: 77.92 +/- 25.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 517500   |
---------------------------------
Eval num_timesteps=518000, episode_reward=2.14 +/- 2.68
Episode length: 74.02 +/- 30.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74       |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.6     |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 253      |
|    time_elapsed    | 4159     |
|    total_timesteps | 518144   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=518500, episode_reward=2.56 +/- 3.67
Episode length: 76.54 +/- 26.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.5         |
|    mean_reward          | 2.56         |
| time/                   |              |
|    total_timesteps      | 518500       |
| train/                  |              |
|    approx_kl            | 0.0034068348 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.135       |
|    explained_variance   | 0.0751       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0207       |
|    n_updates            | 812          |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 0.429        |
------------------------------------------
Eval num_timesteps=519000, episode_reward=2.14 +/- 2.58
Episode length: 78.02 +/- 20.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
Eval num_timesteps=519500, episode_reward=2.48 +/- 3.57
Episode length: 79.24 +/- 27.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.2     |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 519500   |
---------------------------------
Eval num_timesteps=520000, episode_reward=2.60 +/- 3.73
Episode length: 76.02 +/- 27.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.4     |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 254      |
|    time_elapsed    | 4173     |
|    total_timesteps | 520192   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=520500, episode_reward=2.28 +/- 3.24
Episode length: 74.36 +/- 21.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 74.4         |
|    mean_reward          | 2.28         |
| time/                   |              |
|    total_timesteps      | 520500       |
| train/                  |              |
|    approx_kl            | 0.0017208135 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.105       |
|    explained_variance   | 0.193        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.134        |
|    n_updates            | 819          |
|    policy_gradient_loss | -0.000973    |
|    value_loss           | 0.404        |
------------------------------------------
Eval num_timesteps=521000, episode_reward=2.34 +/- 3.04
Episode length: 71.70 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.7     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
Eval num_timesteps=521500, episode_reward=1.78 +/- 2.59
Episode length: 78.50 +/- 24.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 521500   |
---------------------------------
Eval num_timesteps=522000, episode_reward=2.18 +/- 2.56
Episode length: 75.70 +/- 26.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 124      |
|    iterations      | 255      |
|    time_elapsed    | 4187     |
|    total_timesteps | 522240   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=522500, episode_reward=2.90 +/- 3.80
Episode length: 77.68 +/- 26.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.7         |
|    mean_reward          | 2.9          |
| time/                   |              |
|    total_timesteps      | 522500       |
| train/                  |              |
|    approx_kl            | 0.0043071215 |
|    clip_fraction        | 0.0075       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.0469       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.182        |
|    n_updates            | 821          |
|    policy_gradient_loss | 0.000883     |
|    value_loss           | 0.295        |
------------------------------------------
Eval num_timesteps=523000, episode_reward=2.02 +/- 2.52
Episode length: 77.28 +/- 25.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
Eval num_timesteps=523500, episode_reward=2.80 +/- 3.70
Episode length: 79.68 +/- 23.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 523500   |
---------------------------------
Eval num_timesteps=524000, episode_reward=2.20 +/- 3.16
Episode length: 75.38 +/- 24.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.7     |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 256      |
|    time_elapsed    | 4202     |
|    total_timesteps | 524288   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
Eval num_timesteps=524500, episode_reward=2.48 +/- 2.82
Episode length: 76.28 +/- 22.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.3         |
|    mean_reward          | 2.48         |
| time/                   |              |
|    total_timesteps      | 524500       |
| train/                  |              |
|    approx_kl            | 0.0045904336 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.231       |
|    explained_variance   | 0.12         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0713       |
|    n_updates            | 825          |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 0.319        |
------------------------------------------
Eval num_timesteps=525000, episode_reward=2.38 +/- 3.34
Episode length: 76.16 +/- 26.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
Eval num_timesteps=525500, episode_reward=2.38 +/- 2.91
Episode length: 85.40 +/- 25.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.4     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 525500   |
---------------------------------
Eval num_timesteps=526000, episode_reward=2.60 +/- 3.27
Episode length: 82.28 +/- 26.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.8     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 257      |
|    time_elapsed    | 4217     |
|    total_timesteps | 526336   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=526500, episode_reward=2.30 +/- 3.11
Episode length: 72.22 +/- 20.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.2        |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 526500      |
| train/                  |             |
|    approx_kl            | 0.003541723 |
|    clip_fraction        | 0.0205      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.148       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.145       |
|    n_updates            | 828         |
|    policy_gradient_loss | -0.000525   |
|    value_loss           | 0.377       |
-----------------------------------------
Eval num_timesteps=527000, episode_reward=2.56 +/- 3.09
Episode length: 75.10 +/- 23.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
Eval num_timesteps=527500, episode_reward=3.12 +/- 4.63
Episode length: 78.78 +/- 26.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.8     |
|    mean_reward     | 3.12     |
| time/              |          |
|    total_timesteps | 527500   |
---------------------------------
Eval num_timesteps=528000, episode_reward=2.22 +/- 2.65
Episode length: 79.14 +/- 25.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.1     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.8     |
|    ep_rew_mean     | 2.11     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 258      |
|    time_elapsed    | 4231     |
|    total_timesteps | 528384   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=528500, episode_reward=1.92 +/- 2.12
Episode length: 70.44 +/- 22.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.4         |
|    mean_reward          | 1.92         |
| time/                   |              |
|    total_timesteps      | 528500       |
| train/                  |              |
|    approx_kl            | 0.0017849273 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.135       |
|    explained_variance   | 0.143        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0664       |
|    n_updates            | 832          |
|    policy_gradient_loss | -0.000955    |
|    value_loss           | 0.182        |
------------------------------------------
Eval num_timesteps=529000, episode_reward=2.06 +/- 3.01
Episode length: 75.12 +/- 24.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
Eval num_timesteps=529500, episode_reward=1.98 +/- 2.69
Episode length: 79.14 +/- 24.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.1     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 529500   |
---------------------------------
Eval num_timesteps=530000, episode_reward=2.06 +/- 2.73
Episode length: 73.82 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.8     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.5     |
|    ep_rew_mean     | 2.26     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 259      |
|    time_elapsed    | 4245     |
|    total_timesteps | 530432   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=530500, episode_reward=1.78 +/- 2.57
Episode length: 74.52 +/- 26.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 74.5       |
|    mean_reward          | 1.78       |
| time/                   |            |
|    total_timesteps      | 530500     |
| train/                  |            |
|    approx_kl            | 0.02077866 |
|    clip_fraction        | 0.021      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.193     |
|    explained_variance   | 0.144      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.177      |
|    n_updates            | 837        |
|    policy_gradient_loss | -0.00182   |
|    value_loss           | 0.394      |
----------------------------------------
Eval num_timesteps=531000, episode_reward=2.66 +/- 3.59
Episode length: 75.14 +/- 19.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
Eval num_timesteps=531500, episode_reward=2.78 +/- 3.60
Episode length: 80.24 +/- 30.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 531500   |
---------------------------------
Eval num_timesteps=532000, episode_reward=2.58 +/- 2.83
Episode length: 77.72 +/- 23.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.1     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 260      |
|    time_elapsed    | 4260     |
|    total_timesteps | 532480   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=532500, episode_reward=2.78 +/- 3.12
Episode length: 75.48 +/- 23.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.5         |
|    mean_reward          | 2.78         |
| time/                   |              |
|    total_timesteps      | 532500       |
| train/                  |              |
|    approx_kl            | 0.0027091692 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.142       |
|    explained_variance   | 0.17         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.255        |
|    n_updates            | 845          |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 0.354        |
------------------------------------------
Eval num_timesteps=533000, episode_reward=2.38 +/- 3.63
Episode length: 71.96 +/- 25.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72       |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=2.44 +/- 3.00
Episode length: 74.58 +/- 23.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
Eval num_timesteps=534000, episode_reward=2.70 +/- 3.09
Episode length: 73.08 +/- 26.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=534500, episode_reward=2.32 +/- 3.00
Episode length: 77.38 +/- 28.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.5     |
|    ep_rew_mean     | 2.33     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 261      |
|    time_elapsed    | 4277     |
|    total_timesteps | 534528   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=535000, episode_reward=1.48 +/- 2.74
Episode length: 70.04 +/- 24.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70           |
|    mean_reward          | 1.48         |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 0.0048012743 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0756       |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00203     |
|    value_loss           | 0.232        |
------------------------------------------
Eval num_timesteps=535500, episode_reward=2.60 +/- 3.19
Episode length: 79.54 +/- 26.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
Eval num_timesteps=536000, episode_reward=2.64 +/- 2.96
Episode length: 76.26 +/- 23.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | 2.64     |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=536500, episode_reward=3.16 +/- 3.58
Episode length: 84.62 +/- 27.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.6     |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.2     |
|    ep_rew_mean     | 2.22     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 262      |
|    time_elapsed    | 4291     |
|    total_timesteps | 536576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=537000, episode_reward=2.70 +/- 3.92
Episode length: 72.92 +/- 20.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72.9         |
|    mean_reward          | 2.7          |
| time/                   |              |
|    total_timesteps      | 537000       |
| train/                  |              |
|    approx_kl            | 0.0038809082 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.214       |
|    explained_variance   | 0.0549       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.191        |
|    n_updates            | 851          |
|    policy_gradient_loss | 0.000736     |
|    value_loss           | 0.23         |
------------------------------------------
Eval num_timesteps=537500, episode_reward=2.02 +/- 3.59
Episode length: 70.44 +/- 27.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.4     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
Eval num_timesteps=538000, episode_reward=2.30 +/- 3.25
Episode length: 77.96 +/- 23.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=538500, episode_reward=2.32 +/- 3.41
Episode length: 74.76 +/- 26.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.8     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.4     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 263      |
|    time_elapsed    | 4305     |
|    total_timesteps | 538624   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.03
Eval num_timesteps=539000, episode_reward=2.94 +/- 3.55
Episode length: 85.18 +/- 28.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85.2        |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 539000      |
| train/                  |             |
|    approx_kl            | 0.004456925 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 0.0562      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.174       |
|    n_updates            | 859         |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 0.462       |
-----------------------------------------
Eval num_timesteps=539500, episode_reward=1.92 +/- 2.52
Episode length: 73.52 +/- 23.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
Eval num_timesteps=540000, episode_reward=2.04 +/- 2.38
Episode length: 75.92 +/- 24.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=540500, episode_reward=2.32 +/- 3.59
Episode length: 74.48 +/- 23.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.7     |
|    ep_rew_mean     | 2.16     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 264      |
|    time_elapsed    | 4319     |
|    total_timesteps | 540672   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=541000, episode_reward=1.82 +/- 2.46
Episode length: 73.36 +/- 25.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.4        |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 541000      |
| train/                  |             |
|    approx_kl            | 0.006131303 |
|    clip_fraction        | 0.0316      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.284      |
|    explained_variance   | 0.0955      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.104       |
|    n_updates            | 867         |
|    policy_gradient_loss | -0.00384    |
|    value_loss           | 0.229       |
-----------------------------------------
Eval num_timesteps=541500, episode_reward=2.46 +/- 2.91
Episode length: 85.80 +/- 28.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.8     |
|    mean_reward     | 2.46     |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
Eval num_timesteps=542000, episode_reward=3.06 +/- 3.93
Episode length: 79.96 +/- 27.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=542500, episode_reward=2.28 +/- 3.25
Episode length: 74.98 +/- 25.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 2.1      |
| time/              |          |
|    fps             | 125      |
|    iterations      | 265      |
|    time_elapsed    | 4334     |
|    total_timesteps | 542720   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=543000, episode_reward=2.60 +/- 3.29
Episode length: 75.94 +/- 23.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.9        |
|    mean_reward          | 2.6         |
| time/                   |             |
|    total_timesteps      | 543000      |
| train/                  |             |
|    approx_kl            | 0.010498062 |
|    clip_fraction        | 0.0256      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.29       |
|    explained_variance   | 0.195       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.123       |
|    n_updates            | 870         |
|    policy_gradient_loss | 0.000563    |
|    value_loss           | 0.233       |
-----------------------------------------
Eval num_timesteps=543500, episode_reward=2.44 +/- 3.96
Episode length: 78.88 +/- 28.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.9     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
Eval num_timesteps=544000, episode_reward=2.94 +/- 4.48
Episode length: 77.36 +/- 24.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=544500, episode_reward=1.92 +/- 2.90
Episode length: 69.72 +/- 25.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.7     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 2.06     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 266      |
|    time_elapsed    | 4348     |
|    total_timesteps | 544768   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=545000, episode_reward=3.22 +/- 4.09
Episode length: 86.10 +/- 30.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.1        |
|    mean_reward          | 3.22        |
| time/                   |             |
|    total_timesteps      | 545000      |
| train/                  |             |
|    approx_kl            | 0.004748461 |
|    clip_fraction        | 0.0436      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | -0.0571     |
|    learning_rate        | 0.0001      |
|    loss                 | 0.13        |
|    n_updates            | 872         |
|    policy_gradient_loss | -0.000132   |
|    value_loss           | 0.303       |
-----------------------------------------
Eval num_timesteps=545500, episode_reward=1.76 +/- 2.53
Episode length: 70.84 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.8     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
Eval num_timesteps=546000, episode_reward=2.76 +/- 3.50
Episode length: 76.76 +/- 25.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=546500, episode_reward=2.98 +/- 3.44
Episode length: 79.14 +/- 25.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.1     |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.2     |
|    ep_rew_mean     | 1.71     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 267      |
|    time_elapsed    | 4362     |
|    total_timesteps | 546816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=547000, episode_reward=1.46 +/- 2.17
Episode length: 75.48 +/- 26.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.5         |
|    mean_reward          | 1.46         |
| time/                   |              |
|    total_timesteps      | 547000       |
| train/                  |              |
|    approx_kl            | 0.0044162325 |
|    clip_fraction        | 0.0425       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.509       |
|    explained_variance   | 0.11         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0488       |
|    n_updates            | 873          |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 0.176        |
------------------------------------------
Eval num_timesteps=547500, episode_reward=1.68 +/- 2.41
Episode length: 76.60 +/- 24.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
Eval num_timesteps=548000, episode_reward=2.28 +/- 3.79
Episode length: 81.24 +/- 30.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.2     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=548500, episode_reward=2.18 +/- 3.73
Episode length: 70.14 +/- 21.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.1     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 1.85     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 268      |
|    time_elapsed    | 4376     |
|    total_timesteps | 548864   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=549000, episode_reward=2.70 +/- 2.61
Episode length: 82.18 +/- 25.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 82.2         |
|    mean_reward          | 2.7          |
| time/                   |              |
|    total_timesteps      | 549000       |
| train/                  |              |
|    approx_kl            | 0.0053861393 |
|    clip_fraction        | 0.0463       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.445       |
|    explained_variance   | -0.0285      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.383        |
|    n_updates            | 876          |
|    policy_gradient_loss | -0.00177     |
|    value_loss           | 0.449        |
------------------------------------------
Eval num_timesteps=549500, episode_reward=1.30 +/- 2.04
Episode length: 70.82 +/- 26.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.8     |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=2.42 +/- 3.31
Episode length: 73.00 +/- 24.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73       |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=550500, episode_reward=3.10 +/- 4.22
Episode length: 79.22 +/- 22.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.2     |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.8     |
|    ep_rew_mean     | 2        |
| time/              |          |
|    fps             | 125      |
|    iterations      | 269      |
|    time_elapsed    | 4390     |
|    total_timesteps | 550912   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=551000, episode_reward=2.62 +/- 3.62
Episode length: 76.54 +/- 23.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.5        |
|    mean_reward          | 2.62        |
| time/                   |             |
|    total_timesteps      | 551000      |
| train/                  |             |
|    approx_kl            | 0.006365799 |
|    clip_fraction        | 0.0481      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.502      |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.143       |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00377    |
|    value_loss           | 0.42        |
-----------------------------------------
Eval num_timesteps=551500, episode_reward=1.90 +/- 2.32
Episode length: 73.22 +/- 23.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.2     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
Eval num_timesteps=552000, episode_reward=2.52 +/- 3.34
Episode length: 80.98 +/- 30.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81       |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=552500, episode_reward=2.34 +/- 3.30
Episode length: 76.96 +/- 25.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 2.14     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 270      |
|    time_elapsed    | 4404     |
|    total_timesteps | 552960   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=553000, episode_reward=3.28 +/- 4.10
Episode length: 79.78 +/- 31.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79.8         |
|    mean_reward          | 3.28         |
| time/                   |              |
|    total_timesteps      | 553000       |
| train/                  |              |
|    approx_kl            | 0.0061198147 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.702       |
|    explained_variance   | 0.131        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.261        |
|    n_updates            | 883          |
|    policy_gradient_loss | -0.00347     |
|    value_loss           | 0.378        |
------------------------------------------
Eval num_timesteps=553500, episode_reward=2.98 +/- 3.64
Episode length: 78.98 +/- 26.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79       |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
Eval num_timesteps=554000, episode_reward=2.42 +/- 3.21
Episode length: 73.90 +/- 25.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=554500, episode_reward=2.36 +/- 3.46
Episode length: 75.26 +/- 24.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.3     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=3.42 +/- 3.84
Episode length: 83.26 +/- 25.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.3     |
|    mean_reward     | 3.42     |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 271      |
|    time_elapsed    | 4422     |
|    total_timesteps | 555008   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=555500, episode_reward=2.06 +/- 2.56
Episode length: 73.40 +/- 22.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73.4         |
|    mean_reward          | 2.06         |
| time/                   |              |
|    total_timesteps      | 555500       |
| train/                  |              |
|    approx_kl            | 0.0038692013 |
|    clip_fraction        | 0.0476       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.533       |
|    explained_variance   | 0.0765       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.174        |
|    n_updates            | 884          |
|    policy_gradient_loss | -0.00077     |
|    value_loss           | 0.475        |
------------------------------------------
Eval num_timesteps=556000, episode_reward=3.58 +/- 4.90
Episode length: 85.44 +/- 35.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.4     |
|    mean_reward     | 3.58     |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
New best mean reward!
Eval num_timesteps=556500, episode_reward=3.42 +/- 3.94
Episode length: 78.24 +/- 23.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 3.42     |
| time/              |          |
|    total_timesteps | 556500   |
---------------------------------
Eval num_timesteps=557000, episode_reward=2.14 +/- 2.44
Episode length: 76.12 +/- 20.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.1     |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.4     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 272      |
|    time_elapsed    | 4436     |
|    total_timesteps | 557056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=557500, episode_reward=2.46 +/- 2.79
Episode length: 75.92 +/- 22.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.9        |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 557500      |
| train/                  |             |
|    approx_kl            | 0.005273099 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.421      |
|    explained_variance   | 0.181       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.206       |
|    n_updates            | 885         |
|    policy_gradient_loss | 0.00121     |
|    value_loss           | 0.402       |
-----------------------------------------
Eval num_timesteps=558000, episode_reward=2.88 +/- 3.73
Episode length: 80.62 +/- 27.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.6     |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=558500, episode_reward=3.22 +/- 4.08
Episode length: 79.98 +/- 23.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 3.22     |
| time/              |          |
|    total_timesteps | 558500   |
---------------------------------
Eval num_timesteps=559000, episode_reward=2.02 +/- 2.60
Episode length: 76.68 +/- 21.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.7     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.7     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 273      |
|    time_elapsed    | 4451     |
|    total_timesteps | 559104   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=559500, episode_reward=2.34 +/- 2.90
Episode length: 72.98 +/- 27.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73          |
|    mean_reward          | 2.34        |
| time/                   |             |
|    total_timesteps      | 559500      |
| train/                  |             |
|    approx_kl            | 0.005364416 |
|    clip_fraction        | 0.038       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.352      |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.138       |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00396    |
|    value_loss           | 0.345       |
-----------------------------------------
Eval num_timesteps=560000, episode_reward=1.54 +/- 1.91
Episode length: 73.08 +/- 24.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=560500, episode_reward=2.80 +/- 3.03
Episode length: 73.92 +/- 24.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 560500   |
---------------------------------
Eval num_timesteps=561000, episode_reward=2.86 +/- 3.49
Episode length: 82.02 +/- 23.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82       |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.1     |
|    ep_rew_mean     | 2.2      |
| time/              |          |
|    fps             | 125      |
|    iterations      | 274      |
|    time_elapsed    | 4465     |
|    total_timesteps | 561152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=561500, episode_reward=1.96 +/- 3.68
Episode length: 68.76 +/- 21.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 68.8         |
|    mean_reward          | 1.96         |
| time/                   |              |
|    total_timesteps      | 561500       |
| train/                  |              |
|    approx_kl            | 0.0030143345 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.435       |
|    explained_variance   | 0.0782       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.297        |
|    n_updates            | 891          |
|    policy_gradient_loss | 0.00394      |
|    value_loss           | 0.192        |
------------------------------------------
Eval num_timesteps=562000, episode_reward=2.24 +/- 2.72
Episode length: 75.78 +/- 22.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.8     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
Eval num_timesteps=562500, episode_reward=2.30 +/- 2.83
Episode length: 75.70 +/- 22.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 562500   |
---------------------------------
Eval num_timesteps=563000, episode_reward=3.22 +/- 3.57
Episode length: 76.86 +/- 19.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 3.22     |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 2.11     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 275      |
|    time_elapsed    | 4479     |
|    total_timesteps | 563200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=563500, episode_reward=2.22 +/- 3.46
Episode length: 78.66 +/- 27.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.7        |
|    mean_reward          | 2.22        |
| time/                   |             |
|    total_timesteps      | 563500      |
| train/                  |             |
|    approx_kl            | 0.005641951 |
|    clip_fraction        | 0.066       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.0854      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.126       |
|    n_updates            | 892         |
|    policy_gradient_loss | 0.00133     |
|    value_loss           | 0.196       |
-----------------------------------------
Eval num_timesteps=564000, episode_reward=2.30 +/- 3.11
Episode length: 76.82 +/- 20.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=564500, episode_reward=3.36 +/- 3.91
Episode length: 77.64 +/- 29.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 3.36     |
| time/              |          |
|    total_timesteps | 564500   |
---------------------------------
Eval num_timesteps=565000, episode_reward=2.34 +/- 3.04
Episode length: 78.12 +/- 30.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 1.82     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 276      |
|    time_elapsed    | 4493     |
|    total_timesteps | 565248   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=565500, episode_reward=2.46 +/- 2.81
Episode length: 78.16 +/- 25.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.2        |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 565500      |
| train/                  |             |
|    approx_kl            | 0.011389496 |
|    clip_fraction        | 0.0616      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.496      |
|    explained_variance   | 0.0259      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0913      |
|    n_updates            | 895         |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 0.234       |
-----------------------------------------
Eval num_timesteps=566000, episode_reward=1.74 +/- 2.22
Episode length: 77.26 +/- 26.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
Eval num_timesteps=566500, episode_reward=2.76 +/- 3.34
Episode length: 82.36 +/- 29.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 566500   |
---------------------------------
Eval num_timesteps=567000, episode_reward=2.22 +/- 2.87
Episode length: 77.76 +/- 28.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.9     |
|    ep_rew_mean     | 1.66     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 277      |
|    time_elapsed    | 4508     |
|    total_timesteps | 567296   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=567500, episode_reward=2.44 +/- 2.99
Episode length: 77.34 +/- 21.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.3        |
|    mean_reward          | 2.44        |
| time/                   |             |
|    total_timesteps      | 567500      |
| train/                  |             |
|    approx_kl            | 0.008402214 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.469      |
|    explained_variance   | 0.0694      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0367      |
|    n_updates            | 897         |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 0.236       |
-----------------------------------------
Eval num_timesteps=568000, episode_reward=2.58 +/- 3.10
Episode length: 81.64 +/- 27.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.6     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=568500, episode_reward=1.86 +/- 3.52
Episode length: 70.92 +/- 21.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.9     |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 568500   |
---------------------------------
Eval num_timesteps=569000, episode_reward=2.76 +/- 4.03
Episode length: 71.30 +/- 22.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.3     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.5     |
|    ep_rew_mean     | 1.99     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 278      |
|    time_elapsed    | 4521     |
|    total_timesteps | 569344   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=569500, episode_reward=2.34 +/- 3.25
Episode length: 75.84 +/- 24.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 75.8       |
|    mean_reward          | 2.34       |
| time/                   |            |
|    total_timesteps      | 569500     |
| train/                  |            |
|    approx_kl            | 0.01259726 |
|    clip_fraction        | 0.0478     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.38      |
|    explained_variance   | 0.154      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.471      |
|    n_updates            | 899        |
|    policy_gradient_loss | -0.00222   |
|    value_loss           | 0.621      |
----------------------------------------
Eval num_timesteps=570000, episode_reward=3.14 +/- 3.96
Episode length: 80.04 +/- 22.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 3.14     |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
Eval num_timesteps=570500, episode_reward=1.78 +/- 2.19
Episode length: 77.82 +/- 23.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 570500   |
---------------------------------
Eval num_timesteps=571000, episode_reward=1.80 +/- 3.64
Episode length: 77.94 +/- 28.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 1.86     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 279      |
|    time_elapsed    | 4536     |
|    total_timesteps | 571392   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=571500, episode_reward=2.34 +/- 2.93
Episode length: 82.52 +/- 22.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.5        |
|    mean_reward          | 2.34        |
| time/                   |             |
|    total_timesteps      | 571500      |
| train/                  |             |
|    approx_kl            | 0.005061257 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.337      |
|    explained_variance   | 0.206       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.124       |
|    n_updates            | 902         |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 0.215       |
-----------------------------------------
Eval num_timesteps=572000, episode_reward=1.88 +/- 2.41
Episode length: 65.22 +/- 22.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.2     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=572500, episode_reward=2.74 +/- 2.99
Episode length: 75.98 +/- 19.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 572500   |
---------------------------------
Eval num_timesteps=573000, episode_reward=2.12 +/- 3.12
Episode length: 74.74 +/- 23.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.5     |
|    ep_rew_mean     | 2.14     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 280      |
|    time_elapsed    | 4550     |
|    total_timesteps | 573440   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=573500, episode_reward=2.54 +/- 3.96
Episode length: 79.70 +/- 28.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79.7         |
|    mean_reward          | 2.54         |
| time/                   |              |
|    total_timesteps      | 573500       |
| train/                  |              |
|    approx_kl            | 0.0047917133 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.423       |
|    explained_variance   | 0.0658       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.351        |
|    n_updates            | 906          |
|    policy_gradient_loss | -0.00319     |
|    value_loss           | 0.424        |
------------------------------------------
Eval num_timesteps=574000, episode_reward=1.84 +/- 2.64
Episode length: 73.72 +/- 25.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
Eval num_timesteps=574500, episode_reward=2.52 +/- 3.18
Episode length: 77.08 +/- 25.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 574500   |
---------------------------------
Eval num_timesteps=575000, episode_reward=2.80 +/- 3.44
Episode length: 81.10 +/- 19.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.1     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | 1.94     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 281      |
|    time_elapsed    | 4565     |
|    total_timesteps | 575488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=575500, episode_reward=2.18 +/- 3.40
Episode length: 68.48 +/- 22.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 68.5         |
|    mean_reward          | 2.18         |
| time/                   |              |
|    total_timesteps      | 575500       |
| train/                  |              |
|    approx_kl            | 0.0023911586 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.503       |
|    explained_variance   | 0.13         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.103        |
|    n_updates            | 907          |
|    policy_gradient_loss | 0.00147      |
|    value_loss           | 0.239        |
------------------------------------------
Eval num_timesteps=576000, episode_reward=3.42 +/- 4.17
Episode length: 76.02 +/- 19.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 3.42     |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=3.22 +/- 3.80
Episode length: 88.26 +/- 30.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.3     |
|    mean_reward     | 3.22     |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
Eval num_timesteps=577000, episode_reward=2.72 +/- 3.07
Episode length: 71.72 +/- 22.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.7     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=577500, episode_reward=2.58 +/- 3.48
Episode length: 75.66 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 1.98     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 282      |
|    time_elapsed    | 4582     |
|    total_timesteps | 577536   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=578000, episode_reward=1.96 +/- 2.49
Episode length: 74.66 +/- 21.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 74.7         |
|    mean_reward          | 1.96         |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0057969396 |
|    clip_fraction        | 0.0361       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.53        |
|    explained_variance   | 0.0772       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0713       |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00399     |
|    value_loss           | 0.284        |
------------------------------------------
Eval num_timesteps=578500, episode_reward=1.82 +/- 2.90
Episode length: 70.48 +/- 19.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.5     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
Eval num_timesteps=579000, episode_reward=2.80 +/- 3.17
Episode length: 76.88 +/- 24.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=579500, episode_reward=2.38 +/- 2.92
Episode length: 76.58 +/- 27.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.1     |
|    ep_rew_mean     | 1.96     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 283      |
|    time_elapsed    | 4596     |
|    total_timesteps | 579584   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=580000, episode_reward=1.46 +/- 2.06
Episode length: 73.50 +/- 22.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73.5         |
|    mean_reward          | 1.46         |
| time/                   |              |
|    total_timesteps      | 580000       |
| train/                  |              |
|    approx_kl            | 0.0062279366 |
|    clip_fraction        | 0.0509       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.633       |
|    explained_variance   | 0.0525       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00517      |
|    n_updates            | 913          |
|    policy_gradient_loss | -0.00418     |
|    value_loss           | 0.217        |
------------------------------------------
Eval num_timesteps=580500, episode_reward=2.08 +/- 2.81
Episode length: 75.54 +/- 25.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.5     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
Eval num_timesteps=581000, episode_reward=2.30 +/- 2.78
Episode length: 75.86 +/- 24.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=581500, episode_reward=3.24 +/- 4.03
Episode length: 78.04 +/- 25.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 3.24     |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.9     |
|    ep_rew_mean     | 1.74     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 284      |
|    time_elapsed    | 4610     |
|    total_timesteps | 581632   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=582000, episode_reward=2.24 +/- 2.42
Episode length: 77.04 +/- 25.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77          |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 582000      |
| train/                  |             |
|    approx_kl            | 0.009252433 |
|    clip_fraction        | 0.0468      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0.05        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0334      |
|    n_updates            | 918         |
|    policy_gradient_loss | -0.00706    |
|    value_loss           | 0.102       |
-----------------------------------------
Eval num_timesteps=582500, episode_reward=2.26 +/- 3.42
Episode length: 75.06 +/- 24.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
Eval num_timesteps=583000, episode_reward=2.82 +/- 3.48
Episode length: 78.44 +/- 26.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=583500, episode_reward=2.20 +/- 2.91
Episode length: 72.30 +/- 24.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.4     |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 285      |
|    time_elapsed    | 4624     |
|    total_timesteps | 583680   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=584000, episode_reward=1.60 +/- 2.54
Episode length: 71.60 +/- 25.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 71.6         |
|    mean_reward          | 1.6          |
| time/                   |              |
|    total_timesteps      | 584000       |
| train/                  |              |
|    approx_kl            | 0.0062751286 |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.633       |
|    explained_variance   | 0.103        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.052        |
|    n_updates            | 922          |
|    policy_gradient_loss | -0.00787     |
|    value_loss           | 0.38         |
------------------------------------------
Eval num_timesteps=584500, episode_reward=2.50 +/- 3.48
Episode length: 78.28 +/- 26.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
Eval num_timesteps=585000, episode_reward=2.32 +/- 3.41
Episode length: 76.88 +/- 31.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=585500, episode_reward=2.72 +/- 3.71
Episode length: 78.40 +/- 30.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.3     |
|    ep_rew_mean     | 2.01     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 286      |
|    time_elapsed    | 4638     |
|    total_timesteps | 585728   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=586000, episode_reward=2.74 +/- 3.30
Episode length: 75.40 +/- 23.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.4         |
|    mean_reward          | 2.74         |
| time/                   |              |
|    total_timesteps      | 586000       |
| train/                  |              |
|    approx_kl            | 0.0067312275 |
|    clip_fraction        | 0.0642       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.55        |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.357        |
|    n_updates            | 924          |
|    policy_gradient_loss | -0.00183     |
|    value_loss           | 0.472        |
------------------------------------------
Eval num_timesteps=586500, episode_reward=2.98 +/- 3.94
Episode length: 76.62 +/- 25.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
Eval num_timesteps=587000, episode_reward=1.90 +/- 2.67
Episode length: 72.20 +/- 21.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.2     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=587500, episode_reward=2.80 +/- 3.37
Episode length: 81.10 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.1     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.1     |
|    ep_rew_mean     | 1.96     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 287      |
|    time_elapsed    | 4652     |
|    total_timesteps | 587776   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=588000, episode_reward=1.50 +/- 2.17
Episode length: 73.44 +/- 26.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.4        |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 588000      |
| train/                  |             |
|    approx_kl            | 0.004301201 |
|    clip_fraction        | 0.0269      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.447      |
|    explained_variance   | 0.14        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.097       |
|    n_updates            | 926         |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 0.223       |
-----------------------------------------
Eval num_timesteps=588500, episode_reward=3.22 +/- 3.85
Episode length: 80.90 +/- 32.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.9     |
|    mean_reward     | 3.22     |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
Eval num_timesteps=589000, episode_reward=2.48 +/- 3.52
Episode length: 81.00 +/- 25.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81       |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
Eval num_timesteps=589500, episode_reward=2.72 +/- 2.80
Episode length: 76.10 +/- 20.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.1     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.6     |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 288      |
|    time_elapsed    | 4667     |
|    total_timesteps | 589824   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=590000, episode_reward=3.16 +/- 3.57
Episode length: 77.90 +/- 24.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.9        |
|    mean_reward          | 3.16        |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.018762056 |
|    clip_fraction        | 0.0511      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.103       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.128       |
|    n_updates            | 928         |
|    policy_gradient_loss | -0.000682   |
|    value_loss           | 0.335       |
-----------------------------------------
Eval num_timesteps=590500, episode_reward=2.30 +/- 3.31
Episode length: 75.50 +/- 24.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.5     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
Eval num_timesteps=591000, episode_reward=2.06 +/- 2.60
Episode length: 77.88 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=591500, episode_reward=3.12 +/- 3.89
Episode length: 80.18 +/- 27.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 3.12     |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.5     |
|    ep_rew_mean     | 2.21     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 289      |
|    time_elapsed    | 4681     |
|    total_timesteps | 591872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=592000, episode_reward=2.02 +/- 2.97
Episode length: 78.22 +/- 27.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.2         |
|    mean_reward          | 2.02         |
| time/                   |              |
|    total_timesteps      | 592000       |
| train/                  |              |
|    approx_kl            | 0.0036285168 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.404       |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.274        |
|    n_updates            | 929          |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 0.312        |
------------------------------------------
Eval num_timesteps=592500, episode_reward=2.42 +/- 3.31
Episode length: 77.62 +/- 26.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
Eval num_timesteps=593000, episode_reward=3.62 +/- 4.19
Episode length: 79.92 +/- 24.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 3.62     |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
New best mean reward!
Eval num_timesteps=593500, episode_reward=2.42 +/- 3.36
Episode length: 74.14 +/- 25.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.1     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 1.98     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 290      |
|    time_elapsed    | 4696     |
|    total_timesteps | 593920   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=594000, episode_reward=1.96 +/- 2.65
Episode length: 70.82 +/- 24.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 70.8       |
|    mean_reward          | 1.96       |
| time/                   |            |
|    total_timesteps      | 594000     |
| train/                  |            |
|    approx_kl            | 0.00530601 |
|    clip_fraction        | 0.0361     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.477     |
|    explained_variance   | 0.103      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.187      |
|    n_updates            | 932        |
|    policy_gradient_loss | -0.00399   |
|    value_loss           | 0.247      |
----------------------------------------
Eval num_timesteps=594500, episode_reward=2.78 +/- 2.98
Episode length: 84.62 +/- 31.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.6     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
Eval num_timesteps=595000, episode_reward=2.92 +/- 3.42
Episode length: 81.96 +/- 24.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82       |
|    mean_reward     | 2.92     |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=595500, episode_reward=2.32 +/- 2.44
Episode length: 79.14 +/- 28.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.1     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.3     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 291      |
|    time_elapsed    | 4710     |
|    total_timesteps | 595968   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=596000, episode_reward=3.00 +/- 4.49
Episode length: 79.84 +/- 26.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79.8         |
|    mean_reward          | 3            |
| time/                   |              |
|    total_timesteps      | 596000       |
| train/                  |              |
|    approx_kl            | 0.0033773517 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.378       |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.165        |
|    n_updates            | 934          |
|    policy_gradient_loss | 5.65e-05     |
|    value_loss           | 0.508        |
------------------------------------------
Eval num_timesteps=596500, episode_reward=2.72 +/- 3.63
Episode length: 78.66 +/- 28.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.7     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
Eval num_timesteps=597000, episode_reward=2.36 +/- 2.94
Episode length: 75.22 +/- 22.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.2     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=597500, episode_reward=1.84 +/- 2.48
Episode length: 71.84 +/- 27.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=3.44 +/- 3.67
Episode length: 84.02 +/- 25.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84       |
|    mean_reward     | 3.44     |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.5     |
|    ep_rew_mean     | 2.12     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 292      |
|    time_elapsed    | 4727     |
|    total_timesteps | 598016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=598500, episode_reward=3.12 +/- 3.85
Episode length: 81.44 +/- 23.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.4        |
|    mean_reward          | 3.12        |
| time/                   |             |
|    total_timesteps      | 598500      |
| train/                  |             |
|    approx_kl            | 0.004421918 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.469      |
|    explained_variance   | 0.0377      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0757      |
|    n_updates            | 935         |
|    policy_gradient_loss | 0.000527    |
|    value_loss           | 0.244       |
-----------------------------------------
Eval num_timesteps=599000, episode_reward=2.02 +/- 3.04
Episode length: 68.32 +/- 26.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.3     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
Eval num_timesteps=599500, episode_reward=1.92 +/- 2.48
Episode length: 79.24 +/- 24.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.2     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 599500   |
---------------------------------
Eval num_timesteps=600000, episode_reward=2.50 +/- 3.03
Episode length: 73.56 +/- 24.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.6     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.1     |
|    ep_rew_mean     | 2.03     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 293      |
|    time_elapsed    | 4741     |
|    total_timesteps | 600064   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=600500, episode_reward=2.76 +/- 3.33
Episode length: 76.64 +/- 22.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.6        |
|    mean_reward          | 2.76        |
| time/                   |             |
|    total_timesteps      | 600500      |
| train/                  |             |
|    approx_kl            | 0.003934538 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.332      |
|    explained_variance   | 0.142       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.132       |
|    n_updates            | 937         |
|    policy_gradient_loss | -0.000788   |
|    value_loss           | 0.218       |
-----------------------------------------
Eval num_timesteps=601000, episode_reward=1.62 +/- 2.17
Episode length: 72.76 +/- 23.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 601000   |
---------------------------------
Eval num_timesteps=601500, episode_reward=3.24 +/- 3.39
Episode length: 82.90 +/- 26.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.9     |
|    mean_reward     | 3.24     |
| time/              |          |
|    total_timesteps | 601500   |
---------------------------------
Eval num_timesteps=602000, episode_reward=2.50 +/- 3.84
Episode length: 74.22 +/- 26.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.2     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 602000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 1.99     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 294      |
|    time_elapsed    | 4756     |
|    total_timesteps | 602112   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=602500, episode_reward=2.10 +/- 2.84
Episode length: 79.96 +/- 25.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80          |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 602500      |
| train/                  |             |
|    approx_kl            | 0.007368652 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.0336      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.143       |
|    n_updates            | 939         |
|    policy_gradient_loss | -0.00232    |
|    value_loss           | 0.272       |
-----------------------------------------
Eval num_timesteps=603000, episode_reward=2.16 +/- 3.02
Episode length: 77.70 +/- 28.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 603000   |
---------------------------------
Eval num_timesteps=603500, episode_reward=3.12 +/- 3.99
Episode length: 84.96 +/- 28.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85       |
|    mean_reward     | 3.12     |
| time/              |          |
|    total_timesteps | 603500   |
---------------------------------
Eval num_timesteps=604000, episode_reward=1.96 +/- 3.11
Episode length: 70.26 +/- 21.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.3     |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 604000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.5     |
|    ep_rew_mean     | 2.07     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 295      |
|    time_elapsed    | 4770     |
|    total_timesteps | 604160   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=604500, episode_reward=2.52 +/- 3.77
Episode length: 79.98 +/- 22.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 80           |
|    mean_reward          | 2.52         |
| time/                   |              |
|    total_timesteps      | 604500       |
| train/                  |              |
|    approx_kl            | 0.0063222214 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.456       |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.114        |
|    n_updates            | 941          |
|    policy_gradient_loss | 0.00097      |
|    value_loss           | 0.46         |
------------------------------------------
Eval num_timesteps=605000, episode_reward=2.06 +/- 2.66
Episode length: 73.96 +/- 22.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74       |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 605000   |
---------------------------------
Eval num_timesteps=605500, episode_reward=2.68 +/- 3.83
Episode length: 78.60 +/- 28.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 605500   |
---------------------------------
Eval num_timesteps=606000, episode_reward=2.72 +/- 3.72
Episode length: 79.36 +/- 29.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 606000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.2     |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 296      |
|    time_elapsed    | 4784     |
|    total_timesteps | 606208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=606500, episode_reward=1.68 +/- 1.99
Episode length: 73.16 +/- 26.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.2        |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 606500      |
| train/                  |             |
|    approx_kl            | 0.003072938 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.164       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.146       |
|    n_updates            | 942         |
|    policy_gradient_loss | -0.000627   |
|    value_loss           | 0.377       |
-----------------------------------------
Eval num_timesteps=607000, episode_reward=3.18 +/- 4.31
Episode length: 80.72 +/- 30.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.7     |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 607000   |
---------------------------------
Eval num_timesteps=607500, episode_reward=2.16 +/- 2.68
Episode length: 80.34 +/- 25.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 607500   |
---------------------------------
Eval num_timesteps=608000, episode_reward=1.92 +/- 3.49
Episode length: 76.62 +/- 25.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 608000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 297      |
|    time_elapsed    | 4799     |
|    total_timesteps | 608256   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=608500, episode_reward=2.52 +/- 3.21
Episode length: 79.10 +/- 28.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.1        |
|    mean_reward          | 2.52        |
| time/                   |             |
|    total_timesteps      | 608500      |
| train/                  |             |
|    approx_kl            | 0.005296823 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.492      |
|    explained_variance   | 0.109       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.129       |
|    n_updates            | 946         |
|    policy_gradient_loss | -0.00382    |
|    value_loss           | 0.769       |
-----------------------------------------
Eval num_timesteps=609000, episode_reward=3.30 +/- 5.28
Episode length: 77.82 +/- 29.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | 3.3      |
| time/              |          |
|    total_timesteps | 609000   |
---------------------------------
Eval num_timesteps=609500, episode_reward=2.80 +/- 2.79
Episode length: 77.66 +/- 27.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 609500   |
---------------------------------
Eval num_timesteps=610000, episode_reward=2.24 +/- 3.56
Episode length: 72.04 +/- 24.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72       |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 610000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 298      |
|    time_elapsed    | 4814     |
|    total_timesteps | 610304   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=610500, episode_reward=1.84 +/- 3.90
Episode length: 66.72 +/- 19.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 66.7         |
|    mean_reward          | 1.84         |
| time/                   |              |
|    total_timesteps      | 610500       |
| train/                  |              |
|    approx_kl            | 0.0065124906 |
|    clip_fraction        | 0.0448       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.47        |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.243        |
|    n_updates            | 952          |
|    policy_gradient_loss | -0.00706     |
|    value_loss           | 0.366        |
------------------------------------------
Eval num_timesteps=611000, episode_reward=3.30 +/- 3.71
Episode length: 84.02 +/- 28.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84       |
|    mean_reward     | 3.3      |
| time/              |          |
|    total_timesteps | 611000   |
---------------------------------
Eval num_timesteps=611500, episode_reward=2.62 +/- 3.12
Episode length: 76.58 +/- 25.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 611500   |
---------------------------------
Eval num_timesteps=612000, episode_reward=2.30 +/- 3.37
Episode length: 76.82 +/- 26.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 612000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.1     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 299      |
|    time_elapsed    | 4829     |
|    total_timesteps | 612352   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=612500, episode_reward=1.82 +/- 3.40
Episode length: 74.84 +/- 23.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 74.8         |
|    mean_reward          | 1.82         |
| time/                   |              |
|    total_timesteps      | 612500       |
| train/                  |              |
|    approx_kl            | 0.0053749727 |
|    clip_fraction        | 0.0472       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.44        |
|    explained_variance   | -0.0456      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.151        |
|    n_updates            | 954          |
|    policy_gradient_loss | -0.00217     |
|    value_loss           | 0.32         |
------------------------------------------
Eval num_timesteps=613000, episode_reward=1.80 +/- 2.76
Episode length: 73.18 +/- 23.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.2     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 613000   |
---------------------------------
Eval num_timesteps=613500, episode_reward=2.70 +/- 3.27
Episode length: 76.52 +/- 26.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.5     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 613500   |
---------------------------------
Eval num_timesteps=614000, episode_reward=2.34 +/- 3.78
Episode length: 76.98 +/- 28.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 614000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.4     |
|    ep_rew_mean     | 2.31     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 300      |
|    time_elapsed    | 4843     |
|    total_timesteps | 614400   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=614500, episode_reward=2.42 +/- 3.39
Episode length: 77.88 +/- 23.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.9        |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 614500      |
| train/                  |             |
|    approx_kl            | 0.014180444 |
|    clip_fraction        | 0.0411      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.458      |
|    explained_variance   | 0.0457      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.154       |
|    n_updates            | 956         |
|    policy_gradient_loss | -0.00276    |
|    value_loss           | 0.264       |
-----------------------------------------
Eval num_timesteps=615000, episode_reward=1.98 +/- 2.70
Episode length: 72.10 +/- 21.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.1     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 615000   |
---------------------------------
Eval num_timesteps=615500, episode_reward=1.90 +/- 2.26
Episode length: 74.36 +/- 21.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 615500   |
---------------------------------
Eval num_timesteps=616000, episode_reward=2.22 +/- 2.91
Episode length: 76.48 +/- 29.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.5     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 616000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75       |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 301      |
|    time_elapsed    | 4857     |
|    total_timesteps | 616448   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=616500, episode_reward=2.16 +/- 2.54
Episode length: 77.84 +/- 25.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.8         |
|    mean_reward          | 2.16         |
| time/                   |              |
|    total_timesteps      | 616500       |
| train/                  |              |
|    approx_kl            | 0.0074771964 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.319       |
|    explained_variance   | 0.0876       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.188        |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00396     |
|    value_loss           | 0.601        |
------------------------------------------
Eval num_timesteps=617000, episode_reward=2.20 +/- 3.07
Episode length: 74.60 +/- 19.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 617000   |
---------------------------------
Eval num_timesteps=617500, episode_reward=2.38 +/- 3.52
Episode length: 74.96 +/- 26.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 617500   |
---------------------------------
Eval num_timesteps=618000, episode_reward=1.76 +/- 2.53
Episode length: 78.08 +/- 24.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 618000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.8     |
|    ep_rew_mean     | 2.26     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 302      |
|    time_elapsed    | 4872     |
|    total_timesteps | 618496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=618500, episode_reward=2.14 +/- 2.75
Episode length: 73.98 +/- 25.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 74           |
|    mean_reward          | 2.14         |
| time/                   |              |
|    total_timesteps      | 618500       |
| train/                  |              |
|    approx_kl            | 0.0024353103 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.355       |
|    explained_variance   | 0.0841       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.354        |
|    n_updates            | 961          |
|    policy_gradient_loss | 0.00126      |
|    value_loss           | 0.366        |
------------------------------------------
Eval num_timesteps=619000, episode_reward=2.26 +/- 4.15
Episode length: 75.60 +/- 25.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.6     |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 619000   |
---------------------------------
Eval num_timesteps=619500, episode_reward=2.28 +/- 3.02
Episode length: 83.60 +/- 31.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.6     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 619500   |
---------------------------------
Eval num_timesteps=620000, episode_reward=1.98 +/- 2.45
Episode length: 72.62 +/- 23.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.6     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=620500, episode_reward=2.30 +/- 3.52
Episode length: 76.92 +/- 20.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 620500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.1     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 303      |
|    time_elapsed    | 4889     |
|    total_timesteps | 620544   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=621000, episode_reward=2.68 +/- 2.86
Episode length: 79.96 +/- 22.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 80           |
|    mean_reward          | 2.68         |
| time/                   |              |
|    total_timesteps      | 621000       |
| train/                  |              |
|    approx_kl            | 0.0047604684 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.369       |
|    explained_variance   | 0.116        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.142        |
|    n_updates            | 964          |
|    policy_gradient_loss | -0.00081     |
|    value_loss           | 0.27         |
------------------------------------------
Eval num_timesteps=621500, episode_reward=2.26 +/- 2.81
Episode length: 76.02 +/- 26.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 621500   |
---------------------------------
Eval num_timesteps=622000, episode_reward=1.90 +/- 2.21
Episode length: 73.54 +/- 25.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 622000   |
---------------------------------
Eval num_timesteps=622500, episode_reward=2.22 +/- 2.56
Episode length: 82.12 +/- 23.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 622500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.2     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 304      |
|    time_elapsed    | 4903     |
|    total_timesteps | 622592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=623000, episode_reward=2.16 +/- 3.05
Episode length: 78.58 +/- 28.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.6         |
|    mean_reward          | 2.16         |
| time/                   |              |
|    total_timesteps      | 623000       |
| train/                  |              |
|    approx_kl            | 0.0073469398 |
|    clip_fraction        | 0.0582       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.235       |
|    explained_variance   | 0.0543       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.266        |
|    n_updates            | 965          |
|    policy_gradient_loss | -0.00224     |
|    value_loss           | 0.538        |
------------------------------------------
Eval num_timesteps=623500, episode_reward=2.38 +/- 2.40
Episode length: 77.78 +/- 18.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 623500   |
---------------------------------
Eval num_timesteps=624000, episode_reward=2.04 +/- 2.62
Episode length: 77.24 +/- 21.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 624000   |
---------------------------------
Eval num_timesteps=624500, episode_reward=1.28 +/- 2.41
Episode length: 67.16 +/- 23.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.2     |
|    mean_reward     | 1.28     |
| time/              |          |
|    total_timesteps | 624500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.8     |
|    ep_rew_mean     | 2.25     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 305      |
|    time_elapsed    | 4917     |
|    total_timesteps | 624640   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=625000, episode_reward=2.76 +/- 4.09
Episode length: 77.34 +/- 23.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 77.3       |
|    mean_reward          | 2.76       |
| time/                   |            |
|    total_timesteps      | 625000     |
| train/                  |            |
|    approx_kl            | 0.00363648 |
|    clip_fraction        | 0.0174     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.203     |
|    explained_variance   | 0.11       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.62       |
|    n_updates            | 969        |
|    policy_gradient_loss | -0.0031    |
|    value_loss           | 0.468      |
----------------------------------------
Eval num_timesteps=625500, episode_reward=1.94 +/- 3.06
Episode length: 73.06 +/- 22.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 625500   |
---------------------------------
Eval num_timesteps=626000, episode_reward=1.72 +/- 2.38
Episode length: 73.90 +/- 22.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 626000   |
---------------------------------
Eval num_timesteps=626500, episode_reward=2.46 +/- 2.94
Episode length: 74.62 +/- 22.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.46     |
| time/              |          |
|    total_timesteps | 626500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.1     |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 306      |
|    time_elapsed    | 4931     |
|    total_timesteps | 626688   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=627000, episode_reward=2.26 +/- 3.40
Episode length: 73.68 +/- 25.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 73.7       |
|    mean_reward          | 2.26       |
| time/                   |            |
|    total_timesteps      | 627000     |
| train/                  |            |
|    approx_kl            | 0.00359933 |
|    clip_fraction        | 0.026      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.263     |
|    explained_variance   | 0.121      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.162      |
|    n_updates            | 971        |
|    policy_gradient_loss | -0.000373  |
|    value_loss           | 0.347      |
----------------------------------------
Eval num_timesteps=627500, episode_reward=2.56 +/- 3.42
Episode length: 76.96 +/- 28.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 627500   |
---------------------------------
Eval num_timesteps=628000, episode_reward=2.74 +/- 3.46
Episode length: 72.66 +/- 22.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 628000   |
---------------------------------
Eval num_timesteps=628500, episode_reward=2.68 +/- 3.50
Episode length: 79.46 +/- 23.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 628500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.5     |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 127      |
|    iterations      | 307      |
|    time_elapsed    | 4945     |
|    total_timesteps | 628736   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=629000, episode_reward=2.80 +/- 4.04
Episode length: 77.80 +/- 19.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.8        |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 629000      |
| train/                  |             |
|    approx_kl            | 0.006857779 |
|    clip_fraction        | 0.0184      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.3        |
|    explained_variance   | 0.111       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0434      |
|    n_updates            | 974         |
|    policy_gradient_loss | -0.000336   |
|    value_loss           | 0.367       |
-----------------------------------------
Eval num_timesteps=629500, episode_reward=3.56 +/- 4.01
Episode length: 82.64 +/- 24.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.6     |
|    mean_reward     | 3.56     |
| time/              |          |
|    total_timesteps | 629500   |
---------------------------------
Eval num_timesteps=630000, episode_reward=2.84 +/- 3.34
Episode length: 72.82 +/- 20.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 630000   |
---------------------------------
Eval num_timesteps=630500, episode_reward=1.74 +/- 2.42
Episode length: 79.74 +/- 23.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 630500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.4     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 127      |
|    iterations      | 308      |
|    time_elapsed    | 4959     |
|    total_timesteps | 630784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=631000, episode_reward=2.36 +/- 3.50
Episode length: 78.04 +/- 23.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78           |
|    mean_reward          | 2.36         |
| time/                   |              |
|    total_timesteps      | 631000       |
| train/                  |              |
|    approx_kl            | 0.0019828572 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.309       |
|    explained_variance   | 0.137        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0799       |
|    n_updates            | 975          |
|    policy_gradient_loss | 0.00167      |
|    value_loss           | 0.376        |
------------------------------------------
Eval num_timesteps=631500, episode_reward=2.36 +/- 3.67
Episode length: 76.08 +/- 27.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.1     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 631500   |
---------------------------------
Eval num_timesteps=632000, episode_reward=2.68 +/- 3.66
Episode length: 74.78 +/- 23.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.8     |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 632000   |
---------------------------------
Eval num_timesteps=632500, episode_reward=1.72 +/- 3.96
Episode length: 77.18 +/- 28.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 632500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 127      |
|    iterations      | 309      |
|    time_elapsed    | 4973     |
|    total_timesteps | 632832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=633000, episode_reward=2.62 +/- 3.01
Episode length: 76.02 +/- 23.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76          |
|    mean_reward          | 2.62        |
| time/                   |             |
|    total_timesteps      | 633000      |
| train/                  |             |
|    approx_kl            | 0.003732088 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.358      |
|    explained_variance   | 0.0649      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.203       |
|    n_updates            | 976         |
|    policy_gradient_loss | -0.0048     |
|    value_loss           | 0.197       |
-----------------------------------------
Eval num_timesteps=633500, episode_reward=1.94 +/- 2.69
Episode length: 77.22 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 633500   |
---------------------------------
Eval num_timesteps=634000, episode_reward=2.42 +/- 2.74
Episode length: 80.52 +/- 26.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.5     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 634000   |
---------------------------------
Eval num_timesteps=634500, episode_reward=3.18 +/- 4.43
Episode length: 74.50 +/- 27.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 634500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.7     |
|    ep_rew_mean     | 2.22     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 310      |
|    time_elapsed    | 4988     |
|    total_timesteps | 634880   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=635000, episode_reward=2.88 +/- 3.51
Episode length: 79.22 +/- 20.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79.2         |
|    mean_reward          | 2.88         |
| time/                   |              |
|    total_timesteps      | 635000       |
| train/                  |              |
|    approx_kl            | 0.0057107983 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.295       |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0492       |
|    n_updates            | 981          |
|    policy_gradient_loss | -0.00492     |
|    value_loss           | 0.235        |
------------------------------------------
Eval num_timesteps=635500, episode_reward=2.54 +/- 2.99
Episode length: 79.34 +/- 26.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.3     |
|    mean_reward     | 2.54     |
| time/              |          |
|    total_timesteps | 635500   |
---------------------------------
Eval num_timesteps=636000, episode_reward=2.76 +/- 3.87
Episode length: 76.04 +/- 29.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 636000   |
---------------------------------
Eval num_timesteps=636500, episode_reward=3.12 +/- 3.66
Episode length: 76.24 +/- 24.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | 3.12     |
| time/              |          |
|    total_timesteps | 636500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 2.06     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 311      |
|    time_elapsed    | 5002     |
|    total_timesteps | 636928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=637000, episode_reward=2.32 +/- 2.64
Episode length: 74.70 +/- 26.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.7        |
|    mean_reward          | 2.32        |
| time/                   |             |
|    total_timesteps      | 637000      |
| train/                  |             |
|    approx_kl            | 0.004268879 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.0635      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.105       |
|    n_updates            | 982         |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 0.255       |
-----------------------------------------
Eval num_timesteps=637500, episode_reward=1.40 +/- 1.83
Episode length: 75.02 +/- 23.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 637500   |
---------------------------------
Eval num_timesteps=638000, episode_reward=2.58 +/- 3.21
Episode length: 75.98 +/- 24.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 638000   |
---------------------------------
Eval num_timesteps=638500, episode_reward=2.16 +/- 2.80
Episode length: 74.56 +/- 23.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 638500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.2     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 127      |
|    iterations      | 312      |
|    time_elapsed    | 5016     |
|    total_timesteps | 638976   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=639000, episode_reward=2.16 +/- 3.06
Episode length: 78.98 +/- 30.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79          |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 639000      |
| train/                  |             |
|    approx_kl            | 0.004807304 |
|    clip_fraction        | 0.0498      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.122       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.367       |
|    n_updates            | 984         |
|    policy_gradient_loss | 0.00162     |
|    value_loss           | 0.614       |
-----------------------------------------
Eval num_timesteps=639500, episode_reward=2.20 +/- 3.45
Episode length: 78.22 +/- 26.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 639500   |
---------------------------------
Eval num_timesteps=640000, episode_reward=3.06 +/- 5.81
Episode length: 77.48 +/- 24.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.5     |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=640500, episode_reward=1.82 +/- 2.99
Episode length: 73.28 +/- 28.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.3     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 640500   |
---------------------------------
Eval num_timesteps=641000, episode_reward=2.08 +/- 2.63
Episode length: 76.00 +/- 23.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 641000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 313      |
|    time_elapsed    | 5033     |
|    total_timesteps | 641024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=641500, episode_reward=1.48 +/- 2.06
Episode length: 76.86 +/- 25.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.9         |
|    mean_reward          | 1.48         |
| time/                   |              |
|    total_timesteps      | 641500       |
| train/                  |              |
|    approx_kl            | 0.0032404861 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.42        |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.194        |
|    n_updates            | 985          |
|    policy_gradient_loss | 0.000453     |
|    value_loss           | 0.394        |
------------------------------------------
Eval num_timesteps=642000, episode_reward=1.98 +/- 2.40
Episode length: 73.60 +/- 22.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.6     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 642000   |
---------------------------------
Eval num_timesteps=642500, episode_reward=2.56 +/- 3.51
Episode length: 81.86 +/- 28.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.9     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 642500   |
---------------------------------
Eval num_timesteps=643000, episode_reward=2.20 +/- 3.06
Episode length: 69.32 +/- 21.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.3     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 643000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.2     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 314      |
|    time_elapsed    | 5047     |
|    total_timesteps | 643072   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=643500, episode_reward=2.40 +/- 2.95
Episode length: 78.68 +/- 24.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.7         |
|    mean_reward          | 2.4          |
| time/                   |              |
|    total_timesteps      | 643500       |
| train/                  |              |
|    approx_kl            | 0.0051140776 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.483       |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0318       |
|    n_updates            | 988          |
|    policy_gradient_loss | -0.00218     |
|    value_loss           | 0.26         |
------------------------------------------
Eval num_timesteps=644000, episode_reward=2.28 +/- 2.53
Episode length: 74.44 +/- 19.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 644000   |
---------------------------------
Eval num_timesteps=644500, episode_reward=2.78 +/- 3.06
Episode length: 82.38 +/- 29.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 644500   |
---------------------------------
Eval num_timesteps=645000, episode_reward=1.44 +/- 2.29
Episode length: 73.60 +/- 28.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.6     |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 645000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 315      |
|    time_elapsed    | 5061     |
|    total_timesteps | 645120   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=645500, episode_reward=2.12 +/- 3.13
Episode length: 79.82 +/- 29.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.8        |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 645500      |
| train/                  |             |
|    approx_kl            | 0.004554089 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.367      |
|    explained_variance   | 0.166       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0941      |
|    n_updates            | 992         |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 0.25        |
-----------------------------------------
Eval num_timesteps=646000, episode_reward=1.72 +/- 2.27
Episode length: 69.20 +/- 26.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.2     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 646000   |
---------------------------------
Eval num_timesteps=646500, episode_reward=2.32 +/- 2.84
Episode length: 76.96 +/- 25.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 646500   |
---------------------------------
Eval num_timesteps=647000, episode_reward=2.74 +/- 4.15
Episode length: 77.46 +/- 25.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.5     |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 647000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.5     |
|    ep_rew_mean     | 2.03     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 316      |
|    time_elapsed    | 5075     |
|    total_timesteps | 647168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=647500, episode_reward=2.50 +/- 3.52
Episode length: 78.94 +/- 24.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.9         |
|    mean_reward          | 2.5          |
| time/                   |              |
|    total_timesteps      | 647500       |
| train/                  |              |
|    approx_kl            | 0.0034429748 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.452       |
|    explained_variance   | 0.0125       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.268        |
|    n_updates            | 993          |
|    policy_gradient_loss | -0.00096     |
|    value_loss           | 0.344        |
------------------------------------------
Eval num_timesteps=648000, episode_reward=1.94 +/- 2.51
Episode length: 79.94 +/- 25.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 648000   |
---------------------------------
Eval num_timesteps=648500, episode_reward=2.48 +/- 3.09
Episode length: 80.46 +/- 31.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.5     |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 648500   |
---------------------------------
Eval num_timesteps=649000, episode_reward=2.72 +/- 3.52
Episode length: 78.30 +/- 23.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 649000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.6     |
|    ep_rew_mean     | 1.85     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 317      |
|    time_elapsed    | 5089     |
|    total_timesteps | 649216   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=649500, episode_reward=2.20 +/- 2.47
Episode length: 76.34 +/- 25.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 76.3       |
|    mean_reward          | 2.2        |
| time/                   |            |
|    total_timesteps      | 649500     |
| train/                  |            |
|    approx_kl            | 0.00323955 |
|    clip_fraction        | 0.0267     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.0975     |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0177     |
|    n_updates            | 995        |
|    policy_gradient_loss | -0.00181   |
|    value_loss           | 0.147      |
----------------------------------------
Eval num_timesteps=650000, episode_reward=2.78 +/- 3.80
Episode length: 79.96 +/- 27.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 650000   |
---------------------------------
Eval num_timesteps=650500, episode_reward=2.76 +/- 2.91
Episode length: 77.12 +/- 22.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 650500   |
---------------------------------
Eval num_timesteps=651000, episode_reward=2.80 +/- 3.46
Episode length: 80.70 +/- 29.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.7     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 651000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.7     |
|    ep_rew_mean     | 1.74     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 318      |
|    time_elapsed    | 5104     |
|    total_timesteps | 651264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=651500, episode_reward=2.14 +/- 3.05
Episode length: 74.50 +/- 25.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 74.5         |
|    mean_reward          | 2.14         |
| time/                   |              |
|    total_timesteps      | 651500       |
| train/                  |              |
|    approx_kl            | 0.0037409915 |
|    clip_fraction        | 0.0376       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.384       |
|    explained_variance   | 0.0719       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.129        |
|    n_updates            | 996          |
|    policy_gradient_loss | -0.000745    |
|    value_loss           | 0.243        |
------------------------------------------
Eval num_timesteps=652000, episode_reward=2.12 +/- 2.54
Episode length: 80.04 +/- 26.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 652000   |
---------------------------------
Eval num_timesteps=652500, episode_reward=2.26 +/- 2.89
Episode length: 72.74 +/- 21.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 652500   |
---------------------------------
Eval num_timesteps=653000, episode_reward=3.28 +/- 4.39
Episode length: 79.22 +/- 26.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.2     |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 653000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.7     |
|    ep_rew_mean     | 1.73     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 319      |
|    time_elapsed    | 5118     |
|    total_timesteps | 653312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=653500, episode_reward=2.18 +/- 3.49
Episode length: 82.02 +/- 30.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 82           |
|    mean_reward          | 2.18         |
| time/                   |              |
|    total_timesteps      | 653500       |
| train/                  |              |
|    approx_kl            | 0.0028244066 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.258       |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.178        |
|    n_updates            | 997          |
|    policy_gradient_loss | 0.000195     |
|    value_loss           | 0.299        |
------------------------------------------
Eval num_timesteps=654000, episode_reward=2.66 +/- 2.78
Episode length: 75.46 +/- 22.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.5     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 654000   |
---------------------------------
Eval num_timesteps=654500, episode_reward=1.72 +/- 2.48
Episode length: 73.18 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.2     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 654500   |
---------------------------------
Eval num_timesteps=655000, episode_reward=2.16 +/- 2.97
Episode length: 71.70 +/- 25.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.7     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 655000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.5     |
|    ep_rew_mean     | 2.03     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 320      |
|    time_elapsed    | 5132     |
|    total_timesteps | 655360   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=655500, episode_reward=2.28 +/- 3.88
Episode length: 83.78 +/- 28.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 83.8       |
|    mean_reward          | 2.28       |
| time/                   |            |
|    total_timesteps      | 655500     |
| train/                  |            |
|    approx_kl            | 0.01533033 |
|    clip_fraction        | 0.0301     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.0943     |
|    learning_rate        | 0.0001     |
|    loss                 | 0.204      |
|    n_updates            | 1004       |
|    policy_gradient_loss | -0.00341   |
|    value_loss           | 0.37       |
----------------------------------------
Eval num_timesteps=656000, episode_reward=1.84 +/- 2.73
Episode length: 74.30 +/- 24.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.3     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 656000   |
---------------------------------
Eval num_timesteps=656500, episode_reward=1.80 +/- 2.74
Episode length: 73.76 +/- 20.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.8     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 656500   |
---------------------------------
Eval num_timesteps=657000, episode_reward=2.46 +/- 3.11
Episode length: 71.70 +/- 22.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.7     |
|    mean_reward     | 2.46     |
| time/              |          |
|    total_timesteps | 657000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.2     |
|    ep_rew_mean     | 2.17     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 321      |
|    time_elapsed    | 5146     |
|    total_timesteps | 657408   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=657500, episode_reward=1.98 +/- 2.51
Episode length: 72.82 +/- 26.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.8        |
|    mean_reward          | 1.98        |
| time/                   |             |
|    total_timesteps      | 657500      |
| train/                  |             |
|    approx_kl            | 0.004820953 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.231      |
|    explained_variance   | 0.139       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0706      |
|    n_updates            | 1007        |
|    policy_gradient_loss | -0.00187    |
|    value_loss           | 0.207       |
-----------------------------------------
Eval num_timesteps=658000, episode_reward=2.72 +/- 4.34
Episode length: 84.48 +/- 30.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.5     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 658000   |
---------------------------------
Eval num_timesteps=658500, episode_reward=2.28 +/- 2.76
Episode length: 76.52 +/- 23.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.5     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 658500   |
---------------------------------
Eval num_timesteps=659000, episode_reward=1.90 +/- 2.27
Episode length: 71.54 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.5     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 659000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.1     |
|    ep_rew_mean     | 2.12     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 322      |
|    time_elapsed    | 5160     |
|    total_timesteps | 659456   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=659500, episode_reward=1.68 +/- 2.64
Episode length: 74.66 +/- 23.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.7        |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 659500      |
| train/                  |             |
|    approx_kl            | 0.005284479 |
|    clip_fraction        | 0.028       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.408      |
|    explained_variance   | 0.117       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.102       |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 0.305       |
-----------------------------------------
Eval num_timesteps=660000, episode_reward=2.46 +/- 4.00
Episode length: 80.42 +/- 29.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.4     |
|    mean_reward     | 2.46     |
| time/              |          |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=660500, episode_reward=2.18 +/- 2.77
Episode length: 72.88 +/- 23.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 660500   |
---------------------------------
Eval num_timesteps=661000, episode_reward=1.84 +/- 2.54
Episode length: 77.22 +/- 28.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 661000   |
---------------------------------
Eval num_timesteps=661500, episode_reward=1.88 +/- 2.25
Episode length: 70.82 +/- 23.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.8     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 661500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.1     |
|    ep_rew_mean     | 2.13     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 323      |
|    time_elapsed    | 5177     |
|    total_timesteps | 661504   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=662000, episode_reward=3.14 +/- 4.02
Episode length: 81.76 +/- 25.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81.8         |
|    mean_reward          | 3.14         |
| time/                   |              |
|    total_timesteps      | 662000       |
| train/                  |              |
|    approx_kl            | 0.0050464105 |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.455       |
|    explained_variance   | 0.0929       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.389        |
|    n_updates            | 1012         |
|    policy_gradient_loss | -0.00206     |
|    value_loss           | 0.376        |
------------------------------------------
Eval num_timesteps=662500, episode_reward=2.32 +/- 2.76
Episode length: 76.16 +/- 27.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 662500   |
---------------------------------
Eval num_timesteps=663000, episode_reward=3.06 +/- 3.72
Episode length: 80.64 +/- 24.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.6     |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 663000   |
---------------------------------
Eval num_timesteps=663500, episode_reward=2.76 +/- 4.43
Episode length: 78.70 +/- 27.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.7     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 663500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.3     |
|    ep_rew_mean     | 1.75     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 324      |
|    time_elapsed    | 5192     |
|    total_timesteps | 663552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=664000, episode_reward=3.10 +/- 4.25
Episode length: 79.18 +/- 25.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79.2         |
|    mean_reward          | 3.1          |
| time/                   |              |
|    total_timesteps      | 664000       |
| train/                  |              |
|    approx_kl            | 0.0052318326 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.553       |
|    explained_variance   | 0.118        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0878       |
|    n_updates            | 1014         |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 0.241        |
------------------------------------------
Eval num_timesteps=664500, episode_reward=3.00 +/- 3.19
Episode length: 75.22 +/- 19.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.2     |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 664500   |
---------------------------------
Eval num_timesteps=665000, episode_reward=2.44 +/- 3.40
Episode length: 73.52 +/- 22.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 665000   |
---------------------------------
Eval num_timesteps=665500, episode_reward=2.50 +/- 2.79
Episode length: 78.44 +/- 22.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 665500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.2     |
|    ep_rew_mean     | 1.83     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 325      |
|    time_elapsed    | 5206     |
|    total_timesteps | 665600   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=666000, episode_reward=2.82 +/- 2.85
Episode length: 79.16 +/- 26.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.2        |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 666000      |
| train/                  |             |
|    approx_kl            | 0.015464294 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.0781      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.18        |
|    n_updates            | 1016        |
|    policy_gradient_loss | -0.000824   |
|    value_loss           | 0.327       |
-----------------------------------------
Eval num_timesteps=666500, episode_reward=2.44 +/- 3.09
Episode length: 73.34 +/- 20.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.3     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 666500   |
---------------------------------
Eval num_timesteps=667000, episode_reward=3.10 +/- 3.81
Episode length: 82.36 +/- 31.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 667000   |
---------------------------------
Eval num_timesteps=667500, episode_reward=2.04 +/- 2.98
Episode length: 76.30 +/- 26.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 667500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.6     |
|    ep_rew_mean     | 1.76     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 326      |
|    time_elapsed    | 5220     |
|    total_timesteps | 667648   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=668000, episode_reward=2.64 +/- 2.92
Episode length: 76.98 +/- 24.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77           |
|    mean_reward          | 2.64         |
| time/                   |              |
|    total_timesteps      | 668000       |
| train/                  |              |
|    approx_kl            | 0.0057832496 |
|    clip_fraction        | 0.0483       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.776       |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.101        |
|    n_updates            | 1019         |
|    policy_gradient_loss | -0.00688     |
|    value_loss           | 0.331        |
------------------------------------------
Eval num_timesteps=668500, episode_reward=2.66 +/- 3.10
Episode length: 72.30 +/- 22.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 668500   |
---------------------------------
Eval num_timesteps=669000, episode_reward=1.66 +/- 2.07
Episode length: 73.34 +/- 17.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.3     |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 669000   |
---------------------------------
Eval num_timesteps=669500, episode_reward=2.82 +/- 3.12
Episode length: 81.40 +/- 32.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.4     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 669500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.5     |
|    ep_rew_mean     | 1.77     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 327      |
|    time_elapsed    | 5235     |
|    total_timesteps | 669696   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=670000, episode_reward=2.98 +/- 3.67
Episode length: 78.02 +/- 28.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78          |
|    mean_reward          | 2.98        |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.008555208 |
|    clip_fraction        | 0.053       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.18        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0648      |
|    n_updates            | 1021        |
|    policy_gradient_loss | -0.000635   |
|    value_loss           | 0.19        |
-----------------------------------------
Eval num_timesteps=670500, episode_reward=1.82 +/- 2.60
Episode length: 73.08 +/- 24.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 670500   |
---------------------------------
Eval num_timesteps=671000, episode_reward=2.60 +/- 3.38
Episode length: 73.32 +/- 27.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.3     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 671000   |
---------------------------------
Eval num_timesteps=671500, episode_reward=2.62 +/- 3.08
Episode length: 78.10 +/- 29.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 671500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 1.91     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 328      |
|    time_elapsed    | 5249     |
|    total_timesteps | 671744   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=672000, episode_reward=2.16 +/- 3.01
Episode length: 78.48 +/- 29.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.5        |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 672000      |
| train/                  |             |
|    approx_kl            | 0.009592708 |
|    clip_fraction        | 0.0571      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.755      |
|    explained_variance   | 0.177       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.244       |
|    n_updates            | 1025        |
|    policy_gradient_loss | -0.00657    |
|    value_loss           | 0.323       |
-----------------------------------------
Eval num_timesteps=672500, episode_reward=2.44 +/- 3.03
Episode length: 79.48 +/- 23.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 672500   |
---------------------------------
Eval num_timesteps=673000, episode_reward=1.78 +/- 2.50
Episode length: 81.12 +/- 25.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.1     |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 673000   |
---------------------------------
Eval num_timesteps=673500, episode_reward=2.98 +/- 4.28
Episode length: 75.92 +/- 25.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 673500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.7     |
|    ep_rew_mean     | 2.16     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 329      |
|    time_elapsed    | 5263     |
|    total_timesteps | 673792   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=674000, episode_reward=2.60 +/- 3.92
Episode length: 73.68 +/- 25.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.7        |
|    mean_reward          | 2.6         |
| time/                   |             |
|    total_timesteps      | 674000      |
| train/                  |             |
|    approx_kl            | 0.007957709 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.168       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.278       |
|    n_updates            | 1028        |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 0.353       |
-----------------------------------------
Eval num_timesteps=674500, episode_reward=1.96 +/- 2.63
Episode length: 78.02 +/- 23.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 674500   |
---------------------------------
Eval num_timesteps=675000, episode_reward=2.62 +/- 2.58
Episode length: 81.42 +/- 29.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.4     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 675000   |
---------------------------------
Eval num_timesteps=675500, episode_reward=2.74 +/- 3.59
Episode length: 74.68 +/- 27.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 675500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.1     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 330      |
|    time_elapsed    | 5278     |
|    total_timesteps | 675840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=676000, episode_reward=3.00 +/- 3.89
Episode length: 81.26 +/- 28.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.3        |
|    mean_reward          | 3           |
| time/                   |             |
|    total_timesteps      | 676000      |
| train/                  |             |
|    approx_kl            | 0.005154115 |
|    clip_fraction        | 0.0609      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.639      |
|    explained_variance   | 0.107       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.14        |
|    n_updates            | 1029        |
|    policy_gradient_loss | 0.00279     |
|    value_loss           | 0.523       |
-----------------------------------------
Eval num_timesteps=676500, episode_reward=2.24 +/- 2.70
Episode length: 76.38 +/- 23.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.4     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 676500   |
---------------------------------
Eval num_timesteps=677000, episode_reward=3.52 +/- 4.16
Episode length: 79.68 +/- 25.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | 3.52     |
| time/              |          |
|    total_timesteps | 677000   |
---------------------------------
Eval num_timesteps=677500, episode_reward=3.16 +/- 3.47
Episode length: 83.68 +/- 29.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.7     |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 677500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.6     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 331      |
|    time_elapsed    | 5293     |
|    total_timesteps | 677888   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=678000, episode_reward=3.12 +/- 3.39
Episode length: 78.92 +/- 27.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.9         |
|    mean_reward          | 3.12         |
| time/                   |              |
|    total_timesteps      | 678000       |
| train/                  |              |
|    approx_kl            | 0.0040194932 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.646       |
|    explained_variance   | 0.0959       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.154        |
|    n_updates            | 1031         |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 0.431        |
------------------------------------------
Eval num_timesteps=678500, episode_reward=2.70 +/- 2.93
Episode length: 78.34 +/- 24.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 678500   |
---------------------------------
Eval num_timesteps=679000, episode_reward=2.14 +/- 2.51
Episode length: 73.92 +/- 22.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 679000   |
---------------------------------
Eval num_timesteps=679500, episode_reward=2.60 +/- 2.86
Episode length: 81.08 +/- 27.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.1     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 679500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79       |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 128      |
|    iterations      | 332      |
|    time_elapsed    | 5307     |
|    total_timesteps | 679936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=680000, episode_reward=2.36 +/- 3.47
Episode length: 74.38 +/- 19.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 74.4         |
|    mean_reward          | 2.36         |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 0.0037127724 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.594       |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.103        |
|    n_updates            | 1032         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 0.261        |
------------------------------------------
Eval num_timesteps=680500, episode_reward=1.56 +/- 2.32
Episode length: 64.28 +/- 25.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.3     |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 680500   |
---------------------------------
Eval num_timesteps=681000, episode_reward=2.74 +/- 3.84
Episode length: 77.38 +/- 28.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 681000   |
---------------------------------
Eval num_timesteps=681500, episode_reward=2.56 +/- 3.24
Episode length: 78.56 +/- 24.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 681500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.6     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 333      |
|    time_elapsed    | 5321     |
|    total_timesteps | 681984   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=682000, episode_reward=2.30 +/- 2.69
Episode length: 84.40 +/- 29.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.4         |
|    mean_reward          | 2.3          |
| time/                   |              |
|    total_timesteps      | 682000       |
| train/                  |              |
|    approx_kl            | 0.0042933025 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.388       |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.105        |
|    n_updates            | 1035         |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 0.3          |
------------------------------------------
Eval num_timesteps=682500, episode_reward=2.78 +/- 3.35
Episode length: 75.16 +/- 22.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.2     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 682500   |
---------------------------------
Eval num_timesteps=683000, episode_reward=3.04 +/- 4.89
Episode length: 77.84 +/- 27.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 683000   |
---------------------------------
Eval num_timesteps=683500, episode_reward=2.28 +/- 2.75
Episode length: 79.20 +/- 26.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.2     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 683500   |
---------------------------------
Eval num_timesteps=684000, episode_reward=2.86 +/- 3.79
Episode length: 81.26 +/- 29.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.3     |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.4     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 128      |
|    iterations      | 334      |
|    time_elapsed    | 5339     |
|    total_timesteps | 684032   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=684500, episode_reward=2.30 +/- 2.79
Episode length: 72.44 +/- 23.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.4        |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 684500      |
| train/                  |             |
|    approx_kl            | 0.006767923 |
|    clip_fraction        | 0.0581      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.444      |
|    explained_variance   | 0.141       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.37        |
|    n_updates            | 1039        |
|    policy_gradient_loss | -0.00766    |
|    value_loss           | 0.448       |
-----------------------------------------
Eval num_timesteps=685000, episode_reward=2.14 +/- 2.49
Episode length: 78.72 +/- 27.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.7     |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 685000   |
---------------------------------
Eval num_timesteps=685500, episode_reward=2.34 +/- 3.43
Episode length: 69.08 +/- 23.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.1     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 685500   |
---------------------------------
Eval num_timesteps=686000, episode_reward=1.98 +/- 2.77
Episode length: 75.58 +/- 25.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.6     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 686000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.6     |
|    ep_rew_mean     | 2.33     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 335      |
|    time_elapsed    | 5353     |
|    total_timesteps | 686080   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=686500, episode_reward=2.14 +/- 2.35
Episode length: 76.10 +/- 24.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.1        |
|    mean_reward          | 2.14        |
| time/                   |             |
|    total_timesteps      | 686500      |
| train/                  |             |
|    approx_kl            | 0.005793083 |
|    clip_fraction        | 0.039       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.486      |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0441      |
|    n_updates            | 1043        |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 0.238       |
-----------------------------------------
Eval num_timesteps=687000, episode_reward=3.70 +/- 4.11
Episode length: 85.90 +/- 25.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.9     |
|    mean_reward     | 3.7      |
| time/              |          |
|    total_timesteps | 687000   |
---------------------------------
New best mean reward!
Eval num_timesteps=687500, episode_reward=2.20 +/- 2.97
Episode length: 77.14 +/- 25.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 687500   |
---------------------------------
Eval num_timesteps=688000, episode_reward=2.40 +/- 3.38
Episode length: 76.14 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.1     |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 688000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.1     |
|    ep_rew_mean     | 2.31     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 336      |
|    time_elapsed    | 5368     |
|    total_timesteps | 688128   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=688500, episode_reward=2.20 +/- 2.56
Episode length: 75.76 +/- 25.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.8        |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 688500      |
| train/                  |             |
|    approx_kl            | 0.005204725 |
|    clip_fraction        | 0.0436      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.516      |
|    explained_variance   | 0.18        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.067       |
|    n_updates            | 1046        |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 0.206       |
-----------------------------------------
Eval num_timesteps=689000, episode_reward=2.80 +/- 4.10
Episode length: 75.14 +/- 22.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 689000   |
---------------------------------
Eval num_timesteps=689500, episode_reward=2.44 +/- 2.89
Episode length: 74.36 +/- 26.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 689500   |
---------------------------------
Eval num_timesteps=690000, episode_reward=2.06 +/- 2.80
Episode length: 71.60 +/- 26.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.6     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 690000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.8     |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 337      |
|    time_elapsed    | 5382     |
|    total_timesteps | 690176   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=690500, episode_reward=2.06 +/- 3.27
Episode length: 75.92 +/- 21.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.9         |
|    mean_reward          | 2.06         |
| time/                   |              |
|    total_timesteps      | 690500       |
| train/                  |              |
|    approx_kl            | 0.0060132365 |
|    clip_fraction        | 0.0433       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.0924       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.188        |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.00677     |
|    value_loss           | 0.379        |
------------------------------------------
Eval num_timesteps=691000, episode_reward=2.62 +/- 3.06
Episode length: 77.08 +/- 22.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 691000   |
---------------------------------
Eval num_timesteps=691500, episode_reward=2.74 +/- 3.06
Episode length: 77.16 +/- 23.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 691500   |
---------------------------------
Eval num_timesteps=692000, episode_reward=2.32 +/- 2.87
Episode length: 71.48 +/- 22.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.5     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 692000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.4     |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 338      |
|    time_elapsed    | 5396     |
|    total_timesteps | 692224   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=692500, episode_reward=2.98 +/- 3.69
Episode length: 78.20 +/- 22.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.2        |
|    mean_reward          | 2.98        |
| time/                   |             |
|    total_timesteps      | 692500      |
| train/                  |             |
|    approx_kl            | 0.008961094 |
|    clip_fraction        | 0.0421      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.484      |
|    explained_variance   | 0.226       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.258       |
|    n_updates            | 1052        |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 0.381       |
-----------------------------------------
Eval num_timesteps=693000, episode_reward=4.06 +/- 4.73
Episode length: 89.88 +/- 28.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.9     |
|    mean_reward     | 4.06     |
| time/              |          |
|    total_timesteps | 693000   |
---------------------------------
New best mean reward!
Eval num_timesteps=693500, episode_reward=3.14 +/- 3.64
Episode length: 83.26 +/- 25.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.3     |
|    mean_reward     | 3.14     |
| time/              |          |
|    total_timesteps | 693500   |
---------------------------------
Eval num_timesteps=694000, episode_reward=1.92 +/- 2.45
Episode length: 80.52 +/- 25.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.5     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 694000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.6     |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 339      |
|    time_elapsed    | 5412     |
|    total_timesteps | 694272   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=694500, episode_reward=2.28 +/- 3.34
Episode length: 75.58 +/- 33.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.6         |
|    mean_reward          | 2.28         |
| time/                   |              |
|    total_timesteps      | 694500       |
| train/                  |              |
|    approx_kl            | 0.0055128317 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.169        |
|    n_updates            | 1054         |
|    policy_gradient_loss | 9.02e-05     |
|    value_loss           | 0.534        |
------------------------------------------
Eval num_timesteps=695000, episode_reward=2.06 +/- 2.62
Episode length: 74.24 +/- 26.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.2     |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 695000   |
---------------------------------
Eval num_timesteps=695500, episode_reward=2.30 +/- 2.99
Episode length: 74.34 +/- 25.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.3     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 695500   |
---------------------------------
Eval num_timesteps=696000, episode_reward=2.58 +/- 2.71
Episode length: 80.58 +/- 23.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.6     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 696000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.4     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 340      |
|    time_elapsed    | 5426     |
|    total_timesteps | 696320   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=696500, episode_reward=2.62 +/- 3.46
Episode length: 77.94 +/- 25.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.9         |
|    mean_reward          | 2.62         |
| time/                   |              |
|    total_timesteps      | 696500       |
| train/                  |              |
|    approx_kl            | 0.0050018765 |
|    clip_fraction        | 0.0465       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.587       |
|    explained_variance   | 0.0752       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.327        |
|    n_updates            | 1056         |
|    policy_gradient_loss | 0.00139      |
|    value_loss           | 0.287        |
------------------------------------------
Eval num_timesteps=697000, episode_reward=2.80 +/- 3.49
Episode length: 77.26 +/- 21.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 697000   |
---------------------------------
Eval num_timesteps=697500, episode_reward=2.82 +/- 3.79
Episode length: 79.50 +/- 23.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 697500   |
---------------------------------
Eval num_timesteps=698000, episode_reward=2.52 +/- 3.49
Episode length: 74.04 +/- 24.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74       |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 698000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 341      |
|    time_elapsed    | 5441     |
|    total_timesteps | 698368   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=698500, episode_reward=2.80 +/- 3.14
Episode length: 76.42 +/- 24.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.4        |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 698500      |
| train/                  |             |
|    approx_kl            | 0.006087631 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.149       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0921      |
|    n_updates            | 1058        |
|    policy_gradient_loss | -0.00304    |
|    value_loss           | 0.387       |
-----------------------------------------
Eval num_timesteps=699000, episode_reward=1.86 +/- 2.61
Episode length: 69.74 +/- 23.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.7     |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 699000   |
---------------------------------
Eval num_timesteps=699500, episode_reward=2.66 +/- 2.67
Episode length: 74.28 +/- 25.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.3     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 699500   |
---------------------------------
Eval num_timesteps=700000, episode_reward=2.52 +/- 2.74
Episode length: 78.02 +/- 24.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.7     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 128      |
|    iterations      | 342      |
|    time_elapsed    | 5455     |
|    total_timesteps | 700416   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=700500, episode_reward=2.36 +/- 3.08
Episode length: 75.02 +/- 21.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | 2.36         |
| time/                   |              |
|    total_timesteps      | 700500       |
| train/                  |              |
|    approx_kl            | 0.0054613966 |
|    clip_fraction        | 0.0407       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.465       |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.178        |
|    n_updates            | 1061         |
|    policy_gradient_loss | -0.000767    |
|    value_loss           | 0.223        |
------------------------------------------
Eval num_timesteps=701000, episode_reward=2.78 +/- 3.25
Episode length: 75.88 +/- 25.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 701000   |
---------------------------------
Eval num_timesteps=701500, episode_reward=2.78 +/- 3.31
Episode length: 75.38 +/- 29.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 701500   |
---------------------------------
Eval num_timesteps=702000, episode_reward=2.58 +/- 3.83
Episode length: 76.86 +/- 28.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 702000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77       |
|    ep_rew_mean     | 2.07     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 343      |
|    time_elapsed    | 5469     |
|    total_timesteps | 702464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=702500, episode_reward=3.20 +/- 3.50
Episode length: 84.48 +/- 26.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.5         |
|    mean_reward          | 3.2          |
| time/                   |              |
|    total_timesteps      | 702500       |
| train/                  |              |
|    approx_kl            | 0.0032632798 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.419       |
|    explained_variance   | 0.0785       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.381        |
|    n_updates            | 1062         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 0.317        |
------------------------------------------
Eval num_timesteps=703000, episode_reward=1.94 +/- 2.79
Episode length: 73.42 +/- 22.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.4     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 703000   |
---------------------------------
Eval num_timesteps=703500, episode_reward=2.28 +/- 3.33
Episode length: 74.72 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 703500   |
---------------------------------
Eval num_timesteps=704000, episode_reward=2.12 +/- 2.30
Episode length: 74.70 +/- 23.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 704000   |
---------------------------------
Eval num_timesteps=704500, episode_reward=2.48 +/- 3.21
Episode length: 76.94 +/- 20.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 704500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.1     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 344      |
|    time_elapsed    | 5487     |
|    total_timesteps | 704512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=705000, episode_reward=2.02 +/- 3.04
Episode length: 68.32 +/- 23.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 68.3         |
|    mean_reward          | 2.02         |
| time/                   |              |
|    total_timesteps      | 705000       |
| train/                  |              |
|    approx_kl            | 0.0033888665 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.325       |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.191        |
|    n_updates            | 1063         |
|    policy_gradient_loss | -0.00203     |
|    value_loss           | 0.42         |
------------------------------------------
Eval num_timesteps=705500, episode_reward=2.10 +/- 2.82
Episode length: 78.20 +/- 23.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 705500   |
---------------------------------
Eval num_timesteps=706000, episode_reward=2.30 +/- 3.30
Episode length: 75.14 +/- 28.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 706000   |
---------------------------------
Eval num_timesteps=706500, episode_reward=2.78 +/- 2.98
Episode length: 78.88 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.9     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 706500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.6     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 345      |
|    time_elapsed    | 5501     |
|    total_timesteps | 706560   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=707000, episode_reward=2.24 +/- 3.02
Episode length: 76.78 +/- 24.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.8         |
|    mean_reward          | 2.24         |
| time/                   |              |
|    total_timesteps      | 707000       |
| train/                  |              |
|    approx_kl            | 0.0037234065 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.289       |
|    explained_variance   | 0.131        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.186        |
|    n_updates            | 1065         |
|    policy_gradient_loss | -0.0033      |
|    value_loss           | 0.442        |
------------------------------------------
Eval num_timesteps=707500, episode_reward=2.12 +/- 3.03
Episode length: 77.66 +/- 24.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 707500   |
---------------------------------
Eval num_timesteps=708000, episode_reward=2.44 +/- 3.85
Episode length: 77.42 +/- 23.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 708000   |
---------------------------------
Eval num_timesteps=708500, episode_reward=2.20 +/- 2.83
Episode length: 80.70 +/- 24.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.7     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 708500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.8     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 346      |
|    time_elapsed    | 5516     |
|    total_timesteps | 708608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=709000, episode_reward=2.74 +/- 3.05
Episode length: 77.96 +/- 28.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78          |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 709000      |
| train/                  |             |
|    approx_kl            | 0.002720522 |
|    clip_fraction        | 0.0307      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.147       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.185       |
|    n_updates            | 1066        |
|    policy_gradient_loss | -0.00103    |
|    value_loss           | 0.396       |
-----------------------------------------
Eval num_timesteps=709500, episode_reward=1.72 +/- 2.58
Episode length: 70.22 +/- 26.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 709500   |
---------------------------------
Eval num_timesteps=710000, episode_reward=2.42 +/- 2.80
Episode length: 76.12 +/- 21.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.1     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 710000   |
---------------------------------
Eval num_timesteps=710500, episode_reward=2.24 +/- 2.55
Episode length: 77.92 +/- 21.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 710500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.7     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 347      |
|    time_elapsed    | 5530     |
|    total_timesteps | 710656   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=711000, episode_reward=2.46 +/- 3.17
Episode length: 85.78 +/- 28.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 85.8         |
|    mean_reward          | 2.46         |
| time/                   |              |
|    total_timesteps      | 711000       |
| train/                  |              |
|    approx_kl            | 0.0038933605 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0417       |
|    n_updates            | 1069         |
|    policy_gradient_loss | -0.000432    |
|    value_loss           | 0.305        |
------------------------------------------
Eval num_timesteps=711500, episode_reward=2.20 +/- 3.25
Episode length: 80.72 +/- 27.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.7     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 711500   |
---------------------------------
Eval num_timesteps=712000, episode_reward=2.54 +/- 3.11
Episode length: 75.54 +/- 24.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.5     |
|    mean_reward     | 2.54     |
| time/              |          |
|    total_timesteps | 712000   |
---------------------------------
Eval num_timesteps=712500, episode_reward=1.92 +/- 2.97
Episode length: 75.74 +/- 27.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 712500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.4     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 348      |
|    time_elapsed    | 5545     |
|    total_timesteps | 712704   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=713000, episode_reward=1.62 +/- 2.51
Episode length: 68.26 +/- 21.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.3        |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 713000      |
| train/                  |             |
|    approx_kl            | 0.005732591 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.113       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.133       |
|    n_updates            | 1072        |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 0.363       |
-----------------------------------------
Eval num_timesteps=713500, episode_reward=2.90 +/- 3.66
Episode length: 80.12 +/- 25.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 713500   |
---------------------------------
Eval num_timesteps=714000, episode_reward=3.44 +/- 5.12
Episode length: 80.28 +/- 30.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 3.44     |
| time/              |          |
|    total_timesteps | 714000   |
---------------------------------
Eval num_timesteps=714500, episode_reward=2.98 +/- 3.81
Episode length: 80.28 +/- 30.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 714500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.3     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 349      |
|    time_elapsed    | 5560     |
|    total_timesteps | 714752   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=715000, episode_reward=3.56 +/- 3.90
Episode length: 84.72 +/- 26.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.7         |
|    mean_reward          | 3.56         |
| time/                   |              |
|    total_timesteps      | 715000       |
| train/                  |              |
|    approx_kl            | 0.0070122383 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.252       |
|    explained_variance   | 0.194        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.128        |
|    n_updates            | 1075         |
|    policy_gradient_loss | -0.00109     |
|    value_loss           | 0.437        |
------------------------------------------
Eval num_timesteps=715500, episode_reward=2.38 +/- 2.86
Episode length: 76.36 +/- 19.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.4     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 715500   |
---------------------------------
Eval num_timesteps=716000, episode_reward=2.58 +/- 3.25
Episode length: 77.42 +/- 26.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 716000   |
---------------------------------
Eval num_timesteps=716500, episode_reward=2.10 +/- 3.47
Episode length: 77.06 +/- 23.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 716500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.5     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 128      |
|    iterations      | 350      |
|    time_elapsed    | 5574     |
|    total_timesteps | 716800   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=717000, episode_reward=2.66 +/- 3.47
Episode length: 78.10 +/- 25.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.1         |
|    mean_reward          | 2.66         |
| time/                   |              |
|    total_timesteps      | 717000       |
| train/                  |              |
|    approx_kl            | 0.0033835103 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.289       |
|    explained_variance   | 0.0443       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.173        |
|    n_updates            | 1078         |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 0.29         |
------------------------------------------
Eval num_timesteps=717500, episode_reward=1.54 +/- 2.47
Episode length: 70.64 +/- 22.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.6     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 717500   |
---------------------------------
Eval num_timesteps=718000, episode_reward=2.72 +/- 3.54
Episode length: 78.24 +/- 25.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 718000   |
---------------------------------
Eval num_timesteps=718500, episode_reward=3.10 +/- 4.21
Episode length: 73.90 +/- 20.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 718500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.2     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 351      |
|    time_elapsed    | 5588     |
|    total_timesteps | 718848   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=719000, episode_reward=2.08 +/- 2.46
Episode length: 76.48 +/- 24.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.5         |
|    mean_reward          | 2.08         |
| time/                   |              |
|    total_timesteps      | 719000       |
| train/                  |              |
|    approx_kl            | 0.0046036597 |
|    clip_fraction        | 0.0322       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.246       |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0975       |
|    n_updates            | 1081         |
|    policy_gradient_loss | -0.000656    |
|    value_loss           | 0.325        |
------------------------------------------
Eval num_timesteps=719500, episode_reward=1.54 +/- 2.06
Episode length: 73.08 +/- 24.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 719500   |
---------------------------------
Eval num_timesteps=720000, episode_reward=2.32 +/- 2.81
Episode length: 72.92 +/- 25.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=720500, episode_reward=2.52 +/- 2.96
Episode length: 77.40 +/- 23.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 720500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.6     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 128      |
|    iterations      | 352      |
|    time_elapsed    | 5602     |
|    total_timesteps | 720896   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=721000, episode_reward=2.94 +/- 4.04
Episode length: 78.84 +/- 25.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.8         |
|    mean_reward          | 2.94         |
| time/                   |              |
|    total_timesteps      | 721000       |
| train/                  |              |
|    approx_kl            | 0.0043217987 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.203       |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.307        |
|    n_updates            | 1083         |
|    policy_gradient_loss | 1.29e-05     |
|    value_loss           | 0.487        |
------------------------------------------
Eval num_timesteps=721500, episode_reward=2.34 +/- 3.10
Episode length: 82.80 +/- 27.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.8     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 721500   |
---------------------------------
Eval num_timesteps=722000, episode_reward=3.04 +/- 3.32
Episode length: 83.76 +/- 27.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.8     |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 722000   |
---------------------------------
Eval num_timesteps=722500, episode_reward=2.18 +/- 2.74
Episode length: 74.56 +/- 26.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 722500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 353      |
|    time_elapsed    | 5617     |
|    total_timesteps | 722944   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=723000, episode_reward=2.76 +/- 3.64
Episode length: 74.64 +/- 21.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.6        |
|    mean_reward          | 2.76        |
| time/                   |             |
|    total_timesteps      | 723000      |
| train/                  |             |
|    approx_kl            | 0.005183635 |
|    clip_fraction        | 0.0327      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.186       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.631       |
|    n_updates            | 1087        |
|    policy_gradient_loss | -0.00305    |
|    value_loss           | 0.443       |
-----------------------------------------
Eval num_timesteps=723500, episode_reward=2.52 +/- 3.02
Episode length: 74.12 +/- 21.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.1     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 723500   |
---------------------------------
Eval num_timesteps=724000, episode_reward=2.30 +/- 2.93
Episode length: 75.84 +/- 24.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.8     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 724000   |
---------------------------------
Eval num_timesteps=724500, episode_reward=2.10 +/- 2.29
Episode length: 80.96 +/- 25.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81       |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 724500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.3     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 354      |
|    time_elapsed    | 5631     |
|    total_timesteps | 724992   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=725000, episode_reward=2.54 +/- 3.21
Episode length: 77.46 +/- 25.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.5        |
|    mean_reward          | 2.54        |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.014948317 |
|    clip_fraction        | 0.0327      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.295      |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0817      |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00441    |
|    value_loss           | 0.354       |
-----------------------------------------
Eval num_timesteps=725500, episode_reward=2.76 +/- 3.40
Episode length: 80.88 +/- 21.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.9     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 725500   |
---------------------------------
Eval num_timesteps=726000, episode_reward=2.76 +/- 3.55
Episode length: 74.56 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 726000   |
---------------------------------
Eval num_timesteps=726500, episode_reward=2.34 +/- 2.48
Episode length: 79.18 +/- 26.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.2     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 726500   |
---------------------------------
Eval num_timesteps=727000, episode_reward=2.20 +/- 2.78
Episode length: 76.66 +/- 25.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.7     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 727000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.2     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 355      |
|    time_elapsed    | 5649     |
|    total_timesteps | 727040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=727500, episode_reward=2.36 +/- 3.01
Episode length: 74.72 +/- 27.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 74.7         |
|    mean_reward          | 2.36         |
| time/                   |              |
|    total_timesteps      | 727500       |
| train/                  |              |
|    approx_kl            | 0.0059776073 |
|    clip_fraction        | 0.0558       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.406       |
|    explained_variance   | 0.176        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.138        |
|    n_updates            | 1091         |
|    policy_gradient_loss | 0.00101      |
|    value_loss           | 0.469        |
------------------------------------------
Eval num_timesteps=728000, episode_reward=2.72 +/- 3.24
Episode length: 76.00 +/- 21.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 728000   |
---------------------------------
Eval num_timesteps=728500, episode_reward=3.78 +/- 4.10
Episode length: 74.18 +/- 25.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.2     |
|    mean_reward     | 3.78     |
| time/              |          |
|    total_timesteps | 728500   |
---------------------------------
Eval num_timesteps=729000, episode_reward=2.36 +/- 2.99
Episode length: 81.02 +/- 27.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81       |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 729000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.6     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 356      |
|    time_elapsed    | 5664     |
|    total_timesteps | 729088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=729500, episode_reward=2.42 +/- 3.05
Episode length: 77.66 +/- 25.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.7         |
|    mean_reward          | 2.42         |
| time/                   |              |
|    total_timesteps      | 729500       |
| train/                  |              |
|    approx_kl            | 0.0054731304 |
|    clip_fraction        | 0.0551       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.412       |
|    explained_variance   | 0.0408       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.101        |
|    n_updates            | 1092         |
|    policy_gradient_loss | 0.00347      |
|    value_loss           | 0.245        |
------------------------------------------
Eval num_timesteps=730000, episode_reward=2.60 +/- 3.03
Episode length: 74.24 +/- 24.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.2     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 730000   |
---------------------------------
Eval num_timesteps=730500, episode_reward=2.44 +/- 3.14
Episode length: 70.74 +/- 25.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.7     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 730500   |
---------------------------------
Eval num_timesteps=731000, episode_reward=3.04 +/- 3.64
Episode length: 78.60 +/- 24.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 731000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76       |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 357      |
|    time_elapsed    | 5678     |
|    total_timesteps | 731136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=731500, episode_reward=2.46 +/- 3.25
Episode length: 77.72 +/- 24.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.7         |
|    mean_reward          | 2.46         |
| time/                   |              |
|    total_timesteps      | 731500       |
| train/                  |              |
|    approx_kl            | 0.0029038608 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.374       |
|    explained_variance   | 0.129        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.282        |
|    n_updates            | 1093         |
|    policy_gradient_loss | 0.00128      |
|    value_loss           | 0.524        |
------------------------------------------
Eval num_timesteps=732000, episode_reward=2.38 +/- 3.57
Episode length: 75.98 +/- 22.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 732000   |
---------------------------------
Eval num_timesteps=732500, episode_reward=2.28 +/- 3.15
Episode length: 71.14 +/- 28.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.1     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 732500   |
---------------------------------
Eval num_timesteps=733000, episode_reward=3.26 +/- 3.62
Episode length: 81.18 +/- 27.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.2     |
|    mean_reward     | 3.26     |
| time/              |          |
|    total_timesteps | 733000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.2     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 358      |
|    time_elapsed    | 5692     |
|    total_timesteps | 733184   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=733500, episode_reward=3.08 +/- 3.90
Episode length: 84.38 +/- 30.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.4         |
|    mean_reward          | 3.08         |
| time/                   |              |
|    total_timesteps      | 733500       |
| train/                  |              |
|    approx_kl            | 0.0075284205 |
|    clip_fraction        | 0.0558       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.405       |
|    explained_variance   | 0.164        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.179        |
|    n_updates            | 1096         |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 0.307        |
------------------------------------------
Eval num_timesteps=734000, episode_reward=2.46 +/- 3.53
Episode length: 74.88 +/- 25.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | 2.46     |
| time/              |          |
|    total_timesteps | 734000   |
---------------------------------
Eval num_timesteps=734500, episode_reward=3.10 +/- 4.29
Episode length: 82.24 +/- 31.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.2     |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 734500   |
---------------------------------
Eval num_timesteps=735000, episode_reward=3.40 +/- 3.65
Episode length: 80.24 +/- 30.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 3.4      |
| time/              |          |
|    total_timesteps | 735000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.8     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 128      |
|    iterations      | 359      |
|    time_elapsed    | 5708     |
|    total_timesteps | 735232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=735500, episode_reward=2.38 +/- 3.49
Episode length: 74.14 +/- 25.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 74.1         |
|    mean_reward          | 2.38         |
| time/                   |              |
|    total_timesteps      | 735500       |
| train/                  |              |
|    approx_kl            | 0.0027274915 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.351       |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.28         |
|    n_updates            | 1097         |
|    policy_gradient_loss | 0.00346      |
|    value_loss           | 0.47         |
------------------------------------------
Eval num_timesteps=736000, episode_reward=2.88 +/- 3.67
Episode length: 77.64 +/- 27.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 736000   |
---------------------------------
Eval num_timesteps=736500, episode_reward=2.50 +/- 3.05
Episode length: 82.66 +/- 25.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.7     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 736500   |
---------------------------------
Eval num_timesteps=737000, episode_reward=2.02 +/- 2.96
Episode length: 75.34 +/- 25.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.3     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 737000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.7     |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 360      |
|    time_elapsed    | 5722     |
|    total_timesteps | 737280   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=737500, episode_reward=2.84 +/- 3.28
Episode length: 84.12 +/- 22.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.1        |
|    mean_reward          | 2.84        |
| time/                   |             |
|    total_timesteps      | 737500      |
| train/                  |             |
|    approx_kl            | 0.010901797 |
|    clip_fraction        | 0.03        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.323      |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.174       |
|    n_updates            | 1099        |
|    policy_gradient_loss | -0.000326   |
|    value_loss           | 0.505       |
-----------------------------------------
Eval num_timesteps=738000, episode_reward=2.74 +/- 3.51
Episode length: 79.22 +/- 28.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.2     |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 738000   |
---------------------------------
Eval num_timesteps=738500, episode_reward=3.72 +/- 4.04
Episode length: 80.28 +/- 32.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 3.72     |
| time/              |          |
|    total_timesteps | 738500   |
---------------------------------
Eval num_timesteps=739000, episode_reward=2.42 +/- 2.71
Episode length: 75.44 +/- 23.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 739000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.8     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 361      |
|    time_elapsed    | 5737     |
|    total_timesteps | 739328   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=739500, episode_reward=3.22 +/- 3.08
Episode length: 79.06 +/- 29.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79.1         |
|    mean_reward          | 3.22         |
| time/                   |              |
|    total_timesteps      | 739500       |
| train/                  |              |
|    approx_kl            | 0.0044912947 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.316       |
|    explained_variance   | 0.099        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.186        |
|    n_updates            | 1101         |
|    policy_gradient_loss | 0.000697     |
|    value_loss           | 0.375        |
------------------------------------------
Eval num_timesteps=740000, episode_reward=2.94 +/- 4.12
Episode length: 79.48 +/- 26.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=740500, episode_reward=3.06 +/- 3.91
Episode length: 75.02 +/- 24.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 740500   |
---------------------------------
Eval num_timesteps=741000, episode_reward=2.20 +/- 2.91
Episode length: 80.64 +/- 25.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.6     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 741000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.6     |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 362      |
|    time_elapsed    | 5752     |
|    total_timesteps | 741376   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=741500, episode_reward=2.30 +/- 3.57
Episode length: 75.72 +/- 20.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.7         |
|    mean_reward          | 2.3          |
| time/                   |              |
|    total_timesteps      | 741500       |
| train/                  |              |
|    approx_kl            | 0.0034562885 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.219       |
|    explained_variance   | 0.209        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.241        |
|    n_updates            | 1103         |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 0.398        |
------------------------------------------
Eval num_timesteps=742000, episode_reward=2.04 +/- 2.20
Episode length: 72.60 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.6     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 742000   |
---------------------------------
Eval num_timesteps=742500, episode_reward=2.46 +/- 2.49
Episode length: 76.68 +/- 23.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.7     |
|    mean_reward     | 2.46     |
| time/              |          |
|    total_timesteps | 742500   |
---------------------------------
Eval num_timesteps=743000, episode_reward=1.76 +/- 2.41
Episode length: 74.96 +/- 19.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 743000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 363      |
|    time_elapsed    | 5767     |
|    total_timesteps | 743424   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=743500, episode_reward=1.96 +/- 2.70
Episode length: 70.86 +/- 26.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.9         |
|    mean_reward          | 1.96         |
| time/                   |              |
|    total_timesteps      | 743500       |
| train/                  |              |
|    approx_kl            | 0.0047496557 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.239       |
|    explained_variance   | 0.217        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.136        |
|    n_updates            | 1105         |
|    policy_gradient_loss | -0.00028     |
|    value_loss           | 0.34         |
------------------------------------------
Eval num_timesteps=744000, episode_reward=1.92 +/- 2.78
Episode length: 72.60 +/- 23.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.6     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 744000   |
---------------------------------
Eval num_timesteps=744500, episode_reward=2.90 +/- 3.44
Episode length: 76.64 +/- 22.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 744500   |
---------------------------------
Eval num_timesteps=745000, episode_reward=2.80 +/- 3.69
Episode length: 76.88 +/- 29.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 745000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.2     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 364      |
|    time_elapsed    | 5781     |
|    total_timesteps | 745472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=745500, episode_reward=1.42 +/- 2.41
Episode length: 72.60 +/- 24.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.6        |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 745500      |
| train/                  |             |
|    approx_kl            | 0.004937907 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.2        |
|    explained_variance   | 0.0935      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.34        |
|    n_updates            | 1106        |
|    policy_gradient_loss | -0.000264   |
|    value_loss           | 0.636       |
-----------------------------------------
Eval num_timesteps=746000, episode_reward=2.36 +/- 2.99
Episode length: 75.72 +/- 25.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 746000   |
---------------------------------
Eval num_timesteps=746500, episode_reward=2.36 +/- 3.49
Episode length: 83.56 +/- 27.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.6     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 746500   |
---------------------------------
Eval num_timesteps=747000, episode_reward=3.30 +/- 3.99
Episode length: 73.74 +/- 20.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | 3.3      |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=747500, episode_reward=2.44 +/- 2.99
Episode length: 76.94 +/- 21.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 747500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.3     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 365      |
|    time_elapsed    | 5798     |
|    total_timesteps | 747520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=748000, episode_reward=2.44 +/- 3.35
Episode length: 80.36 +/- 25.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 80.4         |
|    mean_reward          | 2.44         |
| time/                   |              |
|    total_timesteps      | 748000       |
| train/                  |              |
|    approx_kl            | 0.0038924403 |
|    clip_fraction        | 0.0422       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.278       |
|    explained_variance   | 0.21         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0941       |
|    n_updates            | 1107         |
|    policy_gradient_loss | -0.000826    |
|    value_loss           | 0.263        |
------------------------------------------
Eval num_timesteps=748500, episode_reward=2.06 +/- 3.73
Episode length: 73.96 +/- 26.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74       |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 748500   |
---------------------------------
Eval num_timesteps=749000, episode_reward=1.96 +/- 3.03
Episode length: 73.02 +/- 28.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73       |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 749000   |
---------------------------------
Eval num_timesteps=749500, episode_reward=2.86 +/- 3.84
Episode length: 73.72 +/- 24.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 749500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.3     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 366      |
|    time_elapsed    | 5812     |
|    total_timesteps | 749568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=750000, episode_reward=2.78 +/- 3.20
Episode length: 74.22 +/- 18.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 74.2         |
|    mean_reward          | 2.78         |
| time/                   |              |
|    total_timesteps      | 750000       |
| train/                  |              |
|    approx_kl            | 0.0028804494 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.201       |
|    explained_variance   | 0.246        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.421        |
|    n_updates            | 1108         |
|    policy_gradient_loss | 0.000927     |
|    value_loss           | 0.352        |
------------------------------------------
Eval num_timesteps=750500, episode_reward=2.72 +/- 3.16
Episode length: 81.36 +/- 27.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.4     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 750500   |
---------------------------------
Eval num_timesteps=751000, episode_reward=2.30 +/- 2.79
Episode length: 77.94 +/- 23.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 751000   |
---------------------------------
Eval num_timesteps=751500, episode_reward=2.54 +/- 3.65
Episode length: 69.80 +/- 21.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.8     |
|    mean_reward     | 2.54     |
| time/              |          |
|    total_timesteps | 751500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.1     |
|    ep_rew_mean     | 2.53     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 367      |
|    time_elapsed    | 5826     |
|    total_timesteps | 751616   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=752000, episode_reward=1.92 +/- 2.34
Episode length: 79.20 +/- 23.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79.2         |
|    mean_reward          | 1.92         |
| time/                   |              |
|    total_timesteps      | 752000       |
| train/                  |              |
|    approx_kl            | 0.0058835414 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.192       |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.116        |
|    n_updates            | 1111         |
|    policy_gradient_loss | -0.000799    |
|    value_loss           | 0.404        |
------------------------------------------
Eval num_timesteps=752500, episode_reward=2.66 +/- 3.28
Episode length: 77.46 +/- 22.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.5     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 752500   |
---------------------------------
Eval num_timesteps=753000, episode_reward=2.06 +/- 2.60
Episode length: 75.04 +/- 27.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 753000   |
---------------------------------
Eval num_timesteps=753500, episode_reward=3.68 +/- 4.47
Episode length: 74.94 +/- 23.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | 3.68     |
| time/              |          |
|    total_timesteps | 753500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.1     |
|    ep_rew_mean     | 2.33     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 368      |
|    time_elapsed    | 5841     |
|    total_timesteps | 753664   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=754000, episode_reward=2.16 +/- 2.32
Episode length: 77.58 +/- 24.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.6         |
|    mean_reward          | 2.16         |
| time/                   |              |
|    total_timesteps      | 754000       |
| train/                  |              |
|    approx_kl            | 0.0073773707 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.144        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0859       |
|    n_updates            | 1114         |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 0.305        |
------------------------------------------
Eval num_timesteps=754500, episode_reward=2.20 +/- 2.84
Episode length: 77.68 +/- 23.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 754500   |
---------------------------------
Eval num_timesteps=755000, episode_reward=3.16 +/- 4.58
Episode length: 83.98 +/- 33.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84       |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 755000   |
---------------------------------
Eval num_timesteps=755500, episode_reward=3.02 +/- 3.59
Episode length: 79.92 +/- 28.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 3.02     |
| time/              |          |
|    total_timesteps | 755500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 369      |
|    time_elapsed    | 5856     |
|    total_timesteps | 755712   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=756000, episode_reward=2.58 +/- 2.91
Episode length: 81.82 +/- 27.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 81.8       |
|    mean_reward          | 2.58       |
| time/                   |            |
|    total_timesteps      | 756000     |
| train/                  |            |
|    approx_kl            | 0.00632335 |
|    clip_fraction        | 0.0427     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.236     |
|    explained_variance   | 0.23       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0984     |
|    n_updates            | 1116       |
|    policy_gradient_loss | -0.000399  |
|    value_loss           | 0.322      |
----------------------------------------
Eval num_timesteps=756500, episode_reward=3.54 +/- 4.08
Episode length: 78.18 +/- 21.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 3.54     |
| time/              |          |
|    total_timesteps | 756500   |
---------------------------------
Eval num_timesteps=757000, episode_reward=1.92 +/- 2.23
Episode length: 72.30 +/- 23.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 757000   |
---------------------------------
Eval num_timesteps=757500, episode_reward=1.88 +/- 2.60
Episode length: 71.34 +/- 23.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.3     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 757500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.3     |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 370      |
|    time_elapsed    | 5870     |
|    total_timesteps | 757760   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=758000, episode_reward=2.62 +/- 3.65
Episode length: 74.48 +/- 24.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.5        |
|    mean_reward          | 2.62        |
| time/                   |             |
|    total_timesteps      | 758000      |
| train/                  |             |
|    approx_kl            | 0.014706664 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.265      |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.599       |
|    n_updates            | 1118        |
|    policy_gradient_loss | 0.000443    |
|    value_loss           | 0.751       |
-----------------------------------------
Eval num_timesteps=758500, episode_reward=2.48 +/- 2.77
Episode length: 73.52 +/- 25.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 758500   |
---------------------------------
Eval num_timesteps=759000, episode_reward=2.14 +/- 3.18
Episode length: 79.74 +/- 31.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 759000   |
---------------------------------
Eval num_timesteps=759500, episode_reward=2.64 +/- 3.55
Episode length: 77.42 +/- 26.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.64     |
| time/              |          |
|    total_timesteps | 759500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.2     |
|    ep_rew_mean     | 3.2      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 371      |
|    time_elapsed    | 5885     |
|    total_timesteps | 759808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=760000, episode_reward=2.12 +/- 2.65
Episode length: 70.74 +/- 23.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.7         |
|    mean_reward          | 2.12         |
| time/                   |              |
|    total_timesteps      | 760000       |
| train/                  |              |
|    approx_kl            | 0.0040934575 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.309       |
|    explained_variance   | 0.24         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.278        |
|    n_updates            | 1119         |
|    policy_gradient_loss | 0.000673     |
|    value_loss           | 0.559        |
------------------------------------------
Eval num_timesteps=760500, episode_reward=2.90 +/- 3.35
Episode length: 84.60 +/- 27.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.6     |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 760500   |
---------------------------------
Eval num_timesteps=761000, episode_reward=2.26 +/- 4.22
Episode length: 72.02 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72       |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 761000   |
---------------------------------
Eval num_timesteps=761500, episode_reward=2.54 +/- 2.99
Episode length: 80.16 +/- 26.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 2.54     |
| time/              |          |
|    total_timesteps | 761500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.8     |
|    ep_rew_mean     | 3.62     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 372      |
|    time_elapsed    | 5900     |
|    total_timesteps | 761856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=762000, episode_reward=2.78 +/- 3.79
Episode length: 78.50 +/- 27.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.5         |
|    mean_reward          | 2.78         |
| time/                   |              |
|    total_timesteps      | 762000       |
| train/                  |              |
|    approx_kl            | 0.0048169694 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.23        |
|    explained_variance   | 0.275        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.198        |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00216     |
|    value_loss           | 0.596        |
------------------------------------------
Eval num_timesteps=762500, episode_reward=2.94 +/- 3.33
Episode length: 79.30 +/- 24.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.3     |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 762500   |
---------------------------------
Eval num_timesteps=763000, episode_reward=2.24 +/- 2.29
Episode length: 74.38 +/- 19.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 763000   |
---------------------------------
Eval num_timesteps=763500, episode_reward=3.78 +/- 4.44
Episode length: 82.06 +/- 27.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 3.78     |
| time/              |          |
|    total_timesteps | 763500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.5     |
|    ep_rew_mean     | 3.65     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 373      |
|    time_elapsed    | 5914     |
|    total_timesteps | 763904   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=764000, episode_reward=2.88 +/- 3.33
Episode length: 76.82 +/- 23.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.8        |
|    mean_reward          | 2.88        |
| time/                   |             |
|    total_timesteps      | 764000      |
| train/                  |             |
|    approx_kl            | 0.008751942 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.18        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.162       |
|    n_updates            | 1122        |
|    policy_gradient_loss | 0.000616    |
|    value_loss           | 0.358       |
-----------------------------------------
Eval num_timesteps=764500, episode_reward=2.08 +/- 2.69
Episode length: 73.56 +/- 23.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.6     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 764500   |
---------------------------------
Eval num_timesteps=765000, episode_reward=2.44 +/- 3.02
Episode length: 73.52 +/- 23.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 765000   |
---------------------------------
Eval num_timesteps=765500, episode_reward=3.34 +/- 3.47
Episode length: 82.90 +/- 22.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.9     |
|    mean_reward     | 3.34     |
| time/              |          |
|    total_timesteps | 765500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80       |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 374      |
|    time_elapsed    | 5929     |
|    total_timesteps | 765952   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=766000, episode_reward=2.44 +/- 3.60
Episode length: 76.76 +/- 24.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.8        |
|    mean_reward          | 2.44        |
| time/                   |             |
|    total_timesteps      | 766000      |
| train/                  |             |
|    approx_kl            | 0.014959848 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.248      |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.293       |
|    n_updates            | 1124        |
|    policy_gradient_loss | 0.00144     |
|    value_loss           | 0.314       |
-----------------------------------------
Eval num_timesteps=766500, episode_reward=3.18 +/- 4.13
Episode length: 78.80 +/- 23.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.8     |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 766500   |
---------------------------------
Eval num_timesteps=767000, episode_reward=2.12 +/- 2.25
Episode length: 80.14 +/- 25.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 767000   |
---------------------------------
Eval num_timesteps=767500, episode_reward=2.98 +/- 3.40
Episode length: 78.98 +/- 27.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79       |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 767500   |
---------------------------------
Eval num_timesteps=768000, episode_reward=2.28 +/- 2.50
Episode length: 74.30 +/- 23.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.3     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83       |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 375      |
|    time_elapsed    | 5947     |
|    total_timesteps | 768000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=768500, episode_reward=4.02 +/- 4.44
Episode length: 83.36 +/- 23.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 83.4         |
|    mean_reward          | 4.02         |
| time/                   |              |
|    total_timesteps      | 768500       |
| train/                  |              |
|    approx_kl            | 0.0035688556 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.261       |
|    explained_variance   | 0.201        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.263        |
|    n_updates            | 1125         |
|    policy_gradient_loss | 0.00246      |
|    value_loss           | 0.379        |
------------------------------------------
Eval num_timesteps=769000, episode_reward=2.22 +/- 2.66
Episode length: 73.06 +/- 20.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 769000   |
---------------------------------
Eval num_timesteps=769500, episode_reward=2.82 +/- 3.02
Episode length: 75.44 +/- 25.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 769500   |
---------------------------------
Eval num_timesteps=770000, episode_reward=3.00 +/- 3.10
Episode length: 75.42 +/- 25.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 770000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.8     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 376      |
|    time_elapsed    | 5961     |
|    total_timesteps | 770048   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=770500, episode_reward=2.98 +/- 3.62
Episode length: 73.28 +/- 23.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73.3         |
|    mean_reward          | 2.98         |
| time/                   |              |
|    total_timesteps      | 770500       |
| train/                  |              |
|    approx_kl            | 0.0057836697 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.329       |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.234        |
|    n_updates            | 1127         |
|    policy_gradient_loss | 0.000187     |
|    value_loss           | 0.374        |
------------------------------------------
Eval num_timesteps=771000, episode_reward=2.66 +/- 2.86
Episode length: 78.18 +/- 23.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 771000   |
---------------------------------
Eval num_timesteps=771500, episode_reward=2.40 +/- 2.61
Episode length: 78.22 +/- 24.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 771500   |
---------------------------------
Eval num_timesteps=772000, episode_reward=3.64 +/- 4.03
Episode length: 85.58 +/- 30.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.6     |
|    mean_reward     | 3.64     |
| time/              |          |
|    total_timesteps | 772000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 377      |
|    time_elapsed    | 5976     |
|    total_timesteps | 772096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=772500, episode_reward=2.56 +/- 4.30
Episode length: 75.24 +/- 27.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.2         |
|    mean_reward          | 2.56         |
| time/                   |              |
|    total_timesteps      | 772500       |
| train/                  |              |
|    approx_kl            | 0.0043545472 |
|    clip_fraction        | 0.053        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.36        |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.235        |
|    n_updates            | 1128         |
|    policy_gradient_loss | -0.00055     |
|    value_loss           | 0.449        |
------------------------------------------
Eval num_timesteps=773000, episode_reward=2.54 +/- 3.40
Episode length: 84.36 +/- 26.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.4     |
|    mean_reward     | 2.54     |
| time/              |          |
|    total_timesteps | 773000   |
---------------------------------
Eval num_timesteps=773500, episode_reward=1.84 +/- 3.23
Episode length: 69.02 +/- 23.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69       |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 773500   |
---------------------------------
Eval num_timesteps=774000, episode_reward=3.00 +/- 3.76
Episode length: 79.16 +/- 22.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.2     |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 774000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78       |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 378      |
|    time_elapsed    | 5991     |
|    total_timesteps | 774144   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=774500, episode_reward=2.18 +/- 2.85
Episode length: 72.04 +/- 23.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | 2.18        |
| time/                   |             |
|    total_timesteps      | 774500      |
| train/                  |             |
|    approx_kl            | 0.007119222 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.379      |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0804      |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00355    |
|    value_loss           | 0.239       |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=1.72 +/- 2.41
Episode length: 73.94 +/- 25.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 775000   |
---------------------------------
Eval num_timesteps=775500, episode_reward=3.48 +/- 4.27
Episode length: 83.64 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.6     |
|    mean_reward     | 3.48     |
| time/              |          |
|    total_timesteps | 775500   |
---------------------------------
Eval num_timesteps=776000, episode_reward=2.22 +/- 2.72
Episode length: 71.52 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.5     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 776000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.9     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 379      |
|    time_elapsed    | 6006     |
|    total_timesteps | 776192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=776500, episode_reward=2.30 +/- 3.23
Episode length: 74.24 +/- 23.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.2        |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 776500      |
| train/                  |             |
|    approx_kl            | 0.002092766 |
|    clip_fraction        | 0.0164      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.201      |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.399       |
|    n_updates            | 1131        |
|    policy_gradient_loss | 0.000769    |
|    value_loss           | 0.525       |
-----------------------------------------
Eval num_timesteps=777000, episode_reward=2.62 +/- 2.81
Episode length: 78.74 +/- 25.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.7     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 777000   |
---------------------------------
Eval num_timesteps=777500, episode_reward=2.22 +/- 3.29
Episode length: 77.44 +/- 27.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 777500   |
---------------------------------
Eval num_timesteps=778000, episode_reward=3.02 +/- 3.61
Episode length: 79.44 +/- 22.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 3.02     |
| time/              |          |
|    total_timesteps | 778000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.2     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 380      |
|    time_elapsed    | 6020     |
|    total_timesteps | 778240   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=778500, episode_reward=2.56 +/- 3.76
Episode length: 71.26 +/- 21.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 71.3         |
|    mean_reward          | 2.56         |
| time/                   |              |
|    total_timesteps      | 778500       |
| train/                  |              |
|    approx_kl            | 0.0039626383 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.264       |
|    explained_variance   | 0.204        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.217        |
|    n_updates            | 1134         |
|    policy_gradient_loss | -0.000923    |
|    value_loss           | 0.302        |
------------------------------------------
Eval num_timesteps=779000, episode_reward=2.28 +/- 2.80
Episode length: 74.70 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 779000   |
---------------------------------
Eval num_timesteps=779500, episode_reward=3.04 +/- 3.63
Episode length: 81.14 +/- 23.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.1     |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 779500   |
---------------------------------
Eval num_timesteps=780000, episode_reward=3.12 +/- 3.40
Episode length: 81.96 +/- 23.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82       |
|    mean_reward     | 3.12     |
| time/              |          |
|    total_timesteps | 780000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.6     |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 381      |
|    time_elapsed    | 6035     |
|    total_timesteps | 780288   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=780500, episode_reward=2.34 +/- 3.60
Episode length: 77.46 +/- 25.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.5        |
|    mean_reward          | 2.34        |
| time/                   |             |
|    total_timesteps      | 780500      |
| train/                  |             |
|    approx_kl            | 0.008016826 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.198       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.087       |
|    n_updates            | 1139        |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 0.316       |
-----------------------------------------
Eval num_timesteps=781000, episode_reward=3.04 +/- 3.08
Episode length: 79.88 +/- 26.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 781000   |
---------------------------------
Eval num_timesteps=781500, episode_reward=2.98 +/- 3.61
Episode length: 80.18 +/- 24.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 781500   |
---------------------------------
Eval num_timesteps=782000, episode_reward=2.66 +/- 3.22
Episode length: 79.48 +/- 25.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 782000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.4     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 382      |
|    time_elapsed    | 6050     |
|    total_timesteps | 782336   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
Eval num_timesteps=782500, episode_reward=1.86 +/- 2.09
Episode length: 70.44 +/- 21.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.4         |
|    mean_reward          | 1.86         |
| time/                   |              |
|    total_timesteps      | 782500       |
| train/                  |              |
|    approx_kl            | 0.0067330506 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.21        |
|    explained_variance   | 0.17         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.297        |
|    n_updates            | 1143         |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 0.602        |
------------------------------------------
Eval num_timesteps=783000, episode_reward=2.64 +/- 3.57
Episode length: 73.96 +/- 24.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74       |
|    mean_reward     | 2.64     |
| time/              |          |
|    total_timesteps | 783000   |
---------------------------------
Eval num_timesteps=783500, episode_reward=2.92 +/- 4.04
Episode length: 80.88 +/- 29.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.9     |
|    mean_reward     | 2.92     |
| time/              |          |
|    total_timesteps | 783500   |
---------------------------------
Eval num_timesteps=784000, episode_reward=2.38 +/- 3.39
Episode length: 74.70 +/- 28.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 784000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.9     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 383      |
|    time_elapsed    | 6065     |
|    total_timesteps | 784384   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=784500, episode_reward=3.24 +/- 3.61
Episode length: 80.22 +/- 22.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 80.2         |
|    mean_reward          | 3.24         |
| time/                   |              |
|    total_timesteps      | 784500       |
| train/                  |              |
|    approx_kl            | 0.0033975241 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.219       |
|    explained_variance   | 0.213        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0663       |
|    n_updates            | 1145         |
|    policy_gradient_loss | -0.000367    |
|    value_loss           | 0.292        |
------------------------------------------
Eval num_timesteps=785000, episode_reward=2.38 +/- 2.98
Episode length: 75.40 +/- 25.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 785000   |
---------------------------------
Eval num_timesteps=785500, episode_reward=2.66 +/- 3.54
Episode length: 85.36 +/- 36.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.4     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 785500   |
---------------------------------
Eval num_timesteps=786000, episode_reward=2.28 +/- 3.58
Episode length: 74.60 +/- 25.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 786000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.1     |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 384      |
|    time_elapsed    | 6080     |
|    total_timesteps | 786432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=786500, episode_reward=1.98 +/- 2.39
Episode length: 75.54 +/- 21.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.5        |
|    mean_reward          | 1.98        |
| time/                   |             |
|    total_timesteps      | 786500      |
| train/                  |             |
|    approx_kl            | 0.004182319 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.298      |
|    explained_variance   | 0.191       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.102       |
|    n_updates            | 1146        |
|    policy_gradient_loss | 0.000709    |
|    value_loss           | 0.252       |
-----------------------------------------
Eval num_timesteps=787000, episode_reward=2.04 +/- 3.07
Episode length: 72.06 +/- 26.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.1     |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 787000   |
---------------------------------
Eval num_timesteps=787500, episode_reward=1.84 +/- 2.60
Episode length: 77.58 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 787500   |
---------------------------------
Eval num_timesteps=788000, episode_reward=2.20 +/- 2.47
Episode length: 75.78 +/- 21.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.8     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 788000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.7     |
|    ep_rew_mean     | 2.13     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 385      |
|    time_elapsed    | 6094     |
|    total_timesteps | 788480   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=788500, episode_reward=2.84 +/- 4.63
Episode length: 73.52 +/- 27.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73.5         |
|    mean_reward          | 2.84         |
| time/                   |              |
|    total_timesteps      | 788500       |
| train/                  |              |
|    approx_kl            | 0.0058459505 |
|    clip_fraction        | 0.0272       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.255       |
|    explained_variance   | 0.299        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0929       |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00214     |
|    value_loss           | 0.245        |
------------------------------------------
Eval num_timesteps=789000, episode_reward=3.36 +/- 3.84
Episode length: 77.98 +/- 22.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 3.36     |
| time/              |          |
|    total_timesteps | 789000   |
---------------------------------
Eval num_timesteps=789500, episode_reward=3.92 +/- 3.57
Episode length: 85.78 +/- 32.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.8     |
|    mean_reward     | 3.92     |
| time/              |          |
|    total_timesteps | 789500   |
---------------------------------
Eval num_timesteps=790000, episode_reward=2.80 +/- 3.47
Episode length: 80.54 +/- 27.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.5     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 790000   |
---------------------------------
Eval num_timesteps=790500, episode_reward=3.28 +/- 3.32
Episode length: 79.68 +/- 28.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 790500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.1     |
|    ep_rew_mean     | 2.14     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 386      |
|    time_elapsed    | 6113     |
|    total_timesteps | 790528   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=791000, episode_reward=3.30 +/- 3.96
Episode length: 81.94 +/- 26.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81.9         |
|    mean_reward          | 3.3          |
| time/                   |              |
|    total_timesteps      | 791000       |
| train/                  |              |
|    approx_kl            | 0.0057642595 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.274       |
|    explained_variance   | 0.304        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.178        |
|    n_updates            | 1152         |
|    policy_gradient_loss | -0.000991    |
|    value_loss           | 0.294        |
------------------------------------------
Eval num_timesteps=791500, episode_reward=2.68 +/- 3.41
Episode length: 75.10 +/- 24.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 791500   |
---------------------------------
Eval num_timesteps=792000, episode_reward=3.72 +/- 3.77
Episode length: 78.94 +/- 27.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.9     |
|    mean_reward     | 3.72     |
| time/              |          |
|    total_timesteps | 792000   |
---------------------------------
Eval num_timesteps=792500, episode_reward=3.04 +/- 3.64
Episode length: 77.22 +/- 24.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 792500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.4     |
|    ep_rew_mean     | 2.03     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 387      |
|    time_elapsed    | 6128     |
|    total_timesteps | 792576   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=793000, episode_reward=2.02 +/- 2.80
Episode length: 70.94 +/- 22.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.9         |
|    mean_reward          | 2.02         |
| time/                   |              |
|    total_timesteps      | 793000       |
| train/                  |              |
|    approx_kl            | 0.0061453907 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.3         |
|    explained_variance   | 0.00884      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.059        |
|    n_updates            | 1154         |
|    policy_gradient_loss | -0.000313    |
|    value_loss           | 0.32         |
------------------------------------------
Eval num_timesteps=793500, episode_reward=3.22 +/- 3.54
Episode length: 84.72 +/- 24.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.7     |
|    mean_reward     | 3.22     |
| time/              |          |
|    total_timesteps | 793500   |
---------------------------------
Eval num_timesteps=794000, episode_reward=2.60 +/- 4.86
Episode length: 71.38 +/- 22.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.4     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 794000   |
---------------------------------
Eval num_timesteps=794500, episode_reward=2.56 +/- 3.18
Episode length: 77.48 +/- 28.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.5     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 794500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.8     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 388      |
|    time_elapsed    | 6142     |
|    total_timesteps | 794624   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=795000, episode_reward=3.30 +/- 2.81
Episode length: 81.32 +/- 26.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.3        |
|    mean_reward          | 3.3         |
| time/                   |             |
|    total_timesteps      | 795000      |
| train/                  |             |
|    approx_kl            | 0.013843072 |
|    clip_fraction        | 0.0384      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.281      |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0675      |
|    n_updates            | 1156        |
|    policy_gradient_loss | -0.0021     |
|    value_loss           | 0.643       |
-----------------------------------------
Eval num_timesteps=795500, episode_reward=2.92 +/- 3.81
Episode length: 80.92 +/- 30.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.9     |
|    mean_reward     | 2.92     |
| time/              |          |
|    total_timesteps | 795500   |
---------------------------------
Eval num_timesteps=796000, episode_reward=2.88 +/- 3.36
Episode length: 81.48 +/- 29.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.5     |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 796000   |
---------------------------------
Eval num_timesteps=796500, episode_reward=3.32 +/- 3.62
Episode length: 77.54 +/- 22.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.5     |
|    mean_reward     | 3.32     |
| time/              |          |
|    total_timesteps | 796500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.3     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 389      |
|    time_elapsed    | 6157     |
|    total_timesteps | 796672   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=797000, episode_reward=2.38 +/- 3.13
Episode length: 75.80 +/- 27.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.8        |
|    mean_reward          | 2.38        |
| time/                   |             |
|    total_timesteps      | 797000      |
| train/                  |             |
|    approx_kl            | 0.005421447 |
|    clip_fraction        | 0.0413      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.246      |
|    explained_variance   | 0.213       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0568      |
|    n_updates            | 1158        |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 0.417       |
-----------------------------------------
Eval num_timesteps=797500, episode_reward=3.44 +/- 4.27
Episode length: 74.22 +/- 23.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.2     |
|    mean_reward     | 3.44     |
| time/              |          |
|    total_timesteps | 797500   |
---------------------------------
Eval num_timesteps=798000, episode_reward=2.78 +/- 3.21
Episode length: 78.64 +/- 26.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 798000   |
---------------------------------
Eval num_timesteps=798500, episode_reward=3.06 +/- 3.49
Episode length: 79.92 +/- 28.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 798500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.6     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 390      |
|    time_elapsed    | 6172     |
|    total_timesteps | 798720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=799000, episode_reward=3.38 +/- 3.74
Episode length: 82.96 +/- 31.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 83         |
|    mean_reward          | 3.38       |
| time/                   |            |
|    total_timesteps      | 799000     |
| train/                  |            |
|    approx_kl            | 0.00437624 |
|    clip_fraction        | 0.0479     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.291     |
|    explained_variance   | 0.207      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0821     |
|    n_updates            | 1159       |
|    policy_gradient_loss | -0.000953  |
|    value_loss           | 0.424      |
----------------------------------------
Eval num_timesteps=799500, episode_reward=2.50 +/- 3.31
Episode length: 78.52 +/- 24.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 799500   |
---------------------------------
Eval num_timesteps=800000, episode_reward=2.44 +/- 2.76
Episode length: 78.10 +/- 22.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=800500, episode_reward=3.28 +/- 3.98
Episode length: 77.18 +/- 24.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 800500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76       |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 391      |
|    time_elapsed    | 6187     |
|    total_timesteps | 800768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=801000, episode_reward=2.40 +/- 3.24
Episode length: 81.14 +/- 25.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81.1         |
|    mean_reward          | 2.4          |
| time/                   |              |
|    total_timesteps      | 801000       |
| train/                  |              |
|    approx_kl            | 0.0035796023 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.342       |
|    explained_variance   | 0.12         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.134        |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.000265    |
|    value_loss           | 0.246        |
------------------------------------------
Eval num_timesteps=801500, episode_reward=4.08 +/- 4.74
Episode length: 83.58 +/- 28.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.6     |
|    mean_reward     | 4.08     |
| time/              |          |
|    total_timesteps | 801500   |
---------------------------------
New best mean reward!
Eval num_timesteps=802000, episode_reward=2.44 +/- 3.04
Episode length: 76.50 +/- 25.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.5     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 802000   |
---------------------------------
Eval num_timesteps=802500, episode_reward=2.64 +/- 3.23
Episode length: 77.84 +/- 34.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | 2.64     |
| time/              |          |
|    total_timesteps | 802500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.2     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 392      |
|    time_elapsed    | 6203     |
|    total_timesteps | 802816   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=803000, episode_reward=2.42 +/- 2.69
Episode length: 77.96 +/- 23.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78          |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 803000      |
| train/                  |             |
|    approx_kl            | 0.009865556 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.26       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.289       |
|    n_updates            | 1162        |
|    policy_gradient_loss | -0.000959   |
|    value_loss           | 0.36        |
-----------------------------------------
Eval num_timesteps=803500, episode_reward=2.88 +/- 3.61
Episode length: 73.44 +/- 22.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.4     |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 803500   |
---------------------------------
Eval num_timesteps=804000, episode_reward=3.10 +/- 3.16
Episode length: 80.10 +/- 22.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 804000   |
---------------------------------
Eval num_timesteps=804500, episode_reward=3.18 +/- 2.64
Episode length: 85.34 +/- 28.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.3     |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 804500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75       |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 393      |
|    time_elapsed    | 6218     |
|    total_timesteps | 804864   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=805000, episode_reward=2.52 +/- 3.02
Episode length: 76.00 +/- 26.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76          |
|    mean_reward          | 2.52        |
| time/                   |             |
|    total_timesteps      | 805000      |
| train/                  |             |
|    approx_kl            | 0.005841522 |
|    clip_fraction        | 0.0351      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.29       |
|    explained_variance   | 0.0934      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.218       |
|    n_updates            | 1164        |
|    policy_gradient_loss | -0.000832   |
|    value_loss           | 0.417       |
-----------------------------------------
Eval num_timesteps=805500, episode_reward=3.70 +/- 4.20
Episode length: 86.94 +/- 29.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.9     |
|    mean_reward     | 3.7      |
| time/              |          |
|    total_timesteps | 805500   |
---------------------------------
Eval num_timesteps=806000, episode_reward=2.84 +/- 3.24
Episode length: 76.76 +/- 26.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 806000   |
---------------------------------
Eval num_timesteps=806500, episode_reward=2.52 +/- 3.52
Episode length: 73.86 +/- 23.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 806500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.5     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 394      |
|    time_elapsed    | 6233     |
|    total_timesteps | 806912   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=807000, episode_reward=1.78 +/- 2.39
Episode length: 80.58 +/- 25.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.6        |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 807000      |
| train/                  |             |
|    approx_kl            | 0.010839901 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.253      |
|    explained_variance   | 0.184       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.164       |
|    n_updates            | 1166        |
|    policy_gradient_loss | -0.00245    |
|    value_loss           | 0.498       |
-----------------------------------------
Eval num_timesteps=807500, episode_reward=2.72 +/- 3.59
Episode length: 79.76 +/- 31.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.8     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 807500   |
---------------------------------
Eval num_timesteps=808000, episode_reward=2.80 +/- 4.06
Episode length: 79.86 +/- 28.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 808000   |
---------------------------------
Eval num_timesteps=808500, episode_reward=3.44 +/- 3.66
Episode length: 83.90 +/- 25.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.9     |
|    mean_reward     | 3.44     |
| time/              |          |
|    total_timesteps | 808500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.3     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 395      |
|    time_elapsed    | 6248     |
|    total_timesteps | 808960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=809000, episode_reward=2.76 +/- 3.67
Episode length: 70.64 +/- 24.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.6         |
|    mean_reward          | 2.76         |
| time/                   |              |
|    total_timesteps      | 809000       |
| train/                  |              |
|    approx_kl            | 0.0034665067 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.315       |
|    explained_variance   | 0.268        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.222        |
|    n_updates            | 1167         |
|    policy_gradient_loss | 0.00239      |
|    value_loss           | 0.321        |
------------------------------------------
Eval num_timesteps=809500, episode_reward=2.20 +/- 2.56
Episode length: 80.78 +/- 26.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.8     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 809500   |
---------------------------------
Eval num_timesteps=810000, episode_reward=3.74 +/- 4.20
Episode length: 85.22 +/- 27.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.2     |
|    mean_reward     | 3.74     |
| time/              |          |
|    total_timesteps | 810000   |
---------------------------------
Eval num_timesteps=810500, episode_reward=2.90 +/- 3.45
Episode length: 79.78 +/- 22.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.8     |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 810500   |
---------------------------------
Eval num_timesteps=811000, episode_reward=2.60 +/- 3.51
Episode length: 74.42 +/- 20.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 811000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.5     |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 396      |
|    time_elapsed    | 6266     |
|    total_timesteps | 811008   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=811500, episode_reward=2.46 +/- 2.97
Episode length: 75.54 +/- 25.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.5        |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 811500      |
| train/                  |             |
|    approx_kl            | 0.004670621 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.249      |
|    explained_variance   | 0.18        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0471      |
|    n_updates            | 1169        |
|    policy_gradient_loss | -0.00184    |
|    value_loss           | 0.308       |
-----------------------------------------
Eval num_timesteps=812000, episode_reward=2.82 +/- 3.85
Episode length: 79.44 +/- 30.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 812000   |
---------------------------------
Eval num_timesteps=812500, episode_reward=2.46 +/- 2.99
Episode length: 80.24 +/- 31.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 2.46     |
| time/              |          |
|    total_timesteps | 812500   |
---------------------------------
Eval num_timesteps=813000, episode_reward=3.24 +/- 3.00
Episode length: 80.40 +/- 23.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.4     |
|    mean_reward     | 3.24     |
| time/              |          |
|    total_timesteps | 813000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.2     |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 397      |
|    time_elapsed    | 6282     |
|    total_timesteps | 813056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=813500, episode_reward=2.32 +/- 3.90
Episode length: 69.96 +/- 24.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70          |
|    mean_reward          | 2.32        |
| time/                   |             |
|    total_timesteps      | 813500      |
| train/                  |             |
|    approx_kl            | 0.002876142 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.279      |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.118       |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 0.436       |
-----------------------------------------
Eval num_timesteps=814000, episode_reward=3.12 +/- 4.16
Episode length: 77.96 +/- 28.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 3.12     |
| time/              |          |
|    total_timesteps | 814000   |
---------------------------------
Eval num_timesteps=814500, episode_reward=1.94 +/- 2.77
Episode length: 73.18 +/- 20.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.2     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 814500   |
---------------------------------
Eval num_timesteps=815000, episode_reward=2.48 +/- 3.11
Episode length: 75.56 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.6     |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 815000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.7     |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 398      |
|    time_elapsed    | 6297     |
|    total_timesteps | 815104   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=815500, episode_reward=2.88 +/- 3.63
Episode length: 80.94 +/- 25.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.9        |
|    mean_reward          | 2.88        |
| time/                   |             |
|    total_timesteps      | 815500      |
| train/                  |             |
|    approx_kl            | 0.011616316 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.274      |
|    explained_variance   | 0.0822      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.234       |
|    n_updates            | 1173        |
|    policy_gradient_loss | -0.00116    |
|    value_loss           | 0.309       |
-----------------------------------------
Eval num_timesteps=816000, episode_reward=2.34 +/- 2.78
Episode length: 70.20 +/- 22.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 816000   |
---------------------------------
Eval num_timesteps=816500, episode_reward=2.58 +/- 3.05
Episode length: 78.58 +/- 27.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 816500   |
---------------------------------
Eval num_timesteps=817000, episode_reward=2.34 +/- 2.72
Episode length: 78.68 +/- 23.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.7     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 817000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.9     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 399      |
|    time_elapsed    | 6312     |
|    total_timesteps | 817152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=817500, episode_reward=2.10 +/- 2.48
Episode length: 73.06 +/- 23.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 73.1       |
|    mean_reward          | 2.1        |
| time/                   |            |
|    total_timesteps      | 817500     |
| train/                  |            |
|    approx_kl            | 0.00451557 |
|    clip_fraction        | 0.0378     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.236     |
|    explained_variance   | 0.229      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.175      |
|    n_updates            | 1174       |
|    policy_gradient_loss | -4.01e-05  |
|    value_loss           | 0.454      |
----------------------------------------
Eval num_timesteps=818000, episode_reward=3.30 +/- 3.00
Episode length: 84.24 +/- 27.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.2     |
|    mean_reward     | 3.3      |
| time/              |          |
|    total_timesteps | 818000   |
---------------------------------
Eval num_timesteps=818500, episode_reward=1.96 +/- 2.97
Episode length: 78.18 +/- 25.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 818500   |
---------------------------------
Eval num_timesteps=819000, episode_reward=3.10 +/- 3.82
Episode length: 77.94 +/- 22.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 819000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.6     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 400      |
|    time_elapsed    | 6327     |
|    total_timesteps | 819200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=819500, episode_reward=2.66 +/- 3.18
Episode length: 75.98 +/- 29.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76           |
|    mean_reward          | 2.66         |
| time/                   |              |
|    total_timesteps      | 819500       |
| train/                  |              |
|    approx_kl            | 0.0030761336 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0.115        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.115        |
|    n_updates            | 1175         |
|    policy_gradient_loss | -0.000206    |
|    value_loss           | 0.255        |
------------------------------------------
Eval num_timesteps=820000, episode_reward=2.86 +/- 3.84
Episode length: 72.82 +/- 25.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=820500, episode_reward=2.80 +/- 3.22
Episode length: 78.38 +/- 26.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 820500   |
---------------------------------
Eval num_timesteps=821000, episode_reward=2.52 +/- 3.26
Episode length: 79.78 +/- 27.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.8     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 821000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78       |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 401      |
|    time_elapsed    | 6342     |
|    total_timesteps | 821248   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=821500, episode_reward=2.90 +/- 4.17
Episode length: 77.98 +/- 21.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78          |
|    mean_reward          | 2.9         |
| time/                   |             |
|    total_timesteps      | 821500      |
| train/                  |             |
|    approx_kl            | 0.006844066 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.2        |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0571      |
|    n_updates            | 1178        |
|    policy_gradient_loss | 0.00047     |
|    value_loss           | 0.397       |
-----------------------------------------
Eval num_timesteps=822000, episode_reward=2.84 +/- 3.43
Episode length: 75.16 +/- 27.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.2     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 822000   |
---------------------------------
Eval num_timesteps=822500, episode_reward=2.24 +/- 2.53
Episode length: 78.52 +/- 25.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 822500   |
---------------------------------
Eval num_timesteps=823000, episode_reward=2.38 +/- 3.17
Episode length: 75.68 +/- 24.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 823000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 402      |
|    time_elapsed    | 6357     |
|    total_timesteps | 823296   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=823500, episode_reward=2.98 +/- 3.74
Episode length: 80.30 +/- 25.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 80.3       |
|    mean_reward          | 2.98       |
| time/                   |            |
|    total_timesteps      | 823500     |
| train/                  |            |
|    approx_kl            | 0.00458555 |
|    clip_fraction        | 0.0305     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.217     |
|    explained_variance   | 0.229      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.1        |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.00194   |
|    value_loss           | 0.361      |
----------------------------------------
Eval num_timesteps=824000, episode_reward=2.28 +/- 3.21
Episode length: 71.94 +/- 25.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.9     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 824000   |
---------------------------------
Eval num_timesteps=824500, episode_reward=2.72 +/- 3.33
Episode length: 80.94 +/- 31.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.9     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 824500   |
---------------------------------
Eval num_timesteps=825000, episode_reward=2.30 +/- 3.03
Episode length: 74.72 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 825000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.2     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 403      |
|    time_elapsed    | 6372     |
|    total_timesteps | 825344   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=825500, episode_reward=2.42 +/- 2.54
Episode length: 69.12 +/- 21.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 69.1        |
|    mean_reward          | 2.42        |
| time/                   |             |
|    total_timesteps      | 825500      |
| train/                  |             |
|    approx_kl            | 0.007306893 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0.173       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.306       |
|    n_updates            | 1182        |
|    policy_gradient_loss | -0.000654   |
|    value_loss           | 0.349       |
-----------------------------------------
Eval num_timesteps=826000, episode_reward=1.84 +/- 2.80
Episode length: 74.36 +/- 22.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 826000   |
---------------------------------
Eval num_timesteps=826500, episode_reward=3.18 +/- 4.12
Episode length: 75.18 +/- 29.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.2     |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 826500   |
---------------------------------
Eval num_timesteps=827000, episode_reward=2.40 +/- 2.99
Episode length: 73.12 +/- 23.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 827000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81       |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 404      |
|    time_elapsed    | 6386     |
|    total_timesteps | 827392   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=827500, episode_reward=3.08 +/- 3.31
Episode length: 76.86 +/- 25.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.9        |
|    mean_reward          | 3.08        |
| time/                   |             |
|    total_timesteps      | 827500      |
| train/                  |             |
|    approx_kl            | 0.004434716 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.235      |
|    explained_variance   | 0.268       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.107       |
|    n_updates            | 1185        |
|    policy_gradient_loss | -0.000579   |
|    value_loss           | 0.595       |
-----------------------------------------
Eval num_timesteps=828000, episode_reward=3.22 +/- 4.67
Episode length: 73.92 +/- 20.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 3.22     |
| time/              |          |
|    total_timesteps | 828000   |
---------------------------------
Eval num_timesteps=828500, episode_reward=3.04 +/- 3.30
Episode length: 82.20 +/- 27.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.2     |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 828500   |
---------------------------------
Eval num_timesteps=829000, episode_reward=2.40 +/- 3.54
Episode length: 79.00 +/- 24.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79       |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 829000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.8     |
|    ep_rew_mean     | 3.3      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 405      |
|    time_elapsed    | 6401     |
|    total_timesteps | 829440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=829500, episode_reward=1.70 +/- 2.09
Episode length: 80.06 +/- 27.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 80.1         |
|    mean_reward          | 1.7          |
| time/                   |              |
|    total_timesteps      | 829500       |
| train/                  |              |
|    approx_kl            | 0.0024912397 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | 0.222        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.28         |
|    n_updates            | 1186         |
|    policy_gradient_loss | 0.000695     |
|    value_loss           | 0.444        |
------------------------------------------
Eval num_timesteps=830000, episode_reward=2.72 +/- 4.62
Episode length: 72.10 +/- 30.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.1     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 830000   |
---------------------------------
Eval num_timesteps=830500, episode_reward=2.42 +/- 2.51
Episode length: 77.62 +/- 24.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 830500   |
---------------------------------
Eval num_timesteps=831000, episode_reward=2.96 +/- 3.33
Episode length: 78.80 +/- 25.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.8     |
|    mean_reward     | 2.96     |
| time/              |          |
|    total_timesteps | 831000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.8     |
|    ep_rew_mean     | 3.41     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 406      |
|    time_elapsed    | 6416     |
|    total_timesteps | 831488   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=831500, episode_reward=3.06 +/- 3.95
Episode length: 78.20 +/- 23.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.2        |
|    mean_reward          | 3.06        |
| time/                   |             |
|    total_timesteps      | 831500      |
| train/                  |             |
|    approx_kl            | 0.006608817 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.292      |
|    explained_variance   | 0.2         |
|    learning_rate        | 0.0001      |
|    loss                 | 0.153       |
|    n_updates            | 1188        |
|    policy_gradient_loss | -0.000863   |
|    value_loss           | 0.237       |
-----------------------------------------
Eval num_timesteps=832000, episode_reward=2.72 +/- 3.17
Episode length: 78.98 +/- 27.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79       |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 832000   |
---------------------------------
Eval num_timesteps=832500, episode_reward=2.56 +/- 3.12
Episode length: 77.34 +/- 22.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 832500   |
---------------------------------
Eval num_timesteps=833000, episode_reward=3.80 +/- 3.85
Episode length: 83.70 +/- 24.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.7     |
|    mean_reward     | 3.8      |
| time/              |          |
|    total_timesteps | 833000   |
---------------------------------
Eval num_timesteps=833500, episode_reward=3.06 +/- 3.73
Episode length: 77.24 +/- 26.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 833500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.4     |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 407      |
|    time_elapsed    | 6434     |
|    total_timesteps | 833536   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=834000, episode_reward=3.00 +/- 3.75
Episode length: 80.34 +/- 28.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.3        |
|    mean_reward          | 3           |
| time/                   |             |
|    total_timesteps      | 834000      |
| train/                  |             |
|    approx_kl            | 0.006689125 |
|    clip_fraction        | 0.0386      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.154       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.151       |
|    n_updates            | 1191        |
|    policy_gradient_loss | -0.00162    |
|    value_loss           | 0.371       |
-----------------------------------------
Eval num_timesteps=834500, episode_reward=2.68 +/- 3.35
Episode length: 77.04 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 834500   |
---------------------------------
Eval num_timesteps=835000, episode_reward=4.08 +/- 5.06
Episode length: 81.20 +/- 30.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.2     |
|    mean_reward     | 4.08     |
| time/              |          |
|    total_timesteps | 835000   |
---------------------------------
Eval num_timesteps=835500, episode_reward=2.84 +/- 4.12
Episode length: 76.98 +/- 26.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 835500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.3     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 408      |
|    time_elapsed    | 6449     |
|    total_timesteps | 835584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=836000, episode_reward=2.26 +/- 3.66
Episode length: 78.14 +/- 22.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.1         |
|    mean_reward          | 2.26         |
| time/                   |              |
|    total_timesteps      | 836000       |
| train/                  |              |
|    approx_kl            | 0.0028476864 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.23        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.512        |
|    n_updates            | 1192         |
|    policy_gradient_loss | 0.00301      |
|    value_loss           | 0.52         |
------------------------------------------
Eval num_timesteps=836500, episode_reward=2.44 +/- 3.24
Episode length: 70.78 +/- 21.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.8     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 836500   |
---------------------------------
Eval num_timesteps=837000, episode_reward=2.20 +/- 4.53
Episode length: 77.24 +/- 23.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 837000   |
---------------------------------
Eval num_timesteps=837500, episode_reward=2.80 +/- 4.28
Episode length: 76.92 +/- 24.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 837500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75       |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 409      |
|    time_elapsed    | 6464     |
|    total_timesteps | 837632   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=838000, episode_reward=2.10 +/- 3.38
Episode length: 76.00 +/- 28.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76           |
|    mean_reward          | 2.1          |
| time/                   |              |
|    total_timesteps      | 838000       |
| train/                  |              |
|    approx_kl            | 0.0062121027 |
|    clip_fraction        | 0.0433       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.162       |
|    explained_variance   | 0.0773       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.21         |
|    n_updates            | 1193         |
|    policy_gradient_loss | 0.000326     |
|    value_loss           | 0.652        |
------------------------------------------
Eval num_timesteps=838500, episode_reward=2.36 +/- 2.74
Episode length: 77.96 +/- 20.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 838500   |
---------------------------------
Eval num_timesteps=839000, episode_reward=2.44 +/- 3.31
Episode length: 72.04 +/- 21.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72       |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 839000   |
---------------------------------
Eval num_timesteps=839500, episode_reward=2.70 +/- 3.18
Episode length: 82.18 +/- 34.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.2     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 839500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.2     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 410      |
|    time_elapsed    | 6479     |
|    total_timesteps | 839680   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=840000, episode_reward=2.26 +/- 3.58
Episode length: 75.30 +/- 24.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.3        |
|    mean_reward          | 2.26        |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.004957774 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.167       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.143       |
|    n_updates            | 1199        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 0.327       |
-----------------------------------------
Eval num_timesteps=840500, episode_reward=2.78 +/- 4.14
Episode length: 80.32 +/- 22.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 840500   |
---------------------------------
Eval num_timesteps=841000, episode_reward=2.70 +/- 2.46
Episode length: 79.44 +/- 22.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 841000   |
---------------------------------
Eval num_timesteps=841500, episode_reward=2.18 +/- 2.79
Episode length: 78.58 +/- 30.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 841500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.1     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 411      |
|    time_elapsed    | 6494     |
|    total_timesteps | 841728   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=842000, episode_reward=3.16 +/- 4.46
Episode length: 81.96 +/- 27.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 82           |
|    mean_reward          | 3.16         |
| time/                   |              |
|    total_timesteps      | 842000       |
| train/                  |              |
|    approx_kl            | 0.0060583963 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | 0.276        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.26         |
|    n_updates            | 1202         |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 0.506        |
------------------------------------------
Eval num_timesteps=842500, episode_reward=2.76 +/- 3.53
Episode length: 72.78 +/- 22.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 842500   |
---------------------------------
Eval num_timesteps=843000, episode_reward=2.08 +/- 2.67
Episode length: 77.38 +/- 27.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 843000   |
---------------------------------
Eval num_timesteps=843500, episode_reward=2.30 +/- 3.29
Episode length: 73.46 +/- 23.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 843500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 412      |
|    time_elapsed    | 6508     |
|    total_timesteps | 843776   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
Eval num_timesteps=844000, episode_reward=2.54 +/- 3.23
Episode length: 80.14 +/- 27.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.1        |
|    mean_reward          | 2.54        |
| time/                   |             |
|    total_timesteps      | 844000      |
| train/                  |             |
|    approx_kl            | 0.005505177 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.22       |
|    explained_variance   | 0.182       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.319       |
|    n_updates            | 1206        |
|    policy_gradient_loss | -0.00193    |
|    value_loss           | 0.36        |
-----------------------------------------
Eval num_timesteps=844500, episode_reward=2.60 +/- 3.68
Episode length: 74.66 +/- 22.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 844500   |
---------------------------------
Eval num_timesteps=845000, episode_reward=2.04 +/- 2.61
Episode length: 71.96 +/- 21.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72       |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 845000   |
---------------------------------
Eval num_timesteps=845500, episode_reward=3.86 +/- 3.86
Episode length: 77.86 +/- 24.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 3.86     |
| time/              |          |
|    total_timesteps | 845500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.3     |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 413      |
|    time_elapsed    | 6523     |
|    total_timesteps | 845824   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=846000, episode_reward=2.98 +/- 3.94
Episode length: 79.12 +/- 32.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79.1         |
|    mean_reward          | 2.98         |
| time/                   |              |
|    total_timesteps      | 846000       |
| train/                  |              |
|    approx_kl            | 0.0030072245 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.182       |
|    explained_variance   | 0.125        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.208        |
|    n_updates            | 1208         |
|    policy_gradient_loss | -0.000291    |
|    value_loss           | 0.526        |
------------------------------------------
Eval num_timesteps=846500, episode_reward=2.84 +/- 3.11
Episode length: 79.26 +/- 24.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.3     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 846500   |
---------------------------------
Eval num_timesteps=847000, episode_reward=2.76 +/- 3.18
Episode length: 78.26 +/- 30.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 847000   |
---------------------------------
Eval num_timesteps=847500, episode_reward=2.52 +/- 2.58
Episode length: 80.34 +/- 25.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 847500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.3     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 414      |
|    time_elapsed    | 6538     |
|    total_timesteps | 847872   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=848000, episode_reward=2.64 +/- 2.79
Episode length: 72.12 +/- 23.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.1        |
|    mean_reward          | 2.64        |
| time/                   |             |
|    total_timesteps      | 848000      |
| train/                  |             |
|    approx_kl            | 0.015760973 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.218      |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.263       |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.000401   |
|    value_loss           | 0.481       |
-----------------------------------------
Eval num_timesteps=848500, episode_reward=2.28 +/- 2.66
Episode length: 75.44 +/- 27.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 848500   |
---------------------------------
Eval num_timesteps=849000, episode_reward=1.84 +/- 3.11
Episode length: 74.08 +/- 22.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.1     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 849000   |
---------------------------------
Eval num_timesteps=849500, episode_reward=2.56 +/- 3.13
Episode length: 77.58 +/- 27.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 849500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.5     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 415      |
|    time_elapsed    | 6552     |
|    total_timesteps | 849920   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=850000, episode_reward=2.96 +/- 2.88
Episode length: 77.02 +/- 24.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77          |
|    mean_reward          | 2.96        |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.005990644 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.215       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.313       |
|    n_updates            | 1213        |
|    policy_gradient_loss | -0.000257   |
|    value_loss           | 0.676       |
-----------------------------------------
Eval num_timesteps=850500, episode_reward=2.28 +/- 2.88
Episode length: 79.44 +/- 27.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 850500   |
---------------------------------
Eval num_timesteps=851000, episode_reward=2.92 +/- 3.77
Episode length: 82.06 +/- 28.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 2.92     |
| time/              |          |
|    total_timesteps | 851000   |
---------------------------------
Eval num_timesteps=851500, episode_reward=2.02 +/- 2.72
Episode length: 73.48 +/- 24.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 851500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.1     |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 416      |
|    time_elapsed    | 6567     |
|    total_timesteps | 851968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=852000, episode_reward=2.54 +/- 3.25
Episode length: 81.40 +/- 30.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81.4         |
|    mean_reward          | 2.54         |
| time/                   |              |
|    total_timesteps      | 852000       |
| train/                  |              |
|    approx_kl            | 0.0020138696 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.204       |
|    explained_variance   | 0.19         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0996       |
|    n_updates            | 1214         |
|    policy_gradient_loss | 0.000177     |
|    value_loss           | 0.254        |
------------------------------------------
Eval num_timesteps=852500, episode_reward=1.86 +/- 2.80
Episode length: 70.32 +/- 21.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.3     |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 852500   |
---------------------------------
Eval num_timesteps=853000, episode_reward=3.88 +/- 3.96
Episode length: 87.28 +/- 29.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.3     |
|    mean_reward     | 3.88     |
| time/              |          |
|    total_timesteps | 853000   |
---------------------------------
Eval num_timesteps=853500, episode_reward=3.04 +/- 3.69
Episode length: 78.40 +/- 24.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 853500   |
---------------------------------
Eval num_timesteps=854000, episode_reward=2.08 +/- 2.23
Episode length: 74.48 +/- 23.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.5     |
|    ep_rew_mean     | 3.71     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 417      |
|    time_elapsed    | 6586     |
|    total_timesteps | 854016   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=854500, episode_reward=2.30 +/- 2.59
Episode length: 74.30 +/- 28.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.3        |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 854500      |
| train/                  |             |
|    approx_kl            | 0.005054129 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.204      |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.428       |
|    n_updates            | 1218        |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 0.662       |
-----------------------------------------
Eval num_timesteps=855000, episode_reward=3.14 +/- 4.00
Episode length: 70.74 +/- 22.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.7     |
|    mean_reward     | 3.14     |
| time/              |          |
|    total_timesteps | 855000   |
---------------------------------
Eval num_timesteps=855500, episode_reward=2.26 +/- 2.91
Episode length: 79.64 +/- 28.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.6     |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 855500   |
---------------------------------
Eval num_timesteps=856000, episode_reward=1.84 +/- 2.60
Episode length: 69.46 +/- 22.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.5     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 856000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 418      |
|    time_elapsed    | 6600     |
|    total_timesteps | 856064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=856500, episode_reward=3.14 +/- 3.54
Episode length: 75.94 +/- 20.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.9         |
|    mean_reward          | 3.14         |
| time/                   |              |
|    total_timesteps      | 856500       |
| train/                  |              |
|    approx_kl            | 0.0026105405 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.265       |
|    explained_variance   | 0.282        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.389        |
|    n_updates            | 1219         |
|    policy_gradient_loss | 0.000276     |
|    value_loss           | 0.45         |
------------------------------------------
Eval num_timesteps=857000, episode_reward=2.60 +/- 3.56
Episode length: 77.14 +/- 25.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 857000   |
---------------------------------
Eval num_timesteps=857500, episode_reward=2.16 +/- 2.89
Episode length: 77.10 +/- 22.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 857500   |
---------------------------------
Eval num_timesteps=858000, episode_reward=2.68 +/- 3.46
Episode length: 77.06 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 858000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.6     |
|    ep_rew_mean     | 3.19     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 419      |
|    time_elapsed    | 6615     |
|    total_timesteps | 858112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=858500, episode_reward=3.46 +/- 4.34
Episode length: 75.82 +/- 29.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.8        |
|    mean_reward          | 3.46        |
| time/                   |             |
|    total_timesteps      | 858500      |
| train/                  |             |
|    approx_kl            | 0.004081668 |
|    clip_fraction        | 0.0542      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.248      |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.167       |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00079    |
|    value_loss           | 0.493       |
-----------------------------------------
Eval num_timesteps=859000, episode_reward=1.74 +/- 2.72
Episode length: 69.78 +/- 25.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.8     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 859000   |
---------------------------------
Eval num_timesteps=859500, episode_reward=2.78 +/- 3.53
Episode length: 75.98 +/- 23.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 859500   |
---------------------------------
Eval num_timesteps=860000, episode_reward=2.86 +/- 3.69
Episode length: 72.86 +/- 23.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.9     |
|    ep_rew_mean     | 3.44     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 420      |
|    time_elapsed    | 6629     |
|    total_timesteps | 860160   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=860500, episode_reward=2.90 +/- 3.93
Episode length: 76.46 +/- 23.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.5        |
|    mean_reward          | 2.9         |
| time/                   |             |
|    total_timesteps      | 860500      |
| train/                  |             |
|    approx_kl            | 0.007884032 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.256      |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.173       |
|    n_updates            | 1223        |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 0.415       |
-----------------------------------------
Eval num_timesteps=861000, episode_reward=2.64 +/- 2.89
Episode length: 82.30 +/- 25.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 2.64     |
| time/              |          |
|    total_timesteps | 861000   |
---------------------------------
Eval num_timesteps=861500, episode_reward=2.56 +/- 3.61
Episode length: 76.04 +/- 22.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 861500   |
---------------------------------
Eval num_timesteps=862000, episode_reward=2.82 +/- 4.11
Episode length: 76.18 +/- 28.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 862000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 3.2      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 421      |
|    time_elapsed    | 6644     |
|    total_timesteps | 862208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=862500, episode_reward=2.44 +/- 3.14
Episode length: 76.06 +/- 24.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 76.1      |
|    mean_reward          | 2.44      |
| time/                   |           |
|    total_timesteps      | 862500    |
| train/                  |           |
|    approx_kl            | 0.0058713 |
|    clip_fraction        | 0.0677    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.325    |
|    explained_variance   | 0.188     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.302     |
|    n_updates            | 1224      |
|    policy_gradient_loss | 0.00508   |
|    value_loss           | 0.571     |
---------------------------------------
Eval num_timesteps=863000, episode_reward=3.64 +/- 3.07
Episode length: 84.18 +/- 23.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.2     |
|    mean_reward     | 3.64     |
| time/              |          |
|    total_timesteps | 863000   |
---------------------------------
Eval num_timesteps=863500, episode_reward=2.36 +/- 2.24
Episode length: 72.46 +/- 23.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.5     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 863500   |
---------------------------------
Eval num_timesteps=864000, episode_reward=2.42 +/- 3.08
Episode length: 73.28 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.3     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 864000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.1     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 422      |
|    time_elapsed    | 6659     |
|    total_timesteps | 864256   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=864500, episode_reward=2.48 +/- 2.82
Episode length: 72.12 +/- 22.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 72.1         |
|    mean_reward          | 2.48         |
| time/                   |              |
|    total_timesteps      | 864500       |
| train/                  |              |
|    approx_kl            | 0.0072870906 |
|    clip_fraction        | 0.0525       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.323       |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.156        |
|    n_updates            | 1226         |
|    policy_gradient_loss | -0.00219     |
|    value_loss           | 0.312        |
------------------------------------------
Eval num_timesteps=865000, episode_reward=3.60 +/- 4.19
Episode length: 76.92 +/- 24.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 3.6      |
| time/              |          |
|    total_timesteps | 865000   |
---------------------------------
Eval num_timesteps=865500, episode_reward=3.26 +/- 4.27
Episode length: 78.48 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 3.26     |
| time/              |          |
|    total_timesteps | 865500   |
---------------------------------
Eval num_timesteps=866000, episode_reward=3.48 +/- 4.10
Episode length: 81.52 +/- 29.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.5     |
|    mean_reward     | 3.48     |
| time/              |          |
|    total_timesteps | 866000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.2     |
|    ep_rew_mean     | 2.73     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 423      |
|    time_elapsed    | 6674     |
|    total_timesteps | 866304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=866500, episode_reward=2.76 +/- 3.00
Episode length: 76.66 +/- 22.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.7         |
|    mean_reward          | 2.76         |
| time/                   |              |
|    total_timesteps      | 866500       |
| train/                  |              |
|    approx_kl            | 0.0034136486 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.271       |
|    explained_variance   | 0.207        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.42         |
|    n_updates            | 1227         |
|    policy_gradient_loss | 0.00198      |
|    value_loss           | 0.422        |
------------------------------------------
Eval num_timesteps=867000, episode_reward=2.16 +/- 2.68
Episode length: 74.40 +/- 26.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 867000   |
---------------------------------
Eval num_timesteps=867500, episode_reward=2.70 +/- 2.84
Episode length: 76.48 +/- 26.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.5     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 867500   |
---------------------------------
Eval num_timesteps=868000, episode_reward=3.58 +/- 3.34
Episode length: 82.06 +/- 26.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 3.58     |
| time/              |          |
|    total_timesteps | 868000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.9     |
|    ep_rew_mean     | 2.74     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 424      |
|    time_elapsed    | 6689     |
|    total_timesteps | 868352   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=868500, episode_reward=2.82 +/- 4.19
Episode length: 74.78 +/- 28.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.8        |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 868500      |
| train/                  |             |
|    approx_kl            | 0.004264089 |
|    clip_fraction        | 0.0301      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.26       |
|    explained_variance   | 0.171       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.113       |
|    n_updates            | 1229        |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 0.37        |
-----------------------------------------
Eval num_timesteps=869000, episode_reward=2.56 +/- 2.56
Episode length: 79.96 +/- 19.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 869000   |
---------------------------------
Eval num_timesteps=869500, episode_reward=2.70 +/- 2.88
Episode length: 78.36 +/- 23.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 869500   |
---------------------------------
Eval num_timesteps=870000, episode_reward=2.98 +/- 3.83
Episode length: 78.30 +/- 22.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 870000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77       |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 425      |
|    time_elapsed    | 6704     |
|    total_timesteps | 870400   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=870500, episode_reward=2.90 +/- 3.48
Episode length: 79.96 +/- 26.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 80           |
|    mean_reward          | 2.9          |
| time/                   |              |
|    total_timesteps      | 870500       |
| train/                  |              |
|    approx_kl            | 0.0063526323 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0.203        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0988       |
|    n_updates            | 1231         |
|    policy_gradient_loss | 5.11e-06     |
|    value_loss           | 0.417        |
------------------------------------------
Eval num_timesteps=871000, episode_reward=2.22 +/- 3.99
Episode length: 75.38 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 871000   |
---------------------------------
Eval num_timesteps=871500, episode_reward=2.76 +/- 3.53
Episode length: 82.50 +/- 26.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.5     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 871500   |
---------------------------------
Eval num_timesteps=872000, episode_reward=2.98 +/- 3.47
Episode length: 83.08 +/- 26.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.1     |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 872000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 426      |
|    time_elapsed    | 6720     |
|    total_timesteps | 872448   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=872500, episode_reward=2.74 +/- 3.12
Episode length: 82.32 +/- 26.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.3        |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 872500      |
| train/                  |             |
|    approx_kl            | 0.005698137 |
|    clip_fraction        | 0.0422      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.255      |
|    explained_variance   | 0.177       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.19        |
|    n_updates            | 1233        |
|    policy_gradient_loss | 0.00374     |
|    value_loss           | 0.405       |
-----------------------------------------
Eval num_timesteps=873000, episode_reward=3.52 +/- 3.75
Episode length: 84.94 +/- 25.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.9     |
|    mean_reward     | 3.52     |
| time/              |          |
|    total_timesteps | 873000   |
---------------------------------
Eval num_timesteps=873500, episode_reward=2.40 +/- 3.21
Episode length: 72.26 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 873500   |
---------------------------------
Eval num_timesteps=874000, episode_reward=3.16 +/- 3.98
Episode length: 77.58 +/- 29.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 874000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 427      |
|    time_elapsed    | 6736     |
|    total_timesteps | 874496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=874500, episode_reward=3.82 +/- 3.71
Episode length: 87.38 +/- 29.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 87.4       |
|    mean_reward          | 3.82       |
| time/                   |            |
|    total_timesteps      | 874500     |
| train/                  |            |
|    approx_kl            | 0.00357527 |
|    clip_fraction        | 0.0442     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.255     |
|    explained_variance   | 0.171      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.226      |
|    n_updates            | 1234       |
|    policy_gradient_loss | 0.00028    |
|    value_loss           | 0.559      |
----------------------------------------
Eval num_timesteps=875000, episode_reward=3.16 +/- 3.87
Episode length: 86.00 +/- 26.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86       |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 875000   |
---------------------------------
Eval num_timesteps=875500, episode_reward=2.70 +/- 2.76
Episode length: 74.56 +/- 19.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 875500   |
---------------------------------
Eval num_timesteps=876000, episode_reward=2.64 +/- 3.50
Episode length: 74.70 +/- 22.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 2.64     |
| time/              |          |
|    total_timesteps | 876000   |
---------------------------------
Eval num_timesteps=876500, episode_reward=2.32 +/- 2.77
Episode length: 76.86 +/- 23.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 876500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.6     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 428      |
|    time_elapsed    | 6755     |
|    total_timesteps | 876544   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=877000, episode_reward=2.68 +/- 2.96
Episode length: 75.62 +/- 23.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.6        |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 877000      |
| train/                  |             |
|    approx_kl            | 0.004078545 |
|    clip_fraction        | 0.0447      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.279      |
|    explained_variance   | 0.222       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.138       |
|    n_updates            | 1236        |
|    policy_gradient_loss | 0.000704    |
|    value_loss           | 0.256       |
-----------------------------------------
Eval num_timesteps=877500, episode_reward=2.58 +/- 3.64
Episode length: 75.98 +/- 30.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 877500   |
---------------------------------
Eval num_timesteps=878000, episode_reward=2.24 +/- 2.93
Episode length: 77.46 +/- 25.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.5     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 878000   |
---------------------------------
Eval num_timesteps=878500, episode_reward=2.24 +/- 2.69
Episode length: 73.44 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.4     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 878500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.4     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 429      |
|    time_elapsed    | 6770     |
|    total_timesteps | 878592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=879000, episode_reward=3.02 +/- 3.69
Episode length: 77.58 +/- 24.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.6        |
|    mean_reward          | 3.02        |
| time/                   |             |
|    total_timesteps      | 879000      |
| train/                  |             |
|    approx_kl            | 0.003954335 |
|    clip_fraction        | 0.0399      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.311      |
|    explained_variance   | 0.176       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.102       |
|    n_updates            | 1237        |
|    policy_gradient_loss | 0.00128     |
|    value_loss           | 0.367       |
-----------------------------------------
Eval num_timesteps=879500, episode_reward=3.04 +/- 3.37
Episode length: 86.40 +/- 24.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.4     |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 879500   |
---------------------------------
Eval num_timesteps=880000, episode_reward=2.18 +/- 2.70
Episode length: 81.20 +/- 25.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.2     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=880500, episode_reward=3.24 +/- 3.40
Episode length: 80.94 +/- 29.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.9     |
|    mean_reward     | 3.24     |
| time/              |          |
|    total_timesteps | 880500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 430      |
|    time_elapsed    | 6785     |
|    total_timesteps | 880640   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=881000, episode_reward=2.34 +/- 2.75
Episode length: 79.02 +/- 22.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 79         |
|    mean_reward          | 2.34       |
| time/                   |            |
|    total_timesteps      | 881000     |
| train/                  |            |
|    approx_kl            | 0.01828008 |
|    clip_fraction        | 0.0528     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.306     |
|    explained_variance   | 0.326      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.141      |
|    n_updates            | 1241       |
|    policy_gradient_loss | -0.00418   |
|    value_loss           | 0.254      |
----------------------------------------
Eval num_timesteps=881500, episode_reward=2.76 +/- 3.44
Episode length: 78.94 +/- 30.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.9     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 881500   |
---------------------------------
Eval num_timesteps=882000, episode_reward=3.50 +/- 3.95
Episode length: 82.06 +/- 28.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 3.5      |
| time/              |          |
|    total_timesteps | 882000   |
---------------------------------
Eval num_timesteps=882500, episode_reward=2.68 +/- 3.54
Episode length: 82.40 +/- 30.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 882500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.5     |
|    ep_rew_mean     | 2.16     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 431      |
|    time_elapsed    | 6801     |
|    total_timesteps | 882688   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=883000, episode_reward=2.82 +/- 3.68
Episode length: 76.58 +/- 26.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.6         |
|    mean_reward          | 2.82         |
| time/                   |              |
|    total_timesteps      | 883000       |
| train/                  |              |
|    approx_kl            | 0.0064397277 |
|    clip_fraction        | 0.0572       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.296       |
|    explained_variance   | 0.246        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.807        |
|    n_updates            | 1243         |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 0.455        |
------------------------------------------
Eval num_timesteps=883500, episode_reward=2.94 +/- 3.42
Episode length: 80.26 +/- 27.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 883500   |
---------------------------------
Eval num_timesteps=884000, episode_reward=2.50 +/- 3.76
Episode length: 75.56 +/- 24.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.6     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 884000   |
---------------------------------
Eval num_timesteps=884500, episode_reward=2.86 +/- 3.85
Episode length: 82.64 +/- 30.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.6     |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 884500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.3     |
|    ep_rew_mean     | 2.25     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 432      |
|    time_elapsed    | 6816     |
|    total_timesteps | 884736   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=885000, episode_reward=2.64 +/- 3.23
Episode length: 84.18 +/- 27.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.2         |
|    mean_reward          | 2.64         |
| time/                   |              |
|    total_timesteps      | 885000       |
| train/                  |              |
|    approx_kl            | 0.0064589763 |
|    clip_fraction        | 0.06         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.271       |
|    explained_variance   | 0.186        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.138        |
|    n_updates            | 1245         |
|    policy_gradient_loss | -0.000475    |
|    value_loss           | 0.261        |
------------------------------------------
Eval num_timesteps=885500, episode_reward=2.64 +/- 2.96
Episode length: 88.00 +/- 27.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88       |
|    mean_reward     | 2.64     |
| time/              |          |
|    total_timesteps | 885500   |
---------------------------------
Eval num_timesteps=886000, episode_reward=2.38 +/- 3.00
Episode length: 77.64 +/- 24.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 886000   |
---------------------------------
Eval num_timesteps=886500, episode_reward=1.80 +/- 2.48
Episode length: 72.70 +/- 20.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 886500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.1     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 433      |
|    time_elapsed    | 6831     |
|    total_timesteps | 886784   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=887000, episode_reward=3.32 +/- 4.10
Episode length: 84.02 +/- 25.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84          |
|    mean_reward          | 3.32        |
| time/                   |             |
|    total_timesteps      | 887000      |
| train/                  |             |
|    approx_kl            | 0.005566908 |
|    clip_fraction        | 0.042       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0946      |
|    n_updates            | 1248        |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 0.397       |
-----------------------------------------
Eval num_timesteps=887500, episode_reward=2.56 +/- 3.20
Episode length: 82.14 +/- 23.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 887500   |
---------------------------------
Eval num_timesteps=888000, episode_reward=2.36 +/- 2.96
Episode length: 77.98 +/- 26.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 888000   |
---------------------------------
Eval num_timesteps=888500, episode_reward=2.56 +/- 3.34
Episode length: 80.12 +/- 26.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 888500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 2.07     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 434      |
|    time_elapsed    | 6846     |
|    total_timesteps | 888832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=889000, episode_reward=3.64 +/- 4.60
Episode length: 88.04 +/- 29.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 88           |
|    mean_reward          | 3.64         |
| time/                   |              |
|    total_timesteps      | 889000       |
| train/                  |              |
|    approx_kl            | 0.0040953285 |
|    clip_fraction        | 0.0539       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.298       |
|    explained_variance   | 0.0747       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0867       |
|    n_updates            | 1249         |
|    policy_gradient_loss | 0.00514      |
|    value_loss           | 0.183        |
------------------------------------------
Eval num_timesteps=889500, episode_reward=2.38 +/- 2.78
Episode length: 74.58 +/- 22.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 889500   |
---------------------------------
Eval num_timesteps=890000, episode_reward=3.46 +/- 3.53
Episode length: 80.86 +/- 27.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.9     |
|    mean_reward     | 3.46     |
| time/              |          |
|    total_timesteps | 890000   |
---------------------------------
Eval num_timesteps=890500, episode_reward=2.16 +/- 2.71
Episode length: 85.02 +/- 29.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85       |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 890500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.2     |
|    ep_rew_mean     | 2.05     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 435      |
|    time_elapsed    | 6862     |
|    total_timesteps | 890880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=891000, episode_reward=3.02 +/- 4.07
Episode length: 75.08 +/- 30.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.1         |
|    mean_reward          | 3.02         |
| time/                   |              |
|    total_timesteps      | 891000       |
| train/                  |              |
|    approx_kl            | 0.0044147144 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.295       |
|    explained_variance   | 0.203        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0519       |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.003       |
|    value_loss           | 0.306        |
------------------------------------------
Eval num_timesteps=891500, episode_reward=3.32 +/- 3.36
Episode length: 86.82 +/- 26.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.8     |
|    mean_reward     | 3.32     |
| time/              |          |
|    total_timesteps | 891500   |
---------------------------------
Eval num_timesteps=892000, episode_reward=1.94 +/- 2.43
Episode length: 76.30 +/- 26.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 892000   |
---------------------------------
Eval num_timesteps=892500, episode_reward=3.40 +/- 3.52
Episode length: 81.00 +/- 27.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81       |
|    mean_reward     | 3.4      |
| time/              |          |
|    total_timesteps | 892500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.6     |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 436      |
|    time_elapsed    | 6877     |
|    total_timesteps | 892928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=893000, episode_reward=3.48 +/- 3.43
Episode length: 79.72 +/- 26.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.7        |
|    mean_reward          | 3.48        |
| time/                   |             |
|    total_timesteps      | 893000      |
| train/                  |             |
|    approx_kl            | 0.007210386 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.261      |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.216       |
|    n_updates            | 1251        |
|    policy_gradient_loss | 0.00218     |
|    value_loss           | 0.409       |
-----------------------------------------
Eval num_timesteps=893500, episode_reward=3.56 +/- 4.36
Episode length: 78.30 +/- 23.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 3.56     |
| time/              |          |
|    total_timesteps | 893500   |
---------------------------------
Eval num_timesteps=894000, episode_reward=2.68 +/- 3.28
Episode length: 84.18 +/- 27.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.2     |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 894000   |
---------------------------------
Eval num_timesteps=894500, episode_reward=2.40 +/- 3.14
Episode length: 76.04 +/- 24.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 894500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.3     |
|    ep_rew_mean     | 2.2      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 437      |
|    time_elapsed    | 6892     |
|    total_timesteps | 894976   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=895000, episode_reward=2.56 +/- 3.20
Episode length: 76.98 +/- 24.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77           |
|    mean_reward          | 2.56         |
| time/                   |              |
|    total_timesteps      | 895000       |
| train/                  |              |
|    approx_kl            | 0.0152890105 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.27        |
|    explained_variance   | 0.198        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.171        |
|    n_updates            | 1253         |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 0.324        |
------------------------------------------
Eval num_timesteps=895500, episode_reward=2.62 +/- 3.79
Episode length: 78.92 +/- 22.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.9     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 895500   |
---------------------------------
Eval num_timesteps=896000, episode_reward=2.96 +/- 3.91
Episode length: 75.90 +/- 23.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 2.96     |
| time/              |          |
|    total_timesteps | 896000   |
---------------------------------
Eval num_timesteps=896500, episode_reward=2.60 +/- 4.00
Episode length: 77.18 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 2.6      |
| time/              |          |
|    total_timesteps | 896500   |
---------------------------------
Eval num_timesteps=897000, episode_reward=2.78 +/- 3.65
Episode length: 79.14 +/- 23.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.1     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 897000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.7     |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 438      |
|    time_elapsed    | 6911     |
|    total_timesteps | 897024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=897500, episode_reward=3.28 +/- 3.12
Episode length: 84.86 +/- 26.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.9         |
|    mean_reward          | 3.28         |
| time/                   |              |
|    total_timesteps      | 897500       |
| train/                  |              |
|    approx_kl            | 0.0041484674 |
|    clip_fraction        | 0.0499       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.232       |
|    explained_variance   | 0.226        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.115        |
|    n_updates            | 1254         |
|    policy_gradient_loss | 0.0025       |
|    value_loss           | 0.292        |
------------------------------------------
Eval num_timesteps=898000, episode_reward=2.80 +/- 2.68
Episode length: 78.04 +/- 23.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 898000   |
---------------------------------
Eval num_timesteps=898500, episode_reward=2.48 +/- 3.10
Episode length: 79.06 +/- 24.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.1     |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 898500   |
---------------------------------
Eval num_timesteps=899000, episode_reward=2.52 +/- 3.76
Episode length: 73.92 +/- 28.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 899000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.5     |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 439      |
|    time_elapsed    | 6926     |
|    total_timesteps | 899072   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=899500, episode_reward=2.32 +/- 2.97
Episode length: 75.50 +/- 22.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.5         |
|    mean_reward          | 2.32         |
| time/                   |              |
|    total_timesteps      | 899500       |
| train/                  |              |
|    approx_kl            | 0.0052283145 |
|    clip_fraction        | 0.0589       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.296       |
|    explained_variance   | 0.0963       |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0864       |
|    n_updates            | 1256         |
|    policy_gradient_loss | -0.000978    |
|    value_loss           | 0.371        |
------------------------------------------
Eval num_timesteps=900000, episode_reward=2.36 +/- 3.58
Episode length: 75.14 +/- 24.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=900500, episode_reward=2.36 +/- 3.37
Episode length: 83.02 +/- 31.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83       |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 900500   |
---------------------------------
Eval num_timesteps=901000, episode_reward=2.88 +/- 2.90
Episode length: 74.74 +/- 27.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 901000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.3     |
|    ep_rew_mean     | 1.95     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 440      |
|    time_elapsed    | 6940     |
|    total_timesteps | 901120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=901500, episode_reward=3.08 +/- 3.09
Episode length: 78.96 +/- 28.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79           |
|    mean_reward          | 3.08         |
| time/                   |              |
|    total_timesteps      | 901500       |
| train/                  |              |
|    approx_kl            | 0.0035662546 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.268       |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0774       |
|    n_updates            | 1257         |
|    policy_gradient_loss | -0.000521    |
|    value_loss           | 0.168        |
------------------------------------------
Eval num_timesteps=902000, episode_reward=3.22 +/- 3.78
Episode length: 76.04 +/- 23.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 3.22     |
| time/              |          |
|    total_timesteps | 902000   |
---------------------------------
Eval num_timesteps=902500, episode_reward=2.52 +/- 2.54
Episode length: 74.16 +/- 21.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.2     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 902500   |
---------------------------------
Eval num_timesteps=903000, episode_reward=3.32 +/- 3.64
Episode length: 85.32 +/- 26.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.3     |
|    mean_reward     | 3.32     |
| time/              |          |
|    total_timesteps | 903000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75       |
|    ep_rew_mean     | 1.86     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 441      |
|    time_elapsed    | 6955     |
|    total_timesteps | 903168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=903500, episode_reward=1.42 +/- 2.20
Episode length: 71.74 +/- 24.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 71.7        |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 903500      |
| train/                  |             |
|    approx_kl            | 0.006729147 |
|    clip_fraction        | 0.0578      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.248      |
|    explained_variance   | 0.229       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.174       |
|    n_updates            | 1258        |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 0.227       |
-----------------------------------------
Eval num_timesteps=904000, episode_reward=2.58 +/- 3.31
Episode length: 73.72 +/- 21.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 904000   |
---------------------------------
Eval num_timesteps=904500, episode_reward=3.68 +/- 3.82
Episode length: 81.68 +/- 28.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.7     |
|    mean_reward     | 3.68     |
| time/              |          |
|    total_timesteps | 904500   |
---------------------------------
Eval num_timesteps=905000, episode_reward=3.32 +/- 3.03
Episode length: 82.46 +/- 24.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.5     |
|    mean_reward     | 3.32     |
| time/              |          |
|    total_timesteps | 905000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.3     |
|    ep_rew_mean     | 1.98     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 442      |
|    time_elapsed    | 6970     |
|    total_timesteps | 905216   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=905500, episode_reward=2.32 +/- 2.85
Episode length: 76.24 +/- 27.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.2         |
|    mean_reward          | 2.32         |
| time/                   |              |
|    total_timesteps      | 905500       |
| train/                  |              |
|    approx_kl            | 0.0039192764 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.183       |
|    explained_variance   | 0.243        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.233        |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 0.438        |
------------------------------------------
Eval num_timesteps=906000, episode_reward=2.84 +/- 4.53
Episode length: 75.24 +/- 25.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.2     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 906000   |
---------------------------------
Eval num_timesteps=906500, episode_reward=3.10 +/- 3.61
Episode length: 79.26 +/- 22.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.3     |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 906500   |
---------------------------------
Eval num_timesteps=907000, episode_reward=3.80 +/- 3.91
Episode length: 82.16 +/- 28.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.2     |
|    mean_reward     | 3.8      |
| time/              |          |
|    total_timesteps | 907000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | 1.77     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 443      |
|    time_elapsed    | 6985     |
|    total_timesteps | 907264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=907500, episode_reward=3.04 +/- 3.65
Episode length: 81.50 +/- 22.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81.5         |
|    mean_reward          | 3.04         |
| time/                   |              |
|    total_timesteps      | 907500       |
| train/                  |              |
|    approx_kl            | 0.0034750402 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.249       |
|    explained_variance   | 0.317        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0906       |
|    n_updates            | 1261         |
|    policy_gradient_loss | 0.000351     |
|    value_loss           | 0.202        |
------------------------------------------
Eval num_timesteps=908000, episode_reward=3.28 +/- 3.32
Episode length: 77.66 +/- 25.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 908000   |
---------------------------------
Eval num_timesteps=908500, episode_reward=3.66 +/- 3.88
Episode length: 77.96 +/- 27.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 3.66     |
| time/              |          |
|    total_timesteps | 908500   |
---------------------------------
Eval num_timesteps=909000, episode_reward=2.78 +/- 3.25
Episode length: 73.24 +/- 22.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.2     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 909000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 444      |
|    time_elapsed    | 7000     |
|    total_timesteps | 909312   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=909500, episode_reward=2.82 +/- 3.82
Episode length: 74.66 +/- 26.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.7        |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 909500      |
| train/                  |             |
|    approx_kl            | 0.006067397 |
|    clip_fraction        | 0.0288      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.205       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.189       |
|    n_updates            | 1263        |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 0.627       |
-----------------------------------------
Eval num_timesteps=910000, episode_reward=2.92 +/- 3.37
Episode length: 77.50 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.5     |
|    mean_reward     | 2.92     |
| time/              |          |
|    total_timesteps | 910000   |
---------------------------------
Eval num_timesteps=910500, episode_reward=3.74 +/- 3.80
Episode length: 88.10 +/- 28.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.1     |
|    mean_reward     | 3.74     |
| time/              |          |
|    total_timesteps | 910500   |
---------------------------------
Eval num_timesteps=911000, episode_reward=1.88 +/- 2.22
Episode length: 72.84 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 911000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.5     |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 445      |
|    time_elapsed    | 7016     |
|    total_timesteps | 911360   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=911500, episode_reward=3.02 +/- 3.60
Episode length: 77.96 +/- 19.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78           |
|    mean_reward          | 3.02         |
| time/                   |              |
|    total_timesteps      | 911500       |
| train/                  |              |
|    approx_kl            | 0.0058712256 |
|    clip_fraction        | 0.0411       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.233       |
|    explained_variance   | 0.21         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.164        |
|    n_updates            | 1265         |
|    policy_gradient_loss | -0.000423    |
|    value_loss           | 0.404        |
------------------------------------------
Eval num_timesteps=912000, episode_reward=2.26 +/- 2.88
Episode length: 74.38 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 912000   |
---------------------------------
Eval num_timesteps=912500, episode_reward=3.60 +/- 4.45
Episode length: 79.40 +/- 25.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 3.6      |
| time/              |          |
|    total_timesteps | 912500   |
---------------------------------
Eval num_timesteps=913000, episode_reward=3.10 +/- 3.76
Episode length: 79.96 +/- 25.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 913000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 446      |
|    time_elapsed    | 7031     |
|    total_timesteps | 913408   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=913500, episode_reward=2.70 +/- 2.84
Episode length: 80.68 +/- 27.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 80.7         |
|    mean_reward          | 2.7          |
| time/                   |              |
|    total_timesteps      | 913500       |
| train/                  |              |
|    approx_kl            | 0.0048992047 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.196       |
|    explained_variance   | 0.36         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0826       |
|    n_updates            | 1267         |
|    policy_gradient_loss | -0.00104     |
|    value_loss           | 0.37         |
------------------------------------------
Eval num_timesteps=914000, episode_reward=3.00 +/- 3.99
Episode length: 78.66 +/- 25.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.7     |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 914000   |
---------------------------------
Eval num_timesteps=914500, episode_reward=3.06 +/- 3.29
Episode length: 81.72 +/- 30.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.7     |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 914500   |
---------------------------------
Eval num_timesteps=915000, episode_reward=2.52 +/- 3.45
Episode length: 67.88 +/- 23.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.9     |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 915000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.8     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 447      |
|    time_elapsed    | 7046     |
|    total_timesteps | 915456   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=915500, episode_reward=3.28 +/- 4.24
Episode length: 84.52 +/- 25.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.5        |
|    mean_reward          | 3.28        |
| time/                   |             |
|    total_timesteps      | 915500      |
| train/                  |             |
|    approx_kl            | 0.005503239 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.16        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.143       |
|    n_updates            | 1269        |
|    policy_gradient_loss | -0.000733   |
|    value_loss           | 0.314       |
-----------------------------------------
Eval num_timesteps=916000, episode_reward=3.34 +/- 3.72
Episode length: 78.52 +/- 26.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 3.34     |
| time/              |          |
|    total_timesteps | 916000   |
---------------------------------
Eval num_timesteps=916500, episode_reward=2.32 +/- 3.72
Episode length: 76.50 +/- 24.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.5     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 916500   |
---------------------------------
Eval num_timesteps=917000, episode_reward=2.14 +/- 2.68
Episode length: 77.10 +/- 23.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 917000   |
---------------------------------
Eval num_timesteps=917500, episode_reward=3.16 +/- 3.54
Episode length: 76.32 +/- 25.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 917500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.4     |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 448      |
|    time_elapsed    | 7064     |
|    total_timesteps | 917504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=918000, episode_reward=3.70 +/- 4.17
Episode length: 75.18 +/- 23.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 75.2       |
|    mean_reward          | 3.7        |
| time/                   |            |
|    total_timesteps      | 918000     |
| train/                  |            |
|    approx_kl            | 0.00427007 |
|    clip_fraction        | 0.0514     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.298     |
|    explained_variance   | 0.207      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.142      |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.000823  |
|    value_loss           | 0.506      |
----------------------------------------
Eval num_timesteps=918500, episode_reward=3.96 +/- 4.31
Episode length: 82.70 +/- 29.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.7     |
|    mean_reward     | 3.96     |
| time/              |          |
|    total_timesteps | 918500   |
---------------------------------
Eval num_timesteps=919000, episode_reward=3.72 +/- 4.85
Episode length: 80.50 +/- 28.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.5     |
|    mean_reward     | 3.72     |
| time/              |          |
|    total_timesteps | 919000   |
---------------------------------
Eval num_timesteps=919500, episode_reward=1.70 +/- 2.26
Episode length: 72.82 +/- 18.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 919500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 449      |
|    time_elapsed    | 7079     |
|    total_timesteps | 919552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=920000, episode_reward=2.86 +/- 3.26
Episode length: 76.34 +/- 23.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.3         |
|    mean_reward          | 2.86         |
| time/                   |              |
|    total_timesteps      | 920000       |
| train/                  |              |
|    approx_kl            | 0.0089298235 |
|    clip_fraction        | 0.06         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.267       |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.256        |
|    n_updates            | 1272         |
|    policy_gradient_loss | 0.000197     |
|    value_loss           | 0.338        |
------------------------------------------
Eval num_timesteps=920500, episode_reward=3.42 +/- 3.97
Episode length: 81.68 +/- 31.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.7     |
|    mean_reward     | 3.42     |
| time/              |          |
|    total_timesteps | 920500   |
---------------------------------
Eval num_timesteps=921000, episode_reward=2.70 +/- 3.55
Episode length: 76.64 +/- 26.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 921000   |
---------------------------------
Eval num_timesteps=921500, episode_reward=3.24 +/- 3.37
Episode length: 84.34 +/- 26.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.3     |
|    mean_reward     | 3.24     |
| time/              |          |
|    total_timesteps | 921500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 450      |
|    time_elapsed    | 7094     |
|    total_timesteps | 921600   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=922000, episode_reward=2.16 +/- 2.52
Episode length: 73.40 +/- 20.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.4        |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 922000      |
| train/                  |             |
|    approx_kl            | 0.007641682 |
|    clip_fraction        | 0.0564      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.318      |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0823      |
|    n_updates            | 1275        |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 0.178       |
-----------------------------------------
Eval num_timesteps=922500, episode_reward=3.08 +/- 3.97
Episode length: 74.18 +/- 22.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.2     |
|    mean_reward     | 3.08     |
| time/              |          |
|    total_timesteps | 922500   |
---------------------------------
Eval num_timesteps=923000, episode_reward=3.28 +/- 4.14
Episode length: 87.22 +/- 33.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.2     |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 923000   |
---------------------------------
Eval num_timesteps=923500, episode_reward=2.34 +/- 3.09
Episode length: 76.94 +/- 23.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.34     |
| time/              |          |
|    total_timesteps | 923500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 451      |
|    time_elapsed    | 7109     |
|    total_timesteps | 923648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=924000, episode_reward=3.36 +/- 3.04
Episode length: 84.68 +/- 27.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.7         |
|    mean_reward          | 3.36         |
| time/                   |              |
|    total_timesteps      | 924000       |
| train/                  |              |
|    approx_kl            | 0.0031081086 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.298       |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.146        |
|    n_updates            | 1276         |
|    policy_gradient_loss | 5.31e-05     |
|    value_loss           | 0.478        |
------------------------------------------
Eval num_timesteps=924500, episode_reward=2.70 +/- 3.33
Episode length: 76.60 +/- 21.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 924500   |
---------------------------------
Eval num_timesteps=925000, episode_reward=2.32 +/- 2.93
Episode length: 76.18 +/- 27.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | 2.32     |
| time/              |          |
|    total_timesteps | 925000   |
---------------------------------
Eval num_timesteps=925500, episode_reward=2.18 +/- 2.61
Episode length: 77.40 +/- 23.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 925500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 452      |
|    time_elapsed    | 7125     |
|    total_timesteps | 925696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=926000, episode_reward=2.80 +/- 4.02
Episode length: 77.04 +/- 25.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77           |
|    mean_reward          | 2.8          |
| time/                   |              |
|    total_timesteps      | 926000       |
| train/                  |              |
|    approx_kl            | 0.0028581358 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.272       |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.18         |
|    n_updates            | 1277         |
|    policy_gradient_loss | 0.00197      |
|    value_loss           | 0.298        |
------------------------------------------
Eval num_timesteps=926500, episode_reward=2.84 +/- 3.04
Episode length: 76.90 +/- 23.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 926500   |
---------------------------------
Eval num_timesteps=927000, episode_reward=2.62 +/- 4.32
Episode length: 76.74 +/- 24.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.7     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 927000   |
---------------------------------
Eval num_timesteps=927500, episode_reward=2.78 +/- 3.18
Episode length: 84.54 +/- 28.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.5     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 927500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.2     |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 453      |
|    time_elapsed    | 7140     |
|    total_timesteps | 927744   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=928000, episode_reward=2.64 +/- 3.43
Episode length: 76.12 +/- 22.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.1         |
|    mean_reward          | 2.64         |
| time/                   |              |
|    total_timesteps      | 928000       |
| train/                  |              |
|    approx_kl            | 0.0044269934 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.353        |
|    n_updates            | 1279         |
|    policy_gradient_loss | 0.00121      |
|    value_loss           | 0.59         |
------------------------------------------
Eval num_timesteps=928500, episode_reward=3.36 +/- 3.75
Episode length: 81.44 +/- 28.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.4     |
|    mean_reward     | 3.36     |
| time/              |          |
|    total_timesteps | 928500   |
---------------------------------
Eval num_timesteps=929000, episode_reward=2.68 +/- 3.52
Episode length: 77.56 +/- 25.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 929000   |
---------------------------------
Eval num_timesteps=929500, episode_reward=4.00 +/- 4.11
Episode length: 88.30 +/- 30.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.3     |
|    mean_reward     | 4        |
| time/              |          |
|    total_timesteps | 929500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 454      |
|    time_elapsed    | 7157     |
|    total_timesteps | 929792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=930000, episode_reward=2.10 +/- 2.74
Episode length: 70.04 +/- 22.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70           |
|    mean_reward          | 2.1          |
| time/                   |              |
|    total_timesteps      | 930000       |
| train/                  |              |
|    approx_kl            | 0.0025013208 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.218       |
|    explained_variance   | 0.341        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.346        |
|    n_updates            | 1280         |
|    policy_gradient_loss | 0.000535     |
|    value_loss           | 0.371        |
------------------------------------------
Eval num_timesteps=930500, episode_reward=3.58 +/- 3.50
Episode length: 86.78 +/- 20.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.8     |
|    mean_reward     | 3.58     |
| time/              |          |
|    total_timesteps | 930500   |
---------------------------------
Eval num_timesteps=931000, episode_reward=2.40 +/- 2.78
Episode length: 74.40 +/- 26.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 2.4      |
| time/              |          |
|    total_timesteps | 931000   |
---------------------------------
Eval num_timesteps=931500, episode_reward=2.16 +/- 3.15
Episode length: 77.68 +/- 29.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 931500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.9     |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 455      |
|    time_elapsed    | 7172     |
|    total_timesteps | 931840   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=932000, episode_reward=2.52 +/- 2.77
Episode length: 76.10 +/- 25.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 76.1         |
|    mean_reward          | 2.52         |
| time/                   |              |
|    total_timesteps      | 932000       |
| train/                  |              |
|    approx_kl            | 0.0053418535 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.206       |
|    explained_variance   | 0.304        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.478        |
|    n_updates            | 1282         |
|    policy_gradient_loss | 9.78e-06     |
|    value_loss           | 0.511        |
------------------------------------------
Eval num_timesteps=932500, episode_reward=2.20 +/- 2.65
Episode length: 78.96 +/- 24.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79       |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 932500   |
---------------------------------
Eval num_timesteps=933000, episode_reward=3.28 +/- 4.15
Episode length: 77.34 +/- 31.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 933000   |
---------------------------------
Eval num_timesteps=933500, episode_reward=3.54 +/- 4.80
Episode length: 80.08 +/- 27.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 3.54     |
| time/              |          |
|    total_timesteps | 933500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 129      |
|    iterations      | 456      |
|    time_elapsed    | 7187     |
|    total_timesteps | 933888   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=934000, episode_reward=2.22 +/- 2.70
Episode length: 72.88 +/- 21.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.9        |
|    mean_reward          | 2.22        |
| time/                   |             |
|    total_timesteps      | 934000      |
| train/                  |             |
|    approx_kl            | 0.008656064 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.228       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.09        |
|    n_updates            | 1284        |
|    policy_gradient_loss | 0.00062     |
|    value_loss           | 0.291       |
-----------------------------------------
Eval num_timesteps=934500, episode_reward=3.68 +/- 4.25
Episode length: 77.68 +/- 21.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 3.68     |
| time/              |          |
|    total_timesteps | 934500   |
---------------------------------
Eval num_timesteps=935000, episode_reward=3.28 +/- 4.09
Episode length: 80.66 +/- 28.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.7     |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 935000   |
---------------------------------
Eval num_timesteps=935500, episode_reward=3.06 +/- 3.25
Episode length: 79.62 +/- 25.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.6     |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 935500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.1     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 457      |
|    time_elapsed    | 7202     |
|    total_timesteps | 935936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=936000, episode_reward=3.46 +/- 3.42
Episode length: 81.70 +/- 30.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.7        |
|    mean_reward          | 3.46        |
| time/                   |             |
|    total_timesteps      | 936000      |
| train/                  |             |
|    approx_kl            | 0.004845434 |
|    clip_fraction        | 0.0513      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.237       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.142       |
|    n_updates            | 1285        |
|    policy_gradient_loss | 0.00136     |
|    value_loss           | 0.539       |
-----------------------------------------
Eval num_timesteps=936500, episode_reward=2.82 +/- 3.34
Episode length: 74.46 +/- 23.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 936500   |
---------------------------------
Eval num_timesteps=937000, episode_reward=2.20 +/- 2.78
Episode length: 69.06 +/- 21.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.1     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 937000   |
---------------------------------
Eval num_timesteps=937500, episode_reward=3.28 +/- 3.75
Episode length: 82.02 +/- 31.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82       |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 937500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.3     |
|    ep_rew_mean     | 2.76     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 458      |
|    time_elapsed    | 7217     |
|    total_timesteps | 937984   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=938000, episode_reward=2.18 +/- 2.86
Episode length: 78.10 +/- 23.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.1        |
|    mean_reward          | 2.18        |
| time/                   |             |
|    total_timesteps      | 938000      |
| train/                  |             |
|    approx_kl            | 0.007076932 |
|    clip_fraction        | 0.0393      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.242       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.249       |
|    n_updates            | 1287        |
|    policy_gradient_loss | -0.000672   |
|    value_loss           | 0.37        |
-----------------------------------------
Eval num_timesteps=938500, episode_reward=3.14 +/- 3.70
Episode length: 75.68 +/- 22.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | 3.14     |
| time/              |          |
|    total_timesteps | 938500   |
---------------------------------
Eval num_timesteps=939000, episode_reward=3.16 +/- 3.43
Episode length: 79.86 +/- 29.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 939000   |
---------------------------------
Eval num_timesteps=939500, episode_reward=2.94 +/- 3.16
Episode length: 80.48 +/- 26.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.5     |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 939500   |
---------------------------------
Eval num_timesteps=940000, episode_reward=2.84 +/- 4.19
Episode length: 79.36 +/- 27.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.8     |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 459      |
|    time_elapsed    | 7235     |
|    total_timesteps | 940032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=940500, episode_reward=2.26 +/- 2.69
Episode length: 70.50 +/- 21.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.5         |
|    mean_reward          | 2.26         |
| time/                   |              |
|    total_timesteps      | 940500       |
| train/                  |              |
|    approx_kl            | 0.0027455555 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.221        |
|    n_updates            | 1288         |
|    policy_gradient_loss | -0.000694    |
|    value_loss           | 0.564        |
------------------------------------------
Eval num_timesteps=941000, episode_reward=2.76 +/- 3.23
Episode length: 77.88 +/- 26.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 941000   |
---------------------------------
Eval num_timesteps=941500, episode_reward=3.44 +/- 5.09
Episode length: 83.80 +/- 29.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.8     |
|    mean_reward     | 3.44     |
| time/              |          |
|    total_timesteps | 941500   |
---------------------------------
Eval num_timesteps=942000, episode_reward=3.36 +/- 4.07
Episode length: 77.28 +/- 26.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 3.36     |
| time/              |          |
|    total_timesteps | 942000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.5     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 460      |
|    time_elapsed    | 7250     |
|    total_timesteps | 942080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=942500, episode_reward=2.82 +/- 4.02
Episode length: 79.42 +/- 28.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.4        |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 942500      |
| train/                  |             |
|    approx_kl            | 0.002494191 |
|    clip_fraction        | 0.0244      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.157      |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.395       |
|    n_updates            | 1289        |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 0.636       |
-----------------------------------------
Eval num_timesteps=943000, episode_reward=3.08 +/- 3.69
Episode length: 73.08 +/- 27.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | 3.08     |
| time/              |          |
|    total_timesteps | 943000   |
---------------------------------
Eval num_timesteps=943500, episode_reward=2.82 +/- 3.78
Episode length: 74.60 +/- 26.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 943500   |
---------------------------------
Eval num_timesteps=944000, episode_reward=2.38 +/- 2.47
Episode length: 82.90 +/- 25.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.9     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 944000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 2.9      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 461      |
|    time_elapsed    | 7265     |
|    total_timesteps | 944128   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=944500, episode_reward=3.18 +/- 4.18
Episode length: 77.48 +/- 25.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.5        |
|    mean_reward          | 3.18        |
| time/                   |             |
|    total_timesteps      | 944500      |
| train/                  |             |
|    approx_kl            | 0.008259709 |
|    clip_fraction        | 0.0375      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.113       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.219       |
|    n_updates            | 1291        |
|    policy_gradient_loss | 0.00187     |
|    value_loss           | 0.502       |
-----------------------------------------
Eval num_timesteps=945000, episode_reward=3.14 +/- 3.23
Episode length: 83.92 +/- 27.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.9     |
|    mean_reward     | 3.14     |
| time/              |          |
|    total_timesteps | 945000   |
---------------------------------
Eval num_timesteps=945500, episode_reward=3.00 +/- 3.44
Episode length: 85.42 +/- 29.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.4     |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 945500   |
---------------------------------
Eval num_timesteps=946000, episode_reward=2.98 +/- 3.70
Episode length: 81.14 +/- 24.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.1     |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 946000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.4     |
|    ep_rew_mean     | 2.98     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 462      |
|    time_elapsed    | 7281     |
|    total_timesteps | 946176   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=946500, episode_reward=3.48 +/- 4.24
Episode length: 83.24 +/- 28.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.2        |
|    mean_reward          | 3.48        |
| time/                   |             |
|    total_timesteps      | 946500      |
| train/                  |             |
|    approx_kl            | 0.005989823 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.204      |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.087       |
|    n_updates            | 1298        |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 0.227       |
-----------------------------------------
Eval num_timesteps=947000, episode_reward=2.54 +/- 3.40
Episode length: 77.66 +/- 30.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.54     |
| time/              |          |
|    total_timesteps | 947000   |
---------------------------------
Eval num_timesteps=947500, episode_reward=2.48 +/- 3.75
Episode length: 77.76 +/- 22.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 947500   |
---------------------------------
Eval num_timesteps=948000, episode_reward=2.90 +/- 2.99
Episode length: 78.94 +/- 22.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.9     |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 948000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.7     |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 463      |
|    time_elapsed    | 7297     |
|    total_timesteps | 948224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=948500, episode_reward=2.66 +/- 3.42
Episode length: 78.46 +/- 23.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.5         |
|    mean_reward          | 2.66         |
| time/                   |              |
|    total_timesteps      | 948500       |
| train/                  |              |
|    approx_kl            | 0.0027577952 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.26        |
|    explained_variance   | 0.148        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.289        |
|    n_updates            | 1299         |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 0.622        |
------------------------------------------
Eval num_timesteps=949000, episode_reward=2.56 +/- 3.16
Episode length: 79.72 +/- 23.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 949000   |
---------------------------------
Eval num_timesteps=949500, episode_reward=3.38 +/- 4.24
Episode length: 83.94 +/- 27.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.9     |
|    mean_reward     | 3.38     |
| time/              |          |
|    total_timesteps | 949500   |
---------------------------------
Eval num_timesteps=950000, episode_reward=3.82 +/- 4.22
Episode length: 87.22 +/- 30.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.2     |
|    mean_reward     | 3.82     |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.5     |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 464      |
|    time_elapsed    | 7313     |
|    total_timesteps | 950272   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=950500, episode_reward=1.92 +/- 2.63
Episode length: 70.68 +/- 22.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.7         |
|    mean_reward          | 1.92         |
| time/                   |              |
|    total_timesteps      | 950500       |
| train/                  |              |
|    approx_kl            | 0.0062702917 |
|    clip_fraction        | 0.0435       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.192       |
|    explained_variance   | -0.0308      |
|    learning_rate        | 0.0001       |
|    loss                 | 0.178        |
|    n_updates            | 1302         |
|    policy_gradient_loss | -0.00206     |
|    value_loss           | 0.331        |
------------------------------------------
Eval num_timesteps=951000, episode_reward=2.88 +/- 3.10
Episode length: 79.86 +/- 25.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 951000   |
---------------------------------
Eval num_timesteps=951500, episode_reward=2.44 +/- 2.73
Episode length: 77.58 +/- 26.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 951500   |
---------------------------------
Eval num_timesteps=952000, episode_reward=3.50 +/- 3.35
Episode length: 75.40 +/- 27.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 3.5      |
| time/              |          |
|    total_timesteps | 952000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 465      |
|    time_elapsed    | 7328     |
|    total_timesteps | 952320   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=952500, episode_reward=2.58 +/- 3.45
Episode length: 79.68 +/- 26.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.7        |
|    mean_reward          | 2.58        |
| time/                   |             |
|    total_timesteps      | 952500      |
| train/                  |             |
|    approx_kl            | 0.009762178 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.195      |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.11        |
|    n_updates            | 1305        |
|    policy_gradient_loss | -0.00264    |
|    value_loss           | 0.307       |
-----------------------------------------
Eval num_timesteps=953000, episode_reward=2.90 +/- 2.97
Episode length: 78.88 +/- 24.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.9     |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 953000   |
---------------------------------
Eval num_timesteps=953500, episode_reward=3.68 +/- 4.11
Episode length: 83.16 +/- 30.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.2     |
|    mean_reward     | 3.68     |
| time/              |          |
|    total_timesteps | 953500   |
---------------------------------
Eval num_timesteps=954000, episode_reward=3.12 +/- 4.18
Episode length: 78.22 +/- 25.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 3.12     |
| time/              |          |
|    total_timesteps | 954000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.6     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 466      |
|    time_elapsed    | 7343     |
|    total_timesteps | 954368   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=954500, episode_reward=2.40 +/- 3.18
Episode length: 75.06 +/- 21.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.1        |
|    mean_reward          | 2.4         |
| time/                   |             |
|    total_timesteps      | 954500      |
| train/                  |             |
|    approx_kl            | 0.010928457 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.11        |
|    n_updates            | 1307        |
|    policy_gradient_loss | 9.88e-05    |
|    value_loss           | 0.371       |
-----------------------------------------
Eval num_timesteps=955000, episode_reward=2.56 +/- 3.28
Episode length: 76.74 +/- 25.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.7     |
|    mean_reward     | 2.56     |
| time/              |          |
|    total_timesteps | 955000   |
---------------------------------
Eval num_timesteps=955500, episode_reward=3.28 +/- 3.82
Episode length: 82.32 +/- 26.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 955500   |
---------------------------------
Eval num_timesteps=956000, episode_reward=2.96 +/- 3.91
Episode length: 79.26 +/- 25.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.3     |
|    mean_reward     | 2.96     |
| time/              |          |
|    total_timesteps | 956000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.1     |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 467      |
|    time_elapsed    | 7358     |
|    total_timesteps | 956416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=956500, episode_reward=2.38 +/- 3.01
Episode length: 76.26 +/- 23.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.3        |
|    mean_reward          | 2.38        |
| time/                   |             |
|    total_timesteps      | 956500      |
| train/                  |             |
|    approx_kl            | 0.004780913 |
|    clip_fraction        | 0.0605      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.188       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.569       |
|    n_updates            | 1308        |
|    policy_gradient_loss | 0.00187     |
|    value_loss           | 0.696       |
-----------------------------------------
Eval num_timesteps=957000, episode_reward=3.54 +/- 4.60
Episode length: 82.36 +/- 26.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | 3.54     |
| time/              |          |
|    total_timesteps | 957000   |
---------------------------------
Eval num_timesteps=957500, episode_reward=3.10 +/- 3.15
Episode length: 77.62 +/- 21.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.6     |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 957500   |
---------------------------------
Eval num_timesteps=958000, episode_reward=2.82 +/- 3.73
Episode length: 76.22 +/- 29.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 958000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.8     |
|    ep_rew_mean     | 3.41     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 468      |
|    time_elapsed    | 7373     |
|    total_timesteps | 958464   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=958500, episode_reward=2.86 +/- 4.06
Episode length: 84.00 +/- 25.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84           |
|    mean_reward          | 2.86         |
| time/                   |              |
|    total_timesteps      | 958500       |
| train/                  |              |
|    approx_kl            | 0.0115447305 |
|    clip_fraction        | 0.0421       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.254       |
|    explained_variance   | 0.23         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.258        |
|    n_updates            | 1310         |
|    policy_gradient_loss | 0.000538     |
|    value_loss           | 0.945        |
------------------------------------------
Eval num_timesteps=959000, episode_reward=2.28 +/- 2.75
Episode length: 77.22 +/- 27.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 959000   |
---------------------------------
Eval num_timesteps=959500, episode_reward=2.36 +/- 3.03
Episode length: 75.06 +/- 22.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 959500   |
---------------------------------
Eval num_timesteps=960000, episode_reward=3.20 +/- 3.53
Episode length: 80.98 +/- 21.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81       |
|    mean_reward     | 3.2      |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=960500, episode_reward=3.20 +/- 5.30
Episode length: 80.72 +/- 28.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.7     |
|    mean_reward     | 3.2      |
| time/              |          |
|    total_timesteps | 960500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.6     |
|    ep_rew_mean     | 3.4      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 469      |
|    time_elapsed    | 7392     |
|    total_timesteps | 960512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=961000, episode_reward=2.40 +/- 3.08
Episode length: 80.56 +/- 24.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 80.6       |
|    mean_reward          | 2.4        |
| time/                   |            |
|    total_timesteps      | 961000     |
| train/                  |            |
|    approx_kl            | 0.00742907 |
|    clip_fraction        | 0.0781     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.263     |
|    explained_variance   | 0.266      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.209      |
|    n_updates            | 1311       |
|    policy_gradient_loss | 0.00606    |
|    value_loss           | 0.29       |
----------------------------------------
Eval num_timesteps=961500, episode_reward=2.10 +/- 3.33
Episode length: 78.70 +/- 25.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.7     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 961500   |
---------------------------------
Eval num_timesteps=962000, episode_reward=2.86 +/- 3.11
Episode length: 84.24 +/- 24.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.2     |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 962000   |
---------------------------------
Eval num_timesteps=962500, episode_reward=2.94 +/- 4.05
Episode length: 79.10 +/- 28.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.1     |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 962500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.8     |
|    ep_rew_mean     | 3.51     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 470      |
|    time_elapsed    | 7407     |
|    total_timesteps | 962560   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=963000, episode_reward=1.78 +/- 2.34
Episode length: 75.10 +/- 22.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75.1         |
|    mean_reward          | 1.78         |
| time/                   |              |
|    total_timesteps      | 963000       |
| train/                  |              |
|    approx_kl            | 0.0065315585 |
|    clip_fraction        | 0.0574       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.243       |
|    explained_variance   | 0.292        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.361        |
|    n_updates            | 1314         |
|    policy_gradient_loss | -0.00701     |
|    value_loss           | 0.46         |
------------------------------------------
Eval num_timesteps=963500, episode_reward=2.84 +/- 4.39
Episode length: 83.12 +/- 24.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.1     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 963500   |
---------------------------------
Eval num_timesteps=964000, episode_reward=2.44 +/- 4.23
Episode length: 74.64 +/- 28.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 964000   |
---------------------------------
Eval num_timesteps=964500, episode_reward=1.84 +/- 2.54
Episode length: 76.86 +/- 22.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 964500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.9     |
|    ep_rew_mean     | 3.44     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 471      |
|    time_elapsed    | 7422     |
|    total_timesteps | 964608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=965000, episode_reward=2.50 +/- 4.24
Episode length: 82.96 +/- 28.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 83           |
|    mean_reward          | 2.5          |
| time/                   |              |
|    total_timesteps      | 965000       |
| train/                  |              |
|    approx_kl            | 0.0035148426 |
|    clip_fraction        | 0.0427       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.276       |
|    explained_variance   | 0.247        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.279        |
|    n_updates            | 1315         |
|    policy_gradient_loss | 0.000732     |
|    value_loss           | 0.384        |
------------------------------------------
Eval num_timesteps=965500, episode_reward=2.72 +/- 3.23
Episode length: 79.90 +/- 22.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.9     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 965500   |
---------------------------------
Eval num_timesteps=966000, episode_reward=2.24 +/- 3.32
Episode length: 76.94 +/- 18.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 966000   |
---------------------------------
Eval num_timesteps=966500, episode_reward=2.58 +/- 3.24
Episode length: 80.04 +/- 29.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 966500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.8     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 472      |
|    time_elapsed    | 7437     |
|    total_timesteps | 966656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=967000, episode_reward=2.72 +/- 2.84
Episode length: 85.64 +/- 28.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85.6        |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 967000      |
| train/                  |             |
|    approx_kl            | 0.002941765 |
|    clip_fraction        | 0.022       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.249      |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.204       |
|    n_updates            | 1316        |
|    policy_gradient_loss | 0.00242     |
|    value_loss           | 0.372       |
-----------------------------------------
Eval num_timesteps=967500, episode_reward=2.72 +/- 3.44
Episode length: 79.84 +/- 23.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.8     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 967500   |
---------------------------------
Eval num_timesteps=968000, episode_reward=2.78 +/- 3.87
Episode length: 79.56 +/- 23.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.6     |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 968000   |
---------------------------------
Eval num_timesteps=968500, episode_reward=1.64 +/- 2.06
Episode length: 77.14 +/- 21.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 968500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.6     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 473      |
|    time_elapsed    | 7452     |
|    total_timesteps | 968704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=969000, episode_reward=2.16 +/- 2.86
Episode length: 81.44 +/- 27.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.4        |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 969000      |
| train/                  |             |
|    approx_kl            | 0.002802007 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.293      |
|    explained_variance   | 0.123       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.104       |
|    n_updates            | 1317        |
|    policy_gradient_loss | 0.00543     |
|    value_loss           | 0.298       |
-----------------------------------------
Eval num_timesteps=969500, episode_reward=2.58 +/- 3.37
Episode length: 82.34 +/- 22.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 969500   |
---------------------------------
Eval num_timesteps=970000, episode_reward=2.12 +/- 3.10
Episode length: 78.40 +/- 24.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 970000   |
---------------------------------
Eval num_timesteps=970500, episode_reward=2.00 +/- 2.84
Episode length: 80.72 +/- 24.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.7     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 970500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85       |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 474      |
|    time_elapsed    | 7467     |
|    total_timesteps | 970752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=971000, episode_reward=3.24 +/- 4.20
Episode length: 81.40 +/- 22.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81.4         |
|    mean_reward          | 3.24         |
| time/                   |              |
|    total_timesteps      | 971000       |
| train/                  |              |
|    approx_kl            | 0.0070586433 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.276       |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.123        |
|    n_updates            | 1318         |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 0.283        |
------------------------------------------
Eval num_timesteps=971500, episode_reward=2.38 +/- 3.24
Episode length: 75.20 +/- 26.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.2     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 971500   |
---------------------------------
Eval num_timesteps=972000, episode_reward=3.16 +/- 3.51
Episode length: 82.64 +/- 21.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.6     |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 972000   |
---------------------------------
Eval num_timesteps=972500, episode_reward=2.94 +/- 4.22
Episode length: 79.52 +/- 26.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 972500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.6     |
|    ep_rew_mean     | 2.22     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 475      |
|    time_elapsed    | 7482     |
|    total_timesteps | 972800   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=973000, episode_reward=2.02 +/- 3.45
Episode length: 78.40 +/- 25.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.4         |
|    mean_reward          | 2.02         |
| time/                   |              |
|    total_timesteps      | 973000       |
| train/                  |              |
|    approx_kl            | 0.0056940527 |
|    clip_fraction        | 0.0596       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.288       |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.123        |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.006       |
|    value_loss           | 0.245        |
------------------------------------------
Eval num_timesteps=973500, episode_reward=2.50 +/- 3.17
Episode length: 75.54 +/- 25.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.5     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 973500   |
---------------------------------
Eval num_timesteps=974000, episode_reward=2.48 +/- 3.10
Episode length: 77.74 +/- 25.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.48     |
| time/              |          |
|    total_timesteps | 974000   |
---------------------------------
Eval num_timesteps=974500, episode_reward=2.88 +/- 3.60
Episode length: 82.08 +/- 26.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 974500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.6     |
|    ep_rew_mean     | 1.93     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 476      |
|    time_elapsed    | 7497     |
|    total_timesteps | 974848   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=975000, episode_reward=1.98 +/- 2.31
Episode length: 78.72 +/- 20.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.7        |
|    mean_reward          | 1.98        |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.010570884 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0578      |
|    n_updates            | 1322        |
|    policy_gradient_loss | 0.00188     |
|    value_loss           | 0.331       |
-----------------------------------------
Eval num_timesteps=975500, episode_reward=2.94 +/- 3.96
Episode length: 83.96 +/- 26.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84       |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 975500   |
---------------------------------
Eval num_timesteps=976000, episode_reward=2.44 +/- 3.38
Episode length: 77.70 +/- 20.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 976000   |
---------------------------------
Eval num_timesteps=976500, episode_reward=1.92 +/- 3.27
Episode length: 75.26 +/- 31.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.3     |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 976500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.4     |
|    ep_rew_mean     | 2.14     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 477      |
|    time_elapsed    | 7512     |
|    total_timesteps | 976896   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=977000, episode_reward=2.28 +/- 3.18
Episode length: 78.90 +/- 27.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.9        |
|    mean_reward          | 2.28        |
| time/                   |             |
|    total_timesteps      | 977000      |
| train/                  |             |
|    approx_kl            | 0.005864447 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.278      |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.119       |
|    n_updates            | 1324        |
|    policy_gradient_loss | 0.000585    |
|    value_loss           | 0.312       |
-----------------------------------------
Eval num_timesteps=977500, episode_reward=2.80 +/- 3.52
Episode length: 82.02 +/- 23.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82       |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 977500   |
---------------------------------
Eval num_timesteps=978000, episode_reward=2.82 +/- 3.10
Episode length: 84.54 +/- 25.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.5     |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 978000   |
---------------------------------
Eval num_timesteps=978500, episode_reward=2.74 +/- 2.92
Episode length: 78.54 +/- 21.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 978500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79       |
|    ep_rew_mean     | 2.16     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 478      |
|    time_elapsed    | 7528     |
|    total_timesteps | 978944   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=979000, episode_reward=2.24 +/- 2.74
Episode length: 77.00 +/- 24.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77          |
|    mean_reward          | 2.24        |
| time/                   |             |
|    total_timesteps      | 979000      |
| train/                  |             |
|    approx_kl            | 0.010843042 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.285      |
|    explained_variance   | 0.202       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.068       |
|    n_updates            | 1326        |
|    policy_gradient_loss | 0.000355    |
|    value_loss           | 0.29        |
-----------------------------------------
Eval num_timesteps=979500, episode_reward=2.16 +/- 2.44
Episode length: 75.60 +/- 21.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.6     |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 979500   |
---------------------------------
Eval num_timesteps=980000, episode_reward=3.12 +/- 3.91
Episode length: 80.82 +/- 21.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.8     |
|    mean_reward     | 3.12     |
| time/              |          |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=980500, episode_reward=2.20 +/- 2.52
Episode length: 74.76 +/- 21.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.8     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 980500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.7     |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 130      |
|    iterations      | 479      |
|    time_elapsed    | 7542     |
|    total_timesteps | 980992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=981000, episode_reward=2.88 +/- 2.41
Episode length: 89.80 +/- 27.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89.8         |
|    mean_reward          | 2.88         |
| time/                   |              |
|    total_timesteps      | 981000       |
| train/                  |              |
|    approx_kl            | 0.0044267233 |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.275       |
|    explained_variance   | 0.28         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.115        |
|    n_updates            | 1327         |
|    policy_gradient_loss | -0.000939    |
|    value_loss           | 0.339        |
------------------------------------------
Eval num_timesteps=981500, episode_reward=2.20 +/- 2.52
Episode length: 82.70 +/- 22.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.7     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 981500   |
---------------------------------
Eval num_timesteps=982000, episode_reward=2.28 +/- 3.14
Episode length: 85.26 +/- 29.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.3     |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 982000   |
---------------------------------
Eval num_timesteps=982500, episode_reward=1.80 +/- 2.76
Episode length: 76.18 +/- 19.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 982500   |
---------------------------------
Eval num_timesteps=983000, episode_reward=2.86 +/- 3.86
Episode length: 86.20 +/- 25.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.2     |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 983000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 480      |
|    time_elapsed    | 7561     |
|    total_timesteps | 983040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=983500, episode_reward=2.44 +/- 3.43
Episode length: 77.84 +/- 23.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.8        |
|    mean_reward          | 2.44        |
| time/                   |             |
|    total_timesteps      | 983500      |
| train/                  |             |
|    approx_kl            | 0.002512167 |
|    clip_fraction        | 0.0272      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.268      |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.183       |
|    n_updates            | 1328        |
|    policy_gradient_loss | 0.00391     |
|    value_loss           | 0.302       |
-----------------------------------------
Eval num_timesteps=984000, episode_reward=2.42 +/- 3.37
Episode length: 86.38 +/- 22.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.4     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 984000   |
---------------------------------
Eval num_timesteps=984500, episode_reward=2.26 +/- 2.86
Episode length: 77.96 +/- 23.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 2.26     |
| time/              |          |
|    total_timesteps | 984500   |
---------------------------------
Eval num_timesteps=985000, episode_reward=1.64 +/- 2.67
Episode length: 80.42 +/- 25.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.4     |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 985000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.5     |
|    ep_rew_mean     | 1.88     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 481      |
|    time_elapsed    | 7577     |
|    total_timesteps | 985088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=985500, episode_reward=2.30 +/- 3.06
Episode length: 78.58 +/- 19.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.6         |
|    mean_reward          | 2.3          |
| time/                   |              |
|    total_timesteps      | 985500       |
| train/                  |              |
|    approx_kl            | 0.0040517976 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.216       |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0889       |
|    n_updates            | 1329         |
|    policy_gradient_loss | -8.6e-05     |
|    value_loss           | 0.174        |
------------------------------------------
Eval num_timesteps=986000, episode_reward=2.00 +/- 3.56
Episode length: 84.48 +/- 21.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.5     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 986000   |
---------------------------------
Eval num_timesteps=986500, episode_reward=1.82 +/- 2.56
Episode length: 89.76 +/- 29.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.8     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 986500   |
---------------------------------
Eval num_timesteps=987000, episode_reward=2.72 +/- 3.33
Episode length: 87.80 +/- 24.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.8     |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 987000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.9     |
|    ep_rew_mean     | 1.68     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 482      |
|    time_elapsed    | 7592     |
|    total_timesteps | 987136   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=987500, episode_reward=1.98 +/- 2.52
Episode length: 77.50 +/- 26.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.5         |
|    mean_reward          | 1.98         |
| time/                   |              |
|    total_timesteps      | 987500       |
| train/                  |              |
|    approx_kl            | 0.0048226286 |
|    clip_fraction        | 0.0382       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.196       |
|    explained_variance   | 0.108        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.199        |
|    n_updates            | 1331         |
|    policy_gradient_loss | 0.00195      |
|    value_loss           | 0.215        |
------------------------------------------
Eval num_timesteps=988000, episode_reward=2.36 +/- 2.81
Episode length: 84.88 +/- 24.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.9     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 988000   |
---------------------------------
Eval num_timesteps=988500, episode_reward=1.90 +/- 3.04
Episode length: 77.10 +/- 23.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 988500   |
---------------------------------
Eval num_timesteps=989000, episode_reward=2.12 +/- 3.25
Episode length: 81.80 +/- 26.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.8     |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 989000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.7     |
|    ep_rew_mean     | 1.88     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 483      |
|    time_elapsed    | 7607     |
|    total_timesteps | 989184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=989500, episode_reward=2.56 +/- 3.60
Episode length: 82.16 +/- 26.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 82.2         |
|    mean_reward          | 2.56         |
| time/                   |              |
|    total_timesteps      | 989500       |
| train/                  |              |
|    approx_kl            | 0.0044194567 |
|    clip_fraction        | 0.0558       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.265       |
|    explained_variance   | 0.28         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0385       |
|    n_updates            | 1332         |
|    policy_gradient_loss | -0.000863    |
|    value_loss           | 0.284        |
------------------------------------------
Eval num_timesteps=990000, episode_reward=2.10 +/- 2.83
Episode length: 80.76 +/- 22.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.8     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 990000   |
---------------------------------
Eval num_timesteps=990500, episode_reward=2.22 +/- 4.15
Episode length: 81.26 +/- 24.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.3     |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 990500   |
---------------------------------
Eval num_timesteps=991000, episode_reward=3.54 +/- 3.44
Episode length: 83.76 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.8     |
|    mean_reward     | 3.54     |
| time/              |          |
|    total_timesteps | 991000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.6     |
|    ep_rew_mean     | 2.15     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 484      |
|    time_elapsed    | 7623     |
|    total_timesteps | 991232   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=991500, episode_reward=2.34 +/- 3.23
Episode length: 76.06 +/- 22.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.1        |
|    mean_reward          | 2.34        |
| time/                   |             |
|    total_timesteps      | 991500      |
| train/                  |             |
|    approx_kl            | 0.009455806 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.246      |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.105       |
|    n_updates            | 1335        |
|    policy_gradient_loss | -0.00298    |
|    value_loss           | 0.583       |
-----------------------------------------
Eval num_timesteps=992000, episode_reward=2.36 +/- 3.43
Episode length: 82.36 +/- 30.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | 2.36     |
| time/              |          |
|    total_timesteps | 992000   |
---------------------------------
Eval num_timesteps=992500, episode_reward=1.78 +/- 2.83
Episode length: 76.00 +/- 22.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 992500   |
---------------------------------
Eval num_timesteps=993000, episode_reward=1.82 +/- 2.41
Episode length: 75.08 +/- 25.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 993000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 485      |
|    time_elapsed    | 7637     |
|    total_timesteps | 993280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=993500, episode_reward=2.92 +/- 3.31
Episode length: 84.94 +/- 22.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.9        |
|    mean_reward          | 2.92        |
| time/                   |             |
|    total_timesteps      | 993500      |
| train/                  |             |
|    approx_kl            | 0.003652568 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.122       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.193       |
|    n_updates            | 1336        |
|    policy_gradient_loss | 0.00166     |
|    value_loss           | 0.451       |
-----------------------------------------
Eval num_timesteps=994000, episode_reward=2.50 +/- 3.91
Episode length: 78.56 +/- 31.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 2.5      |
| time/              |          |
|    total_timesteps | 994000   |
---------------------------------
Eval num_timesteps=994500, episode_reward=2.66 +/- 3.13
Episode length: 86.32 +/- 25.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.3     |
|    mean_reward     | 2.66     |
| time/              |          |
|    total_timesteps | 994500   |
---------------------------------
Eval num_timesteps=995000, episode_reward=2.42 +/- 3.42
Episode length: 76.36 +/- 21.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.4     |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 995000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.8     |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 486      |
|    time_elapsed    | 7653     |
|    total_timesteps | 995328   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=995500, episode_reward=1.96 +/- 2.34
Episode length: 74.78 +/- 19.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 74.8       |
|    mean_reward          | 1.96       |
| time/                   |            |
|    total_timesteps      | 995500     |
| train/                  |            |
|    approx_kl            | 0.00500747 |
|    clip_fraction        | 0.05       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.271     |
|    explained_variance   | 0.223      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0676     |
|    n_updates            | 1338       |
|    policy_gradient_loss | -0.00223   |
|    value_loss           | 0.286      |
----------------------------------------
Eval num_timesteps=996000, episode_reward=2.38 +/- 3.37
Episode length: 76.34 +/- 24.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=996500, episode_reward=2.38 +/- 2.74
Episode length: 78.16 +/- 23.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | 2.38     |
| time/              |          |
|    total_timesteps | 996500   |
---------------------------------
Eval num_timesteps=997000, episode_reward=2.22 +/- 3.09
Episode length: 80.98 +/- 27.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81       |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 997000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.8     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 487      |
|    time_elapsed    | 7668     |
|    total_timesteps | 997376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=997500, episode_reward=2.06 +/- 2.96
Episode length: 77.06 +/- 25.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.1         |
|    mean_reward          | 2.06         |
| time/                   |              |
|    total_timesteps      | 997500       |
| train/                  |              |
|    approx_kl            | 0.0029357937 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.249       |
|    explained_variance   | 0.274        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.168        |
|    n_updates            | 1339         |
|    policy_gradient_loss | -0.000824    |
|    value_loss           | 0.48         |
------------------------------------------
Eval num_timesteps=998000, episode_reward=3.18 +/- 3.24
Episode length: 86.66 +/- 30.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.7     |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 998000   |
---------------------------------
Eval num_timesteps=998500, episode_reward=2.84 +/- 3.75
Episode length: 81.14 +/- 24.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.1     |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 998500   |
---------------------------------
Eval num_timesteps=999000, episode_reward=3.34 +/- 3.97
Episode length: 85.46 +/- 22.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.5     |
|    mean_reward     | 3.34     |
| time/              |          |
|    total_timesteps | 999000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.6     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 130      |
|    iterations      | 488      |
|    time_elapsed    | 7684     |
|    total_timesteps | 999424   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=999500, episode_reward=2.84 +/- 3.41
Episode length: 83.14 +/- 24.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.1        |
|    mean_reward          | 2.84        |
| time/                   |             |
|    total_timesteps      | 999500      |
| train/                  |             |
|    approx_kl            | 0.008163481 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.114       |
|    n_updates            | 1341        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 0.387       |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=1.98 +/- 2.83
Episode length: 81.10 +/- 26.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.1     |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1000500, episode_reward=2.62 +/- 2.83
Episode length: 88.12 +/- 34.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.1     |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 1000500  |
---------------------------------
Eval num_timesteps=1001000, episode_reward=2.08 +/- 2.96
Episode length: 78.60 +/- 26.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 1001000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 489      |
|    time_elapsed    | 7700     |
|    total_timesteps | 1001472  |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/deathmatch/ppo-2-btn(menos)-fs(7)-steps(1000000)/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Early stopping at step 1 due to reaching max kl: 0.02
