/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-4221.94 +/- 623.68
Episode length: 47.50 +/- 14.79
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 47.5      |
|    mean_reward      | -4.22e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 500       |
-----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | -4.3e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 4        |
|    fps              | 313      |
|    time_elapsed     | 1        |
|    total_timesteps  | 503      |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 100       |
|    ep_rew_mean      | -4.21e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 8         |
|    fps              | 475       |
|    time_elapsed     | 1         |
|    total_timesteps  | 804       |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 79.8      |
|    ep_rew_mean      | -4.27e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 12        |
|    fps              | 551       |
|    time_elapsed     | 1         |
|    total_timesteps  | 958       |
-----------------------------------
Eval num_timesteps=1000, episode_reward=-4201.95 +/- 904.91
Episode length: 51.42 +/- 16.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | -4.2e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=-4497.94 +/- 683.44
Episode length: 48.40 +/- 18.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.4     |
|    mean_reward      | -4.5e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | -4.1e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 16       |
|    fps              | 337      |
|    time_elapsed     | 4        |
|    total_timesteps  | 1631     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 99        |
|    ep_rew_mean      | -4.11e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 20        |
|    fps              | 401       |
|    time_elapsed     | 4         |
|    total_timesteps  | 1979      |
-----------------------------------
Eval num_timesteps=2000, episode_reward=-4391.54 +/- 681.91
Episode length: 48.54 +/- 17.02
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 48.5      |
|    mean_reward      | -4.39e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 2000      |
-----------------------------------
Eval num_timesteps=2500, episode_reward=-4425.95 +/- 707.38
Episode length: 47.26 +/- 13.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 47.3      |
|    mean_reward      | -4.43e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 2500      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 113       |
|    ep_rew_mean      | -4.16e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 24        |
|    fps              | 342       |
|    time_elapsed     | 7         |
|    total_timesteps  | 2704      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 103       |
|    ep_rew_mean      | -4.19e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 28        |
|    fps              | 362       |
|    time_elapsed     | 7         |
|    total_timesteps  | 2888      |
-----------------------------------
Eval num_timesteps=3000, episode_reward=-4535.14 +/- 655.99
Episode length: 48.56 +/- 15.09
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 48.6      |
|    mean_reward      | -4.54e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 3000      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 99.3      |
|    ep_rew_mean      | -4.24e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 32        |
|    fps              | 336       |
|    time_elapsed     | 9         |
|    total_timesteps  | 3178      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95.2      |
|    ep_rew_mean      | -4.22e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 36        |
|    fps              | 359       |
|    time_elapsed     | 9         |
|    total_timesteps  | 3428      |
-----------------------------------
Eval num_timesteps=3500, episode_reward=-4165.94 +/- 782.23
Episode length: 46.04 +/- 14.01
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 46        |
|    mean_reward      | -4.17e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 3500      |
-----------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=-4373.14 +/- 797.97
Episode length: 48.50 +/- 19.14
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 48.5      |
|    mean_reward      | -4.37e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 4000      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 103       |
|    ep_rew_mean      | -4.18e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 40        |
|    fps              | 329       |
|    time_elapsed     | 12        |
|    total_timesteps  | 4107      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 101       |
|    ep_rew_mean      | -4.19e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 44        |
|    fps              | 355       |
|    time_elapsed     | 12        |
|    total_timesteps  | 4464      |
-----------------------------------
Eval num_timesteps=4500, episode_reward=-4454.74 +/- 772.62
Episode length: 44.22 +/- 15.98
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 44.2      |
|    mean_reward      | -4.45e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 4500      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 97.1      |
|    ep_rew_mean      | -4.19e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 48        |
|    fps              | 335       |
|    time_elapsed     | 13        |
|    total_timesteps  | 4662      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 92.8      |
|    ep_rew_mean      | -4.14e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 52        |
|    fps              | 346       |
|    time_elapsed     | 13        |
|    total_timesteps  | 4824      |
-----------------------------------
Eval num_timesteps=5000, episode_reward=-4560.35 +/- 727.12
Episode length: 53.64 +/- 18.12
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 53.6      |
|    mean_reward      | -4.56e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 5000      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 94        |
|    ep_rew_mean      | -4.15e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 56        |
|    fps              | 337       |
|    time_elapsed     | 15        |
|    total_timesteps  | 5263      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 91.3      |
|    ep_rew_mean      | -4.17e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 60        |
|    fps              | 349       |
|    time_elapsed     | 15        |
|    total_timesteps  | 5479      |
-----------------------------------
Eval num_timesteps=5500, episode_reward=-4403.14 +/- 715.73
Episode length: 50.64 +/- 19.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.6     |
|    mean_reward      | -4.4e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5500     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 88.2      |
|    ep_rew_mean      | -4.16e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 64        |
|    fps              | 328       |
|    time_elapsed     | 17        |
|    total_timesteps  | 5646      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 86.3      |
|    ep_rew_mean      | -4.14e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 68        |
|    fps              | 340       |
|    time_elapsed     | 17        |
|    total_timesteps  | 5869      |
-----------------------------------
Eval num_timesteps=6000, episode_reward=-4612.74 +/- 649.27
Episode length: 54.26 +/- 20.75
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 54.3      |
|    mean_reward      | -4.61e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 6000      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 86.5      |
|    ep_rew_mean      | -4.14e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 72        |
|    fps              | 329       |
|    time_elapsed     | 18        |
|    total_timesteps  | 6230      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.5      |
|    ep_rew_mean      | -4.14e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 76        |
|    fps              | 342       |
|    time_elapsed     | 18        |
|    total_timesteps  | 6497      |
-----------------------------------
Eval num_timesteps=6500, episode_reward=-4350.73 +/- 649.89
Episode length: 50.08 +/- 14.02
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50.1      |
|    mean_reward      | -4.35e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 6500      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 84.2      |
|    ep_rew_mean      | -4.13e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 80        |
|    fps              | 328       |
|    time_elapsed     | 20        |
|    total_timesteps  | 6733      |
-----------------------------------
Eval num_timesteps=7000, episode_reward=-4432.74 +/- 756.84
Episode length: 47.26 +/- 16.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 47.3      |
|    mean_reward      | -4.43e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 7000      |
-----------------------------------
Eval num_timesteps=7500, episode_reward=-4513.95 +/- 661.67
Episode length: 54.26 +/- 18.39
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 54.3      |
|    mean_reward      | -4.51e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 7500      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 91.9      |
|    ep_rew_mean      | -4.11e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 84        |
|    fps              | 326       |
|    time_elapsed     | 23        |
|    total_timesteps  | 7716      |
-----------------------------------
Eval num_timesteps=8000, episode_reward=-4329.15 +/- 703.56
Episode length: 48.74 +/- 17.09
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 48.7      |
|    mean_reward      | -4.33e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 8000      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | -4.1e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 88       |
|    fps              | 327      |
|    time_elapsed     | 25       |
|    total_timesteps  | 8271     |
----------------------------------
Eval num_timesteps=8500, episode_reward=-4419.95 +/- 735.25
Episode length: 51.10 +/- 19.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 51.1      |
|    mean_reward      | -4.42e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 8500      |
-----------------------------------
Eval num_timesteps=9000, episode_reward=-4529.15 +/- 670.54
Episode length: 51.06 +/- 17.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 51.1      |
|    mean_reward      | -4.53e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 9000      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 99.9      |
|    ep_rew_mean      | -4.08e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 92        |
|    fps              | 323       |
|    time_elapsed     | 28        |
|    total_timesteps  | 9194      |
-----------------------------------
Eval num_timesteps=9500, episode_reward=-4347.94 +/- 842.21
Episode length: 50.04 +/- 20.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50        |
|    mean_reward      | -4.35e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 9500      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | -4.1e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 96       |
|    fps              | 318      |
|    time_elapsed     | 29       |
|    total_timesteps  | 9555     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | -4.1e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 100      |
|    fps              | 325      |
|    time_elapsed     | 30       |
|    total_timesteps  | 9768     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 94.7      |
|    ep_rew_mean      | -4.06e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 104       |
|    fps              | 331       |
|    time_elapsed     | 30        |
|    total_timesteps  | 9973      |
-----------------------------------
Eval num_timesteps=10000, episode_reward=-4505.54 +/- 598.00
Episode length: 49.10 +/- 13.93
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 49.1      |
|    mean_reward      | -4.51e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 10000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 93.4      |
|    ep_rew_mean      | -4.05e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 108       |
|    fps              | 320       |
|    time_elapsed     | 31        |
|    total_timesteps  | 10143     |
-----------------------------------
Eval num_timesteps=10500, episode_reward=-4483.95 +/- 677.88
Episode length: 49.94 +/- 19.68
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 49.9      |
|    mean_reward      | -4.48e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 10500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 99.3      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 112       |
|    fps              | 327       |
|    time_elapsed     | 33        |
|    total_timesteps  | 10888     |
-----------------------------------
Eval num_timesteps=11000, episode_reward=-4459.14 +/- 650.34
Episode length: 52.20 +/- 17.11
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 52.2      |
|    mean_reward      | -4.46e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 11000     |
-----------------------------------
Eval num_timesteps=11500, episode_reward=-4345.94 +/- 757.73
Episode length: 49.54 +/- 16.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 49.5      |
|    mean_reward      | -4.35e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 11500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 101       |
|    ep_rew_mean      | -4.04e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 116       |
|    fps              | 322       |
|    time_elapsed     | 36        |
|    total_timesteps  | 11766     |
-----------------------------------
Eval num_timesteps=12000, episode_reward=-4337.93 +/- 764.08
Episode length: 47.70 +/- 15.28
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 47.7      |
|    mean_reward      | -4.34e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 12000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 100       |
|    ep_rew_mean      | -4.04e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 120       |
|    fps              | 316       |
|    time_elapsed     | 37        |
|    total_timesteps  | 12018     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95.9      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 124       |
|    fps              | 323       |
|    time_elapsed     | 38        |
|    total_timesteps  | 12297     |
-----------------------------------
Eval num_timesteps=12500, episode_reward=-4373.41 +/- 844.24
Episode length: 50.08 +/- 18.42
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50.1      |
|    mean_reward      | -4.37e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 12500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 96.7      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 128       |
|    fps              | 317       |
|    time_elapsed     | 39        |
|    total_timesteps  | 12562     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95.7      |
|    ep_rew_mean      | -4.01e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 132       |
|    fps              | 321       |
|    time_elapsed     | 39        |
|    total_timesteps  | 12743     |
-----------------------------------
Eval num_timesteps=13000, episode_reward=-4377.94 +/- 720.62
Episode length: 50.84 +/- 17.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50.8      |
|    mean_reward      | -4.38e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 13000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 100       |
|    ep_rew_mean      | -3.99e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 136       |
|    fps              | 325       |
|    time_elapsed     | 41        |
|    total_timesteps  | 13423     |
-----------------------------------
Eval num_timesteps=13500, episode_reward=-4210.34 +/- 822.96
Episode length: 46.50 +/- 18.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 46.5      |
|    mean_reward      | -4.21e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 13500     |
-----------------------------------
Eval num_timesteps=14000, episode_reward=-4491.95 +/- 591.25
Episode length: 53.92 +/- 20.25
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 53.9      |
|    mean_reward      | -4.49e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 14000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 100       |
|    ep_rew_mean      | -3.98e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 140       |
|    fps              | 318       |
|    time_elapsed     | 44        |
|    total_timesteps  | 14107     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 98.9      |
|    ep_rew_mean      | -3.98e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 144       |
|    fps              | 323       |
|    time_elapsed     | 44        |
|    total_timesteps  | 14351     |
-----------------------------------
Eval num_timesteps=14500, episode_reward=-4477.55 +/- 734.66
Episode length: 53.44 +/- 16.72
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 53.4      |
|    mean_reward      | -4.48e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 14500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 98.8      |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 148       |
|    fps              | 316       |
|    time_elapsed     | 45        |
|    total_timesteps  | 14537     |
-----------------------------------
Eval num_timesteps=15000, episode_reward=-4667.15 +/- 528.59
Episode length: 52.84 +/- 17.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 52.8      |
|    mean_reward      | -4.67e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 15000     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | -4e+03   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 152      |
|    fps              | 319      |
|    time_elapsed     | 47       |
|    total_timesteps  | 15262    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 102       |
|    ep_rew_mean      | -3.98e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 156       |
|    fps              | 323       |
|    time_elapsed     | 47        |
|    total_timesteps  | 15448     |
-----------------------------------
Eval num_timesteps=15500, episode_reward=-4300.34 +/- 839.79
Episode length: 49.62 +/- 18.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.6     |
|    mean_reward      | -4.3e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15500    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 103       |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 160       |
|    fps              | 319       |
|    time_elapsed     | 49        |
|    total_timesteps  | 15737     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 103       |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 164       |
|    fps              | 322       |
|    time_elapsed     | 49        |
|    total_timesteps  | 15933     |
-----------------------------------
Eval num_timesteps=16000, episode_reward=-4570.98 +/- 795.71
Episode length: 50.96 +/- 19.40
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 51        |
|    mean_reward      | -4.57e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 16000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 103       |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 168       |
|    fps              | 316       |
|    time_elapsed     | 50        |
|    total_timesteps  | 16126     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 101       |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 172       |
|    fps              | 320       |
|    time_elapsed     | 50        |
|    total_timesteps  | 16311     |
-----------------------------------
Eval num_timesteps=16500, episode_reward=-4500.34 +/- 716.88
Episode length: 54.34 +/- 22.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.3     |
|    mean_reward      | -4.5e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16500    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 101       |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 176       |
|    fps              | 315       |
|    time_elapsed     | 52        |
|    total_timesteps  | 16578     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 102       |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 180       |
|    fps              | 321       |
|    time_elapsed     | 52        |
|    total_timesteps  | 16944     |
-----------------------------------
Eval num_timesteps=17000, episode_reward=-4304.74 +/- 761.20
Episode length: 47.02 +/- 14.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | -4.3e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17000    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95        |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 184       |
|    fps              | 318       |
|    time_elapsed     | 54        |
|    total_timesteps  | 17214     |
-----------------------------------
Eval num_timesteps=17500, episode_reward=-4352.75 +/- 744.59
Episode length: 50.78 +/- 19.31
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50.8      |
|    mean_reward      | -4.35e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 17500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95.5      |
|    ep_rew_mean      | -3.99e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 188       |
|    fps              | 319       |
|    time_elapsed     | 55        |
|    total_timesteps  | 17819     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 87.8      |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 192       |
|    fps              | 322       |
|    time_elapsed     | 55        |
|    total_timesteps  | 17977     |
-----------------------------------
Eval num_timesteps=18000, episode_reward=-4459.95 +/- 648.76
Episode length: 52.74 +/- 16.60
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 52.7      |
|    mean_reward      | -4.46e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 18000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 86.8      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 196       |
|    fps              | 317       |
|    time_elapsed     | 57        |
|    total_timesteps  | 18232     |
-----------------------------------
Eval num_timesteps=18500, episode_reward=-4346.34 +/- 672.28
Episode length: 48.16 +/- 15.83
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 48.2      |
|    mean_reward      | -4.35e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 18500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 88.4      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 200       |
|    fps              | 316       |
|    time_elapsed     | 58        |
|    total_timesteps  | 18607     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 87.9      |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 204       |
|    fps              | 318       |
|    time_elapsed     | 58        |
|    total_timesteps  | 18761     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 88        |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 208       |
|    fps              | 321       |
|    time_elapsed     | 58        |
|    total_timesteps  | 18940     |
-----------------------------------
Eval num_timesteps=19000, episode_reward=-4511.94 +/- 683.25
Episode length: 49.74 +/- 14.64
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 49.7      |
|    mean_reward      | -4.51e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 19000     |
-----------------------------------
Eval num_timesteps=19500, episode_reward=-4314.35 +/- 725.28
Episode length: 51.14 +/- 16.90
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 51.1      |
|    mean_reward      | -4.31e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 19500     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 212      |
|    fps              | 315      |
|    time_elapsed     | 62       |
|    total_timesteps  | 19567    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 81.2      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 216       |
|    fps              | 320       |
|    time_elapsed     | 62        |
|    total_timesteps  | 19886     |
-----------------------------------
Eval num_timesteps=20000, episode_reward=-4366.32 +/- 826.30
Episode length: 52.68 +/- 16.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 52.7      |
|    mean_reward      | -4.37e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 20000     |
-----------------------------------
Eval num_timesteps=20500, episode_reward=-4460.75 +/- 716.26
Episode length: 50.30 +/- 14.79
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50.3      |
|    mean_reward      | -4.46e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 20500     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 220      |
|    fps              | 314      |
|    time_elapsed     | 65       |
|    total_timesteps  | 20518    |
----------------------------------
Eval num_timesteps=21000, episode_reward=-4321.54 +/- 864.54
Episode length: 49.86 +/- 19.98
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 49.9      |
|    mean_reward      | -4.32e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 21000     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.8     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 224      |
|    fps              | 316      |
|    time_elapsed     | 66       |
|    total_timesteps  | 21175    |
----------------------------------
Eval num_timesteps=21500, episode_reward=-4496.74 +/- 688.48
Episode length: 50.46 +/- 17.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.5     |
|    mean_reward      | -4.5e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.2     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 228      |
|    fps              | 315      |
|    time_elapsed     | 68       |
|    total_timesteps  | 21580    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 90.7      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 232       |
|    fps              | 318       |
|    time_elapsed     | 68        |
|    total_timesteps  | 21808     |
-----------------------------------
Eval num_timesteps=22000, episode_reward=-4363.55 +/- 761.01
Episode length: 50.10 +/- 14.42
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50.1      |
|    mean_reward      | -4.36e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 22000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 86.2      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 236       |
|    fps              | 314       |
|    time_elapsed     | 70        |
|    total_timesteps  | 22038     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 82.4      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 240       |
|    fps              | 318       |
|    time_elapsed     | 70        |
|    total_timesteps  | 22343     |
-----------------------------------
Eval num_timesteps=22500, episode_reward=-4231.14 +/- 846.65
Episode length: 50.32 +/- 17.39
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50.3      |
|    mean_reward      | -4.23e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 22500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 82.2      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 244       |
|    fps              | 314       |
|    time_elapsed     | 71        |
|    total_timesteps  | 22568     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 82        |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 248       |
|    fps              | 317       |
|    time_elapsed     | 71        |
|    total_timesteps  | 22742     |
-----------------------------------
Eval num_timesteps=23000, episode_reward=-4425.94 +/- 637.40
Episode length: 48.38 +/- 18.68
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 48.4      |
|    mean_reward      | -4.43e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 23000     |
-----------------------------------
Eval num_timesteps=23500, episode_reward=-4446.34 +/- 697.85
Episode length: 49.86 +/- 17.10
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 49.9      |
|    mean_reward      | -4.45e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 23500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 86.2      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 252       |
|    fps              | 319       |
|    time_elapsed     | 74        |
|    total_timesteps  | 23887     |
-----------------------------------
Eval num_timesteps=24000, episode_reward=-4529.15 +/- 604.99
Episode length: 52.16 +/- 16.72
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 52.2      |
|    mean_reward      | -4.53e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 24000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 87.1      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 256       |
|    fps              | 316       |
|    time_elapsed     | 76        |
|    total_timesteps  | 24157     |
-----------------------------------
Eval num_timesteps=24500, episode_reward=-4371.95 +/- 884.80
Episode length: 49.62 +/- 17.80
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 49.6      |
|    mean_reward      | -4.37e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 24500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 90.6      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 260       |
|    fps              | 317       |
|    time_elapsed     | 78        |
|    total_timesteps  | 24797     |
-----------------------------------
Eval num_timesteps=25000, episode_reward=-4459.94 +/- 571.14
Episode length: 52.40 +/- 13.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 52.4      |
|    mean_reward      | -4.46e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 25000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95.3      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 264       |
|    fps              | 317       |
|    time_elapsed     | 80        |
|    total_timesteps  | 25467     |
-----------------------------------
Eval num_timesteps=25500, episode_reward=-4440.74 +/- 717.45
Episode length: 50.60 +/- 19.35
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50.6      |
|    mean_reward      | -4.44e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 25500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 98.6      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 268       |
|    fps              | 316       |
|    time_elapsed     | 82        |
|    total_timesteps  | 25990     |
-----------------------------------
Eval num_timesteps=26000, episode_reward=-4417.94 +/- 716.59
Episode length: 51.14 +/- 16.87
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 51.1      |
|    mean_reward      | -4.42e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 26000     |
-----------------------------------
Eval num_timesteps=26500, episode_reward=-4261.54 +/- 945.04
Episode length: 48.44 +/- 15.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 48.4      |
|    mean_reward      | -4.26e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 26500     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 272      |
|    fps              | 309      |
|    time_elapsed     | 85       |
|    total_timesteps  | 26517    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 102       |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 276       |
|    fps              | 311       |
|    time_elapsed     | 85        |
|    total_timesteps  | 26731     |
-----------------------------------
Eval num_timesteps=27000, episode_reward=-4481.94 +/- 689.97
Episode length: 49.78 +/- 14.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 49.8      |
|    mean_reward      | -4.48e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 27000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 101       |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 280       |
|    fps              | 308       |
|    time_elapsed     | 87        |
|    total_timesteps  | 27042     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 284      |
|    fps              | 311      |
|    time_elapsed     | 87       |
|    total_timesteps  | 27264    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95.9      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 288       |
|    fps              | 312       |
|    time_elapsed     | 87        |
|    total_timesteps  | 27412     |
-----------------------------------
Eval num_timesteps=27500, episode_reward=-4584.75 +/- 653.82
Episode length: 53.50 +/- 18.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 53.5      |
|    mean_reward      | -4.58e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 27500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 96.2      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 292       |
|    fps              | 308       |
|    time_elapsed     | 89        |
|    total_timesteps  | 27602     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95.6      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 296       |
|    fps              | 310       |
|    time_elapsed     | 89        |
|    total_timesteps  | 27789     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 93.4      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 300       |
|    fps              | 311       |
|    time_elapsed     | 89        |
|    total_timesteps  | 27949     |
-----------------------------------
Eval num_timesteps=28000, episode_reward=-4276.35 +/- 771.87
Episode length: 45.40 +/- 13.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 45.4      |
|    mean_reward      | -4.28e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 28000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 93.9      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 304       |
|    fps              | 308       |
|    time_elapsed     | 91        |
|    total_timesteps  | 28147     |
-----------------------------------
Eval num_timesteps=28500, episode_reward=-4147.14 +/- 965.56
Episode length: 47.92 +/- 15.67
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 47.9      |
|    mean_reward      | -4.15e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 28500     |
-----------------------------------
New best mean reward!
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 98.8      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 308       |
|    fps              | 309       |
|    time_elapsed     | 93        |
|    total_timesteps  | 28822     |
-----------------------------------
Eval num_timesteps=29000, episode_reward=-4441.15 +/- 772.35
Episode length: 52.56 +/- 18.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 52.6      |
|    mean_reward      | -4.44e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 29000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 94.9      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 312       |
|    fps              | 305       |
|    time_elapsed     | 95        |
|    total_timesteps  | 29060     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 93.3      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 316       |
|    fps              | 307       |
|    time_elapsed     | 95        |
|    total_timesteps  | 29220     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 88.8      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 320       |
|    fps              | 309       |
|    time_elapsed     | 95        |
|    total_timesteps  | 29399     |
-----------------------------------
Eval num_timesteps=29500, episode_reward=-4359.53 +/- 669.22
Episode length: 46.32 +/- 14.04
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 46.3      |
|    mean_reward      | -4.36e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 29500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 84.6      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 324       |
|    fps              | 306       |
|    time_elapsed     | 96        |
|    total_timesteps  | 29634     |
-----------------------------------
Eval num_timesteps=30000, episode_reward=-4623.96 +/- 527.11
Episode length: 52.76 +/- 16.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 52.8      |
|    mean_reward      | -4.62e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 30000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 87.6      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 328       |
|    fps              | 307       |
|    time_elapsed     | 98        |
|    total_timesteps  | 30338     |
-----------------------------------
Eval num_timesteps=30500, episode_reward=-4322.74 +/- 696.91
Episode length: 48.56 +/- 17.75
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 48.6      |
|    mean_reward      | -4.32e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 30500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 87.2      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 332       |
|    fps              | 303       |
|    time_elapsed     | 100       |
|    total_timesteps  | 30532     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 86.5      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 336       |
|    fps              | 305       |
|    time_elapsed     | 100       |
|    total_timesteps  | 30688     |
-----------------------------------
Eval num_timesteps=31000, episode_reward=-4495.96 +/- 760.23
Episode length: 54.66 +/- 18.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.7     |
|    mean_reward      | -4.5e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31000    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 89.7      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 340       |
|    fps              | 305       |
|    time_elapsed     | 102       |
|    total_timesteps  | 31308     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 89.2      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 344       |
|    fps              | 306       |
|    time_elapsed     | 102       |
|    total_timesteps  | 31486     |
-----------------------------------
Eval num_timesteps=31500, episode_reward=-4539.95 +/- 641.73
Episode length: 48.54 +/- 16.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 48.5      |
|    mean_reward      | -4.54e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 31500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 90.9      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 348       |
|    fps              | 305       |
|    time_elapsed     | 104       |
|    total_timesteps  | 31835     |
-----------------------------------
Eval num_timesteps=32000, episode_reward=-4503.95 +/- 554.83
Episode length: 50.86 +/- 17.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.9     |
|    mean_reward      | -4.5e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32000    |
----------------------------------
Eval num_timesteps=32500, episode_reward=-4477.95 +/- 775.45
Episode length: 51.24 +/- 15.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 51.2      |
|    mean_reward      | -4.48e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 32500     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.9     |
|    ep_rew_mean      | -4e+03   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 352      |
|    fps              | 302      |
|    time_elapsed     | 107      |
|    total_timesteps  | 32673    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 87.2      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 356       |
|    fps              | 304       |
|    time_elapsed     | 107       |
|    total_timesteps  | 32879     |
-----------------------------------
Eval num_timesteps=33000, episode_reward=-4395.54 +/- 724.12
Episode length: 49.50 +/- 17.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.5     |
|    mean_reward      | -4.4e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | -4.1e+03 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 360      |
|    fps              | 301      |
|    time_elapsed     | 109      |
|    total_timesteps  | 33128    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 78.4      |
|    ep_rew_mean      | -4.08e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 364       |
|    fps              | 303       |
|    time_elapsed     | 109       |
|    total_timesteps  | 33306     |
-----------------------------------
Eval num_timesteps=33500, episode_reward=-4441.55 +/- 784.56
Episode length: 49.76 +/- 14.26
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 49.8      |
|    mean_reward      | -4.44e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 33500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 76.3      |
|    ep_rew_mean      | -4.11e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 368       |
|    fps              | 301       |
|    time_elapsed     | 111       |
|    total_timesteps  | 33616     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 72.8      |
|    ep_rew_mean      | -4.12e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 372       |
|    fps              | 302       |
|    time_elapsed     | 111       |
|    total_timesteps  | 33797     |
-----------------------------------
Eval num_timesteps=34000, episode_reward=-4543.94 +/- 638.57
Episode length: 52.66 +/- 17.59
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 52.7      |
|    mean_reward      | -4.54e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 34000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 73        |
|    ep_rew_mean      | -4.18e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 376       |
|    fps              | 299       |
|    time_elapsed     | 113       |
|    total_timesteps  | 34029     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 74.2      |
|    ep_rew_mean      | -4.15e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 380       |
|    fps              | 303       |
|    time_elapsed     | 113       |
|    total_timesteps  | 34467     |
-----------------------------------
Eval num_timesteps=34500, episode_reward=-4533.56 +/- 622.03
Episode length: 51.54 +/- 13.76
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 51.5      |
|    mean_reward      | -4.53e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 34500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 73.8      |
|    ep_rew_mean      | -4.14e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 384       |
|    fps              | 300       |
|    time_elapsed     | 115       |
|    total_timesteps  | 34640     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 74.4      |
|    ep_rew_mean      | -4.13e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 388       |
|    fps              | 301       |
|    time_elapsed     | 115       |
|    total_timesteps  | 34854     |
-----------------------------------
Eval num_timesteps=35000, episode_reward=-4630.35 +/- 692.11
Episode length: 53.78 +/- 17.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 53.8      |
|    mean_reward      | -4.63e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 35000     |
-----------------------------------
Eval num_timesteps=35500, episode_reward=-4418.16 +/- 824.61
Episode length: 50.22 +/- 14.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50.2      |
|    mean_reward      | -4.42e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 35500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 79.4      |
|    ep_rew_mean      | -4.13e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 392       |
|    fps              | 298       |
|    time_elapsed     | 119       |
|    total_timesteps  | 35539     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 79.6      |
|    ep_rew_mean      | -4.14e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 396       |
|    fps              | 300       |
|    time_elapsed     | 119       |
|    total_timesteps  | 35749     |
-----------------------------------
Eval num_timesteps=36000, episode_reward=-4393.16 +/- 697.86
Episode length: 51.58 +/- 17.49
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 51.6      |
|    mean_reward      | -4.39e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 36000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 84.7      |
|    ep_rew_mean      | -4.11e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 400       |
|    fps              | 301       |
|    time_elapsed     | 120       |
|    total_timesteps  | 36422     |
-----------------------------------
Eval num_timesteps=36500, episode_reward=-4276.76 +/- 981.12
Episode length: 51.84 +/- 16.77
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 51.8      |
|    mean_reward      | -4.28e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 36500     |
-----------------------------------
Eval num_timesteps=37000, episode_reward=-4532.76 +/- 682.19
Episode length: 55.28 +/- 14.25
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 55.3      |
|    mean_reward      | -4.53e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 37000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 89.5      |
|    ep_rew_mean      | -4.11e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 404       |
|    fps              | 297       |
|    time_elapsed     | 124       |
|    total_timesteps  | 37100     |
-----------------------------------
Eval num_timesteps=37500, episode_reward=-4541.95 +/- 792.37
Episode length: 51.14 +/- 16.57
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 51.1      |
|    mean_reward      | -4.54e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 37500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 89.6      |
|    ep_rew_mean      | -4.09e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 408       |
|    fps              | 298       |
|    time_elapsed     | 126       |
|    total_timesteps  | 37781     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 89.3      |
|    ep_rew_mean      | -4.07e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 412       |
|    fps              | 299       |
|    time_elapsed     | 126       |
|    total_timesteps  | 37989     |
-----------------------------------
Eval num_timesteps=38000, episode_reward=-4395.15 +/- 695.30
Episode length: 48.36 +/- 15.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.4     |
|    mean_reward      | -4.4e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38000    |
----------------------------------
Eval num_timesteps=38500, episode_reward=-4004.34 +/- 839.15
Episode length: 44.94 +/- 13.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | -4e+03   |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38500    |
----------------------------------
New best mean reward!
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 96.3      |
|    ep_rew_mean      | -4.06e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 416       |
|    fps              | 298       |
|    time_elapsed     | 130       |
|    total_timesteps  | 38847     |
-----------------------------------
Eval num_timesteps=39000, episode_reward=-4300.75 +/- 765.35
Episode length: 45.76 +/- 17.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | -4.3e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39000    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 96.6      |
|    ep_rew_mean      | -4.09e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 420       |
|    fps              | 296       |
|    time_elapsed     | 131       |
|    total_timesteps  | 39057     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95.9      |
|    ep_rew_mean      | -4.08e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 424       |
|    fps              | 297       |
|    time_elapsed     | 131       |
|    total_timesteps  | 39226     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 91.3      |
|    ep_rew_mean      | -4.07e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 428       |
|    fps              | 299       |
|    time_elapsed     | 131       |
|    total_timesteps  | 39469     |
-----------------------------------
Eval num_timesteps=39500, episode_reward=-4306.35 +/- 830.29
Episode length: 50.08 +/- 18.76
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 50.1      |
|    mean_reward      | -4.31e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 39500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 90.7      |
|    ep_rew_mean      | -4.05e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 432       |
|    fps              | 296       |
|    time_elapsed     | 133       |
|    total_timesteps  | 39597     |
-----------------------------------
Eval num_timesteps=40000, episode_reward=-4400.75 +/- 726.25
Episode length: 47.26 +/- 14.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | -4.4e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40000    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95.9      |
|    ep_rew_mean      | -4.06e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 436       |
|    fps              | 296       |
|    time_elapsed     | 135       |
|    total_timesteps  | 40274     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.8      |
|    n_updates        | 68        |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 91.4      |
|    ep_rew_mean      | -4.06e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 440       |
|    fps              | 297       |
|    time_elapsed     | 135       |
|    total_timesteps  | 40452     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.8      |
|    n_updates        | 112       |
-----------------------------------
Eval num_timesteps=40500, episode_reward=-2811.40 +/- 1213.43
Episode length: 34.00 +/- 6.57
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 40500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 5.23      |
|    n_updates        | 124       |
-----------------------------------
New best mean reward!
Eval num_timesteps=41000, episode_reward=-2883.49 +/- 999.93
Episode length: 35.64 +/- 6.12
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 1         |
| time/               |           |
|    total_timesteps  | 41000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 17.7      |
|    n_updates        | 249       |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 95.4      |
|    ep_rew_mean      | -4.07e+03 |
|    exploration_rate | 1         |
| time/               |           |
|    episodes         | 444       |
|    fps              | 295       |
|    time_elapsed     | 138       |
|    total_timesteps  | 41029     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.4      |
|    n_updates        | 257       |
-----------------------------------
Eval num_timesteps=41500, episode_reward=-2998.77 +/- 961.74
Episode length: 35.42 +/- 6.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.4     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 110      |
|    n_updates        | 374      |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 99.9      |
|    ep_rew_mean      | -4.06e+03 |
|    exploration_rate | 0.999     |
| time/               |           |
|    episodes         | 448       |
|    fps              | 297       |
|    time_elapsed     | 140       |
|    total_timesteps  | 41828     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.1      |
|    n_updates        | 456       |
-----------------------------------
Eval num_timesteps=42000, episode_reward=-2574.55 +/- 1245.90
Episode length: 36.06 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.57e+03 |
| rollout/            |           |
|    exploration_rate | 0.999     |
| time/               |           |
|    total_timesteps  | 42000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 31.8      |
|    n_updates        | 499       |
-----------------------------------
New best mean reward!
Eval num_timesteps=42500, episode_reward=-2766.42 +/- 1445.80
Episode length: 33.96 +/- 7.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.999     |
| time/               |           |
|    total_timesteps  | 42500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 42.1      |
|    n_updates        | 624       |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 99.3      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 0.999     |
| time/               |           |
|    episodes         | 452       |
|    fps              | 296       |
|    time_elapsed     | 143       |
|    total_timesteps  | 42606     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47.2      |
|    n_updates        | 651       |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | -4e+03   |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 456      |
|    fps              | 297      |
|    time_elapsed     | 143      |
|    total_timesteps  | 42795    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 152      |
|    n_updates        | 698      |
----------------------------------
Eval num_timesteps=43000, episode_reward=-2863.09 +/- 1226.44
Episode length: 35.14 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.999     |
| time/               |           |
|    total_timesteps  | 43000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.6      |
|    n_updates        | 749       |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 99.4      |
|    ep_rew_mean      | -4.01e+03 |
|    exploration_rate | 0.999     |
| time/               |           |
|    episodes         | 460       |
|    fps              | 296       |
|    time_elapsed     | 145       |
|    total_timesteps  | 43065     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 55.3      |
|    n_updates        | 766       |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | -4e+03   |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 464      |
|    fps              | 297      |
|    time_elapsed     | 145      |
|    total_timesteps  | 43270    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 57.4     |
|    n_updates        | 817      |
----------------------------------
Eval num_timesteps=43500, episode_reward=-2688.98 +/- 1276.49
Episode length: 36.68 +/- 5.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.7      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.998     |
| time/               |           |
|    total_timesteps  | 43500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 6.63      |
|    n_updates        | 874       |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 100       |
|    ep_rew_mean      | -3.98e+03 |
|    exploration_rate | 0.998     |
| time/               |           |
|    episodes         | 468       |
|    fps              | 297       |
|    time_elapsed     | 146       |
|    total_timesteps  | 43620     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.2      |
|    n_updates        | 904       |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 100       |
|    ep_rew_mean      | -3.99e+03 |
|    exploration_rate | 0.998     |
| time/               |           |
|    episodes         | 472       |
|    fps              | 298       |
|    time_elapsed     | 146       |
|    total_timesteps  | 43831     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 21.8      |
|    n_updates        | 957       |
-----------------------------------
Eval num_timesteps=44000, episode_reward=-2635.15 +/- 1405.39
Episode length: 34.98 +/- 6.57
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.998     |
| time/               |           |
|    total_timesteps  | 44000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 4.69      |
|    n_updates        | 999       |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 101       |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 0.998     |
| time/               |           |
|    episodes         | 476       |
|    fps              | 297       |
|    time_elapsed     | 148       |
|    total_timesteps  | 44126     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.1      |
|    n_updates        | 1031      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 98.9      |
|    ep_rew_mean      | -3.99e+03 |
|    exploration_rate | 0.998     |
| time/               |           |
|    episodes         | 480       |
|    fps              | 298       |
|    time_elapsed     | 148       |
|    total_timesteps  | 44357     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.1      |
|    n_updates        | 1089      |
-----------------------------------
Eval num_timesteps=44500, episode_reward=-2833.22 +/- 1217.18
Episode length: 36.36 +/- 6.11
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.997     |
| time/               |           |
|    total_timesteps  | 44500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.5      |
|    n_updates        | 1124      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 103       |
|    ep_rew_mean      | -4.01e+03 |
|    exploration_rate | 0.997     |
| time/               |           |
|    episodes         | 484       |
|    fps              | 299       |
|    time_elapsed     | 150       |
|    total_timesteps  | 44940     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37        |
|    n_updates        | 1234      |
-----------------------------------
Eval num_timesteps=45000, episode_reward=-2908.68 +/- 1175.65
Episode length: 33.86 +/- 7.26
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.997     |
| time/               |           |
|    total_timesteps  | 45000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.8      |
|    n_updates        | 1249      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 103       |
|    ep_rew_mean      | -4.05e+03 |
|    exploration_rate | 0.997     |
| time/               |           |
|    episodes         | 488       |
|    fps              | 298       |
|    time_elapsed     | 151       |
|    total_timesteps  | 45189     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 14.2      |
|    n_updates        | 1297      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 98.1      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 0.997     |
| time/               |           |
|    episodes         | 492       |
|    fps              | 299       |
|    time_elapsed     | 151       |
|    total_timesteps  | 45347     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37.1      |
|    n_updates        | 1336      |
-----------------------------------
Eval num_timesteps=45500, episode_reward=-2881.73 +/- 1268.61
Episode length: 34.64 +/- 6.38
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.997     |
| time/               |           |
|    total_timesteps  | 45500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.6      |
|    n_updates        | 1374      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 98.5      |
|    ep_rew_mean      | -4.01e+03 |
|    exploration_rate | 0.996     |
| time/               |           |
|    episodes         | 496       |
|    fps              | 298       |
|    time_elapsed     | 152       |
|    total_timesteps  | 45598     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 18.4      |
|    n_updates        | 1399      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 94.7      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 0.996     |
| time/               |           |
|    episodes         | 500       |
|    fps              | 299       |
|    time_elapsed     | 153       |
|    total_timesteps  | 45894     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44        |
|    n_updates        | 1473      |
-----------------------------------
Eval num_timesteps=46000, episode_reward=-2680.41 +/- 1167.17
Episode length: 35.02 +/- 7.54
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.996     |
| time/               |           |
|    total_timesteps  | 46000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 9.62      |
|    n_updates        | 1499      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 89.8      |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.996     |
| time/               |           |
|    episodes         | 504       |
|    fps              | 298       |
|    time_elapsed     | 154       |
|    total_timesteps  | 46078     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 1519      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.2      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 0.996     |
| time/               |           |
|    episodes         | 508       |
|    fps              | 299       |
|    time_elapsed     | 154       |
|    total_timesteps  | 46298     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94        |
|    n_updates        | 1574      |
-----------------------------------
Eval num_timesteps=46500, episode_reward=-2556.96 +/- 1340.51
Episode length: 35.92 +/- 6.49
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.56e+03 |
| rollout/            |           |
|    exploration_rate | 0.996     |
| time/               |           |
|    total_timesteps  | 46500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.3      |
|    n_updates        | 1624      |
-----------------------------------
New best mean reward!
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.3      |
|    ep_rew_mean      | -4.05e+03 |
|    exploration_rate | 0.996     |
| time/               |           |
|    episodes         | 512       |
|    fps              | 298       |
|    time_elapsed     | 156       |
|    total_timesteps  | 46521     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.2      |
|    n_updates        | 1630      |
-----------------------------------
Eval num_timesteps=47000, episode_reward=-2493.37 +/- 1439.37
Episode length: 34.68 +/- 7.85
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.49e+03 |
| rollout/            |           |
|    exploration_rate | 0.995     |
| time/               |           |
|    total_timesteps  | 47000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64.4      |
|    n_updates        | 1749      |
-----------------------------------
New best mean reward!
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 82.7      |
|    ep_rew_mean      | -4.04e+03 |
|    exploration_rate | 0.995     |
| time/               |           |
|    episodes         | 516       |
|    fps              | 298       |
|    time_elapsed     | 157       |
|    total_timesteps  | 47116     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 25.1      |
|    n_updates        | 1778      |
-----------------------------------
Eval num_timesteps=47500, episode_reward=-2578.98 +/- 1286.96
Episode length: 35.30 +/- 6.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.58e+03 |
| rollout/            |           |
|    exploration_rate | 0.994     |
| time/               |           |
|    total_timesteps  | 47500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 10.9      |
|    n_updates        | 1874      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 88.3      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 0.994     |
| time/               |           |
|    episodes         | 520       |
|    fps              | 300       |
|    time_elapsed     | 159       |
|    total_timesteps  | 47890     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.8      |
|    n_updates        | 1972      |
-----------------------------------
Eval num_timesteps=48000, episode_reward=-2743.46 +/- 1322.56
Episode length: 33.24 +/- 6.80
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.2      |
|    mean_reward      | -2.74e+03 |
| rollout/            |           |
|    exploration_rate | 0.994     |
| time/               |           |
|    total_timesteps  | 48000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.7      |
|    n_updates        | 1999      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 90.6      |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.994     |
| time/               |           |
|    episodes         | 524       |
|    fps              | 300       |
|    time_elapsed     | 160       |
|    total_timesteps  | 48282     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 30.6      |
|    n_updates        | 2070      |
-----------------------------------
Eval num_timesteps=48500, episode_reward=-2986.18 +/- 1210.02
Episode length: 34.66 +/- 6.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.993     |
| time/               |           |
|    total_timesteps  | 48500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.8      |
|    n_updates        | 2124      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 90.6      |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.993     |
| time/               |           |
|    episodes         | 528       |
|    fps              | 299       |
|    time_elapsed     | 162       |
|    total_timesteps  | 48525     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 14        |
|    n_updates        | 2131      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 91.8      |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.993     |
| time/               |           |
|    episodes         | 532       |
|    fps              | 300       |
|    time_elapsed     | 162       |
|    total_timesteps  | 48780     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 4.89      |
|    n_updates        | 2194      |
-----------------------------------
Eval num_timesteps=49000, episode_reward=-2942.17 +/- 925.91
Episode length: 33.34 +/- 6.87
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.3      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.993     |
| time/               |           |
|    total_timesteps  | 49000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.3      |
|    n_updates        | 2249      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 89        |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.993     |
| time/               |           |
|    episodes         | 536       |
|    fps              | 300       |
|    time_elapsed     | 163       |
|    total_timesteps  | 49178     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.3      |
|    n_updates        | 2294      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 89.6      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 0.992     |
| time/               |           |
|    episodes         | 540       |
|    fps              | 301       |
|    time_elapsed     | 164       |
|    total_timesteps  | 49410     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 2352      |
-----------------------------------
Eval num_timesteps=49500, episode_reward=-3017.48 +/- 957.95
Episode length: 35.34 +/- 6.12
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -3.02e+03 |
| rollout/            |           |
|    exploration_rate | 0.992     |
| time/               |           |
|    total_timesteps  | 49500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 12.4      |
|    n_updates        | 2374      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.5      |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.992     |
| time/               |           |
|    episodes         | 544       |
|    fps              | 299       |
|    time_elapsed     | 165       |
|    total_timesteps  | 49582     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28.4      |
|    n_updates        | 2395      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 79.5      |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.992     |
| time/               |           |
|    episodes         | 548       |
|    fps              | 300       |
|    time_elapsed     | 165       |
|    total_timesteps  | 49778     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 22        |
|    n_updates        | 2444      |
-----------------------------------
Eval num_timesteps=50000, episode_reward=-2517.47 +/- 1427.93
Episode length: 36.46 +/- 5.34
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -2.52e+03 |
| rollout/            |           |
|    exploration_rate | 0.992     |
| time/               |           |
|    total_timesteps  | 50000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.6      |
|    n_updates        | 2499      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 74.2      |
|    ep_rew_mean      | -4.06e+03 |
|    exploration_rate | 0.991     |
| time/               |           |
|    episodes         | 552       |
|    fps              | 299       |
|    time_elapsed     | 166       |
|    total_timesteps  | 50030     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.1      |
|    n_updates        | 2507      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 76.6      |
|    ep_rew_mean      | -4.09e+03 |
|    exploration_rate | 0.991     |
| time/               |           |
|    episodes         | 556       |
|    fps              | 301       |
|    time_elapsed     | 167       |
|    total_timesteps  | 50459     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.8      |
|    n_updates        | 2614      |
-----------------------------------
Eval num_timesteps=50500, episode_reward=-2850.65 +/- 1256.32
Episode length: 34.80 +/- 6.28
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.991     |
| time/               |           |
|    total_timesteps  | 50500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 2624      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | -4.1e+03 |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 560      |
|    fps              | 300      |
|    time_elapsed     | 168      |
|    total_timesteps  | 50714    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 55.2     |
|    n_updates        | 2678     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 76        |
|    ep_rew_mean      | -4.08e+03 |
|    exploration_rate | 0.99      |
| time/               |           |
|    episodes         | 564       |
|    fps              | 301       |
|    time_elapsed     | 168       |
|    total_timesteps  | 50868     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.5      |
|    n_updates        | 2716      |
-----------------------------------
Eval num_timesteps=51000, episode_reward=-2948.35 +/- 952.52
Episode length: 34.50 +/- 7.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.99      |
| time/               |           |
|    total_timesteps  | 51000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57        |
|    n_updates        | 2749      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 74.5      |
|    ep_rew_mean      | -4.07e+03 |
|    exploration_rate | 0.99      |
| time/               |           |
|    episodes         | 568       |
|    fps              | 300       |
|    time_elapsed     | 170       |
|    total_timesteps  | 51073     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.9      |
|    n_updates        | 2768      |
-----------------------------------
Eval num_timesteps=51500, episode_reward=-3226.31 +/- 884.05
Episode length: 33.20 +/- 6.17
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.2      |
|    mean_reward      | -3.23e+03 |
| rollout/            |           |
|    exploration_rate | 0.99      |
| time/               |           |
|    total_timesteps  | 51500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28.9      |
|    n_updates        | 2874      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 77.8      |
|    ep_rew_mean      | -4.05e+03 |
|    exploration_rate | 0.989     |
| time/               |           |
|    episodes         | 572       |
|    fps              | 300       |
|    time_elapsed     | 171       |
|    total_timesteps  | 51615     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 22.5      |
|    n_updates        | 2903      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 76.3      |
|    ep_rew_mean      | -4.04e+03 |
|    exploration_rate | 0.989     |
| time/               |           |
|    episodes         | 576       |
|    fps              | 301       |
|    time_elapsed     | 171       |
|    total_timesteps  | 51752     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53.1      |
|    n_updates        | 2937      |
-----------------------------------
Eval num_timesteps=52000, episode_reward=-2854.23 +/- 1006.77
Episode length: 34.56 +/- 7.12
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.989     |
| time/               |           |
|    total_timesteps  | 52000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 18.4      |
|    n_updates        | 2999      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 81.2      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 0.988     |
| time/               |           |
|    episodes         | 580       |
|    fps              | 302       |
|    time_elapsed     | 173       |
|    total_timesteps  | 52473     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 3118      |
-----------------------------------
Eval num_timesteps=52500, episode_reward=-2769.52 +/- 1281.50
Episode length: 34.78 +/- 6.56
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.988     |
| time/               |           |
|    total_timesteps  | 52500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 38.7      |
|    n_updates        | 3124      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 77        |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.988     |
| time/               |           |
|    episodes         | 584       |
|    fps              | 301       |
|    time_elapsed     | 174       |
|    total_timesteps  | 52643     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.8      |
|    n_updates        | 3160      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 77.7      |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.987     |
| time/               |           |
|    episodes         | 588       |
|    fps              | 302       |
|    time_elapsed     | 174       |
|    total_timesteps  | 52963     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67        |
|    n_updates        | 3240      |
-----------------------------------
Eval num_timesteps=53000, episode_reward=-3000.45 +/- 1090.80
Episode length: 35.48 +/- 5.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.5     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 53000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 10.9     |
|    n_updates        | 3249     |
----------------------------------
Eval num_timesteps=53500, episode_reward=-2990.97 +/- 996.26
Episode length: 34.14 +/- 7.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.987     |
| time/               |           |
|    total_timesteps  | 53500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 33.8      |
|    n_updates        | 3374      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 83        |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.986     |
| time/               |           |
|    episodes         | 592       |
|    fps              | 301       |
|    time_elapsed     | 177       |
|    total_timesteps  | 53645     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 40.9      |
|    n_updates        | 3411      |
-----------------------------------
Eval num_timesteps=54000, episode_reward=-2718.04 +/- 1421.26
Episode length: 35.92 +/- 6.56
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.986     |
| time/               |           |
|    total_timesteps  | 54000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.2      |
|    n_updates        | 3499      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 86.7      |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 0.986     |
| time/               |           |
|    episodes         | 596       |
|    fps              | 302       |
|    time_elapsed     | 179       |
|    total_timesteps  | 54263     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 22.7      |
|    n_updates        | 3565      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.7      |
|    ep_rew_mean      | -3.99e+03 |
|    exploration_rate | 0.985     |
| time/               |           |
|    episodes         | 600       |
|    fps              | 303       |
|    time_elapsed     | 179       |
|    total_timesteps  | 54459     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.2      |
|    n_updates        | 3614      |
-----------------------------------
Eval num_timesteps=54500, episode_reward=-2699.83 +/- 1405.20
Episode length: 35.02 +/- 6.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35       |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.985    |
| time/               |          |
|    total_timesteps  | 54500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 41.8     |
|    n_updates        | 3624     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.8      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.985     |
| time/               |           |
|    episodes         | 604       |
|    fps              | 301       |
|    time_elapsed     | 181       |
|    total_timesteps  | 54653     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 15.4      |
|    n_updates        | 3663      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.6      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.985     |
| time/               |           |
|    episodes         | 608       |
|    fps              | 302       |
|    time_elapsed     | 181       |
|    total_timesteps  | 54855     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 22.6      |
|    n_updates        | 3713      |
-----------------------------------
Eval num_timesteps=55000, episode_reward=-2511.75 +/- 1379.50
Episode length: 37.42 +/- 5.86
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.4      |
|    mean_reward      | -2.51e+03 |
| rollout/            |           |
|    exploration_rate | 0.984     |
| time/               |           |
|    total_timesteps  | 55000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.5      |
|    n_updates        | 3749      |
-----------------------------------
Eval num_timesteps=55500, episode_reward=-2673.08 +/- 1245.57
Episode length: 36.14 +/- 6.40
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.984     |
| time/               |           |
|    total_timesteps  | 55500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88.5      |
|    n_updates        | 3874      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 90        |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 0.984     |
| time/               |           |
|    episodes         | 612       |
|    fps              | 301       |
|    time_elapsed     | 184       |
|    total_timesteps  | 55521     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.1      |
|    n_updates        | 3880      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.6      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.983     |
| time/               |           |
|    episodes         | 616       |
|    fps              | 302       |
|    time_elapsed     | 184       |
|    total_timesteps  | 55675     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.6      |
|    n_updates        | 3918      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 79.8      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 0.983     |
| time/               |           |
|    episodes         | 620       |
|    fps              | 302       |
|    time_elapsed     | 184       |
|    total_timesteps  | 55866     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 24.3      |
|    n_updates        | 3966      |
-----------------------------------
Eval num_timesteps=56000, episode_reward=-2911.62 +/- 963.66
Episode length: 45.70 +/- 68.75
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 45.7      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.983     |
| time/               |           |
|    total_timesteps  | 56000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 3999      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 78.3      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 0.983     |
| time/               |           |
|    episodes         | 624       |
|    fps              | 301       |
|    time_elapsed     | 186       |
|    total_timesteps  | 56110     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 32.8      |
|    n_updates        | 4027      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 79        |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.982     |
| time/               |           |
|    episodes         | 628       |
|    fps              | 302       |
|    time_elapsed     | 186       |
|    total_timesteps  | 56420     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 26.2      |
|    n_updates        | 4104      |
-----------------------------------
Eval num_timesteps=56500, episode_reward=-2948.76 +/- 877.74
Episode length: 34.86 +/- 5.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.982     |
| time/               |           |
|    total_timesteps  | 56500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.3      |
|    n_updates        | 4124      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 77.8      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.982     |
| time/               |           |
|    episodes         | 632       |
|    fps              | 301       |
|    time_elapsed     | 187       |
|    total_timesteps  | 56563     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.9      |
|    n_updates        | 4140      |
-----------------------------------
Eval num_timesteps=57000, episode_reward=-2693.47 +/- 1341.06
Episode length: 46.02 +/- 68.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 46        |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.981     |
| time/               |           |
|    total_timesteps  | 57000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 6.54      |
|    n_updates        | 4249      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 80.6      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.981     |
| time/               |           |
|    episodes         | 636       |
|    fps              | 301       |
|    time_elapsed     | 189       |
|    total_timesteps  | 57241     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90        |
|    n_updates        | 4310      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 80.4      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.98      |
| time/               |           |
|    episodes         | 640       |
|    fps              | 302       |
|    time_elapsed     | 189       |
|    total_timesteps  | 57449     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65.1      |
|    n_updates        | 4362      |
-----------------------------------
Eval num_timesteps=57500, episode_reward=-2662.52 +/- 1445.23
Episode length: 34.84 +/- 6.84
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.66e+03 |
| rollout/            |           |
|    exploration_rate | 0.98      |
| time/               |           |
|    total_timesteps  | 57500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37.7      |
|    n_updates        | 4374      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 81.2      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.98      |
| time/               |           |
|    episodes         | 644       |
|    fps              | 301       |
|    time_elapsed     | 191       |
|    total_timesteps  | 57706     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 31.7      |
|    n_updates        | 4426      |
-----------------------------------
Eval num_timesteps=58000, episode_reward=-2826.03 +/- 940.56
Episode length: 36.26 +/- 6.01
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.979     |
| time/               |           |
|    total_timesteps  | 58000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.1      |
|    n_updates        | 4499      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.5      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.979     |
| time/               |           |
|    episodes         | 648       |
|    fps              | 302       |
|    time_elapsed     | 193       |
|    total_timesteps  | 58329     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 11.9      |
|    n_updates        | 4582      |
-----------------------------------
Eval num_timesteps=58500, episode_reward=-2828.72 +/- 1306.14
Episode length: 33.76 +/- 6.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.979     |
| time/               |           |
|    total_timesteps  | 58500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 25.7      |
|    n_updates        | 4624      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.1     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 652      |
|    fps              | 301      |
|    time_elapsed     | 194      |
|    total_timesteps  | 58542    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 42.8     |
|    n_updates        | 4635     |
----------------------------------
Eval num_timesteps=59000, episode_reward=-3052.64 +/- 805.49
Episode length: 34.96 +/- 6.46
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -3.05e+03 |
| rollout/            |           |
|    exploration_rate | 0.978     |
| time/               |           |
|    total_timesteps  | 59000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 4749      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 87.9      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.977     |
| time/               |           |
|    episodes         | 656       |
|    fps              | 302       |
|    time_elapsed     | 196       |
|    total_timesteps  | 59245     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.5      |
|    n_updates        | 4811      |
-----------------------------------
Eval num_timesteps=59500, episode_reward=-2694.16 +/- 1344.14
Episode length: 36.12 +/- 7.04
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.977     |
| time/               |           |
|    total_timesteps  | 59500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.1      |
|    n_updates        | 4874      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 88.3      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.977     |
| time/               |           |
|    episodes         | 660       |
|    fps              | 301       |
|    time_elapsed     | 197       |
|    total_timesteps  | 59544     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 20        |
|    n_updates        | 4885      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 88.2      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.977     |
| time/               |           |
|    episodes         | 664       |
|    fps              | 302       |
|    time_elapsed     | 197       |
|    total_timesteps  | 59691     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 36.7      |
|    n_updates        | 4922      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 88        |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.976     |
| time/               |           |
|    episodes         | 668       |
|    fps              | 302       |
|    time_elapsed     | 197       |
|    total_timesteps  | 59874     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.9      |
|    n_updates        | 4968      |
-----------------------------------
Eval num_timesteps=60000, episode_reward=-2709.61 +/- 1240.66
Episode length: 36.00 +/- 6.86
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.71e+03 |
| rollout/            |           |
|    exploration_rate | 0.976     |
| time/               |           |
|    total_timesteps  | 60000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37.1      |
|    n_updates        | 4999      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 84.5      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.976     |
| time/               |           |
|    episodes         | 672       |
|    fps              | 301       |
|    time_elapsed     | 199       |
|    total_timesteps  | 60062     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.6      |
|    n_updates        | 5015      |
-----------------------------------
Eval num_timesteps=60500, episode_reward=-2862.58 +/- 1110.71
Episode length: 34.78 +/- 7.10
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.975     |
| time/               |           |
|    total_timesteps  | 60500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 42.8      |
|    n_updates        | 5124      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 90.3      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.975     |
| time/               |           |
|    episodes         | 676       |
|    fps              | 302       |
|    time_elapsed     | 200       |
|    total_timesteps  | 60778     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 29.4      |
|    n_updates        | 5194      |
-----------------------------------
Eval num_timesteps=61000, episode_reward=-3164.42 +/- 879.83
Episode length: 33.66 +/- 6.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -3.16e+03 |
| rollout/            |           |
|    exploration_rate | 0.974     |
| time/               |           |
|    total_timesteps  | 61000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 5.1       |
|    n_updates        | 5249      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.9      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.974     |
| time/               |           |
|    episodes         | 680       |
|    fps              | 301       |
|    time_elapsed     | 202       |
|    total_timesteps  | 61059     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64.4      |
|    n_updates        | 5264      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 87.2      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.973     |
| time/               |           |
|    episodes         | 684       |
|    fps              | 303       |
|    time_elapsed     | 202       |
|    total_timesteps  | 61367     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28.1      |
|    n_updates        | 5341      |
-----------------------------------
Eval num_timesteps=61500, episode_reward=-3086.42 +/- 1133.32
Episode length: 33.94 +/- 8.13
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -3.09e+03 |
| rollout/            |           |
|    exploration_rate | 0.973     |
| time/               |           |
|    total_timesteps  | 61500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 5.64      |
|    n_updates        | 5374      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 85.8      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.973     |
| time/               |           |
|    episodes         | 688       |
|    fps              | 302       |
|    time_elapsed     | 203       |
|    total_timesteps  | 61542     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.4      |
|    n_updates        | 5385      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 83.4      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.972     |
| time/               |           |
|    episodes         | 692       |
|    fps              | 303       |
|    time_elapsed     | 204       |
|    total_timesteps  | 61981     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 26.7      |
|    n_updates        | 5495      |
-----------------------------------
Eval num_timesteps=62000, episode_reward=-2814.39 +/- 1292.74
Episode length: 34.00 +/- 7.75
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.972     |
| time/               |           |
|    total_timesteps  | 62000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 6         |
|    n_updates        | 5499      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 79        |
|    ep_rew_mean      | -3.99e+03 |
|    exploration_rate | 0.972     |
| time/               |           |
|    episodes         | 696       |
|    fps              | 302       |
|    time_elapsed     | 205       |
|    total_timesteps  | 62167     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28.2      |
|    n_updates        | 5541      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 78.9      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.972     |
| time/               |           |
|    episodes         | 700       |
|    fps              | 303       |
|    time_elapsed     | 205       |
|    total_timesteps  | 62346     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.4      |
|    n_updates        | 5586      |
-----------------------------------
Eval num_timesteps=62500, episode_reward=-3031.04 +/- 973.89
Episode length: 35.48 +/- 6.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -3.03e+03 |
| rollout/            |           |
|    exploration_rate | 0.971     |
| time/               |           |
|    total_timesteps  | 62500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 8.07      |
|    n_updates        | 5624      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 79.8      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.971     |
| time/               |           |
|    episodes         | 704       |
|    fps              | 302       |
|    time_elapsed     | 206       |
|    total_timesteps  | 62630     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.9      |
|    n_updates        | 5657      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 79.2      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.971     |
| time/               |           |
|    episodes         | 708       |
|    fps              | 303       |
|    time_elapsed     | 207       |
|    total_timesteps  | 62773     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.3      |
|    n_updates        | 5693      |
-----------------------------------
Eval num_timesteps=63000, episode_reward=-3362.52 +/- 843.05
Episode length: 33.52 +/- 7.15
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.5      |
|    mean_reward      | -3.36e+03 |
| rollout/            |           |
|    exploration_rate | 0.97      |
| time/               |           |
|    total_timesteps  | 63000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52.4      |
|    n_updates        | 5749      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 78.5      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.97      |
| time/               |           |
|    episodes         | 712       |
|    fps              | 303       |
|    time_elapsed     | 208       |
|    total_timesteps  | 63375     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 5843      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 78.2      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.969     |
| time/               |           |
|    episodes         | 716       |
|    fps              | 304       |
|    time_elapsed     | 208       |
|    total_timesteps  | 63495     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 6.27      |
|    n_updates        | 5873      |
-----------------------------------
Eval num_timesteps=63500, episode_reward=-2971.51 +/- 1061.05
Episode length: 35.34 +/- 6.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.969     |
| time/               |           |
|    total_timesteps  | 63500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 5874      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 720      |
|    fps              | 303      |
|    time_elapsed     | 210      |
|    total_timesteps  | 63684    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 63.8     |
|    n_updates        | 5920     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 77.6      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.969     |
| time/               |           |
|    episodes         | 724       |
|    fps              | 303       |
|    time_elapsed     | 210       |
|    total_timesteps  | 63873     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.9      |
|    n_updates        | 5968      |
-----------------------------------
Eval num_timesteps=64000, episode_reward=-3120.70 +/- 1070.37
Episode length: 33.74 +/- 5.83
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -3.12e+03 |
| rollout/            |           |
|    exploration_rate | 0.968     |
| time/               |           |
|    total_timesteps  | 64000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.7      |
|    n_updates        | 5999      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 76.1      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.968     |
| time/               |           |
|    episodes         | 728       |
|    fps              | 302       |
|    time_elapsed     | 211       |
|    total_timesteps  | 64029     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.3      |
|    n_updates        | 6007      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 76.2      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.968     |
| time/               |           |
|    episodes         | 732       |
|    fps              | 303       |
|    time_elapsed     | 211       |
|    total_timesteps  | 64188     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 15.8      |
|    n_updates        | 6046      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 71.1      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.968     |
| time/               |           |
|    episodes         | 736       |
|    fps              | 303       |
|    time_elapsed     | 211       |
|    total_timesteps  | 64354     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 4.86      |
|    n_updates        | 6088      |
-----------------------------------
Eval num_timesteps=64500, episode_reward=-3109.47 +/- 965.78
Episode length: 34.36 +/- 7.75
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.967     |
| time/               |           |
|    total_timesteps  | 64500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.4      |
|    n_updates        | 6124      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 71.1      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.967     |
| time/               |           |
|    episodes         | 740       |
|    fps              | 302       |
|    time_elapsed     | 213       |
|    total_timesteps  | 64562     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 11.8      |
|    n_updates        | 6140      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 70.6      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.967     |
| time/               |           |
|    episodes         | 744       |
|    fps              | 303       |
|    time_elapsed     | 213       |
|    total_timesteps  | 64762     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.5      |
|    n_updates        | 6190      |
-----------------------------------
Eval num_timesteps=65000, episode_reward=-2731.07 +/- 1366.23
Episode length: 34.98 +/- 6.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.966     |
| time/               |           |
|    total_timesteps  | 65000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 7.16      |
|    n_updates        | 6249      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 66.9      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 0.966     |
| time/               |           |
|    episodes         | 748       |
|    fps              | 302       |
|    time_elapsed     | 214       |
|    total_timesteps  | 65020     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 6254      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 66.2      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.966     |
| time/               |           |
|    episodes         | 752       |
|    fps              | 303       |
|    time_elapsed     | 214       |
|    total_timesteps  | 65159     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.2      |
|    n_updates        | 6289      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.7      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.966     |
| time/               |           |
|    episodes         | 756       |
|    fps              | 304       |
|    time_elapsed     | 214       |
|    total_timesteps  | 65415     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.3      |
|    n_updates        | 6353      |
-----------------------------------
Eval num_timesteps=65500, episode_reward=-3052.18 +/- 1204.58
Episode length: 33.54 +/- 6.19
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.5      |
|    mean_reward      | -3.05e+03 |
| rollout/            |           |
|    exploration_rate | 0.965     |
| time/               |           |
|    total_timesteps  | 65500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.6      |
|    n_updates        | 6374      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.6     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 760      |
|    fps              | 303      |
|    time_elapsed     | 216      |
|    total_timesteps  | 65707    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 11.2     |
|    n_updates        | 6426     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.4      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.965     |
| time/               |           |
|    episodes         | 764       |
|    fps              | 304       |
|    time_elapsed     | 216       |
|    total_timesteps  | 65831     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 6457      |
-----------------------------------
Eval num_timesteps=66000, episode_reward=-2841.16 +/- 1284.38
Episode length: 34.22 +/- 7.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.964     |
| time/               |           |
|    total_timesteps  | 66000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 36.8      |
|    n_updates        | 6499      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.3      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.964     |
| time/               |           |
|    episodes         | 768       |
|    fps              | 303       |
|    time_elapsed     | 217       |
|    total_timesteps  | 66002     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 56.1      |
|    n_updates        | 6500      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 60.9      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.964     |
| time/               |           |
|    episodes         | 772       |
|    fps              | 303       |
|    time_elapsed     | 217       |
|    total_timesteps  | 66154     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 49        |
|    n_updates        | 6538      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 56.8      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.963     |
| time/               |           |
|    episodes         | 776       |
|    fps              | 304       |
|    time_elapsed     | 218       |
|    total_timesteps  | 66458     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 6614      |
-----------------------------------
Eval num_timesteps=66500, episode_reward=-2777.27 +/- 1115.95
Episode length: 35.60 +/- 6.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.963     |
| time/               |           |
|    total_timesteps  | 66500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 22.1      |
|    n_updates        | 6624      |
-----------------------------------
Eval num_timesteps=67000, episode_reward=-2886.52 +/- 1136.81
Episode length: 36.82 +/- 7.39
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.962     |
| time/               |           |
|    total_timesteps  | 67000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 6749      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 60.3      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.962     |
| time/               |           |
|    episodes         | 780       |
|    fps              | 303       |
|    time_elapsed     | 220       |
|    total_timesteps  | 67088     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 6771      |
-----------------------------------
Eval num_timesteps=67500, episode_reward=-2934.71 +/- 1108.59
Episode length: 33.58 +/- 6.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.6      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.961     |
| time/               |           |
|    total_timesteps  | 67500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 32.8      |
|    n_updates        | 6874      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 63.8      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.961     |
| time/               |           |
|    episodes         | 784       |
|    fps              | 304       |
|    time_elapsed     | 222       |
|    total_timesteps  | 67745     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 46.1      |
|    n_updates        | 6936      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 64.3      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.96      |
| time/               |           |
|    episodes         | 788       |
|    fps              | 305       |
|    time_elapsed     | 222       |
|    total_timesteps  | 67975     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 6993      |
-----------------------------------
Eval num_timesteps=68000, episode_reward=-3020.32 +/- 1217.21
Episode length: 34.46 +/- 6.76
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -3.02e+03 |
| rollout/            |           |
|    exploration_rate | 0.96      |
| time/               |           |
|    total_timesteps  | 68000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52        |
|    n_updates        | 6999      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.4      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.96      |
| time/               |           |
|    episodes         | 792       |
|    fps              | 304       |
|    time_elapsed     | 224       |
|    total_timesteps  | 68118     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 40.2      |
|    n_updates        | 7029      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.8      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.959     |
| time/               |           |
|    episodes         | 796       |
|    fps              | 305       |
|    time_elapsed     | 224       |
|    total_timesteps  | 68445     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.7      |
|    n_updates        | 7111      |
-----------------------------------
Eval num_timesteps=68500, episode_reward=-2896.63 +/- 1210.13
Episode length: 34.52 +/- 6.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.5     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.959    |
| time/               |          |
|    total_timesteps  | 68500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 54.9     |
|    n_updates        | 7124     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 63        |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 0.959     |
| time/               |           |
|    episodes         | 800       |
|    fps              | 304       |
|    time_elapsed     | 225       |
|    total_timesteps  | 68647     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 7161      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.1      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.958     |
| time/               |           |
|    episodes         | 804       |
|    fps              | 304       |
|    time_elapsed     | 225       |
|    total_timesteps  | 68836     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.2      |
|    n_updates        | 7208      |
-----------------------------------
Eval num_timesteps=69000, episode_reward=-2608.13 +/- 1248.00
Episode length: 35.10 +/- 8.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.61e+03 |
| rollout/            |           |
|    exploration_rate | 0.958     |
| time/               |           |
|    total_timesteps  | 69000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 19.5      |
|    n_updates        | 7249      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.6      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.958     |
| time/               |           |
|    episodes         | 808       |
|    fps              | 303       |
|    time_elapsed     | 227       |
|    total_timesteps  | 69034     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.3      |
|    n_updates        | 7258      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.9      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.957     |
| time/               |           |
|    episodes         | 812       |
|    fps              | 304       |
|    time_elapsed     | 227       |
|    total_timesteps  | 69266     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 25.8      |
|    n_updates        | 7316      |
-----------------------------------
Eval num_timesteps=69500, episode_reward=-2921.96 +/- 1199.66
Episode length: 33.92 +/- 6.97
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.957     |
| time/               |           |
|    total_timesteps  | 69500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66        |
|    n_updates        | 7374      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 60.1      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.957     |
| time/               |           |
|    episodes         | 816       |
|    fps              | 303       |
|    time_elapsed     | 228       |
|    total_timesteps  | 69505     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 7376      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 59.8      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.957     |
| time/               |           |
|    episodes         | 820       |
|    fps              | 304       |
|    time_elapsed     | 228       |
|    total_timesteps  | 69662     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.9      |
|    n_updates        | 7415      |
-----------------------------------
Eval num_timesteps=70000, episode_reward=-3090.21 +/- 1121.87
Episode length: 32.10 +/- 7.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.1      |
|    mean_reward      | -3.09e+03 |
| rollout/            |           |
|    exploration_rate | 0.956     |
| time/               |           |
|    total_timesteps  | 70000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 31.1      |
|    n_updates        | 7499      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.6      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.956     |
| time/               |           |
|    episodes         | 824       |
|    fps              | 304       |
|    time_elapsed     | 230       |
|    total_timesteps  | 70135     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.5      |
|    n_updates        | 7533      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.5      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.955     |
| time/               |           |
|    episodes         | 828       |
|    fps              | 305       |
|    time_elapsed     | 230       |
|    total_timesteps  | 70275     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 23.1      |
|    n_updates        | 7568      |
-----------------------------------
Eval num_timesteps=70500, episode_reward=-3105.35 +/- 912.31
Episode length: 34.96 +/- 6.86
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.955     |
| time/               |           |
|    total_timesteps  | 70500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80        |
|    n_updates        | 7624      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 63.5      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.955     |
| time/               |           |
|    episodes         | 832       |
|    fps              | 304       |
|    time_elapsed     | 231       |
|    total_timesteps  | 70535     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.3      |
|    n_updates        | 7633      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 63.9      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.954     |
| time/               |           |
|    episodes         | 836       |
|    fps              | 305       |
|    time_elapsed     | 231       |
|    total_timesteps  | 70746     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.8      |
|    n_updates        | 7686      |
-----------------------------------
Eval num_timesteps=71000, episode_reward=-2637.29 +/- 1256.46
Episode length: 36.52 +/- 6.13
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.954     |
| time/               |           |
|    total_timesteps  | 71000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 38.8      |
|    n_updates        | 7749      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 64.9      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.954     |
| time/               |           |
|    episodes         | 840       |
|    fps              | 304       |
|    time_elapsed     | 233       |
|    total_timesteps  | 71055     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.7      |
|    n_updates        | 7763      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 65.9      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 0.953     |
| time/               |           |
|    episodes         | 844       |
|    fps              | 305       |
|    time_elapsed     | 233       |
|    total_timesteps  | 71353     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.9      |
|    n_updates        | 7838      |
-----------------------------------
Eval num_timesteps=71500, episode_reward=-3106.04 +/- 1098.13
Episode length: 33.20 +/- 7.45
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.2      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.952     |
| time/               |           |
|    total_timesteps  | 71500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.8      |
|    n_updates        | 7874      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 65.4      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.952     |
| time/               |           |
|    episodes         | 848       |
|    fps              | 304       |
|    time_elapsed     | 234       |
|    total_timesteps  | 71561     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.8      |
|    n_updates        | 7890      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.5      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.952     |
| time/               |           |
|    episodes         | 852       |
|    fps              | 305       |
|    time_elapsed     | 235       |
|    total_timesteps  | 71904     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.8      |
|    n_updates        | 7975      |
-----------------------------------
Eval num_timesteps=72000, episode_reward=-2833.18 +/- 939.49
Episode length: 35.40 +/- 7.08
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.951     |
| time/               |           |
|    total_timesteps  | 72000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 17.3      |
|    n_updates        | 7999      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.3      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.951     |
| time/               |           |
|    episodes         | 856       |
|    fps              | 304       |
|    time_elapsed     | 236       |
|    total_timesteps  | 72150     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 10.9      |
|    n_updates        | 8037      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 66.3      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.951     |
| time/               |           |
|    episodes         | 860       |
|    fps              | 305       |
|    time_elapsed     | 236       |
|    total_timesteps  | 72333     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.9      |
|    n_updates        | 8083      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 66.4      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.95      |
| time/               |           |
|    episodes         | 864       |
|    fps              | 305       |
|    time_elapsed     | 236       |
|    total_timesteps  | 72474     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97        |
|    n_updates        | 8118      |
-----------------------------------
Eval num_timesteps=72500, episode_reward=-3156.66 +/- 1102.15
Episode length: 33.00 +/- 5.89
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33        |
|    mean_reward      | -3.16e+03 |
| rollout/            |           |
|    exploration_rate | 0.95      |
| time/               |           |
|    total_timesteps  | 72500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 8124      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 66.1      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.95      |
| time/               |           |
|    episodes         | 868       |
|    fps              | 304       |
|    time_elapsed     | 238       |
|    total_timesteps  | 72615     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 32.3      |
|    n_updates        | 8153      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 66        |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.95      |
| time/               |           |
|    episodes         | 872       |
|    fps              | 305       |
|    time_elapsed     | 238       |
|    total_timesteps  | 72754     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.6      |
|    n_updates        | 8188      |
-----------------------------------
Eval num_timesteps=73000, episode_reward=-3039.24 +/- 839.95
Episode length: 35.34 +/- 5.77
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.949     |
| time/               |           |
|    total_timesteps  | 73000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.4      |
|    n_updates        | 8249      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.5     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.948    |
| time/               |          |
|    episodes         | 876      |
|    fps              | 305      |
|    time_elapsed     | 239      |
|    total_timesteps  | 73404    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 31.5     |
|    n_updates        | 8350     |
----------------------------------
Eval num_timesteps=73500, episode_reward=-3262.02 +/- 699.14
Episode length: 33.68 +/- 8.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -3.26e+03 |
| rollout/            |           |
|    exploration_rate | 0.948     |
| time/               |           |
|    total_timesteps  | 73500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.6      |
|    n_updates        | 8374      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 65.7      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.948     |
| time/               |           |
|    episodes         | 880       |
|    fps              | 305       |
|    time_elapsed     | 241       |
|    total_timesteps  | 73657     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.4      |
|    n_updates        | 8414      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.9      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.947     |
| time/               |           |
|    episodes         | 884       |
|    fps              | 306       |
|    time_elapsed     | 241       |
|    total_timesteps  | 73930     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.6      |
|    n_updates        | 8482      |
-----------------------------------
Eval num_timesteps=74000, episode_reward=-2413.08 +/- 1207.72
Episode length: 37.68 +/- 5.85
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.7      |
|    mean_reward      | -2.41e+03 |
| rollout/            |           |
|    exploration_rate | 0.947     |
| time/               |           |
|    total_timesteps  | 74000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58        |
|    n_updates        | 8499      |
-----------------------------------
New best mean reward!
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.7      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.946     |
| time/               |           |
|    episodes         | 888       |
|    fps              | 305       |
|    time_elapsed     | 242       |
|    total_timesteps  | 74146     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.3      |
|    n_updates        | 8536      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.3     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 892      |
|    fps              | 305      |
|    time_elapsed     | 243      |
|    total_timesteps  | 74351    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 81.3     |
|    n_updates        | 8587     |
----------------------------------
Eval num_timesteps=74500, episode_reward=-2812.67 +/- 1437.08
Episode length: 34.46 +/- 6.31
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.946     |
| time/               |           |
|    total_timesteps  | 74500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 8624      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61       |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 896      |
|    fps              | 304      |
|    time_elapsed     | 244      |
|    total_timesteps  | 74546    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 81.4     |
|    n_updates        | 8636     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.3      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.945     |
| time/               |           |
|    episodes         | 900       |
|    fps              | 305       |
|    time_elapsed     | 244       |
|    total_timesteps  | 74778     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.8      |
|    n_updates        | 8694      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.2      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.944     |
| time/               |           |
|    episodes         | 904       |
|    fps              | 306       |
|    time_elapsed     | 244       |
|    total_timesteps  | 74952     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.1      |
|    n_updates        | 8737      |
-----------------------------------
Eval num_timesteps=75000, episode_reward=-3103.47 +/- 1068.98
Episode length: 34.06 +/- 6.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.1     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.944    |
| time/               |          |
|    total_timesteps  | 75000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 89.7     |
|    n_updates        | 8749     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.5      |
|    ep_rew_mean      | -3.98e+03 |
|    exploration_rate | 0.944     |
| time/               |           |
|    episodes         | 908       |
|    fps              | 305       |
|    time_elapsed     | 246       |
|    total_timesteps  | 75282     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 32        |
|    n_updates        | 8820      |
-----------------------------------
Eval num_timesteps=75500, episode_reward=-2756.38 +/- 1329.32
Episode length: 36.06 +/- 5.96
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.943     |
| time/               |           |
|    total_timesteps  | 75500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.7      |
|    n_updates        | 8874      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.5      |
|    ep_rew_mean      | -3.99e+03 |
|    exploration_rate | 0.943     |
| time/               |           |
|    episodes         | 912       |
|    fps              | 305       |
|    time_elapsed     | 247       |
|    total_timesteps  | 75512     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 8877      |
-----------------------------------
Eval num_timesteps=76000, episode_reward=-3010.91 +/- 1141.00
Episode length: 33.74 +/- 6.91
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.942     |
| time/               |           |
|    total_timesteps  | 76000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80        |
|    n_updates        | 8999      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 66.9      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.941     |
| time/               |           |
|    episodes         | 916       |
|    fps              | 305       |
|    time_elapsed     | 249       |
|    total_timesteps  | 76193     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.7      |
|    n_updates        | 9048      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67        |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.941     |
| time/               |           |
|    episodes         | 920       |
|    fps              | 306       |
|    time_elapsed     | 249       |
|    total_timesteps  | 76363     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 9090      |
-----------------------------------
Eval num_timesteps=76500, episode_reward=-3072.45 +/- 941.05
Episode length: 34.40 +/- 6.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -3.07e+03 |
| rollout/            |           |
|    exploration_rate | 0.941     |
| time/               |           |
|    total_timesteps  | 76500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 9124      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 64.6      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 0.941     |
| time/               |           |
|    episodes         | 924       |
|    fps              | 305       |
|    time_elapsed     | 250       |
|    total_timesteps  | 76592     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.6      |
|    n_updates        | 9147      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.1      |
|    ep_rew_mean      | -3.98e+03 |
|    exploration_rate | 0.94      |
| time/               |           |
|    episodes         | 928       |
|    fps              | 306       |
|    time_elapsed     | 251       |
|    total_timesteps  | 76989     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 9247      |
-----------------------------------
Eval num_timesteps=77000, episode_reward=-2969.09 +/- 1207.70
Episode length: 34.70 +/- 7.40
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.94      |
| time/               |           |
|    total_timesteps  | 77000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 9249      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 65.8      |
|    ep_rew_mean      | -3.99e+03 |
|    exploration_rate | 0.939     |
| time/               |           |
|    episodes         | 932       |
|    fps              | 305       |
|    time_elapsed     | 252       |
|    total_timesteps  | 77116     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.3      |
|    n_updates        | 9278      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.1     |
|    ep_rew_mean      | -4e+03   |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 936      |
|    fps              | 306      |
|    time_elapsed     | 252      |
|    total_timesteps  | 77253    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 50.8     |
|    n_updates        | 9313     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.4     |
|    ep_rew_mean      | -4e+03   |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 940      |
|    fps              | 306      |
|    time_elapsed     | 252      |
|    total_timesteps  | 77495    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 77.4     |
|    n_updates        | 9373     |
----------------------------------
Eval num_timesteps=77500, episode_reward=-3026.58 +/- 1051.92
Episode length: 33.88 +/- 6.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -3.03e+03 |
| rollout/            |           |
|    exploration_rate | 0.938     |
| time/               |           |
|    total_timesteps  | 77500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 9374      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 63.4      |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 0.938     |
| time/               |           |
|    episodes         | 944       |
|    fps              | 306       |
|    time_elapsed     | 253       |
|    total_timesteps  | 77696     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 5.17      |
|    n_updates        | 9423      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 63.6      |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 0.937     |
| time/               |           |
|    episodes         | 948       |
|    fps              | 306       |
|    time_elapsed     | 254       |
|    total_timesteps  | 77921     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.3      |
|    n_updates        | 9480      |
-----------------------------------
Eval num_timesteps=78000, episode_reward=-2817.54 +/- 1097.61
Episode length: 34.88 +/- 7.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.937     |
| time/               |           |
|    total_timesteps  | 78000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28.8      |
|    n_updates        | 9499      |
-----------------------------------
Eval num_timesteps=78500, episode_reward=-2731.07 +/- 1104.22
Episode length: 36.02 +/- 6.45
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.936     |
| time/               |           |
|    total_timesteps  | 78500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79        |
|    n_updates        | 9624      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.3      |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 0.935     |
| time/               |           |
|    episodes         | 952       |
|    fps              | 305       |
|    time_elapsed     | 257       |
|    total_timesteps  | 78637     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47.7      |
|    n_updates        | 9659      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 66.3      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 0.935     |
| time/               |           |
|    episodes         | 956       |
|    fps              | 306       |
|    time_elapsed     | 257       |
|    total_timesteps  | 78777     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 39.2      |
|    n_updates        | 9694      |
-----------------------------------
Eval num_timesteps=79000, episode_reward=-3102.72 +/- 977.57
Episode length: 34.76 +/- 6.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.8     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.935    |
| time/               |          |
|    total_timesteps  | 79000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 104      |
|    n_updates        | 9749     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.4      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 0.934     |
| time/               |           |
|    episodes         | 960       |
|    fps              | 305       |
|    time_elapsed     | 258       |
|    total_timesteps  | 79070     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.6      |
|    n_updates        | 9767      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 68.7      |
|    ep_rew_mean      | -3.95e+03 |
|    exploration_rate | 0.934     |
| time/               |           |
|    episodes         | 964       |
|    fps              | 306       |
|    time_elapsed     | 258       |
|    total_timesteps  | 79340     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 9834      |
-----------------------------------
Eval num_timesteps=79500, episode_reward=-2692.92 +/- 1163.41
Episode length: 37.14 +/- 5.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.1      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.933     |
| time/               |           |
|    total_timesteps  | 79500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 9874      |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 69.7      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.933     |
| time/               |           |
|    episodes         | 968       |
|    fps              | 305       |
|    time_elapsed     | 260       |
|    total_timesteps  | 79581     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 9895      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 972      |
|    fps              | 306      |
|    time_elapsed     | 260      |
|    total_timesteps  | 79757    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 74.3     |
|    n_updates        | 9939     |
----------------------------------
Eval num_timesteps=80000, episode_reward=-2703.31 +/- 1223.52
Episode length: 36.24 +/- 6.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.2     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.932    |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 73.6     |
|    n_updates        | 9999     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.9      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.932     |
| time/               |           |
|    episodes         | 976       |
|    fps              | 306       |
|    time_elapsed     | 261       |
|    total_timesteps  | 80193     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 50.3      |
|    n_updates        | 10048     |
-----------------------------------
Eval num_timesteps=80500, episode_reward=-2757.75 +/- 1251.71
Episode length: 36.30 +/- 5.88
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.931     |
| time/               |           |
|    total_timesteps  | 80500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.1      |
|    n_updates        | 10124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 71.5      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.93      |
| time/               |           |
|    episodes         | 980       |
|    fps              | 306       |
|    time_elapsed     | 263       |
|    total_timesteps  | 80811     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 50.2      |
|    n_updates        | 10202     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 70.4      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.93      |
| time/               |           |
|    episodes         | 984       |
|    fps              | 307       |
|    time_elapsed     | 263       |
|    total_timesteps  | 80970     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.8      |
|    n_updates        | 10242     |
-----------------------------------
Eval num_timesteps=81000, episode_reward=-3151.75 +/- 936.80
Episode length: 34.46 +/- 6.30
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -3.15e+03 |
| rollout/            |           |
|    exploration_rate | 0.929     |
| time/               |           |
|    total_timesteps  | 81000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28        |
|    n_updates        | 10249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 71.8      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.929     |
| time/               |           |
|    episodes         | 988       |
|    fps              | 306       |
|    time_elapsed     | 265       |
|    total_timesteps  | 81324     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 10330     |
-----------------------------------
Eval num_timesteps=81500, episode_reward=-2849.34 +/- 1225.03
Episode length: 34.44 +/- 6.97
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.928     |
| time/               |           |
|    total_timesteps  | 81500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.9      |
|    n_updates        | 10374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 71.6      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.928     |
| time/               |           |
|    episodes         | 992       |
|    fps              | 305       |
|    time_elapsed     | 266       |
|    total_timesteps  | 81507     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 10376     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 71.6      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.928     |
| time/               |           |
|    episodes         | 996       |
|    fps              | 306       |
|    time_elapsed     | 266       |
|    total_timesteps  | 81706     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 55.7      |
|    n_updates        | 10426     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 70.9      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.927     |
| time/               |           |
|    episodes         | 1000      |
|    fps              | 306       |
|    time_elapsed     | 266       |
|    total_timesteps  | 81867     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.1      |
|    n_updates        | 10466     |
-----------------------------------
Eval num_timesteps=82000, episode_reward=-2887.61 +/- 1118.64
Episode length: 34.68 +/- 6.34
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.927     |
| time/               |           |
|    total_timesteps  | 82000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 10499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 71        |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.927     |
| time/               |           |
|    episodes         | 1004      |
|    fps              | 306       |
|    time_elapsed     | 268       |
|    total_timesteps  | 82056     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.7      |
|    n_updates        | 10513     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 70.1      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.926     |
| time/               |           |
|    episodes         | 1008      |
|    fps              | 306       |
|    time_elapsed     | 268       |
|    total_timesteps  | 82296     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 10.7      |
|    n_updates        | 10573     |
-----------------------------------
Eval num_timesteps=82500, episode_reward=-2848.31 +/- 1103.26
Episode length: 35.56 +/- 6.25
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.926     |
| time/               |           |
|    total_timesteps  | 82500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.5      |
|    n_updates        | 10624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 70        |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.926     |
| time/               |           |
|    episodes         | 1012      |
|    fps              | 306       |
|    time_elapsed     | 269       |
|    total_timesteps  | 82512     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.3      |
|    n_updates        | 10627     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.1      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.924     |
| time/               |           |
|    episodes         | 1016      |
|    fps              | 307       |
|    time_elapsed     | 269       |
|    total_timesteps  | 82906     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.1      |
|    n_updates        | 10726     |
-----------------------------------
Eval num_timesteps=83000, episode_reward=-2948.00 +/- 1168.07
Episode length: 34.24 +/- 7.03
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.924     |
| time/               |           |
|    total_timesteps  | 83000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.8      |
|    n_updates        | 10749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 68        |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.924     |
| time/               |           |
|    episodes         | 1020      |
|    fps              | 306       |
|    time_elapsed     | 271       |
|    total_timesteps  | 83159     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.5      |
|    n_updates        | 10789     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.5      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.923     |
| time/               |           |
|    episodes         | 1024      |
|    fps              | 307       |
|    time_elapsed     | 271       |
|    total_timesteps  | 83345     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.3      |
|    n_updates        | 10836     |
-----------------------------------
Eval num_timesteps=83500, episode_reward=-2894.01 +/- 1035.58
Episode length: 34.60 +/- 6.56
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.923     |
| time/               |           |
|    total_timesteps  | 83500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60        |
|    n_updates        | 10874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 65.6      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.923     |
| time/               |           |
|    episodes         | 1028      |
|    fps              | 306       |
|    time_elapsed     | 272       |
|    total_timesteps  | 83545     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 8.7       |
|    n_updates        | 10886     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.9      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.922     |
| time/               |           |
|    episodes         | 1032      |
|    fps              | 307       |
|    time_elapsed     | 273       |
|    total_timesteps  | 83902     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 16.9      |
|    n_updates        | 10975     |
-----------------------------------
Eval num_timesteps=84000, episode_reward=-2903.01 +/- 1045.86
Episode length: 35.80 +/- 5.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.8     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.922    |
| time/               |          |
|    total_timesteps  | 84000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 104      |
|    n_updates        | 10999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68       |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 306      |
|    time_elapsed     | 274      |
|    total_timesteps  | 84054    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 14.1     |
|    n_updates        | 11013    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 69.3      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.92      |
| time/               |           |
|    episodes         | 1040      |
|    fps              | 307       |
|    time_elapsed     | 274       |
|    total_timesteps  | 84424     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.5      |
|    n_updates        | 11105     |
-----------------------------------
Eval num_timesteps=84500, episode_reward=-2846.12 +/- 1089.56
Episode length: 34.80 +/- 6.74
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.92      |
| time/               |           |
|    total_timesteps  | 84500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.5      |
|    n_updates        | 11124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 68.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.92      |
| time/               |           |
|    episodes         | 1044      |
|    fps              | 306       |
|    time_elapsed     | 276       |
|    total_timesteps  | 84579     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.8      |
|    n_updates        | 11144     |
-----------------------------------
Eval num_timesteps=85000, episode_reward=-2474.81 +/- 1450.73
Episode length: 36.30 +/- 7.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.47e+03 |
| rollout/            |           |
|    exploration_rate | 0.919     |
| time/               |           |
|    total_timesteps  | 85000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.4      |
|    n_updates        | 11249     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.7     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 306      |
|    time_elapsed     | 277      |
|    total_timesteps  | 85086    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 67.7     |
|    n_updates        | 11271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.5     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 307      |
|    time_elapsed     | 277      |
|    total_timesteps  | 85283    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 72.4     |
|    n_updates        | 11320    |
----------------------------------
Eval num_timesteps=85500, episode_reward=-3089.16 +/- 855.51
Episode length: 34.16 +/- 7.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -3.09e+03 |
| rollout/            |           |
|    exploration_rate | 0.918     |
| time/               |           |
|    total_timesteps  | 85500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 11374     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.7     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 306      |
|    time_elapsed     | 279      |
|    total_timesteps  | 85549    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 99.7     |
|    n_updates        | 11387    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.5      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.917     |
| time/               |           |
|    episodes         | 1060      |
|    fps              | 307       |
|    time_elapsed     | 279       |
|    total_timesteps  | 85816     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 9.08      |
|    n_updates        | 11453     |
-----------------------------------
Eval num_timesteps=86000, episode_reward=-2855.88 +/- 1290.81
Episode length: 35.02 +/- 6.21
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.916     |
| time/               |           |
|    total_timesteps  | 86000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 46.5      |
|    n_updates        | 11499     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.8     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 306      |
|    time_elapsed     | 280      |
|    total_timesteps  | 86024    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 169      |
|    n_updates        | 11505    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 68.5      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.915     |
| time/               |           |
|    episodes         | 1068      |
|    fps              | 307       |
|    time_elapsed     | 281       |
|    total_timesteps  | 86433     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 56.4      |
|    n_updates        | 11608     |
-----------------------------------
Eval num_timesteps=86500, episode_reward=-3114.04 +/- 875.65
Episode length: 34.74 +/- 7.06
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.915     |
| time/               |           |
|    total_timesteps  | 86500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.2      |
|    n_updates        | 11624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 70.8      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.914     |
| time/               |           |
|    episodes         | 1072      |
|    fps              | 307       |
|    time_elapsed     | 282       |
|    total_timesteps  | 86835     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 14.5      |
|    n_updates        | 11708     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 67.4      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.914     |
| time/               |           |
|    episodes         | 1076      |
|    fps              | 307       |
|    time_elapsed     | 282       |
|    total_timesteps  | 86937     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.7      |
|    n_updates        | 11734     |
-----------------------------------
Eval num_timesteps=87000, episode_reward=-2855.11 +/- 1083.20
Episode length: 36.44 +/- 5.49
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.913     |
| time/               |           |
|    total_timesteps  | 87000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.5      |
|    n_updates        | 11749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.9      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.913     |
| time/               |           |
|    episodes         | 1080      |
|    fps              | 306       |
|    time_elapsed     | 283       |
|    total_timesteps  | 87103     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 16        |
|    n_updates        | 11775     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 63        |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.913     |
| time/               |           |
|    episodes         | 1084      |
|    fps              | 307       |
|    time_elapsed     | 284       |
|    total_timesteps  | 87272     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 11817     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.1      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.912     |
| time/               |           |
|    episodes         | 1088      |
|    fps              | 307       |
|    time_elapsed     | 284       |
|    total_timesteps  | 87437     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 11859     |
-----------------------------------
Eval num_timesteps=87500, episode_reward=-3021.13 +/- 1014.35
Episode length: 34.14 +/- 6.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -3.02e+03 |
| rollout/            |           |
|    exploration_rate | 0.912     |
| time/               |           |
|    total_timesteps  | 87500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 19        |
|    n_updates        | 11874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.1      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.911     |
| time/               |           |
|    episodes         | 1092      |
|    fps              | 307       |
|    time_elapsed     | 285       |
|    total_timesteps  | 87718     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.8      |
|    n_updates        | 11929     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.1      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.911     |
| time/               |           |
|    episodes         | 1096      |
|    fps              | 307       |
|    time_elapsed     | 285       |
|    total_timesteps  | 87920     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 11979     |
-----------------------------------
Eval num_timesteps=88000, episode_reward=-2836.80 +/- 1179.73
Episode length: 34.94 +/- 7.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.911     |
| time/               |           |
|    total_timesteps  | 88000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 11999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 64.1      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.91      |
| time/               |           |
|    episodes         | 1100      |
|    fps              | 307       |
|    time_elapsed     | 287       |
|    total_timesteps  | 88281     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.4      |
|    n_updates        | 12070     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.7     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 307      |
|    time_elapsed     | 287      |
|    total_timesteps  | 88424    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 55.9     |
|    n_updates        | 12105    |
----------------------------------
Eval num_timesteps=88500, episode_reward=-3168.85 +/- 950.32
Episode length: 33.38 +/- 6.53
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.4      |
|    mean_reward      | -3.17e+03 |
| rollout/            |           |
|    exploration_rate | 0.909     |
| time/               |           |
|    total_timesteps  | 88500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.6      |
|    n_updates        | 12124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.6      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.909     |
| time/               |           |
|    episodes         | 1108      |
|    fps              | 306       |
|    time_elapsed     | 288       |
|    total_timesteps  | 88558     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.1      |
|    n_updates        | 12139     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.6     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 307      |
|    time_elapsed     | 288      |
|    total_timesteps  | 88773    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 74.4     |
|    n_updates        | 12193    |
----------------------------------
Eval num_timesteps=89000, episode_reward=-2654.69 +/- 1307.23
Episode length: 35.40 +/- 7.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.908     |
| time/               |           |
|    total_timesteps  | 89000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.3      |
|    n_updates        | 12249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.5      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.907     |
| time/               |           |
|    episodes         | 1116      |
|    fps              | 307       |
|    time_elapsed     | 290       |
|    total_timesteps  | 89151     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.2      |
|    n_updates        | 12287     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 62.9      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.907     |
| time/               |           |
|    episodes         | 1120      |
|    fps              | 307       |
|    time_elapsed     | 290       |
|    total_timesteps  | 89448     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.7      |
|    n_updates        | 12361     |
-----------------------------------
Eval num_timesteps=89500, episode_reward=-2828.78 +/- 1126.04
Episode length: 35.04 +/- 7.31
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.906     |
| time/               |           |
|    total_timesteps  | 89500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98        |
|    n_updates        | 12374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 63.6      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.906     |
| time/               |           |
|    episodes         | 1124      |
|    fps              | 307       |
|    time_elapsed     | 291       |
|    total_timesteps  | 89710     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.4      |
|    n_updates        | 12427     |
-----------------------------------
Eval num_timesteps=90000, episode_reward=-3273.82 +/- 959.42
Episode length: 34.26 +/- 8.89
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -3.27e+03 |
| rollout/            |           |
|    exploration_rate | 0.905     |
| time/               |           |
|    total_timesteps  | 90000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.2      |
|    n_updates        | 12499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 64.7      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.905     |
| time/               |           |
|    episodes         | 1128      |
|    fps              | 306       |
|    time_elapsed     | 293       |
|    total_timesteps  | 90011     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 12502     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 63.8      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.904     |
| time/               |           |
|    episodes         | 1132      |
|    fps              | 307       |
|    time_elapsed     | 293       |
|    total_timesteps  | 90281     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.4      |
|    n_updates        | 12570     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 63.4      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.904     |
| time/               |           |
|    episodes         | 1136      |
|    fps              | 307       |
|    time_elapsed     | 293       |
|    total_timesteps  | 90392     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 12597     |
-----------------------------------
Eval num_timesteps=90500, episode_reward=-3057.38 +/- 1007.28
Episode length: 34.22 +/- 7.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -3.06e+03 |
| rollout/            |           |
|    exploration_rate | 0.904     |
| time/               |           |
|    total_timesteps  | 90500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.6      |
|    n_updates        | 12624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.3     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 307      |
|    time_elapsed     | 294      |
|    total_timesteps  | 90555    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 121      |
|    n_updates        | 12638    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.3      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.903     |
| time/               |           |
|    episodes         | 1144      |
|    fps              | 307       |
|    time_elapsed     | 294       |
|    total_timesteps  | 90710     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 38        |
|    n_updates        | 12677     |
-----------------------------------
Eval num_timesteps=91000, episode_reward=-3253.62 +/- 882.84
Episode length: 33.18 +/- 6.12
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.2      |
|    mean_reward      | -3.25e+03 |
| rollout/            |           |
|    exploration_rate | 0.902     |
| time/               |           |
|    total_timesteps  | 91000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79        |
|    n_updates        | 12749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.7      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.901     |
| time/               |           |
|    episodes         | 1148      |
|    fps              | 307       |
|    time_elapsed     | 296       |
|    total_timesteps  | 91253     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 12813     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.1      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.901     |
| time/               |           |
|    episodes         | 1152      |
|    fps              | 308       |
|    time_elapsed     | 296       |
|    total_timesteps  | 91389     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 17.1      |
|    n_updates        | 12847     |
-----------------------------------
Eval num_timesteps=91500, episode_reward=-3063.72 +/- 1270.66
Episode length: 32.86 +/- 6.97
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.9      |
|    mean_reward      | -3.06e+03 |
| rollout/            |           |
|    exploration_rate | 0.901     |
| time/               |           |
|    total_timesteps  | 91500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.1      |
|    n_updates        | 12874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 61.1      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.9       |
| time/               |           |
|    episodes         | 1156      |
|    fps              | 307       |
|    time_elapsed     | 297       |
|    total_timesteps  | 91660     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.3      |
|    n_updates        | 12914     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 59.9      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.9       |
| time/               |           |
|    episodes         | 1160      |
|    fps              | 307       |
|    time_elapsed     | 298       |
|    total_timesteps  | 91806     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.2      |
|    n_updates        | 12951     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 59.3      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.899     |
| time/               |           |
|    episodes         | 1164      |
|    fps              | 308       |
|    time_elapsed     | 298       |
|    total_timesteps  | 91957     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 12989     |
-----------------------------------
Eval num_timesteps=92000, episode_reward=-2674.94 +/- 1222.25
Episode length: 36.98 +/- 6.15
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37        |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.899     |
| time/               |           |
|    total_timesteps  | 92000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 12999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 57.1      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.899     |
| time/               |           |
|    episodes         | 1168      |
|    fps              | 307       |
|    time_elapsed     | 299       |
|    total_timesteps  | 92145     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 35.7      |
|    n_updates        | 13036     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 56        |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.898     |
| time/               |           |
|    episodes         | 1172      |
|    fps              | 308       |
|    time_elapsed     | 299       |
|    total_timesteps  | 92435     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 25        |
|    n_updates        | 13108     |
-----------------------------------
Eval num_timesteps=92500, episode_reward=-2855.42 +/- 1233.60
Episode length: 34.50 +/- 7.30
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.898     |
| time/               |           |
|    total_timesteps  | 92500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 13124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 56.5      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.898     |
| time/               |           |
|    episodes         | 1176      |
|    fps              | 307       |
|    time_elapsed     | 301       |
|    total_timesteps  | 92584     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 13145     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.3      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.897     |
| time/               |           |
|    episodes         | 1180      |
|    fps              | 308       |
|    time_elapsed     | 301       |
|    total_timesteps  | 92937     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 13234     |
-----------------------------------
Eval num_timesteps=93000, episode_reward=-2750.30 +/- 1130.96
Episode length: 34.36 +/- 7.55
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.896     |
| time/               |           |
|    total_timesteps  | 93000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 13249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.5      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.896     |
| time/               |           |
|    episodes         | 1184      |
|    fps              | 307       |
|    time_elapsed     | 302       |
|    total_timesteps  | 93123     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 13280     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 59.1      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.895     |
| time/               |           |
|    episodes         | 1188      |
|    fps              | 308       |
|    time_elapsed     | 302       |
|    total_timesteps  | 93351     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 13.4      |
|    n_updates        | 13337     |
-----------------------------------
Eval num_timesteps=93500, episode_reward=-3026.13 +/- 890.22
Episode length: 34.14 +/- 7.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -3.03e+03 |
| rollout/            |           |
|    exploration_rate | 0.895     |
| time/               |           |
|    total_timesteps  | 93500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 32.4      |
|    n_updates        | 13374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.5      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.895     |
| time/               |           |
|    episodes         | 1192      |
|    fps              | 307       |
|    time_elapsed     | 304       |
|    total_timesteps  | 93568     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37.4      |
|    n_updates        | 13391     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.3      |
|    ep_rew_mean      | -3.98e+03 |
|    exploration_rate | 0.894     |
| time/               |           |
|    episodes         | 1196      |
|    fps              | 307       |
|    time_elapsed     | 304       |
|    total_timesteps  | 93751     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.8      |
|    n_updates        | 13437     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 56.1      |
|    ep_rew_mean      | -4.01e+03 |
|    exploration_rate | 0.894     |
| time/               |           |
|    episodes         | 1200      |
|    fps              | 308       |
|    time_elapsed     | 304       |
|    total_timesteps  | 93892     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61        |
|    n_updates        | 13472     |
-----------------------------------
Eval num_timesteps=94000, episode_reward=-3093.21 +/- 884.74
Episode length: 33.84 +/- 5.88
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -3.09e+03 |
| rollout/            |           |
|    exploration_rate | 0.893     |
| time/               |           |
|    total_timesteps  | 94000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 11        |
|    n_updates        | 13499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 56.3      |
|    ep_rew_mean      | -4.04e+03 |
|    exploration_rate | 0.893     |
| time/               |           |
|    episodes         | 1204      |
|    fps              | 307       |
|    time_elapsed     | 305       |
|    total_timesteps  | 94056     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57        |
|    n_updates        | 13513     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 57.3      |
|    ep_rew_mean      | -4.06e+03 |
|    exploration_rate | 0.893     |
| time/               |           |
|    episodes         | 1208      |
|    fps              | 308       |
|    time_elapsed     | 305       |
|    total_timesteps  | 94288     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 33.5      |
|    n_updates        | 13571     |
-----------------------------------
Eval num_timesteps=94500, episode_reward=-2936.66 +/- 1258.78
Episode length: 33.92 +/- 6.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.892     |
| time/               |           |
|    total_timesteps  | 94500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 13624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.1      |
|    ep_rew_mean      | -4.06e+03 |
|    exploration_rate | 0.892     |
| time/               |           |
|    episodes         | 1212      |
|    fps              | 307       |
|    time_elapsed     | 307       |
|    total_timesteps  | 94581     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28        |
|    n_updates        | 13645     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 56.7      |
|    ep_rew_mean      | -4.06e+03 |
|    exploration_rate | 0.891     |
| time/               |           |
|    episodes         | 1216      |
|    fps              | 308       |
|    time_elapsed     | 307       |
|    total_timesteps  | 94818     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64.9      |
|    n_updates        | 13704     |
-----------------------------------
Eval num_timesteps=95000, episode_reward=-3030.26 +/- 1156.54
Episode length: 33.28 +/- 7.26
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.3      |
|    mean_reward      | -3.03e+03 |
| rollout/            |           |
|    exploration_rate | 0.89      |
| time/               |           |
|    total_timesteps  | 95000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.8      |
|    n_updates        | 13749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.9      |
|    ep_rew_mean      | -4.01e+03 |
|    exploration_rate | 0.889     |
| time/               |           |
|    episodes         | 1220      |
|    fps              | 308       |
|    time_elapsed     | 309       |
|    total_timesteps  | 95335     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 25.2      |
|    n_updates        | 13833     |
-----------------------------------
Eval num_timesteps=95500, episode_reward=-2777.20 +/- 1282.50
Episode length: 34.72 +/- 5.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.889     |
| time/               |           |
|    total_timesteps  | 95500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.5      |
|    n_updates        | 13874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.9      |
|    ep_rew_mean      | -4.04e+03 |
|    exploration_rate | 0.889     |
| time/               |           |
|    episodes         | 1224      |
|    fps              | 307       |
|    time_elapsed     | 310       |
|    total_timesteps  | 95602     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 13900     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 57.2      |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.888     |
| time/               |           |
|    episodes         | 1228      |
|    fps              | 308       |
|    time_elapsed     | 310       |
|    total_timesteps  | 95736     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 13933     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 55.9      |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.888     |
| time/               |           |
|    episodes         | 1232      |
|    fps              | 308       |
|    time_elapsed     | 310       |
|    total_timesteps  | 95873     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 56.7      |
|    n_updates        | 13968     |
-----------------------------------
Eval num_timesteps=96000, episode_reward=-2495.23 +/- 1506.35
Episode length: 35.50 +/- 7.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.5     |
|    mean_reward      | -2.5e+03 |
| rollout/            |          |
|    exploration_rate | 0.887    |
| time/               |          |
|    total_timesteps  | 96000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 19.9     |
|    n_updates        | 13999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 57.4      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 0.887     |
| time/               |           |
|    episodes         | 1236      |
|    fps              | 307       |
|    time_elapsed     | 312       |
|    total_timesteps  | 96135     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 14033     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 57.3      |
|    ep_rew_mean      | -4.05e+03 |
|    exploration_rate | 0.887     |
| time/               |           |
|    episodes         | 1240      |
|    fps              | 308       |
|    time_elapsed     | 312       |
|    total_timesteps  | 96289     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.4      |
|    n_updates        | 14072     |
-----------------------------------
Eval num_timesteps=96500, episode_reward=-2693.35 +/- 1109.07
Episode length: 35.98 +/- 6.80
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.886     |
| time/               |           |
|    total_timesteps  | 96500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 14124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.7      |
|    ep_rew_mean      | -4.03e+03 |
|    exploration_rate | 0.886     |
| time/               |           |
|    episodes         | 1244      |
|    fps              | 307       |
|    time_elapsed     | 313       |
|    total_timesteps  | 96580     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 42        |
|    n_updates        | 14144     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 55        |
|    ep_rew_mean      | -4.01e+03 |
|    exploration_rate | 0.885     |
| time/               |           |
|    episodes         | 1248      |
|    fps              | 308       |
|    time_elapsed     | 313       |
|    total_timesteps  | 96748     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.7      |
|    n_updates        | 14186     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 54.8      |
|    ep_rew_mean      | -4.01e+03 |
|    exploration_rate | 0.885     |
| time/               |           |
|    episodes         | 1252      |
|    fps              | 308       |
|    time_elapsed     | 313       |
|    total_timesteps  | 96871     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 14217     |
-----------------------------------
Eval num_timesteps=97000, episode_reward=-3096.45 +/- 829.51
Episode length: 35.74 +/- 6.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.7     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 97000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 13.4     |
|    n_updates        | 14249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 55.4      |
|    ep_rew_mean      | -4.02e+03 |
|    exploration_rate | 0.884     |
| time/               |           |
|    episodes         | 1256      |
|    fps              | 308       |
|    time_elapsed     | 315       |
|    total_timesteps  | 97204     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 11.6      |
|    n_updates        | 14300     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 56        |
|    ep_rew_mean      | -3.99e+03 |
|    exploration_rate | 0.883     |
| time/               |           |
|    episodes         | 1260      |
|    fps              | 308       |
|    time_elapsed     | 315       |
|    total_timesteps  | 97411     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 39.6      |
|    n_updates        | 14352     |
-----------------------------------
Eval num_timesteps=97500, episode_reward=-3184.62 +/- 864.77
Episode length: 34.70 +/- 6.98
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -3.18e+03 |
| rollout/            |           |
|    exploration_rate | 0.883     |
| time/               |           |
|    total_timesteps  | 97500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.8      |
|    n_updates        | 14374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 57.5      |
|    ep_rew_mean      | -3.98e+03 |
|    exploration_rate | 0.882     |
| time/               |           |
|    episodes         | 1264      |
|    fps              | 308       |
|    time_elapsed     | 317       |
|    total_timesteps  | 97710     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62.7      |
|    n_updates        | 14427     |
-----------------------------------
Eval num_timesteps=98000, episode_reward=-2942.59 +/- 1031.48
Episode length: 34.42 +/- 6.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.881     |
| time/               |           |
|    total_timesteps  | 98000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.5      |
|    n_updates        | 14499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 59.3      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.881     |
| time/               |           |
|    episodes         | 1268      |
|    fps              | 307       |
|    time_elapsed     | 318       |
|    total_timesteps  | 98076     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.5      |
|    n_updates        | 14518     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.7      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.88      |
| time/               |           |
|    episodes         | 1272      |
|    fps              | 308       |
|    time_elapsed     | 318       |
|    total_timesteps  | 98309     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52.2      |
|    n_updates        | 14577     |
-----------------------------------
Eval num_timesteps=98500, episode_reward=-2756.87 +/- 1289.91
Episode length: 35.66 +/- 6.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.88      |
| time/               |           |
|    total_timesteps  | 98500     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 34.6      |
|    n_updates        | 14624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 59.9      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.88      |
| time/               |           |
|    episodes         | 1276      |
|    fps              | 307       |
|    time_elapsed     | 320       |
|    total_timesteps  | 98575     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 14643     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 59.1      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.879     |
| time/               |           |
|    episodes         | 1280      |
|    fps              | 308       |
|    time_elapsed     | 320       |
|    total_timesteps  | 98846     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 14711     |
-----------------------------------
Eval num_timesteps=99000, episode_reward=-2866.10 +/- 1137.06
Episode length: 33.94 +/- 6.74
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.878     |
| time/               |           |
|    total_timesteps  | 99000     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.3      |
|    n_updates        | 14749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 59        |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.878     |
| time/               |           |
|    episodes         | 1284      |
|    fps              | 307       |
|    time_elapsed     | 321       |
|    total_timesteps  | 99024     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.8      |
|    n_updates        | 14755     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58.7      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.878     |
| time/               |           |
|    episodes         | 1288      |
|    fps              | 308       |
|    time_elapsed     | 321       |
|    total_timesteps  | 99217     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 56.9      |
|    n_updates        | 14804     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 57.9      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.877     |
| time/               |           |
|    episodes         | 1292      |
|    fps              | 308       |
|    time_elapsed     | 321       |
|    total_timesteps  | 99357     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 36.9      |
|    n_updates        | 14839     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 57.5      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.877     |
| time/               |           |
|    episodes         | 1296      |
|    fps              | 308       |
|    time_elapsed     | 322       |
|    total_timesteps  | 99499     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.9      |
|    n_updates        | 14874     |
-----------------------------------
Eval num_timesteps=99500, episode_reward=-2862.39 +/- 1254.63
Episode length: 34.08 +/- 6.86
----------------------------------
| eval/              |           |
|    mean_ep_length  | 34.1      |
|    mean_reward     | -2.86e+03 |
| time/              |           |
|    total_timesteps | 99500     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 57.7      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.876     |
| time/               |           |
|    episodes         | 1300      |
|    fps              | 308       |
|    time_elapsed     | 323       |
|    total_timesteps  | 99666     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 32.7      |
|    n_updates        | 14916     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 58        |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.876     |
| time/               |           |
|    episodes         | 1304      |
|    fps              | 308       |
|    time_elapsed     | 323       |
|    total_timesteps  | 99854     |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 26.2      |
|    n_updates        | 14963     |
-----------------------------------
Eval num_timesteps=100000, episode_reward=-2986.56 +/- 1090.94
Episode length: 32.98 +/- 6.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33        |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.875     |
| time/               |           |
|    total_timesteps  | 100000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 14999     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.4     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 308      |
|    time_elapsed     | 324      |
|    total_timesteps  | 100032   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 47.3     |
|    n_updates        | 15007    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.9     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 308      |
|    time_elapsed     | 324      |
|    total_timesteps  | 100166   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 54.6     |
|    n_updates        | 15041    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 55.9      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.874     |
| time/               |           |
|    episodes         | 1316      |
|    fps              | 308       |
|    time_elapsed     | 325       |
|    total_timesteps  | 100408    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.4      |
|    n_updates        | 15101     |
-----------------------------------
Eval num_timesteps=100500, episode_reward=-2892.09 +/- 899.76
Episode length: 34.68 +/- 7.56
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.874     |
| time/               |           |
|    total_timesteps  | 100500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.3      |
|    n_updates        | 15124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 52.6      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.873     |
| time/               |           |
|    episodes         | 1320      |
|    fps              | 308       |
|    time_elapsed     | 326       |
|    total_timesteps  | 100599    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.4      |
|    n_updates        | 15149     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 51.2      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.873     |
| time/               |           |
|    episodes         | 1324      |
|    fps              | 308       |
|    time_elapsed     | 326       |
|    total_timesteps  | 100720    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 15179     |
-----------------------------------
Eval num_timesteps=101000, episode_reward=-2670.03 +/- 1186.79
Episode length: 36.82 +/- 5.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.872     |
| time/               |           |
|    total_timesteps  | 101000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73        |
|    n_updates        | 15249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 52.7      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.872     |
| time/               |           |
|    episodes         | 1328      |
|    fps              | 307       |
|    time_elapsed     | 328       |
|    total_timesteps  | 101009    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.3      |
|    n_updates        | 15252     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 52.6      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.872     |
| time/               |           |
|    episodes         | 1332      |
|    fps              | 308       |
|    time_elapsed     | 328       |
|    total_timesteps  | 101133    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 15283     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 51.8      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.871     |
| time/               |           |
|    episodes         | 1336      |
|    fps              | 308       |
|    time_elapsed     | 328       |
|    total_timesteps  | 101317    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 15329     |
-----------------------------------
Eval num_timesteps=101500, episode_reward=-2817.45 +/- 1138.09
Episode length: 35.42 +/- 5.74
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.87      |
| time/               |           |
|    total_timesteps  | 101500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 5.14      |
|    n_updates        | 15374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 52.9      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.87      |
| time/               |           |
|    episodes         | 1340      |
|    fps              | 308       |
|    time_elapsed     | 329       |
|    total_timesteps  | 101578    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 39.1      |
|    n_updates        | 15394     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 51.5      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.87      |
| time/               |           |
|    episodes         | 1344      |
|    fps              | 308       |
|    time_elapsed     | 329       |
|    total_timesteps  | 101725    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.4      |
|    n_updates        | 15431     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.5     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 308      |
|    time_elapsed     | 329      |
|    total_timesteps  | 101903   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 105      |
|    n_updates        | 15475    |
----------------------------------
Eval num_timesteps=102000, episode_reward=-2824.22 +/- 1356.50
Episode length: 34.74 +/- 7.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.869     |
| time/               |           |
|    total_timesteps  | 102000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.4      |
|    n_updates        | 15499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 51.5      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.869     |
| time/               |           |
|    episodes         | 1352      |
|    fps              | 307       |
|    time_elapsed     | 331       |
|    total_timesteps  | 102026    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 15506     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 50.9      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.868     |
| time/               |           |
|    episodes         | 1356      |
|    fps              | 308       |
|    time_elapsed     | 331       |
|    total_timesteps  | 102295    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.2      |
|    n_updates        | 15573     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.6     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 308      |
|    time_elapsed     | 331      |
|    total_timesteps  | 102471   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 73       |
|    n_updates        | 15617    |
----------------------------------
Eval num_timesteps=102500, episode_reward=-2996.56 +/- 1189.00
Episode length: 34.36 +/- 7.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.4     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.867    |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 145      |
|    n_updates        | 15624    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 49.9      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.867     |
| time/               |           |
|    episodes         | 1364      |
|    fps              | 308       |
|    time_elapsed     | 333       |
|    total_timesteps  | 102700    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.7      |
|    n_updates        | 15674     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.6      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.866     |
| time/               |           |
|    episodes         | 1368      |
|    fps              | 308       |
|    time_elapsed     | 333       |
|    total_timesteps  | 102833    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.6      |
|    n_updates        | 15708     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 46.7      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.866     |
| time/               |           |
|    episodes         | 1372      |
|    fps              | 308       |
|    time_elapsed     | 333       |
|    total_timesteps  | 102978    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.7      |
|    n_updates        | 15744     |
-----------------------------------
Eval num_timesteps=103000, episode_reward=-2524.80 +/- 1263.16
Episode length: 37.50 +/- 6.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.5      |
|    mean_reward      | -2.52e+03 |
| rollout/            |           |
|    exploration_rate | 0.866     |
| time/               |           |
|    total_timesteps  | 103000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 15749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 46.1      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.865     |
| time/               |           |
|    episodes         | 1376      |
|    fps              | 308       |
|    time_elapsed     | 334       |
|    total_timesteps  | 103185    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.5      |
|    n_updates        | 15796     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.1      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.865     |
| time/               |           |
|    episodes         | 1380      |
|    fps              | 308       |
|    time_elapsed     | 334       |
|    total_timesteps  | 103354    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 20.6      |
|    n_updates        | 15838     |
-----------------------------------
Eval num_timesteps=103500, episode_reward=-2694.70 +/- 1243.09
Episode length: 35.90 +/- 6.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.864     |
| time/               |           |
|    total_timesteps  | 103500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.5      |
|    n_updates        | 15874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.9      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.864     |
| time/               |           |
|    episodes         | 1384      |
|    fps              | 307       |
|    time_elapsed     | 336       |
|    total_timesteps  | 103516    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.6      |
|    n_updates        | 15878     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.7      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.863     |
| time/               |           |
|    episodes         | 1388      |
|    fps              | 308       |
|    time_elapsed     | 336       |
|    total_timesteps  | 103790    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 15947     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 46.3      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.862     |
| time/               |           |
|    episodes         | 1392      |
|    fps              | 308       |
|    time_elapsed     | 336       |
|    total_timesteps  | 103990    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 15997     |
-----------------------------------
Eval num_timesteps=104000, episode_reward=-2599.40 +/- 1498.95
Episode length: 34.64 +/- 7.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.6     |
|    mean_reward      | -2.6e+03 |
| rollout/            |          |
|    exploration_rate | 0.862    |
| time/               |          |
|    total_timesteps  | 104000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 66.8     |
|    n_updates        | 15999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47        |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.862     |
| time/               |           |
|    episodes         | 1396      |
|    fps              | 308       |
|    time_elapsed     | 338       |
|    total_timesteps  | 104203    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.7      |
|    n_updates        | 16050     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.3      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.861     |
| time/               |           |
|    episodes         | 1400      |
|    fps              | 308       |
|    time_elapsed     | 338       |
|    total_timesteps  | 104392    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 11.2      |
|    n_updates        | 16097     |
-----------------------------------
Eval num_timesteps=104500, episode_reward=-3035.62 +/- 800.88
Episode length: 34.84 +/- 6.87
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.861     |
| time/               |           |
|    total_timesteps  | 104500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 14.4      |
|    n_updates        | 16124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.6      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.86      |
| time/               |           |
|    episodes         | 1404      |
|    fps              | 307       |
|    time_elapsed     | 339       |
|    total_timesteps  | 104619    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 16154     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.8      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.86      |
| time/               |           |
|    episodes         | 1408      |
|    fps              | 308       |
|    time_elapsed     | 339       |
|    total_timesteps  | 104815    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 16203     |
-----------------------------------
Eval num_timesteps=105000, episode_reward=-3005.41 +/- 1179.57
Episode length: 33.64 +/- 6.76
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.6      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.859     |
| time/               |           |
|    total_timesteps  | 105000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 12.9      |
|    n_updates        | 16249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 49.1      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.859     |
| time/               |           |
|    episodes         | 1412      |
|    fps              | 307       |
|    time_elapsed     | 341       |
|    total_timesteps  | 105079    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.1      |
|    n_updates        | 16269     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 48.1      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.858     |
| time/               |           |
|    episodes         | 1416      |
|    fps              | 308       |
|    time_elapsed     | 341       |
|    total_timesteps  | 105215    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 16303     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.7      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.858     |
| time/               |           |
|    episodes         | 1420      |
|    fps              | 308       |
|    time_elapsed     | 341       |
|    total_timesteps  | 105367    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 55.8      |
|    n_updates        | 16341     |
-----------------------------------
Eval num_timesteps=105500, episode_reward=-2827.14 +/- 1130.51
Episode length: 34.74 +/- 6.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.858     |
| time/               |           |
|    total_timesteps  | 105500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 23.1      |
|    n_updates        | 16374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.9      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.858     |
| time/               |           |
|    episodes         | 1424      |
|    fps              | 307       |
|    time_elapsed     | 342       |
|    total_timesteps  | 105514    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 24.8      |
|    n_updates        | 16378     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 46.6      |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 0.857     |
| time/               |           |
|    episodes         | 1428      |
|    fps              | 308       |
|    time_elapsed     | 342       |
|    total_timesteps  | 105665    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 16416     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.9      |
|    ep_rew_mean      | -3.99e+03 |
|    exploration_rate | 0.856     |
| time/               |           |
|    episodes         | 1432      |
|    fps              | 308       |
|    time_elapsed     | 343       |
|    total_timesteps  | 105926    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 12.7      |
|    n_updates        | 16481     |
-----------------------------------
Eval num_timesteps=106000, episode_reward=-2853.77 +/- 1091.78
Episode length: 33.96 +/- 6.46
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.856     |
| time/               |           |
|    total_timesteps  | 106000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 16499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.7      |
|    ep_rew_mean      | -4.01e+03 |
|    exploration_rate | 0.856     |
| time/               |           |
|    episodes         | 1436      |
|    fps              | 307       |
|    time_elapsed     | 344       |
|    total_timesteps  | 106086    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.9      |
|    n_updates        | 16521     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.2      |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 0.855     |
| time/               |           |
|    episodes         | 1440      |
|    fps              | 308       |
|    time_elapsed     | 344       |
|    total_timesteps  | 106300    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.4      |
|    n_updates        | 16574     |
-----------------------------------
Eval num_timesteps=106500, episode_reward=-2817.16 +/- 1404.03
Episode length: 34.92 +/- 7.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.854     |
| time/               |           |
|    total_timesteps  | 106500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.8      |
|    n_updates        | 16624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 48.1      |
|    ep_rew_mean      | -3.97e+03 |
|    exploration_rate | 0.854     |
| time/               |           |
|    episodes         | 1444      |
|    fps              | 307       |
|    time_elapsed     | 345       |
|    total_timesteps  | 106532    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 16632     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.8      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.854     |
| time/               |           |
|    episodes         | 1448      |
|    fps              | 308       |
|    time_elapsed     | 346       |
|    total_timesteps  | 106680    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.5      |
|    n_updates        | 16669     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 48        |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.853     |
| time/               |           |
|    episodes         | 1452      |
|    fps              | 308       |
|    time_elapsed     | 346       |
|    total_timesteps  | 106829    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.2      |
|    n_updates        | 16707     |
-----------------------------------
Eval num_timesteps=107000, episode_reward=-2963.01 +/- 1200.85
Episode length: 34.04 +/- 6.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.853     |
| time/               |           |
|    total_timesteps  | 107000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.6      |
|    n_updates        | 16749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.2      |
|    ep_rew_mean      | -3.96e+03 |
|    exploration_rate | 0.853     |
| time/               |           |
|    episodes         | 1456      |
|    fps              | 307       |
|    time_elapsed     | 347       |
|    total_timesteps  | 107013    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85        |
|    n_updates        | 16753     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.4     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.852    |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 308      |
|    time_elapsed     | 347      |
|    total_timesteps  | 107210   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 65.9     |
|    n_updates        | 16802    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.7      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.851     |
| time/               |           |
|    episodes         | 1464      |
|    fps              | 308       |
|    time_elapsed     | 347       |
|    total_timesteps  | 107473    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52.4      |
|    n_updates        | 16868     |
-----------------------------------
Eval num_timesteps=107500, episode_reward=-2857.47 +/- 1110.02
Episode length: 34.34 +/- 6.75
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.851     |
| time/               |           |
|    total_timesteps  | 107500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 21.2      |
|    n_updates        | 16874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.8      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.851     |
| time/               |           |
|    episodes         | 1468      |
|    fps              | 308       |
|    time_elapsed     | 349       |
|    total_timesteps  | 107612    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 16902     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 308      |
|    time_elapsed     | 349      |
|    total_timesteps  | 107899   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 43.8     |
|    n_updates        | 16974    |
----------------------------------
Eval num_timesteps=108000, episode_reward=-2820.56 +/- 1221.44
Episode length: 34.92 +/- 6.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.849     |
| time/               |           |
|    total_timesteps  | 108000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.2      |
|    n_updates        | 16999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 48.5      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.849     |
| time/               |           |
|    episodes         | 1476      |
|    fps              | 308       |
|    time_elapsed     | 350       |
|    total_timesteps  | 108032    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 21.3      |
|    n_updates        | 17007     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 48.4      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.849     |
| time/               |           |
|    episodes         | 1480      |
|    fps              | 308       |
|    time_elapsed     | 350       |
|    total_timesteps  | 108190    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 17047     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 48.8      |
|    ep_rew_mean      | -3.94e+03 |
|    exploration_rate | 0.848     |
| time/               |           |
|    episodes         | 1484      |
|    fps              | 308       |
|    time_elapsed     | 351       |
|    total_timesteps  | 108400    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 17099     |
-----------------------------------
Eval num_timesteps=108500, episode_reward=-2767.39 +/- 1104.95
Episode length: 36.78 +/- 6.02
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.848     |
| time/               |           |
|    total_timesteps  | 108500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.6      |
|    n_updates        | 17124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.5      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.848     |
| time/               |           |
|    episodes         | 1488      |
|    fps              | 307       |
|    time_elapsed     | 352       |
|    total_timesteps  | 108538    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.1      |
|    n_updates        | 17134     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.9      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.847     |
| time/               |           |
|    episodes         | 1492      |
|    fps              | 308       |
|    time_elapsed     | 352       |
|    total_timesteps  | 108781    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 17195     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.3      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.846     |
| time/               |           |
|    episodes         | 1496      |
|    fps              | 308       |
|    time_elapsed     | 352       |
|    total_timesteps  | 108935    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 11.8      |
|    n_updates        | 17233     |
-----------------------------------
Eval num_timesteps=109000, episode_reward=-2823.26 +/- 1083.82
Episode length: 34.62 +/- 6.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.846     |
| time/               |           |
|    total_timesteps  | 109000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98        |
|    n_updates        | 17249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 47.3      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.846     |
| time/               |           |
|    episodes         | 1500      |
|    fps              | 308       |
|    time_elapsed     | 354       |
|    total_timesteps  | 109125    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 38.6      |
|    n_updates        | 17281     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 46.6      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.845     |
| time/               |           |
|    episodes         | 1504      |
|    fps              | 308       |
|    time_elapsed     | 354       |
|    total_timesteps  | 109278    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.3      |
|    n_updates        | 17319     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 46.7      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.844     |
| time/               |           |
|    episodes         | 1508      |
|    fps              | 308       |
|    time_elapsed     | 354       |
|    total_timesteps  | 109481    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 217       |
|    n_updates        | 17370     |
-----------------------------------
Eval num_timesteps=109500, episode_reward=-2762.38 +/- 1181.25
Episode length: 35.08 +/- 6.25
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.844     |
| time/               |           |
|    total_timesteps  | 109500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 17374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.3      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.844     |
| time/               |           |
|    episodes         | 1512      |
|    fps              | 308       |
|    time_elapsed     | 355       |
|    total_timesteps  | 109613    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 17403     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 46.1      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.843     |
| time/               |           |
|    episodes         | 1516      |
|    fps              | 308       |
|    time_elapsed     | 355       |
|    total_timesteps  | 109825    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.3      |
|    n_updates        | 17456     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 46        |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.843     |
| time/               |           |
|    episodes         | 1520      |
|    fps              | 308       |
|    time_elapsed     | 355       |
|    total_timesteps  | 109962    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28.4      |
|    n_updates        | 17490     |
-----------------------------------
Eval num_timesteps=110000, episode_reward=-2853.06 +/- 1140.63
Episode length: 35.00 +/- 6.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.843     |
| time/               |           |
|    total_timesteps  | 110000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 17499     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.8     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.842    |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 308      |
|    time_elapsed     | 357      |
|    total_timesteps  | 110094   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 88.2     |
|    n_updates        | 17523    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 46.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.842     |
| time/               |           |
|    episodes         | 1528      |
|    fps              | 308       |
|    time_elapsed     | 357       |
|    total_timesteps  | 110272    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 20.5      |
|    n_updates        | 17567     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.6      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.841     |
| time/               |           |
|    episodes         | 1532      |
|    fps              | 308       |
|    time_elapsed     | 357       |
|    total_timesteps  | 110491    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.5      |
|    n_updates        | 17622     |
-----------------------------------
Eval num_timesteps=110500, episode_reward=-2572.09 +/- 1594.20
Episode length: 34.24 +/- 8.10
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.57e+03 |
| rollout/            |           |
|    exploration_rate | 0.841     |
| time/               |           |
|    total_timesteps  | 110500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 17624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.5      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.84      |
| time/               |           |
|    episodes         | 1536      |
|    fps              | 308       |
|    time_elapsed     | 358       |
|    total_timesteps  | 110638    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 17659     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.7      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.84      |
| time/               |           |
|    episodes         | 1540      |
|    fps              | 308       |
|    time_elapsed     | 359       |
|    total_timesteps  | 110770    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.4      |
|    n_updates        | 17692     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.1      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.839     |
| time/               |           |
|    episodes         | 1544      |
|    fps              | 308       |
|    time_elapsed     | 359       |
|    total_timesteps  | 110944    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 17735     |
-----------------------------------
Eval num_timesteps=111000, episode_reward=-2904.52 +/- 1085.70
Episode length: 34.18 +/- 6.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.2     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.839    |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 96.2     |
|    n_updates        | 17749    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.3      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.839     |
| time/               |           |
|    episodes         | 1548      |
|    fps              | 308       |
|    time_elapsed     | 360       |
|    total_timesteps  | 111109    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.4      |
|    n_updates        | 17777     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.7      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.838     |
| time/               |           |
|    episodes         | 1552      |
|    fps              | 308       |
|    time_elapsed     | 360       |
|    total_timesteps  | 111303    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 26.9      |
|    n_updates        | 17825     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.3     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 308      |
|    time_elapsed     | 360      |
|    total_timesteps  | 111445   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 31.1     |
|    n_updates        | 17861    |
----------------------------------
Eval num_timesteps=111500, episode_reward=-3097.40 +/- 896.19
Episode length: 33.68 +/- 6.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.7     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.838    |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 85.2     |
|    n_updates        | 17874    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.4      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.837     |
| time/               |           |
|    episodes         | 1560      |
|    fps              | 308       |
|    time_elapsed     | 362       |
|    total_timesteps  | 111648    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 17911     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.8      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.836     |
| time/               |           |
|    episodes         | 1564      |
|    fps              | 308       |
|    time_elapsed     | 362       |
|    total_timesteps  | 111853    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 17963     |
-----------------------------------
Eval num_timesteps=112000, episode_reward=-2635.52 +/- 1247.89
Episode length: 35.26 +/- 5.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.836     |
| time/               |           |
|    total_timesteps  | 112000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71        |
|    n_updates        | 17999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.1      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.836     |
| time/               |           |
|    episodes         | 1568      |
|    fps              | 308       |
|    time_elapsed     | 363       |
|    total_timesteps  | 112027    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.2      |
|    n_updates        | 18006     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.7      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.835     |
| time/               |           |
|    episodes         | 1572      |
|    fps              | 308       |
|    time_elapsed     | 363       |
|    total_timesteps  | 112172    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 35.5      |
|    n_updates        | 18042     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43        |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.835     |
| time/               |           |
|    episodes         | 1576      |
|    fps              | 308       |
|    time_elapsed     | 363       |
|    total_timesteps  | 112337    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.9      |
|    n_updates        | 18084     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.8      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.834     |
| time/               |           |
|    episodes         | 1580      |
|    fps              | 308       |
|    time_elapsed     | 364       |
|    total_timesteps  | 112469    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.3      |
|    n_updates        | 18117     |
-----------------------------------
Eval num_timesteps=112500, episode_reward=-2974.03 +/- 1176.06
Episode length: 33.68 +/- 7.13
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.834     |
| time/               |           |
|    total_timesteps  | 112500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 12.8      |
|    n_updates        | 18124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.6      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.834     |
| time/               |           |
|    episodes         | 1584      |
|    fps              | 308       |
|    time_elapsed     | 365       |
|    total_timesteps  | 112663    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.8      |
|    n_updates        | 18165     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.4     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.833    |
| time/               |          |
|    episodes         | 1588     |
|    fps              | 308      |
|    time_elapsed     | 365      |
|    total_timesteps  | 112878   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 68.5     |
|    n_updates        | 18219    |
----------------------------------
Eval num_timesteps=113000, episode_reward=-2944.34 +/- 1162.80
Episode length: 36.20 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.832     |
| time/               |           |
|    total_timesteps  | 113000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 18249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.7      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.832     |
| time/               |           |
|    episodes         | 1592      |
|    fps              | 308       |
|    time_elapsed     | 366       |
|    total_timesteps  | 113054    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 25.5      |
|    n_updates        | 18263     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.6      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.832     |
| time/               |           |
|    episodes         | 1596      |
|    fps              | 308       |
|    time_elapsed     | 367       |
|    total_timesteps  | 113199    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.9      |
|    n_updates        | 18299     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43       |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.831    |
| time/               |          |
|    episodes         | 1600     |
|    fps              | 308      |
|    time_elapsed     | 367      |
|    total_timesteps  | 113423   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 162      |
|    n_updates        | 18355    |
----------------------------------
Eval num_timesteps=113500, episode_reward=-3050.46 +/- 1060.52
Episode length: 35.12 +/- 6.61
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -3.05e+03 |
| rollout/            |           |
|    exploration_rate | 0.831     |
| time/               |           |
|    total_timesteps  | 113500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 55.3      |
|    n_updates        | 18374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.8      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.83      |
| time/               |           |
|    episodes         | 1604      |
|    fps              | 308       |
|    time_elapsed     | 368       |
|    total_timesteps  | 113660    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.8      |
|    n_updates        | 18414     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.8     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.83     |
| time/               |          |
|    episodes         | 1608     |
|    fps              | 308      |
|    time_elapsed     | 368      |
|    total_timesteps  | 113760   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 162      |
|    n_updates        | 18439    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.1      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.829     |
| time/               |           |
|    episodes         | 1612      |
|    fps              | 308       |
|    time_elapsed     | 368       |
|    total_timesteps  | 113921    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.6      |
|    n_updates        | 18480     |
-----------------------------------
Eval num_timesteps=114000, episode_reward=-2929.40 +/- 897.31
Episode length: 35.32 +/- 7.55
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.829     |
| time/               |           |
|    total_timesteps  | 114000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 18499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.7      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.829     |
| time/               |           |
|    episodes         | 1616      |
|    fps              | 308       |
|    time_elapsed     | 370       |
|    total_timesteps  | 114093    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 18523     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.6     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.828    |
| time/               |          |
|    episodes         | 1620     |
|    fps              | 308      |
|    time_elapsed     | 370      |
|    total_timesteps  | 114222   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 87.2     |
|    n_updates        | 18555    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43        |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.828     |
| time/               |           |
|    episodes         | 1624      |
|    fps              | 308       |
|    time_elapsed     | 370       |
|    total_timesteps  | 114396    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 18598     |
-----------------------------------
Eval num_timesteps=114500, episode_reward=-3115.90 +/- 1083.00
Episode length: 34.24 +/- 6.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -3.12e+03 |
| rollout/            |           |
|    exploration_rate | 0.827     |
| time/               |           |
|    total_timesteps  | 114500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 18624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.8      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.827     |
| time/               |           |
|    episodes         | 1628      |
|    fps              | 308       |
|    time_elapsed     | 371       |
|    total_timesteps  | 114547    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 18636     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.2      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.826     |
| time/               |           |
|    episodes         | 1632      |
|    fps              | 308       |
|    time_elapsed     | 371       |
|    total_timesteps  | 114713    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.1      |
|    n_updates        | 18678     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.9      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.826     |
| time/               |           |
|    episodes         | 1636      |
|    fps              | 308       |
|    time_elapsed     | 371       |
|    total_timesteps  | 114823    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54        |
|    n_updates        | 18705     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.2      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.826     |
| time/               |           |
|    episodes         | 1640      |
|    fps              | 309       |
|    time_elapsed     | 372       |
|    total_timesteps  | 114991    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.5      |
|    n_updates        | 18747     |
-----------------------------------
Eval num_timesteps=115000, episode_reward=-3198.91 +/- 1166.29
Episode length: 32.72 +/- 6.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 32.7     |
|    mean_reward      | -3.2e+03 |
| rollout/            |          |
|    exploration_rate | 0.825    |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 162      |
|    n_updates        | 18749    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.4      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.825     |
| time/               |           |
|    episodes         | 1644      |
|    fps              | 308       |
|    time_elapsed     | 373       |
|    total_timesteps  | 115183    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.5      |
|    n_updates        | 18795     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.7      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.824     |
| time/               |           |
|    episodes         | 1648      |
|    fps              | 308       |
|    time_elapsed     | 373       |
|    total_timesteps  | 115376    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 18843     |
-----------------------------------
Eval num_timesteps=115500, episode_reward=-3054.98 +/- 1201.57
Episode length: 33.44 +/- 6.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.4      |
|    mean_reward      | -3.05e+03 |
| rollout/            |           |
|    exploration_rate | 0.824     |
| time/               |           |
|    total_timesteps  | 115500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 18874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.4      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.824     |
| time/               |           |
|    episodes         | 1652      |
|    fps              | 308       |
|    time_elapsed     | 374       |
|    total_timesteps  | 115538    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.4      |
|    n_updates        | 18884     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.4      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.823     |
| time/               |           |
|    episodes         | 1656      |
|    fps              | 308       |
|    time_elapsed     | 374       |
|    total_timesteps  | 115685    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 35.9      |
|    n_updates        | 18921     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.6      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.823     |
| time/               |           |
|    episodes         | 1660      |
|    fps              | 308       |
|    time_elapsed     | 375       |
|    total_timesteps  | 115809    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 18952     |
-----------------------------------
Eval num_timesteps=116000, episode_reward=-2524.24 +/- 1340.71
Episode length: 37.06 +/- 6.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.1      |
|    mean_reward      | -2.52e+03 |
| rollout/            |           |
|    exploration_rate | 0.822     |
| time/               |           |
|    total_timesteps  | 116000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 18999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.4      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.822     |
| time/               |           |
|    episodes         | 1664      |
|    fps              | 308       |
|    time_elapsed     | 376       |
|    total_timesteps  | 116088    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.4      |
|    n_updates        | 19021     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.7      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.821     |
| time/               |           |
|    episodes         | 1668      |
|    fps              | 308       |
|    time_elapsed     | 376       |
|    total_timesteps  | 116198    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.9      |
|    n_updates        | 19049     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.821     |
| time/               |           |
|    episodes         | 1672      |
|    fps              | 308       |
|    time_elapsed     | 376       |
|    total_timesteps  | 116350    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 19087     |
-----------------------------------
Eval num_timesteps=116500, episode_reward=-3065.45 +/- 1099.88
Episode length: 33.96 +/- 7.15
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -3.07e+03 |
| rollout/            |           |
|    exploration_rate | 0.82      |
| time/               |           |
|    total_timesteps  | 116500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 219       |
|    n_updates        | 19124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.8      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.82      |
| time/               |           |
|    episodes         | 1676      |
|    fps              | 308       |
|    time_elapsed     | 378       |
|    total_timesteps  | 116513    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 19128     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.5      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.819     |
| time/               |           |
|    episodes         | 1680      |
|    fps              | 308       |
|    time_elapsed     | 378       |
|    total_timesteps  | 116716    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.2      |
|    n_updates        | 19178     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.9      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.819     |
| time/               |           |
|    episodes         | 1684      |
|    fps              | 309       |
|    time_elapsed     | 378       |
|    total_timesteps  | 116952    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 19237     |
-----------------------------------
Eval num_timesteps=117000, episode_reward=-3104.43 +/- 845.98
Episode length: 33.72 +/- 7.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.7     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.818    |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 247      |
|    n_updates        | 19249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.3      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.818     |
| time/               |           |
|    episodes         | 1688      |
|    fps              | 308       |
|    time_elapsed     | 379       |
|    total_timesteps  | 117111    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62.8      |
|    n_updates        | 19277     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.2      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.817     |
| time/               |           |
|    episodes         | 1692      |
|    fps              | 308       |
|    time_elapsed     | 379       |
|    total_timesteps  | 117275    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 19318     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.3      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.817     |
| time/               |           |
|    episodes         | 1696      |
|    fps              | 308       |
|    time_elapsed     | 380       |
|    total_timesteps  | 117429    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.2      |
|    n_updates        | 19357     |
-----------------------------------
Eval num_timesteps=117500, episode_reward=-2850.26 +/- 1019.46
Episode length: 34.70 +/- 6.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.817     |
| time/               |           |
|    total_timesteps  | 117500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 38.8      |
|    n_updates        | 19374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.7      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.816     |
| time/               |           |
|    episodes         | 1700      |
|    fps              | 308       |
|    time_elapsed     | 381       |
|    total_timesteps  | 117593    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 19398     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.2      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.815     |
| time/               |           |
|    episodes         | 1704      |
|    fps              | 308       |
|    time_elapsed     | 381       |
|    total_timesteps  | 117881    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 19470     |
-----------------------------------
Eval num_timesteps=118000, episode_reward=-2762.46 +/- 1030.68
Episode length: 35.48 +/- 6.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.815     |
| time/               |           |
|    total_timesteps  | 118000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 19499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.5      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.815     |
| time/               |           |
|    episodes         | 1708      |
|    fps              | 308       |
|    time_elapsed     | 383       |
|    total_timesteps  | 118113    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.9      |
|    n_updates        | 19528     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.9      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.814     |
| time/               |           |
|    episodes         | 1712      |
|    fps              | 308       |
|    time_elapsed     | 383       |
|    total_timesteps  | 118307    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 49.3      |
|    n_updates        | 19576     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.9      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.813     |
| time/               |           |
|    episodes         | 1716      |
|    fps              | 309       |
|    time_elapsed     | 383       |
|    total_timesteps  | 118480    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.6      |
|    n_updates        | 19619     |
-----------------------------------
Eval num_timesteps=118500, episode_reward=-2796.91 +/- 1075.24
Episode length: 34.56 +/- 6.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.6     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.813    |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 146      |
|    n_updates        | 19624    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.6      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.813     |
| time/               |           |
|    episodes         | 1720      |
|    fps              | 308       |
|    time_elapsed     | 384       |
|    total_timesteps  | 118587    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.4      |
|    n_updates        | 19646     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.1      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.812     |
| time/               |           |
|    episodes         | 1724      |
|    fps              | 308       |
|    time_elapsed     | 384       |
|    total_timesteps  | 118706    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 19676     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.9      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.812     |
| time/               |           |
|    episodes         | 1728      |
|    fps              | 308       |
|    time_elapsed     | 384       |
|    total_timesteps  | 118832    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 262       |
|    n_updates        | 19707     |
-----------------------------------
Eval num_timesteps=119000, episode_reward=-2725.91 +/- 1183.44
Episode length: 35.50 +/- 7.77
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.811     |
| time/               |           |
|    total_timesteps  | 119000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84        |
|    n_updates        | 19749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43        |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.811     |
| time/               |           |
|    episodes         | 1732      |
|    fps              | 308       |
|    time_elapsed     | 386       |
|    total_timesteps  | 119014    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.7      |
|    n_updates        | 19753     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.3      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.811     |
| time/               |           |
|    episodes         | 1736      |
|    fps              | 308       |
|    time_elapsed     | 386       |
|    total_timesteps  | 119157    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.1      |
|    n_updates        | 19789     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.4      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.81      |
| time/               |           |
|    episodes         | 1740      |
|    fps              | 308       |
|    time_elapsed     | 386       |
|    total_timesteps  | 119327    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 19831     |
-----------------------------------
Eval num_timesteps=119500, episode_reward=-2858.76 +/- 1086.49
Episode length: 35.82 +/- 6.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.81      |
| time/               |           |
|    total_timesteps  | 119500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 19874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.5      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.809     |
| time/               |           |
|    episodes         | 1744      |
|    fps              | 308       |
|    time_elapsed     | 387       |
|    total_timesteps  | 119529    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.7      |
|    n_updates        | 19882     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.9      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.809     |
| time/               |           |
|    episodes         | 1748      |
|    fps              | 308       |
|    time_elapsed     | 387       |
|    total_timesteps  | 119665    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 19916     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.6      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.808     |
| time/               |           |
|    episodes         | 1752      |
|    fps              | 308       |
|    time_elapsed     | 388       |
|    total_timesteps  | 119903    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 19975     |
-----------------------------------
Eval num_timesteps=120000, episode_reward=-2808.84 +/- 1228.97
Episode length: 33.50 +/- 7.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.5      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.808     |
| time/               |           |
|    total_timesteps  | 120000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 19999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.6      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.808     |
| time/               |           |
|    episodes         | 1756      |
|    fps              | 308       |
|    time_elapsed     | 389       |
|    total_timesteps  | 120050    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 20012     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.5      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.807     |
| time/               |           |
|    episodes         | 1760      |
|    fps              | 308       |
|    time_elapsed     | 389       |
|    total_timesteps  | 120257    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 20064     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.3      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.806     |
| time/               |           |
|    episodes         | 1764      |
|    fps              | 309       |
|    time_elapsed     | 389       |
|    total_timesteps  | 120422    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82        |
|    n_updates        | 20105     |
-----------------------------------
Eval num_timesteps=120500, episode_reward=-2964.36 +/- 1077.89
Episode length: 33.78 +/- 7.33
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.806     |
| time/               |           |
|    total_timesteps  | 120500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 20124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44        |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.806     |
| time/               |           |
|    episodes         | 1768      |
|    fps              | 308       |
|    time_elapsed     | 390       |
|    total_timesteps  | 120594    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 20148     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.7      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.805     |
| time/               |           |
|    episodes         | 1772      |
|    fps              | 308       |
|    time_elapsed     | 391       |
|    total_timesteps  | 120720    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.3      |
|    n_updates        | 20179     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.5     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.805    |
| time/               |          |
|    episodes         | 1776     |
|    fps              | 308      |
|    time_elapsed     | 391      |
|    total_timesteps  | 120864   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 91       |
|    n_updates        | 20215    |
----------------------------------
Eval num_timesteps=121000, episode_reward=-3064.64 +/- 1139.85
Episode length: 34.44 +/- 5.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -3.06e+03 |
| rollout/            |           |
|    exploration_rate | 0.804     |
| time/               |           |
|    total_timesteps  | 121000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.7      |
|    n_updates        | 20249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.8      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.804     |
| time/               |           |
|    episodes         | 1780      |
|    fps              | 308       |
|    time_elapsed     | 392       |
|    total_timesteps  | 121000    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.1      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.804     |
| time/               |           |
|    episodes         | 1784      |
|    fps              | 308       |
|    time_elapsed     | 392       |
|    total_timesteps  | 121163    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 20290     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.6      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.803     |
| time/               |           |
|    episodes         | 1788      |
|    fps              | 308       |
|    time_elapsed     | 392       |
|    total_timesteps  | 121368    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84        |
|    n_updates        | 20341     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.2      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.802     |
| time/               |           |
|    episodes         | 1792      |
|    fps              | 309       |
|    time_elapsed     | 392       |
|    total_timesteps  | 121496    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.3      |
|    n_updates        | 20373     |
-----------------------------------
Eval num_timesteps=121500, episode_reward=-2944.63 +/- 1088.72
Episode length: 33.20 +/- 7.11
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.2      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.802     |
| time/               |           |
|    total_timesteps  | 121500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 13.4      |
|    n_updates        | 20374     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.4     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.802    |
| time/               |          |
|    episodes         | 1796     |
|    fps              | 308      |
|    time_elapsed     | 394      |
|    total_timesteps  | 121671   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 178      |
|    n_updates        | 20417    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.7      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.801     |
| time/               |           |
|    episodes         | 1800      |
|    fps              | 309       |
|    time_elapsed     | 394       |
|    total_timesteps  | 121862    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 20465     |
-----------------------------------
Eval num_timesteps=122000, episode_reward=-3125.05 +/- 1011.12
Episode length: 35.26 +/- 6.30
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -3.13e+03 |
| rollout/            |           |
|    exploration_rate | 0.8       |
| time/               |           |
|    total_timesteps  | 122000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 32.8      |
|    n_updates        | 20499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.3      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.8       |
| time/               |           |
|    episodes         | 1804      |
|    fps              | 308       |
|    time_elapsed     | 395       |
|    total_timesteps  | 122011    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 18.6      |
|    n_updates        | 20502     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.3      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.8       |
| time/               |           |
|    episodes         | 1808      |
|    fps              | 308       |
|    time_elapsed     | 395       |
|    total_timesteps  | 122245    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 20561     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.8      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.799     |
| time/               |           |
|    episodes         | 1812      |
|    fps              | 309       |
|    time_elapsed     | 396       |
|    total_timesteps  | 122386    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.1      |
|    n_updates        | 20596     |
-----------------------------------
Eval num_timesteps=122500, episode_reward=-2849.42 +/- 1137.81
Episode length: 35.90 +/- 6.90
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.799     |
| time/               |           |
|    total_timesteps  | 122500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 20624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.6     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.798    |
| time/               |          |
|    episodes         | 1816     |
|    fps              | 308      |
|    time_elapsed     | 397      |
|    total_timesteps  | 122545   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 207      |
|    n_updates        | 20636    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41        |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.798     |
| time/               |           |
|    episodes         | 1820      |
|    fps              | 308       |
|    time_elapsed     | 397       |
|    total_timesteps  | 122684    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.2      |
|    n_updates        | 20670     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.8      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.797     |
| time/               |           |
|    episodes         | 1824      |
|    fps              | 309       |
|    time_elapsed     | 397       |
|    total_timesteps  | 122986    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.8      |
|    n_updates        | 20746     |
-----------------------------------
Eval num_timesteps=123000, episode_reward=-2898.75 +/- 1026.52
Episode length: 36.24 +/- 6.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.2     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.797    |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 109      |
|    n_updates        | 20749    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.2      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.796     |
| time/               |           |
|    episodes         | 1828      |
|    fps              | 308       |
|    time_elapsed     | 399       |
|    total_timesteps  | 123153    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.5      |
|    n_updates        | 20788     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.4      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.796     |
| time/               |           |
|    episodes         | 1832      |
|    fps              | 308       |
|    time_elapsed     | 399       |
|    total_timesteps  | 123352    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.3      |
|    n_updates        | 20837     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.1      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.795     |
| time/               |           |
|    episodes         | 1836      |
|    fps              | 309       |
|    time_elapsed     | 399       |
|    total_timesteps  | 123468    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 20866     |
-----------------------------------
Eval num_timesteps=123500, episode_reward=-2606.97 +/- 1233.10
Episode length: 36.70 +/- 5.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.7      |
|    mean_reward      | -2.61e+03 |
| rollout/            |           |
|    exploration_rate | 0.795     |
| time/               |           |
|    total_timesteps  | 123500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 20874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.6      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.794     |
| time/               |           |
|    episodes         | 1840      |
|    fps              | 308       |
|    time_elapsed     | 400       |
|    total_timesteps  | 123683    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 20920     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43        |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.794     |
| time/               |           |
|    episodes         | 1844      |
|    fps              | 308       |
|    time_elapsed     | 400       |
|    total_timesteps  | 123824    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 20955     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.3      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.793     |
| time/               |           |
|    episodes         | 1848      |
|    fps              | 309       |
|    time_elapsed     | 400       |
|    total_timesteps  | 123998    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 20999     |
-----------------------------------
Eval num_timesteps=124000, episode_reward=-2641.23 +/- 1315.89
Episode length: 35.70 +/- 7.14
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.793     |
| time/               |           |
|    total_timesteps  | 124000    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.8      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.792     |
| time/               |           |
|    episodes         | 1852      |
|    fps              | 308       |
|    time_elapsed     | 402       |
|    total_timesteps  | 124184    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 34        |
|    n_updates        | 21045     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.7      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.792     |
| time/               |           |
|    episodes         | 1856      |
|    fps              | 309       |
|    time_elapsed     | 402       |
|    total_timesteps  | 124419    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 21104     |
-----------------------------------
Eval num_timesteps=124500, episode_reward=-2859.82 +/- 1029.57
Episode length: 35.54 +/- 5.72
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.791     |
| time/               |           |
|    total_timesteps  | 124500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.4      |
|    n_updates        | 21124     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.1     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.791    |
| time/               |          |
|    episodes         | 1860     |
|    fps              | 308      |
|    time_elapsed     | 403      |
|    total_timesteps  | 124570   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 327      |
|    n_updates        | 21142    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.6      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.791     |
| time/               |           |
|    episodes         | 1864      |
|    fps              | 308       |
|    time_elapsed     | 403       |
|    total_timesteps  | 124683    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 21170     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.3      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.79      |
| time/               |           |
|    episodes         | 1868      |
|    fps              | 308       |
|    time_elapsed     | 404       |
|    total_timesteps  | 124822    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.6      |
|    n_updates        | 21205     |
-----------------------------------
Eval num_timesteps=125000, episode_reward=-3106.48 +/- 894.80
Episode length: 35.14 +/- 6.79
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.789     |
| time/               |           |
|    total_timesteps  | 125000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 21249     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43       |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.789    |
| time/               |          |
|    episodes         | 1872     |
|    fps              | 308      |
|    time_elapsed     | 405      |
|    total_timesteps  | 125022   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 86.6     |
|    n_updates        | 21255    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.8      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.789     |
| time/               |           |
|    episodes         | 1876      |
|    fps              | 308       |
|    time_elapsed     | 405       |
|    total_timesteps  | 125246    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 174       |
|    n_updates        | 21311     |
-----------------------------------
Eval num_timesteps=125500, episode_reward=-2954.14 +/- 1104.19
Episode length: 34.24 +/- 6.88
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.788     |
| time/               |           |
|    total_timesteps  | 125500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 21374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.2      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.787     |
| time/               |           |
|    episodes         | 1880      |
|    fps              | 308       |
|    time_elapsed     | 407       |
|    total_timesteps  | 125525    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 21381     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.4      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.787     |
| time/               |           |
|    episodes         | 1884      |
|    fps              | 308       |
|    time_elapsed     | 407       |
|    total_timesteps  | 125702    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 21425     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.9     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 1888     |
|    fps              | 308      |
|    time_elapsed     | 407      |
|    total_timesteps  | 125857   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 100      |
|    n_updates        | 21464    |
----------------------------------
Eval num_timesteps=126000, episode_reward=-3062.10 +/- 1092.71
Episode length: 33.96 +/- 6.88
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -3.06e+03 |
| rollout/            |           |
|    exploration_rate | 0.786     |
| time/               |           |
|    total_timesteps  | 126000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 21499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.2      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.786     |
| time/               |           |
|    episodes         | 1892      |
|    fps              | 308       |
|    time_elapsed     | 408       |
|    total_timesteps  | 126017    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.9      |
|    n_updates        | 21504     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.8     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.785    |
| time/               |          |
|    episodes         | 1896     |
|    fps              | 308      |
|    time_elapsed     | 408      |
|    total_timesteps  | 126154   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 192      |
|    n_updates        | 21538    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.5      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.785     |
| time/               |           |
|    episodes         | 1900      |
|    fps              | 308       |
|    time_elapsed     | 408       |
|    total_timesteps  | 126311    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 21577     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.1      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.784     |
| time/               |           |
|    episodes         | 1904      |
|    fps              | 309       |
|    time_elapsed     | 408       |
|    total_timesteps  | 126426    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 21606     |
-----------------------------------
Eval num_timesteps=126500, episode_reward=-2982.50 +/- 1139.15
Episode length: 34.24 +/- 6.67
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.784     |
| time/               |           |
|    total_timesteps  | 126500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.7      |
|    n_updates        | 21624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45        |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.783     |
| time/               |           |
|    episodes         | 1908      |
|    fps              | 308       |
|    time_elapsed     | 410       |
|    total_timesteps  | 126742    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.6      |
|    n_updates        | 21685     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.4      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.782     |
| time/               |           |
|    episodes         | 1912      |
|    fps              | 309       |
|    time_elapsed     | 410       |
|    total_timesteps  | 126930    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 21732     |
-----------------------------------
Eval num_timesteps=127000, episode_reward=-2859.23 +/- 1191.12
Episode length: 35.48 +/- 6.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.782     |
| time/               |           |
|    total_timesteps  | 127000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.7      |
|    n_updates        | 21749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.2      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.782     |
| time/               |           |
|    episodes         | 1916      |
|    fps              | 308       |
|    time_elapsed     | 411       |
|    total_timesteps  | 127069    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.4      |
|    n_updates        | 21767     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 45.4      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.781     |
| time/               |           |
|    episodes         | 1920      |
|    fps              | 308       |
|    time_elapsed     | 412       |
|    total_timesteps  | 127222    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 29.2      |
|    n_updates        | 21805     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.5      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.78      |
| time/               |           |
|    episodes         | 1924      |
|    fps              | 309       |
|    time_elapsed     | 412       |
|    total_timesteps  | 127436    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.7      |
|    n_updates        | 21858     |
-----------------------------------
Eval num_timesteps=127500, episode_reward=-3157.11 +/- 837.94
Episode length: 34.06 +/- 6.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -3.16e+03 |
| rollout/            |           |
|    exploration_rate | 0.78      |
| time/               |           |
|    total_timesteps  | 127500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.9      |
|    n_updates        | 21874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.2      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.78      |
| time/               |           |
|    episodes         | 1928      |
|    fps              | 308       |
|    time_elapsed     | 413       |
|    total_timesteps  | 127570    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.6      |
|    n_updates        | 21892     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.3      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.779     |
| time/               |           |
|    episodes         | 1932      |
|    fps              | 308       |
|    time_elapsed     | 413       |
|    total_timesteps  | 127783    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 21945     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 44.4      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.779     |
| time/               |           |
|    episodes         | 1936      |
|    fps              | 309       |
|    time_elapsed     | 413       |
|    total_timesteps  | 127906    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.5      |
|    n_updates        | 21976     |
-----------------------------------
Eval num_timesteps=128000, episode_reward=-3218.31 +/- 782.84
Episode length: 35.68 +/- 7.30
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -3.22e+03 |
| rollout/            |           |
|    exploration_rate | 0.778     |
| time/               |           |
|    total_timesteps  | 128000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 21999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.4      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.778     |
| time/               |           |
|    episodes         | 1940      |
|    fps              | 308       |
|    time_elapsed     | 415       |
|    total_timesteps  | 128026    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68        |
|    n_updates        | 22006     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43.6      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.777     |
| time/               |           |
|    episodes         | 1944      |
|    fps              | 308       |
|    time_elapsed     | 415       |
|    total_timesteps  | 128187    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 22046     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 43        |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.777     |
| time/               |           |
|    episodes         | 1948      |
|    fps              | 308       |
|    time_elapsed     | 415       |
|    total_timesteps  | 128300    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.3      |
|    n_updates        | 22074     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.5      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.777     |
| time/               |           |
|    episodes         | 1952      |
|    fps              | 309       |
|    time_elapsed     | 415       |
|    total_timesteps  | 128438    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53.3      |
|    n_updates        | 22109     |
-----------------------------------
Eval num_timesteps=128500, episode_reward=-2841.81 +/- 1090.50
Episode length: 35.92 +/- 6.93
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.776     |
| time/               |           |
|    total_timesteps  | 128500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 22124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.3      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.776     |
| time/               |           |
|    episodes         | 1956      |
|    fps              | 308       |
|    time_elapsed     | 416       |
|    total_timesteps  | 128553    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52        |
|    n_updates        | 22138     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.9      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.776     |
| time/               |           |
|    episodes         | 1960      |
|    fps              | 308       |
|    time_elapsed     | 416       |
|    total_timesteps  | 128661    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 220       |
|    n_updates        | 22165     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.1      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.775     |
| time/               |           |
|    episodes         | 1964      |
|    fps              | 308       |
|    time_elapsed     | 417       |
|    total_timesteps  | 128790    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.5      |
|    n_updates        | 22197     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.775     |
| time/               |           |
|    episodes         | 1968      |
|    fps              | 309       |
|    time_elapsed     | 417       |
|    total_timesteps  | 128924    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.7      |
|    n_updates        | 22230     |
-----------------------------------
Eval num_timesteps=129000, episode_reward=-2753.59 +/- 1353.86
Episode length: 35.52 +/- 6.44
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.774     |
| time/               |           |
|    total_timesteps  | 129000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 22249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.5      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.774     |
| time/               |           |
|    episodes         | 1972      |
|    fps              | 308       |
|    time_elapsed     | 418       |
|    total_timesteps  | 129076    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 22268     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.8      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.774     |
| time/               |           |
|    episodes         | 1976      |
|    fps              | 308       |
|    time_elapsed     | 418       |
|    total_timesteps  | 129221    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.8      |
|    n_updates        | 22305     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.4      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.773     |
| time/               |           |
|    episodes         | 1980      |
|    fps              | 308       |
|    time_elapsed     | 418       |
|    total_timesteps  | 129368    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 207       |
|    n_updates        | 22341     |
-----------------------------------
Eval num_timesteps=129500, episode_reward=-3146.46 +/- 1013.81
Episode length: 36.52 +/- 6.76
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -3.15e+03 |
| rollout/            |           |
|    exploration_rate | 0.773     |
| time/               |           |
|    total_timesteps  | 129500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.2      |
|    n_updates        | 22374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.4      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.772     |
| time/               |           |
|    episodes         | 1984      |
|    fps              | 308       |
|    time_elapsed     | 420       |
|    total_timesteps  | 129541    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 22385     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.2     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.772    |
| time/               |          |
|    episodes         | 1988     |
|    fps              | 308      |
|    time_elapsed     | 420      |
|    total_timesteps  | 129681   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 158      |
|    n_updates        | 22420    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.5      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.771     |
| time/               |           |
|    episodes         | 1992      |
|    fps              | 308       |
|    time_elapsed     | 420       |
|    total_timesteps  | 129871    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 22467     |
-----------------------------------
Eval num_timesteps=130000, episode_reward=-2753.97 +/- 1156.96
Episode length: 36.24 +/- 6.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.771     |
| time/               |           |
|    total_timesteps  | 130000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.9      |
|    n_updates        | 22499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.7      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.771     |
| time/               |           |
|    episodes         | 1996      |
|    fps              | 308       |
|    time_elapsed     | 421       |
|    total_timesteps  | 130020    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57        |
|    n_updates        | 22504     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.9      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.77      |
| time/               |           |
|    episodes         | 2000      |
|    fps              | 308       |
|    time_elapsed     | 421       |
|    total_timesteps  | 130203    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 22550     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.2      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.769     |
| time/               |           |
|    episodes         | 2004      |
|    fps              | 308       |
|    time_elapsed     | 422       |
|    total_timesteps  | 130347    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53.8      |
|    n_updates        | 22586     |
-----------------------------------
Eval num_timesteps=130500, episode_reward=-2666.43 +/- 1283.08
Episode length: 35.68 +/- 7.30
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.769     |
| time/               |           |
|    total_timesteps  | 130500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 22624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.8      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.769     |
| time/               |           |
|    episodes         | 2008      |
|    fps              | 308       |
|    time_elapsed     | 423       |
|    total_timesteps  | 130519    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 22629     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.1      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.768     |
| time/               |           |
|    episodes         | 2012      |
|    fps              | 308       |
|    time_elapsed     | 423       |
|    total_timesteps  | 130637    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 22659     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.768     |
| time/               |           |
|    episodes         | 2016      |
|    fps              | 308       |
|    time_elapsed     | 423       |
|    total_timesteps  | 130755    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.9      |
|    n_updates        | 22688     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.2      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.767     |
| time/               |           |
|    episodes         | 2020      |
|    fps              | 309       |
|    time_elapsed     | 423       |
|    total_timesteps  | 130945    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.7      |
|    n_updates        | 22736     |
-----------------------------------
Eval num_timesteps=131000, episode_reward=-2610.66 +/- 1106.42
Episode length: 36.18 +/- 6.10
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.61e+03 |
| rollout/            |           |
|    exploration_rate | 0.767     |
| time/               |           |
|    total_timesteps  | 131000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 30.9      |
|    n_updates        | 22749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.766     |
| time/               |           |
|    episodes         | 2024      |
|    fps              | 308       |
|    time_elapsed     | 425       |
|    total_timesteps  | 131074    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 213       |
|    n_updates        | 22768     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.3      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.766     |
| time/               |           |
|    episodes         | 2028      |
|    fps              | 308       |
|    time_elapsed     | 425       |
|    total_timesteps  | 131296    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 22823     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.765     |
| time/               |           |
|    episodes         | 2032      |
|    fps              | 308       |
|    time_elapsed     | 425       |
|    total_timesteps  | 131435    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 22858     |
-----------------------------------
Eval num_timesteps=131500, episode_reward=-3057.53 +/- 1020.10
Episode length: 35.12 +/- 5.51
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -3.06e+03 |
| rollout/            |           |
|    exploration_rate | 0.765     |
| time/               |           |
|    total_timesteps  | 131500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 22874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.764     |
| time/               |           |
|    episodes         | 2036      |
|    fps              | 308       |
|    time_elapsed     | 426       |
|    total_timesteps  | 131607    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 22901     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.1      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.764     |
| time/               |           |
|    episodes         | 2040      |
|    fps              | 308       |
|    time_elapsed     | 426       |
|    total_timesteps  | 131735    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 22933     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.763     |
| time/               |           |
|    episodes         | 2044      |
|    fps              | 308       |
|    time_elapsed     | 426       |
|    total_timesteps  | 131863    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.9      |
|    n_updates        | 22965     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.763     |
| time/               |           |
|    episodes         | 2048      |
|    fps              | 309       |
|    time_elapsed     | 427       |
|    total_timesteps  | 131980    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.8      |
|    n_updates        | 22994     |
-----------------------------------
Eval num_timesteps=132000, episode_reward=-2730.12 +/- 975.34
Episode length: 34.86 +/- 6.55
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.763     |
| time/               |           |
|    total_timesteps  | 132000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.6      |
|    n_updates        | 22999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.762     |
| time/               |           |
|    episodes         | 2052      |
|    fps              | 308       |
|    time_elapsed     | 428       |
|    total_timesteps  | 132107    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 290       |
|    n_updates        | 23026     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.762     |
| time/               |           |
|    episodes         | 2056      |
|    fps              | 308       |
|    time_elapsed     | 428       |
|    total_timesteps  | 132240    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 23059     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.4      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.761     |
| time/               |           |
|    episodes         | 2060      |
|    fps              | 308       |
|    time_elapsed     | 428       |
|    total_timesteps  | 132398    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.4      |
|    n_updates        | 23099     |
-----------------------------------
Eval num_timesteps=132500, episode_reward=-2827.03 +/- 1006.85
Episode length: 35.40 +/- 6.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.761     |
| time/               |           |
|    total_timesteps  | 132500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 250       |
|    n_updates        | 23124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.6      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.761     |
| time/               |           |
|    episodes         | 2064      |
|    fps              | 308       |
|    time_elapsed     | 429       |
|    total_timesteps  | 132549    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.1      |
|    n_updates        | 23137     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.2      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.76      |
| time/               |           |
|    episodes         | 2068      |
|    fps              | 308       |
|    time_elapsed     | 430       |
|    total_timesteps  | 132748    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 199       |
|    n_updates        | 23186     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.759     |
| time/               |           |
|    episodes         | 2072      |
|    fps              | 308       |
|    time_elapsed     | 430       |
|    total_timesteps  | 132879    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.5      |
|    n_updates        | 23219     |
-----------------------------------
Eval num_timesteps=133000, episode_reward=-3026.29 +/- 1005.78
Episode length: 36.88 +/- 8.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.9      |
|    mean_reward      | -3.03e+03 |
| rollout/            |           |
|    exploration_rate | 0.759     |
| time/               |           |
|    total_timesteps  | 133000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 23249     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.2     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.759    |
| time/               |          |
|    episodes         | 2076     |
|    fps              | 308      |
|    time_elapsed     | 431      |
|    total_timesteps  | 133042   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 43.4     |
|    n_updates        | 23260    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.3     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.758    |
| time/               |          |
|    episodes         | 2080     |
|    fps              | 308      |
|    time_elapsed     | 431      |
|    total_timesteps  | 133197   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 165      |
|    n_updates        | 23299    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.9      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.757     |
| time/               |           |
|    episodes         | 2084      |
|    fps              | 308       |
|    time_elapsed     | 431       |
|    total_timesteps  | 133429    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.6      |
|    n_updates        | 23357     |
-----------------------------------
Eval num_timesteps=133500, episode_reward=-2926.08 +/- 992.15
Episode length: 34.70 +/- 6.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.757     |
| time/               |           |
|    total_timesteps  | 133500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.4      |
|    n_updates        | 23374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.9      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.757     |
| time/               |           |
|    episodes         | 2088      |
|    fps              | 308       |
|    time_elapsed     | 433       |
|    total_timesteps  | 133570    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71        |
|    n_updates        | 23392     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.9     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.756    |
| time/               |          |
|    episodes         | 2092     |
|    fps              | 308      |
|    time_elapsed     | 433      |
|    total_timesteps  | 133763   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 118      |
|    n_updates        | 23440    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.3      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.755     |
| time/               |           |
|    episodes         | 2096      |
|    fps              | 308       |
|    time_elapsed     | 433       |
|    total_timesteps  | 133948    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 23486     |
-----------------------------------
Eval num_timesteps=134000, episode_reward=-2525.04 +/- 1435.06
Episode length: 35.46 +/- 7.06
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.53e+03 |
| rollout/            |           |
|    exploration_rate | 0.755     |
| time/               |           |
|    total_timesteps  | 134000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.9      |
|    n_updates        | 23499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39        |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.755     |
| time/               |           |
|    episodes         | 2100      |
|    fps              | 308       |
|    time_elapsed     | 434       |
|    total_timesteps  | 134103    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.4      |
|    n_updates        | 23525     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.7      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.754     |
| time/               |           |
|    episodes         | 2104      |
|    fps              | 308       |
|    time_elapsed     | 435       |
|    total_timesteps  | 134318    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 23579     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.4      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.753     |
| time/               |           |
|    episodes         | 2108      |
|    fps              | 308       |
|    time_elapsed     | 435       |
|    total_timesteps  | 134458    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.8      |
|    n_updates        | 23614     |
-----------------------------------
Eval num_timesteps=134500, episode_reward=-2975.28 +/- 1112.29
Episode length: 34.76 +/- 7.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.753     |
| time/               |           |
|    total_timesteps  | 134500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 23624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.7      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.753     |
| time/               |           |
|    episodes         | 2112      |
|    fps              | 308       |
|    time_elapsed     | 436       |
|    total_timesteps  | 134610    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 23652     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.5      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.752     |
| time/               |           |
|    episodes         | 2116      |
|    fps              | 308       |
|    time_elapsed     | 436       |
|    total_timesteps  | 134805    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37.2      |
|    n_updates        | 23701     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.2      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.751     |
| time/               |           |
|    episodes         | 2120      |
|    fps              | 308       |
|    time_elapsed     | 436       |
|    total_timesteps  | 134967    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96        |
|    n_updates        | 23741     |
-----------------------------------
Eval num_timesteps=135000, episode_reward=-2871.35 +/- 1227.99
Episode length: 34.84 +/- 6.80
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.751     |
| time/               |           |
|    total_timesteps  | 135000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.1      |
|    n_updates        | 23749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.1      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.751     |
| time/               |           |
|    episodes         | 2124      |
|    fps              | 308       |
|    time_elapsed     | 438       |
|    total_timesteps  | 135083    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 23770     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.4      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.75      |
| time/               |           |
|    episodes         | 2128      |
|    fps              | 308       |
|    time_elapsed     | 438       |
|    total_timesteps  | 135234    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 23808     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.4     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.75     |
| time/               |          |
|    episodes         | 2132     |
|    fps              | 308      |
|    time_elapsed     | 438      |
|    total_timesteps  | 135370   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 102      |
|    n_updates        | 23842    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.9      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.749     |
| time/               |           |
|    episodes         | 2136      |
|    fps              | 308       |
|    time_elapsed     | 438       |
|    total_timesteps  | 135495    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53.5      |
|    n_updates        | 23873     |
-----------------------------------
Eval num_timesteps=135500, episode_reward=-3223.00 +/- 973.58
Episode length: 35.30 +/- 7.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -3.22e+03 |
| rollout/            |           |
|    exploration_rate | 0.749     |
| time/               |           |
|    total_timesteps  | 135500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 36.1      |
|    n_updates        | 23874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39        |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.749     |
| time/               |           |
|    episodes         | 2140      |
|    fps              | 308       |
|    time_elapsed     | 439       |
|    total_timesteps  | 135632    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 36.2      |
|    n_updates        | 23907     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.9      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.748     |
| time/               |           |
|    episodes         | 2144      |
|    fps              | 308       |
|    time_elapsed     | 439       |
|    total_timesteps  | 135749    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 23937     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.4      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.748     |
| time/               |           |
|    episodes         | 2148      |
|    fps              | 308       |
|    time_elapsed     | 440       |
|    total_timesteps  | 135918    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 23979     |
-----------------------------------
Eval num_timesteps=136000, episode_reward=-2839.51 +/- 1293.93
Episode length: 34.92 +/- 6.44
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.747     |
| time/               |           |
|    total_timesteps  | 136000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.4      |
|    n_updates        | 23999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.6      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.747     |
| time/               |           |
|    episodes         | 2152      |
|    fps              | 308       |
|    time_elapsed     | 441       |
|    total_timesteps  | 136068    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 24016     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.7      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.746     |
| time/               |           |
|    episodes         | 2156      |
|    fps              | 308       |
|    time_elapsed     | 441       |
|    total_timesteps  | 136212    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 200       |
|    n_updates        | 24052     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.1      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.746     |
| time/               |           |
|    episodes         | 2160      |
|    fps              | 308       |
|    time_elapsed     | 441       |
|    total_timesteps  | 136406    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.1      |
|    n_updates        | 24101     |
-----------------------------------
Eval num_timesteps=136500, episode_reward=-2790.19 +/- 1150.85
Episode length: 35.96 +/- 6.61
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.745     |
| time/               |           |
|    total_timesteps  | 136500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 24124     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.5     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.745    |
| time/               |          |
|    episodes         | 2164     |
|    fps              | 308      |
|    time_elapsed     | 443      |
|    total_timesteps  | 136604   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 110      |
|    n_updates        | 24150    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.9      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.744     |
| time/               |           |
|    episodes         | 2168      |
|    fps              | 308       |
|    time_elapsed     | 443       |
|    total_timesteps  | 136838    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 24209     |
-----------------------------------
Eval num_timesteps=137000, episode_reward=-3029.80 +/- 984.24
Episode length: 37.60 +/- 7.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.6      |
|    mean_reward      | -3.03e+03 |
| rollout/            |           |
|    exploration_rate | 0.743     |
| time/               |           |
|    total_timesteps  | 137000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 24249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.7      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.743     |
| time/               |           |
|    episodes         | 2172      |
|    fps              | 308       |
|    time_elapsed     | 444       |
|    total_timesteps  | 137047    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 24261     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 42.2      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.742     |
| time/               |           |
|    episodes         | 2176      |
|    fps              | 308       |
|    time_elapsed     | 445       |
|    total_timesteps  | 137266    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98        |
|    n_updates        | 24316     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.742     |
| time/               |           |
|    episodes         | 2180      |
|    fps              | 308       |
|    time_elapsed     | 445       |
|    total_timesteps  | 137387    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.9      |
|    n_updates        | 24346     |
-----------------------------------
Eval num_timesteps=137500, episode_reward=-2875.10 +/- 1100.82
Episode length: 34.14 +/- 7.42
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.741     |
| time/               |           |
|    total_timesteps  | 137500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 24374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.3      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.741     |
| time/               |           |
|    episodes         | 2184      |
|    fps              | 308       |
|    time_elapsed     | 446       |
|    total_timesteps  | 137561    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 24390     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.4      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.741     |
| time/               |           |
|    episodes         | 2188      |
|    fps              | 308       |
|    time_elapsed     | 446       |
|    total_timesteps  | 137707    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 24426     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.6      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.74      |
| time/               |           |
|    episodes         | 2192      |
|    fps              | 308       |
|    time_elapsed     | 446       |
|    total_timesteps  | 137826    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87        |
|    n_updates        | 24456     |
-----------------------------------
Eval num_timesteps=138000, episode_reward=-2897.34 +/- 1120.99
Episode length: 36.48 +/- 6.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.5     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.739    |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 67.9     |
|    n_updates        | 24499    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.6      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.739     |
| time/               |           |
|    episodes         | 2196      |
|    fps              | 308       |
|    time_elapsed     | 448       |
|    total_timesteps  | 138013    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 24503     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 41.2      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.738     |
| time/               |           |
|    episodes         | 2200      |
|    fps              | 308       |
|    time_elapsed     | 448       |
|    total_timesteps  | 138225    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 205       |
|    n_updates        | 24556     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.4      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.738     |
| time/               |           |
|    episodes         | 2204      |
|    fps              | 308       |
|    time_elapsed     | 448       |
|    total_timesteps  | 138356    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 209       |
|    n_updates        | 24588     |
-----------------------------------
Eval num_timesteps=138500, episode_reward=-2793.51 +/- 1440.37
Episode length: 34.64 +/- 6.49
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.737     |
| time/               |           |
|    total_timesteps  | 138500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.1      |
|    n_updates        | 24624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.8      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.737     |
| time/               |           |
|    episodes         | 2208      |
|    fps              | 308       |
|    time_elapsed     | 449       |
|    total_timesteps  | 138538    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52.4      |
|    n_updates        | 24634     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.6      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.737     |
| time/               |           |
|    episodes         | 2212      |
|    fps              | 308       |
|    time_elapsed     | 449       |
|    total_timesteps  | 138671    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 256       |
|    n_updates        | 24667     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.1      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.736     |
| time/               |           |
|    episodes         | 2216      |
|    fps              | 308       |
|    time_elapsed     | 449       |
|    total_timesteps  | 138819    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.9      |
|    n_updates        | 24704     |
-----------------------------------
Eval num_timesteps=139000, episode_reward=-2675.65 +/- 1019.30
Episode length: 37.10 +/- 5.68
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.1      |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.735     |
| time/               |           |
|    total_timesteps  | 139000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 24749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.3      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.735     |
| time/               |           |
|    episodes         | 2220      |
|    fps              | 308       |
|    time_elapsed     | 451       |
|    total_timesteps  | 139001    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 24750     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.2      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.735     |
| time/               |           |
|    episodes         | 2224      |
|    fps              | 308       |
|    time_elapsed     | 451       |
|    total_timesteps  | 139104    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 24775     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.734     |
| time/               |           |
|    episodes         | 2228      |
|    fps              | 308       |
|    time_elapsed     | 451       |
|    total_timesteps  | 139232    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62.4      |
|    n_updates        | 24807     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.734     |
| time/               |           |
|    episodes         | 2232      |
|    fps              | 308       |
|    time_elapsed     | 451       |
|    total_timesteps  | 139355    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 27.6      |
|    n_updates        | 24838     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.733     |
| time/               |           |
|    episodes         | 2236      |
|    fps              | 308       |
|    time_elapsed     | 451       |
|    total_timesteps  | 139488    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 46.1      |
|    n_updates        | 24871     |
-----------------------------------
Eval num_timesteps=139500, episode_reward=-2788.22 +/- 1187.85
Episode length: 35.14 +/- 6.17
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.733     |
| time/               |           |
|    total_timesteps  | 139500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83        |
|    n_updates        | 24874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.4      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.733     |
| time/               |           |
|    episodes         | 2240      |
|    fps              | 308       |
|    time_elapsed     | 453       |
|    total_timesteps  | 139668    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 295       |
|    n_updates        | 24916     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.6      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.732     |
| time/               |           |
|    episodes         | 2244      |
|    fps              | 308       |
|    time_elapsed     | 453       |
|    total_timesteps  | 139814    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.3      |
|    n_updates        | 24953     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.1      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.732     |
| time/               |           |
|    episodes         | 2248      |
|    fps              | 308       |
|    time_elapsed     | 453       |
|    total_timesteps  | 139930    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 214       |
|    n_updates        | 24982     |
-----------------------------------
Eval num_timesteps=140000, episode_reward=-3111.48 +/- 881.54
Episode length: 34.04 +/- 6.52
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.731     |
| time/               |           |
|    total_timesteps  | 140000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 195       |
|    n_updates        | 24999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.1      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.731     |
| time/               |           |
|    episodes         | 2252      |
|    fps              | 308       |
|    time_elapsed     | 454       |
|    total_timesteps  | 140075    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.3      |
|    n_updates        | 25018     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 40.1      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.73      |
| time/               |           |
|    episodes         | 2256      |
|    fps              | 308       |
|    time_elapsed     | 454       |
|    total_timesteps  | 140218    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 25054     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.5      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.73      |
| time/               |           |
|    episodes         | 2260      |
|    fps              | 308       |
|    time_elapsed     | 454       |
|    total_timesteps  | 140358    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 25089     |
-----------------------------------
Eval num_timesteps=140500, episode_reward=-2923.69 +/- 1097.96
Episode length: 33.58 +/- 7.29
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.6      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.729     |
| time/               |           |
|    total_timesteps  | 140500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 200       |
|    n_updates        | 25124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.1      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.729     |
| time/               |           |
|    episodes         | 2264      |
|    fps              | 307       |
|    time_elapsed     | 456       |
|    total_timesteps  | 140512    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.6      |
|    n_updates        | 25127     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.6      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.729     |
| time/               |           |
|    episodes         | 2268      |
|    fps              | 308       |
|    time_elapsed     | 456       |
|    total_timesteps  | 140694    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 26.5      |
|    n_updates        | 25173     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.8      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.728     |
| time/               |           |
|    episodes         | 2272      |
|    fps              | 308       |
|    time_elapsed     | 456       |
|    total_timesteps  | 140828    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.2      |
|    n_updates        | 25206     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.727     |
| time/               |           |
|    episodes         | 2276      |
|    fps              | 308       |
|    time_elapsed     | 456       |
|    total_timesteps  | 140953    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.2      |
|    n_updates        | 25238     |
-----------------------------------
Eval num_timesteps=141000, episode_reward=-2533.02 +/- 1303.41
Episode length: 37.40 +/- 5.51
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.4      |
|    mean_reward      | -2.53e+03 |
| rollout/            |           |
|    exploration_rate | 0.727     |
| time/               |           |
|    total_timesteps  | 141000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.4      |
|    n_updates        | 25249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.727     |
| time/               |           |
|    episodes         | 2280      |
|    fps              | 308       |
|    time_elapsed     | 458       |
|    total_timesteps  | 141087    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.9      |
|    n_updates        | 25271     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.1      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.726     |
| time/               |           |
|    episodes         | 2284      |
|    fps              | 308       |
|    time_elapsed     | 458       |
|    total_timesteps  | 141270    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.5      |
|    n_updates        | 25317     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.1      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.726     |
| time/               |           |
|    episodes         | 2288      |
|    fps              | 308       |
|    time_elapsed     | 458       |
|    total_timesteps  | 141413    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66        |
|    n_updates        | 25353     |
-----------------------------------
Eval num_timesteps=141500, episode_reward=-2747.65 +/- 1143.59
Episode length: 35.80 +/- 6.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.725     |
| time/               |           |
|    total_timesteps  | 141500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92        |
|    n_updates        | 25374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.7      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.725     |
| time/               |           |
|    episodes         | 2292      |
|    fps              | 307       |
|    time_elapsed     | 459       |
|    total_timesteps  | 141592    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.4      |
|    n_updates        | 25397     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37       |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.724    |
| time/               |          |
|    episodes         | 2296     |
|    fps              | 308      |
|    time_elapsed     | 459      |
|    total_timesteps  | 141709   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 109      |
|    n_updates        | 25427    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.724     |
| time/               |           |
|    episodes         | 2300      |
|    fps              | 308       |
|    time_elapsed     | 459       |
|    total_timesteps  | 141878    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.3      |
|    n_updates        | 25469     |
-----------------------------------
Eval num_timesteps=142000, episode_reward=-2708.89 +/- 1266.43
Episode length: 34.94 +/- 6.41
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.71e+03 |
| rollout/            |           |
|    exploration_rate | 0.723     |
| time/               |           |
|    total_timesteps  | 142000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 25499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.1      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.723     |
| time/               |           |
|    episodes         | 2304      |
|    fps              | 307       |
|    time_elapsed     | 461       |
|    total_timesteps  | 142065    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 25516     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.722     |
| time/               |           |
|    episodes         | 2308      |
|    fps              | 308       |
|    time_elapsed     | 461       |
|    total_timesteps  | 142212    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 25552     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.8      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.721     |
| time/               |           |
|    episodes         | 2312      |
|    fps              | 308       |
|    time_elapsed     | 461       |
|    total_timesteps  | 142448    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.3      |
|    n_updates        | 25611     |
-----------------------------------
Eval num_timesteps=142500, episode_reward=-2971.78 +/- 871.19
Episode length: 34.52 +/- 6.36
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.721     |
| time/               |           |
|    total_timesteps  | 142500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52.8      |
|    n_updates        | 25624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.8      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.721     |
| time/               |           |
|    episodes         | 2316      |
|    fps              | 307       |
|    time_elapsed     | 462       |
|    total_timesteps  | 142594    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 55.7      |
|    n_updates        | 25648     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.72      |
| time/               |           |
|    episodes         | 2320      |
|    fps              | 308       |
|    time_elapsed     | 463       |
|    total_timesteps  | 142698    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.3      |
|    n_updates        | 25674     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.9      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.72      |
| time/               |           |
|    episodes         | 2324      |
|    fps              | 308       |
|    time_elapsed     | 463       |
|    total_timesteps  | 142891    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 25722     |
-----------------------------------
Eval num_timesteps=143000, episode_reward=-2513.50 +/- 1392.83
Episode length: 35.84 +/- 7.00
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.51e+03 |
| rollout/            |           |
|    exploration_rate | 0.719     |
| time/               |           |
|    total_timesteps  | 143000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 34.3      |
|    n_updates        | 25749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.2      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.719     |
| time/               |           |
|    episodes         | 2328      |
|    fps              | 307       |
|    time_elapsed     | 464       |
|    total_timesteps  | 143049    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 25762     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.4      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.718     |
| time/               |           |
|    episodes         | 2332      |
|    fps              | 308       |
|    time_elapsed     | 464       |
|    total_timesteps  | 143193    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 25798     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.3      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.718     |
| time/               |           |
|    episodes         | 2336      |
|    fps              | 308       |
|    time_elapsed     | 464       |
|    total_timesteps  | 143321    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 25830     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.5      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.717     |
| time/               |           |
|    episodes         | 2340      |
|    fps              | 308       |
|    time_elapsed     | 464       |
|    total_timesteps  | 143423    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 25855     |
-----------------------------------
Eval num_timesteps=143500, episode_reward=-3060.40 +/- 926.53
Episode length: 33.84 +/- 6.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -3.06e+03 |
| rollout/            |           |
|    exploration_rate | 0.717     |
| time/               |           |
|    total_timesteps  | 143500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 25874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.3      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.717     |
| time/               |           |
|    episodes         | 2344      |
|    fps              | 307       |
|    time_elapsed     | 466       |
|    total_timesteps  | 143543    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 25885     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.2      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.716     |
| time/               |           |
|    episodes         | 2348      |
|    fps              | 308       |
|    time_elapsed     | 466       |
|    total_timesteps  | 143653    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 25913     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.716     |
| time/               |           |
|    episodes         | 2352      |
|    fps              | 308       |
|    time_elapsed     | 466       |
|    total_timesteps  | 143779    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.1      |
|    n_updates        | 25944     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.715     |
| time/               |           |
|    episodes         | 2356      |
|    fps              | 308       |
|    time_elapsed     | 466       |
|    total_timesteps  | 143905    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65        |
|    n_updates        | 25976     |
-----------------------------------
Eval num_timesteps=144000, episode_reward=-2708.96 +/- 1270.66
Episode length: 36.36 +/- 6.38
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.71e+03 |
| rollout/            |           |
|    exploration_rate | 0.715     |
| time/               |           |
|    total_timesteps  | 144000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.3      |
|    n_updates        | 25999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.715     |
| time/               |           |
|    episodes         | 2360      |
|    fps              | 307       |
|    time_elapsed     | 467       |
|    total_timesteps  | 144033    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 271       |
|    n_updates        | 26008     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.714     |
| time/               |           |
|    episodes         | 2364      |
|    fps              | 308       |
|    time_elapsed     | 468       |
|    total_timesteps  | 144160    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 167       |
|    n_updates        | 26039     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.714     |
| time/               |           |
|    episodes         | 2368      |
|    fps              | 308       |
|    time_elapsed     | 468       |
|    total_timesteps  | 144310    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 13.8      |
|    n_updates        | 26077     |
-----------------------------------
Eval num_timesteps=144500, episode_reward=-3002.12 +/- 1116.10
Episode length: 34.30 +/- 6.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.3     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.713    |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 45       |
|    n_updates        | 26124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.8     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.713    |
| time/               |          |
|    episodes         | 2372     |
|    fps              | 307      |
|    time_elapsed     | 469      |
|    total_timesteps  | 144512   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 95.2     |
|    n_updates        | 26127    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.6      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.712     |
| time/               |           |
|    episodes         | 2376      |
|    fps              | 308       |
|    time_elapsed     | 469       |
|    total_timesteps  | 144709    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.8      |
|    n_updates        | 26177     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.3     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.712    |
| time/               |          |
|    episodes         | 2380     |
|    fps              | 308      |
|    time_elapsed     | 469      |
|    total_timesteps  | 144817   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 204      |
|    n_updates        | 26204    |
----------------------------------
Eval num_timesteps=145000, episode_reward=-2602.50 +/- 1223.76
Episode length: 35.52 +/- 7.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.5     |
|    mean_reward      | -2.6e+03 |
| rollout/            |          |
|    exploration_rate | 0.711    |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 174      |
|    n_updates        | 26249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.4      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.711     |
| time/               |           |
|    episodes         | 2384      |
|    fps              | 307       |
|    time_elapsed     | 471       |
|    total_timesteps  | 145005    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.7      |
|    n_updates        | 26251     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.7      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.71      |
| time/               |           |
|    episodes         | 2388      |
|    fps              | 307       |
|    time_elapsed     | 471       |
|    total_timesteps  | 145183    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.1      |
|    n_updates        | 26295     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.1      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.709     |
| time/               |           |
|    episodes         | 2392      |
|    fps              | 308       |
|    time_elapsed     | 471       |
|    total_timesteps  | 145398    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.3      |
|    n_updates        | 26349     |
-----------------------------------
Eval num_timesteps=145500, episode_reward=-3040.15 +/- 1093.23
Episode length: 33.52 +/- 6.87
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.5      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.709     |
| time/               |           |
|    total_timesteps  | 145500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.6      |
|    n_updates        | 26374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.7      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.709     |
| time/               |           |
|    episodes         | 2396      |
|    fps              | 307       |
|    time_elapsed     | 472       |
|    total_timesteps  | 145576    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 26393     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.4      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.708     |
| time/               |           |
|    episodes         | 2400      |
|    fps              | 308       |
|    time_elapsed     | 472       |
|    total_timesteps  | 145715    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 26428     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.8      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.707     |
| time/               |           |
|    episodes         | 2404      |
|    fps              | 308       |
|    time_elapsed     | 473       |
|    total_timesteps  | 145849    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 26462     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.2      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.707     |
| time/               |           |
|    episodes         | 2408      |
|    fps              | 308       |
|    time_elapsed     | 473       |
|    total_timesteps  | 145936    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76        |
|    n_updates        | 26483     |
-----------------------------------
Eval num_timesteps=146000, episode_reward=-3114.28 +/- 1008.40
Episode length: 32.48 +/- 6.87
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.5      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.707     |
| time/               |           |
|    total_timesteps  | 146000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.8      |
|    n_updates        | 26499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.706     |
| time/               |           |
|    episodes         | 2412      |
|    fps              | 307       |
|    time_elapsed     | 474       |
|    total_timesteps  | 146066    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.4      |
|    n_updates        | 26516     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.706     |
| time/               |           |
|    episodes         | 2416      |
|    fps              | 308       |
|    time_elapsed     | 474       |
|    total_timesteps  | 146298    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65.6      |
|    n_updates        | 26574     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.3     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.705    |
| time/               |          |
|    episodes         | 2420     |
|    fps              | 308      |
|    time_elapsed     | 474      |
|    total_timesteps  | 146426   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 119      |
|    n_updates        | 26606    |
----------------------------------
Eval num_timesteps=146500, episode_reward=-2976.85 +/- 1128.45
Episode length: 32.10 +/- 7.11
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.1      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.705     |
| time/               |           |
|    total_timesteps  | 146500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 26624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.5     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.705    |
| time/               |          |
|    episodes         | 2424     |
|    fps              | 307      |
|    time_elapsed     | 475      |
|    total_timesteps  | 146537   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 86.2     |
|    n_updates        | 26634    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.704     |
| time/               |           |
|    episodes         | 2428      |
|    fps              | 308       |
|    time_elapsed     | 476       |
|    total_timesteps  | 146665    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 26666     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.703     |
| time/               |           |
|    episodes         | 2432      |
|    fps              | 308       |
|    time_elapsed     | 476       |
|    total_timesteps  | 146818    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 26704     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.703     |
| time/               |           |
|    episodes         | 2436      |
|    fps              | 308       |
|    time_elapsed     | 476       |
|    total_timesteps  | 146946    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 24.7      |
|    n_updates        | 26736     |
-----------------------------------
Eval num_timesteps=147000, episode_reward=-2953.58 +/- 1062.83
Episode length: 37.96 +/- 6.83
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 38        |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.703     |
| time/               |           |
|    total_timesteps  | 147000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 239       |
|    n_updates        | 26749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.1      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.702     |
| time/               |           |
|    episodes         | 2440      |
|    fps              | 307       |
|    time_elapsed     | 477       |
|    total_timesteps  | 147130    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 32.9      |
|    n_updates        | 26782     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.5      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.701     |
| time/               |           |
|    episodes         | 2444      |
|    fps              | 308       |
|    time_elapsed     | 477       |
|    total_timesteps  | 147289    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.1      |
|    n_updates        | 26822     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.4      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.701     |
| time/               |           |
|    episodes         | 2448      |
|    fps              | 308       |
|    time_elapsed     | 477       |
|    total_timesteps  | 147396    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.9      |
|    n_updates        | 26848     |
-----------------------------------
Eval num_timesteps=147500, episode_reward=-2747.83 +/- 1162.32
Episode length: 36.02 +/- 5.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.701     |
| time/               |           |
|    total_timesteps  | 147500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.5      |
|    n_updates        | 26874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.3      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.7       |
| time/               |           |
|    episodes         | 2452      |
|    fps              | 307       |
|    time_elapsed     | 479       |
|    total_timesteps  | 147507    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 26876     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.7      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.7       |
| time/               |           |
|    episodes         | 2456      |
|    fps              | 307       |
|    time_elapsed     | 479       |
|    total_timesteps  | 147672    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 26917     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.6      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.699     |
| time/               |           |
|    episodes         | 2460      |
|    fps              | 308       |
|    time_elapsed     | 479       |
|    total_timesteps  | 147796    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 26948     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.9      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.699     |
| time/               |           |
|    episodes         | 2464      |
|    fps              | 308       |
|    time_elapsed     | 479       |
|    total_timesteps  | 147946    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 26986     |
-----------------------------------
Eval num_timesteps=148000, episode_reward=-2426.91 +/- 1330.69
Episode length: 36.30 +/- 6.56
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.43e+03 |
| rollout/            |           |
|    exploration_rate | 0.698     |
| time/               |           |
|    total_timesteps  | 148000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 26999     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.6     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.698    |
| time/               |          |
|    episodes         | 2468     |
|    fps              | 307      |
|    time_elapsed     | 481      |
|    total_timesteps  | 148067   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 105      |
|    n_updates        | 27016    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37       |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.698    |
| time/               |          |
|    episodes         | 2472     |
|    fps              | 307      |
|    time_elapsed     | 481      |
|    total_timesteps  | 148215   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 153      |
|    n_updates        | 27053    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.697     |
| time/               |           |
|    episodes         | 2476      |
|    fps              | 308       |
|    time_elapsed     | 481       |
|    total_timesteps  | 148349    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 192       |
|    n_updates        | 27087     |
-----------------------------------
Eval num_timesteps=148500, episode_reward=-2843.85 +/- 1211.28
Episode length: 35.54 +/- 5.53
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.696     |
| time/               |           |
|    total_timesteps  | 148500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.2      |
|    n_updates        | 27124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.696     |
| time/               |           |
|    episodes         | 2480      |
|    fps              | 307       |
|    time_elapsed     | 482       |
|    total_timesteps  | 148512    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 207       |
|    n_updates        | 27127     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.696     |
| time/               |           |
|    episodes         | 2484      |
|    fps              | 307       |
|    time_elapsed     | 482       |
|    total_timesteps  | 148678    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 27169     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.695     |
| time/               |           |
|    episodes         | 2488      |
|    fps              | 308       |
|    time_elapsed     | 483       |
|    total_timesteps  | 148820    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 27204     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.694     |
| time/               |           |
|    episodes         | 2492      |
|    fps              | 308       |
|    time_elapsed     | 483       |
|    total_timesteps  | 148970    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.3      |
|    n_updates        | 27242     |
-----------------------------------
Eval num_timesteps=149000, episode_reward=-2497.09 +/- 1292.53
Episode length: 36.42 +/- 6.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.4     |
|    mean_reward      | -2.5e+03 |
| rollout/            |          |
|    exploration_rate | 0.694    |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 150      |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.6     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.693    |
| time/               |          |
|    episodes         | 2496     |
|    fps              | 307      |
|    time_elapsed     | 484      |
|    total_timesteps  | 149238   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 182      |
|    n_updates        | 27309    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.5      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.692     |
| time/               |           |
|    episodes         | 2500      |
|    fps              | 308       |
|    time_elapsed     | 484       |
|    total_timesteps  | 149460    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.6      |
|    n_updates        | 27364     |
-----------------------------------
Eval num_timesteps=149500, episode_reward=-2658.11 +/- 1127.43
Episode length: 35.10 +/- 6.88
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.66e+03 |
| rollout/            |           |
|    exploration_rate | 0.692     |
| time/               |           |
|    total_timesteps  | 149500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.1      |
|    n_updates        | 27374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.6      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.692     |
| time/               |           |
|    episodes         | 2504      |
|    fps              | 307       |
|    time_elapsed     | 486       |
|    total_timesteps  | 149606    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.6      |
|    n_updates        | 27401     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.2      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.691     |
| time/               |           |
|    episodes         | 2508      |
|    fps              | 307       |
|    time_elapsed     | 486       |
|    total_timesteps  | 149756    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.7      |
|    n_updates        | 27438     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.69      |
| time/               |           |
|    episodes         | 2512      |
|    fps              | 308       |
|    time_elapsed     | 486       |
|    total_timesteps  | 149942    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.9      |
|    n_updates        | 27485     |
-----------------------------------
Eval num_timesteps=150000, episode_reward=-2546.75 +/- 1252.65
Episode length: 35.90 +/- 6.89
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.55e+03 |
| rollout/            |           |
|    exploration_rate | 0.69      |
| time/               |           |
|    total_timesteps  | 150000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 27.3      |
|    n_updates        | 27499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.3      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.689     |
| time/               |           |
|    episodes         | 2516      |
|    fps              | 307       |
|    time_elapsed     | 487       |
|    total_timesteps  | 150128    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.8      |
|    n_updates        | 27531     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.4      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.689     |
| time/               |           |
|    episodes         | 2520      |
|    fps              | 307       |
|    time_elapsed     | 487       |
|    total_timesteps  | 150268    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 27566     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.1      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.688     |
| time/               |           |
|    episodes         | 2524      |
|    fps              | 308       |
|    time_elapsed     | 488       |
|    total_timesteps  | 150446    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 27611     |
-----------------------------------
Eval num_timesteps=150500, episode_reward=-2883.52 +/- 1064.51
Episode length: 33.60 +/- 7.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.6      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.688     |
| time/               |           |
|    total_timesteps  | 150500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 27624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.688     |
| time/               |           |
|    episodes         | 2528      |
|    fps              | 307       |
|    time_elapsed     | 489       |
|    total_timesteps  | 150580    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.3      |
|    n_updates        | 27644     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39        |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.687     |
| time/               |           |
|    episodes         | 2532      |
|    fps              | 307       |
|    time_elapsed     | 489       |
|    total_timesteps  | 150723    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 27680     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.686     |
| time/               |           |
|    episodes         | 2536      |
|    fps              | 308       |
|    time_elapsed     | 489       |
|    total_timesteps  | 150849    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65.7      |
|    n_updates        | 27712     |
-----------------------------------
Eval num_timesteps=151000, episode_reward=-2899.41 +/- 1148.68
Episode length: 34.02 +/- 5.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34       |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.686    |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 87.2     |
|    n_updates        | 27749    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.686     |
| time/               |           |
|    episodes         | 2540      |
|    fps              | 307       |
|    time_elapsed     | 490       |
|    total_timesteps  | 151033    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97        |
|    n_updates        | 27758     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.6      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.685     |
| time/               |           |
|    episodes         | 2544      |
|    fps              | 307       |
|    time_elapsed     | 491       |
|    total_timesteps  | 151149    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.8      |
|    n_updates        | 27787     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.6      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.685     |
| time/               |           |
|    episodes         | 2548      |
|    fps              | 307       |
|    time_elapsed     | 491       |
|    total_timesteps  | 151254    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 27813     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.2      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.684     |
| time/               |           |
|    episodes         | 2552      |
|    fps              | 308       |
|    time_elapsed     | 491       |
|    total_timesteps  | 151423    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80        |
|    n_updates        | 27855     |
-----------------------------------
Eval num_timesteps=151500, episode_reward=-3194.39 +/- 934.89
Episode length: 36.70 +/- 6.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.7      |
|    mean_reward      | -3.19e+03 |
| rollout/            |           |
|    exploration_rate | 0.684     |
| time/               |           |
|    total_timesteps  | 151500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 27874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.8      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.683     |
| time/               |           |
|    episodes         | 2556      |
|    fps              | 307       |
|    time_elapsed     | 492       |
|    total_timesteps  | 151547    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 27886     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.7      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.683     |
| time/               |           |
|    episodes         | 2560      |
|    fps              | 307       |
|    time_elapsed     | 492       |
|    total_timesteps  | 151670    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 27917     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.1      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.682     |
| time/               |           |
|    episodes         | 2564      |
|    fps              | 308       |
|    time_elapsed     | 492       |
|    total_timesteps  | 151858    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 27964     |
-----------------------------------
Eval num_timesteps=152000, episode_reward=-2858.60 +/- 1111.82
Episode length: 35.42 +/- 6.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.682     |
| time/               |           |
|    total_timesteps  | 152000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 249       |
|    n_updates        | 27999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.4      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.681     |
| time/               |           |
|    episodes         | 2568      |
|    fps              | 307       |
|    time_elapsed     | 494       |
|    total_timesteps  | 152010    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 248       |
|    n_updates        | 28002     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.5      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.681     |
| time/               |           |
|    episodes         | 2572      |
|    fps              | 307       |
|    time_elapsed     | 494       |
|    total_timesteps  | 152161    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.2      |
|    n_updates        | 28040     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.3      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.68      |
| time/               |           |
|    episodes         | 2576      |
|    fps              | 307       |
|    time_elapsed     | 494       |
|    total_timesteps  | 152279    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 28069     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.6      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.68      |
| time/               |           |
|    episodes         | 2580      |
|    fps              | 308       |
|    time_elapsed     | 494       |
|    total_timesteps  | 152469    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 28117     |
-----------------------------------
Eval num_timesteps=152500, episode_reward=-2850.70 +/- 1222.04
Episode length: 34.26 +/- 6.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.679     |
| time/               |           |
|    total_timesteps  | 152500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 19.4      |
|    n_updates        | 28124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39        |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.679     |
| time/               |           |
|    episodes         | 2584      |
|    fps              | 307       |
|    time_elapsed     | 495       |
|    total_timesteps  | 152575    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92        |
|    n_updates        | 28143     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.9      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.678     |
| time/               |           |
|    episodes         | 2588      |
|    fps              | 307       |
|    time_elapsed     | 496       |
|    total_timesteps  | 152711    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.1      |
|    n_updates        | 28177     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 39.4      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.678     |
| time/               |           |
|    episodes         | 2592      |
|    fps              | 308       |
|    time_elapsed     | 496       |
|    total_timesteps  | 152908    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 24.3      |
|    n_updates        | 28226     |
-----------------------------------
Eval num_timesteps=153000, episode_reward=-2722.82 +/- 1318.19
Episode length: 34.72 +/- 6.77
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.677     |
| time/               |           |
|    total_timesteps  | 153000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 25.1      |
|    n_updates        | 28249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.9      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.677     |
| time/               |           |
|    episodes         | 2596      |
|    fps              | 307       |
|    time_elapsed     | 497       |
|    total_timesteps  | 153023    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 356       |
|    n_updates        | 28255     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.677     |
| time/               |           |
|    episodes         | 2600      |
|    fps              | 307       |
|    time_elapsed     | 497       |
|    total_timesteps  | 153142    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 28285     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.676     |
| time/               |           |
|    episodes         | 2604      |
|    fps              | 307       |
|    time_elapsed     | 497       |
|    total_timesteps  | 153272    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.3      |
|    n_updates        | 28317     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.676     |
| time/               |           |
|    episodes         | 2608      |
|    fps              | 308       |
|    time_elapsed     | 497       |
|    total_timesteps  | 153394    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 28348     |
-----------------------------------
Eval num_timesteps=153500, episode_reward=-2967.88 +/- 1080.50
Episode length: 35.70 +/- 5.46
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.675     |
| time/               |           |
|    total_timesteps  | 153500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 28374     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.8     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.675    |
| time/               |          |
|    episodes         | 2612     |
|    fps              | 307      |
|    time_elapsed     | 499      |
|    total_timesteps  | 153518   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 115      |
|    n_updates        | 28379    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.4     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.674    |
| time/               |          |
|    episodes         | 2616     |
|    fps              | 307      |
|    time_elapsed     | 499      |
|    total_timesteps  | 153665   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 188      |
|    n_updates        | 28416    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.674     |
| time/               |           |
|    episodes         | 2620      |
|    fps              | 307       |
|    time_elapsed     | 499       |
|    total_timesteps  | 153774    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.5      |
|    n_updates        | 28443     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.673     |
| time/               |           |
|    episodes         | 2624      |
|    fps              | 308       |
|    time_elapsed     | 499       |
|    total_timesteps  | 153895    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 28473     |
-----------------------------------
Eval num_timesteps=154000, episode_reward=-2704.74 +/- 1294.75
Episode length: 36.52 +/- 5.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.5     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.673    |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 119      |
|    n_updates        | 28499    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.673     |
| time/               |           |
|    episodes         | 2628      |
|    fps              | 307       |
|    time_elapsed     | 500       |
|    total_timesteps  | 154042    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 192       |
|    n_updates        | 28510     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.672     |
| time/               |           |
|    episodes         | 2632      |
|    fps              | 307       |
|    time_elapsed     | 500       |
|    total_timesteps  | 154145    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 28536     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.672     |
| time/               |           |
|    episodes         | 2636      |
|    fps              | 307       |
|    time_elapsed     | 501       |
|    total_timesteps  | 154285    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 36.7      |
|    n_updates        | 28571     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.671     |
| time/               |           |
|    episodes         | 2640      |
|    fps              | 308       |
|    time_elapsed     | 501       |
|    total_timesteps  | 154424    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 28605     |
-----------------------------------
Eval num_timesteps=154500, episode_reward=-2859.26 +/- 1141.02
Episode length: 34.56 +/- 6.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.671     |
| time/               |           |
|    total_timesteps  | 154500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 28624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.671     |
| time/               |           |
|    episodes         | 2644      |
|    fps              | 307       |
|    time_elapsed     | 502       |
|    total_timesteps  | 154534    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 28633     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.67      |
| time/               |           |
|    episodes         | 2648      |
|    fps              | 307       |
|    time_elapsed     | 502       |
|    total_timesteps  | 154702    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48        |
|    n_updates        | 28675     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.669     |
| time/               |           |
|    episodes         | 2652      |
|    fps              | 307       |
|    time_elapsed     | 502       |
|    total_timesteps  | 154846    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88        |
|    n_updates        | 28711     |
-----------------------------------
Eval num_timesteps=155000, episode_reward=-3161.51 +/- 913.70
Episode length: 34.40 +/- 7.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -3.16e+03 |
| rollout/            |           |
|    exploration_rate | 0.669     |
| time/               |           |
|    total_timesteps  | 155000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.3      |
|    n_updates        | 28749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.669     |
| time/               |           |
|    episodes         | 2656      |
|    fps              | 307       |
|    time_elapsed     | 504       |
|    total_timesteps  | 155020    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 260       |
|    n_updates        | 28754     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.668     |
| time/               |           |
|    episodes         | 2660      |
|    fps              | 307       |
|    time_elapsed     | 504       |
|    total_timesteps  | 155149    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 28787     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.667     |
| time/               |           |
|    episodes         | 2664      |
|    fps              | 307       |
|    time_elapsed     | 504       |
|    total_timesteps  | 155295    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 181       |
|    n_updates        | 28823     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.667     |
| time/               |           |
|    episodes         | 2668      |
|    fps              | 308       |
|    time_elapsed     | 504       |
|    total_timesteps  | 155426    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 20.1      |
|    n_updates        | 28856     |
-----------------------------------
Eval num_timesteps=155500, episode_reward=-2805.60 +/- 1115.20
Episode length: 34.86 +/- 6.26
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.666     |
| time/               |           |
|    total_timesteps  | 155500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 28874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.666     |
| time/               |           |
|    episodes         | 2672      |
|    fps              | 307       |
|    time_elapsed     | 505       |
|    total_timesteps  | 155572    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 28892     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.666     |
| time/               |           |
|    episodes         | 2676      |
|    fps              | 307       |
|    time_elapsed     | 505       |
|    total_timesteps  | 155721    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 50.4      |
|    n_updates        | 28930     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.665     |
| time/               |           |
|    episodes         | 2680      |
|    fps              | 307       |
|    time_elapsed     | 506       |
|    total_timesteps  | 155850    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 28962     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.664     |
| time/               |           |
|    episodes         | 2684      |
|    fps              | 308       |
|    time_elapsed     | 506       |
|    total_timesteps  | 155991    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.1      |
|    n_updates        | 28997     |
-----------------------------------
Eval num_timesteps=156000, episode_reward=-2649.96 +/- 1156.39
Episode length: 35.84 +/- 6.23
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.664     |
| time/               |           |
|    total_timesteps  | 156000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.2      |
|    n_updates        | 28999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.663     |
| time/               |           |
|    episodes         | 2688      |
|    fps              | 307       |
|    time_elapsed     | 507       |
|    total_timesteps  | 156198    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93        |
|    n_updates        | 29049     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.663     |
| time/               |           |
|    episodes         | 2692      |
|    fps              | 307       |
|    time_elapsed     | 507       |
|    total_timesteps  | 156316    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.2      |
|    n_updates        | 29078     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.662     |
| time/               |           |
|    episodes         | 2696      |
|    fps              | 308       |
|    time_elapsed     | 507       |
|    total_timesteps  | 156457    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 29114     |
-----------------------------------
Eval num_timesteps=156500, episode_reward=-2992.07 +/- 1020.95
Episode length: 35.84 +/- 7.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.662     |
| time/               |           |
|    total_timesteps  | 156500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 29124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.662     |
| time/               |           |
|    episodes         | 2700      |
|    fps              | 307       |
|    time_elapsed     | 509       |
|    total_timesteps  | 156602    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 29150     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.661     |
| time/               |           |
|    episodes         | 2704      |
|    fps              | 307       |
|    time_elapsed     | 509       |
|    total_timesteps  | 156783    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.3      |
|    n_updates        | 29195     |
-----------------------------------
Eval num_timesteps=157000, episode_reward=-3212.63 +/- 904.80
Episode length: 35.96 +/- 8.18
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -3.21e+03 |
| rollout/            |           |
|    exploration_rate | 0.66      |
| time/               |           |
|    total_timesteps  | 157000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 29249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.66      |
| time/               |           |
|    episodes         | 2708      |
|    fps              | 307       |
|    time_elapsed     | 510       |
|    total_timesteps  | 157006    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 29251     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.659     |
| time/               |           |
|    episodes         | 2712      |
|    fps              | 307       |
|    time_elapsed     | 510       |
|    total_timesteps  | 157129    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 29282     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.659     |
| time/               |           |
|    episodes         | 2716      |
|    fps              | 307       |
|    time_elapsed     | 510       |
|    total_timesteps  | 157291    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 29322     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.658     |
| time/               |           |
|    episodes         | 2720      |
|    fps              | 308       |
|    time_elapsed     | 511       |
|    total_timesteps  | 157425    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 29356     |
-----------------------------------
Eval num_timesteps=157500, episode_reward=-3289.48 +/- 759.06
Episode length: 35.50 +/- 7.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -3.29e+03 |
| rollout/            |           |
|    exploration_rate | 0.658     |
| time/               |           |
|    total_timesteps  | 157500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.6      |
|    n_updates        | 29374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.657     |
| time/               |           |
|    episodes         | 2724      |
|    fps              | 307       |
|    time_elapsed     | 512       |
|    total_timesteps  | 157567    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 29391     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.657     |
| time/               |           |
|    episodes         | 2728      |
|    fps              | 307       |
|    time_elapsed     | 512       |
|    total_timesteps  | 157687    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 29421     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.3      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.656     |
| time/               |           |
|    episodes         | 2732      |
|    fps              | 307       |
|    time_elapsed     | 512       |
|    total_timesteps  | 157872    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 29467     |
-----------------------------------
Eval num_timesteps=158000, episode_reward=-2743.26 +/- 1193.22
Episode length: 34.78 +/- 6.33
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.74e+03 |
| rollout/            |           |
|    exploration_rate | 0.656     |
| time/               |           |
|    total_timesteps  | 158000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.9      |
|    n_updates        | 29499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.4      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.655     |
| time/               |           |
|    episodes         | 2736      |
|    fps              | 307       |
|    time_elapsed     | 513       |
|    total_timesteps  | 158029    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 29507     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.2      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.655     |
| time/               |           |
|    episodes         | 2740      |
|    fps              | 307       |
|    time_elapsed     | 514       |
|    total_timesteps  | 158144    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.2      |
|    n_updates        | 29535     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 38.5      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.654     |
| time/               |           |
|    episodes         | 2744      |
|    fps              | 307       |
|    time_elapsed     | 514       |
|    total_timesteps  | 158381    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.6      |
|    n_updates        | 29595     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.9      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.653     |
| time/               |           |
|    episodes         | 2748      |
|    fps              | 308       |
|    time_elapsed     | 514       |
|    total_timesteps  | 158490    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 29622     |
-----------------------------------
Eval num_timesteps=158500, episode_reward=-2727.12 +/- 1256.65
Episode length: 36.78 +/- 5.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.653     |
| time/               |           |
|    total_timesteps  | 158500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 29624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.6      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.653     |
| time/               |           |
|    episodes         | 2752      |
|    fps              | 307       |
|    time_elapsed     | 515       |
|    total_timesteps  | 158605    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 29651     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.652     |
| time/               |           |
|    episodes         | 2756      |
|    fps              | 307       |
|    time_elapsed     | 515       |
|    total_timesteps  | 158735    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 29683     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.652     |
| time/               |           |
|    episodes         | 2760      |
|    fps              | 307       |
|    time_elapsed     | 515       |
|    total_timesteps  | 158854    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 29713     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.9     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.651    |
| time/               |          |
|    episodes         | 2764     |
|    fps              | 308      |
|    time_elapsed     | 516      |
|    total_timesteps  | 158983   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 44.8     |
|    n_updates        | 29745    |
----------------------------------
Eval num_timesteps=159000, episode_reward=-3182.52 +/- 1005.06
Episode length: 37.48 +/- 7.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.5      |
|    mean_reward      | -3.18e+03 |
| rollout/            |           |
|    exploration_rate | 0.651     |
| time/               |           |
|    total_timesteps  | 159000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.7      |
|    n_updates        | 29749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.651     |
| time/               |           |
|    episodes         | 2768      |
|    fps              | 307       |
|    time_elapsed     | 517       |
|    total_timesteps  | 159127    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 203       |
|    n_updates        | 29781     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.65      |
| time/               |           |
|    episodes         | 2772      |
|    fps              | 307       |
|    time_elapsed     | 517       |
|    total_timesteps  | 159252    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 29812     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.8     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.649    |
| time/               |          |
|    episodes         | 2776     |
|    fps              | 307      |
|    time_elapsed     | 517      |
|    total_timesteps  | 159399   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 139      |
|    n_updates        | 29849    |
----------------------------------
Eval num_timesteps=159500, episode_reward=-2529.35 +/- 1330.52
Episode length: 36.74 +/- 5.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.7      |
|    mean_reward      | -2.53e+03 |
| rollout/            |           |
|    exploration_rate | 0.649     |
| time/               |           |
|    total_timesteps  | 159500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 29874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.3      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.649     |
| time/               |           |
|    episodes         | 2780      |
|    fps              | 307       |
|    time_elapsed     | 519       |
|    total_timesteps  | 159581    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 200       |
|    n_updates        | 29895     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37.2      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.648     |
| time/               |           |
|    episodes         | 2784      |
|    fps              | 307       |
|    time_elapsed     | 519       |
|    total_timesteps  | 159708    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47.3      |
|    n_updates        | 29926     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.648     |
| time/               |           |
|    episodes         | 2788      |
|    fps              | 307       |
|    time_elapsed     | 519       |
|    total_timesteps  | 159837    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 29959     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.647     |
| time/               |           |
|    episodes         | 2792      |
|    fps              | 307       |
|    time_elapsed     | 519       |
|    total_timesteps  | 159954    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 295       |
|    n_updates        | 29988     |
-----------------------------------
Eval num_timesteps=160000, episode_reward=-3436.54 +/- 705.78
Episode length: 35.78 +/- 7.04
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -3.44e+03 |
| rollout/            |           |
|    exploration_rate | 0.647     |
| time/               |           |
|    total_timesteps  | 160000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 239       |
|    n_updates        | 29999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.646     |
| time/               |           |
|    episodes         | 2796      |
|    fps              | 307       |
|    time_elapsed     | 520       |
|    total_timesteps  | 160070    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64.3      |
|    n_updates        | 30017     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.646     |
| time/               |           |
|    episodes         | 2800      |
|    fps              | 307       |
|    time_elapsed     | 520       |
|    total_timesteps  | 160214    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 30053     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.1     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.645    |
| time/               |          |
|    episodes         | 2804     |
|    fps              | 307      |
|    time_elapsed     | 521      |
|    total_timesteps  | 160397   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 106      |
|    n_updates        | 30099    |
----------------------------------
Eval num_timesteps=160500, episode_reward=-2751.90 +/- 1274.46
Episode length: 35.88 +/- 6.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.645     |
| time/               |           |
|    total_timesteps  | 160500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 218       |
|    n_updates        | 30124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.644     |
| time/               |           |
|    episodes         | 2808      |
|    fps              | 307       |
|    time_elapsed     | 522       |
|    total_timesteps  | 160585    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 30146     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.644     |
| time/               |           |
|    episodes         | 2812      |
|    fps              | 307       |
|    time_elapsed     | 522       |
|    total_timesteps  | 160704    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 231       |
|    n_updates        | 30175     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.643     |
| time/               |           |
|    episodes         | 2816      |
|    fps              | 307       |
|    time_elapsed     | 522       |
|    total_timesteps  | 160839    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.6      |
|    n_updates        | 30209     |
-----------------------------------
Eval num_timesteps=161000, episode_reward=-2690.50 +/- 1266.49
Episode length: 34.44 +/- 7.93
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.642     |
| time/               |           |
|    total_timesteps  | 161000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 30249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.642     |
| time/               |           |
|    episodes         | 2820      |
|    fps              | 307       |
|    time_elapsed     | 524       |
|    total_timesteps  | 161020    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 334       |
|    n_updates        | 30254     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.642     |
| time/               |           |
|    episodes         | 2824      |
|    fps              | 307       |
|    time_elapsed     | 524       |
|    total_timesteps  | 161148    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 30286     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.641     |
| time/               |           |
|    episodes         | 2828      |
|    fps              | 307       |
|    time_elapsed     | 524       |
|    total_timesteps  | 161252    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.9      |
|    n_updates        | 30312     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.64      |
| time/               |           |
|    episodes         | 2832      |
|    fps              | 307       |
|    time_elapsed     | 524       |
|    total_timesteps  | 161448    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 30361     |
-----------------------------------
Eval num_timesteps=161500, episode_reward=-2783.45 +/- 986.00
Episode length: 36.00 +/- 5.52
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.64      |
| time/               |           |
|    total_timesteps  | 161500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 30374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.64      |
| time/               |           |
|    episodes         | 2836      |
|    fps              | 307       |
|    time_elapsed     | 525       |
|    total_timesteps  | 161568    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99        |
|    n_updates        | 30391     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.639     |
| time/               |           |
|    episodes         | 2840      |
|    fps              | 307       |
|    time_elapsed     | 525       |
|    total_timesteps  | 161705    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54        |
|    n_updates        | 30426     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.639     |
| time/               |           |
|    episodes         | 2844      |
|    fps              | 307       |
|    time_elapsed     | 526       |
|    total_timesteps  | 161870    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.8      |
|    n_updates        | 30467     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.638     |
| time/               |           |
|    episodes         | 2848      |
|    fps              | 307       |
|    time_elapsed     | 526       |
|    total_timesteps  | 161985    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 30496     |
-----------------------------------
Eval num_timesteps=162000, episode_reward=-2829.99 +/- 1238.01
Episode length: 34.34 +/- 7.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.638     |
| time/               |           |
|    total_timesteps  | 162000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93        |
|    n_updates        | 30499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.638     |
| time/               |           |
|    episodes         | 2852      |
|    fps              | 307       |
|    time_elapsed     | 527       |
|    total_timesteps  | 162095    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 30523     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.637     |
| time/               |           |
|    episodes         | 2856      |
|    fps              | 307       |
|    time_elapsed     | 527       |
|    total_timesteps  | 162266    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 30566     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.636     |
| time/               |           |
|    episodes         | 2860      |
|    fps              | 307       |
|    time_elapsed     | 527       |
|    total_timesteps  | 162406    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 30601     |
-----------------------------------
Eval num_timesteps=162500, episode_reward=-2843.28 +/- 1112.09
Episode length: 34.16 +/- 6.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.636     |
| time/               |           |
|    total_timesteps  | 162500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 30624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.636     |
| time/               |           |
|    episodes         | 2864      |
|    fps              | 307       |
|    time_elapsed     | 529       |
|    total_timesteps  | 162536    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 30633     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.635     |
| time/               |           |
|    episodes         | 2868      |
|    fps              | 307       |
|    time_elapsed     | 529       |
|    total_timesteps  | 162665    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.9      |
|    n_updates        | 30666     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.634     |
| time/               |           |
|    episodes         | 2872      |
|    fps              | 307       |
|    time_elapsed     | 529       |
|    total_timesteps  | 162813    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.3      |
|    n_updates        | 30703     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.634     |
| time/               |           |
|    episodes         | 2876      |
|    fps              | 307       |
|    time_elapsed     | 529       |
|    total_timesteps  | 162913    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 30728     |
-----------------------------------
Eval num_timesteps=163000, episode_reward=-3073.81 +/- 1002.44
Episode length: 35.88 +/- 7.89
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -3.07e+03 |
| rollout/            |           |
|    exploration_rate | 0.633     |
| time/               |           |
|    total_timesteps  | 163000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.3      |
|    n_updates        | 30749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.633     |
| time/               |           |
|    episodes         | 2880      |
|    fps              | 307       |
|    time_elapsed     | 530       |
|    total_timesteps  | 163022    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 30755     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.633     |
| time/               |           |
|    episodes         | 2884      |
|    fps              | 307       |
|    time_elapsed     | 530       |
|    total_timesteps  | 163131    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 30782     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.632     |
| time/               |           |
|    episodes         | 2888      |
|    fps              | 307       |
|    time_elapsed     | 530       |
|    total_timesteps  | 163231    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81        |
|    n_updates        | 30807     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.632     |
| time/               |           |
|    episodes         | 2892      |
|    fps              | 307       |
|    time_elapsed     | 530       |
|    total_timesteps  | 163350    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 30837     |
-----------------------------------
Eval num_timesteps=163500, episode_reward=-2627.04 +/- 1329.43
Episode length: 36.24 +/- 6.33
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.63e+03 |
| rollout/            |           |
|    exploration_rate | 0.631     |
| time/               |           |
|    total_timesteps  | 163500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 30874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.631     |
| time/               |           |
|    episodes         | 2896      |
|    fps              | 307       |
|    time_elapsed     | 532       |
|    total_timesteps  | 163560    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 32.5      |
|    n_updates        | 30889     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.631     |
| time/               |           |
|    episodes         | 2900      |
|    fps              | 307       |
|    time_elapsed     | 532       |
|    total_timesteps  | 163664    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 40.5      |
|    n_updates        | 30915     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.63      |
| time/               |           |
|    episodes         | 2904      |
|    fps              | 307       |
|    time_elapsed     | 532       |
|    total_timesteps  | 163801    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 40.4      |
|    n_updates        | 30950     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.629     |
| time/               |           |
|    episodes         | 2908      |
|    fps              | 307       |
|    time_elapsed     | 532       |
|    total_timesteps  | 163935    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.2      |
|    n_updates        | 30983     |
-----------------------------------
Eval num_timesteps=164000, episode_reward=-2882.95 +/- 1003.06
Episode length: 34.30 +/- 6.78
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.629     |
| time/               |           |
|    total_timesteps  | 164000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 30999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.629     |
| time/               |           |
|    episodes         | 2912      |
|    fps              | 307       |
|    time_elapsed     | 534       |
|    total_timesteps  | 164091    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 31022     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.628     |
| time/               |           |
|    episodes         | 2916      |
|    fps              | 307       |
|    time_elapsed     | 534       |
|    total_timesteps  | 164221    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.4      |
|    n_updates        | 31055     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.627     |
| time/               |           |
|    episodes         | 2920      |
|    fps              | 307       |
|    time_elapsed     | 534       |
|    total_timesteps  | 164350    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 31087     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.627     |
| time/               |           |
|    episodes         | 2924      |
|    fps              | 307       |
|    time_elapsed     | 534       |
|    total_timesteps  | 164494    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 233       |
|    n_updates        | 31123     |
-----------------------------------
Eval num_timesteps=164500, episode_reward=-2536.47 +/- 1277.12
Episode length: 35.76 +/- 7.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.54e+03 |
| rollout/            |           |
|    exploration_rate | 0.627     |
| time/               |           |
|    total_timesteps  | 164500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 31124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.626     |
| time/               |           |
|    episodes         | 2928      |
|    fps              | 307       |
|    time_elapsed     | 535       |
|    total_timesteps  | 164633    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.5      |
|    n_updates        | 31158     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.626     |
| time/               |           |
|    episodes         | 2932      |
|    fps              | 307       |
|    time_elapsed     | 535       |
|    total_timesteps  | 164746    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 31186     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.625     |
| time/               |           |
|    episodes         | 2936      |
|    fps              | 307       |
|    time_elapsed     | 535       |
|    total_timesteps  | 164863    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 31215     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.625     |
| time/               |           |
|    episodes         | 2940      |
|    fps              | 307       |
|    time_elapsed     | 536       |
|    total_timesteps  | 164979    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 219       |
|    n_updates        | 31244     |
-----------------------------------
Eval num_timesteps=165000, episode_reward=-2593.02 +/- 1271.28
Episode length: 35.66 +/- 6.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.59e+03 |
| rollout/            |           |
|    exploration_rate | 0.625     |
| time/               |           |
|    total_timesteps  | 165000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 31249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.624     |
| time/               |           |
|    episodes         | 2944      |
|    fps              | 307       |
|    time_elapsed     | 537       |
|    total_timesteps  | 165144    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 31285     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.623     |
| time/               |           |
|    episodes         | 2948      |
|    fps              | 307       |
|    time_elapsed     | 537       |
|    total_timesteps  | 165329    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73        |
|    n_updates        | 31332     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.622     |
| time/               |           |
|    episodes         | 2952      |
|    fps              | 307       |
|    time_elapsed     | 537       |
|    total_timesteps  | 165464    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 31365     |
-----------------------------------
Eval num_timesteps=165500, episode_reward=-2840.18 +/- 1321.71
Episode length: 35.96 +/- 6.46
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.622     |
| time/               |           |
|    total_timesteps  | 165500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.1      |
|    n_updates        | 31374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.622     |
| time/               |           |
|    episodes         | 2956      |
|    fps              | 307       |
|    time_elapsed     | 539       |
|    total_timesteps  | 165605    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 31401     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.621     |
| time/               |           |
|    episodes         | 2960      |
|    fps              | 307       |
|    time_elapsed     | 539       |
|    total_timesteps  | 165704    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 31425     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.621     |
| time/               |           |
|    episodes         | 2964      |
|    fps              | 307       |
|    time_elapsed     | 539       |
|    total_timesteps  | 165845    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.1      |
|    n_updates        | 31461     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.62      |
| time/               |           |
|    episodes         | 2968      |
|    fps              | 307       |
|    time_elapsed     | 539       |
|    total_timesteps  | 165995    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 31498     |
-----------------------------------
Eval num_timesteps=166000, episode_reward=-2640.66 +/- 1029.40
Episode length: 35.00 +/- 6.51
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.62      |
| time/               |           |
|    total_timesteps  | 166000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.9      |
|    n_updates        | 31499     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.2     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.619    |
| time/               |          |
|    episodes         | 2972     |
|    fps              | 307      |
|    time_elapsed     | 540      |
|    total_timesteps  | 166138   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 167      |
|    n_updates        | 31534    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.619     |
| time/               |           |
|    episodes         | 2976      |
|    fps              | 307       |
|    time_elapsed     | 540       |
|    total_timesteps  | 166271    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 31567     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.5     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 2980     |
|    fps              | 307      |
|    time_elapsed     | 541      |
|    total_timesteps  | 166375   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 122      |
|    n_updates        | 31593    |
----------------------------------
Eval num_timesteps=166500, episode_reward=-2966.31 +/- 1287.65
Episode length: 34.24 +/- 7.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.618     |
| time/               |           |
|    total_timesteps  | 166500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 31624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.618     |
| time/               |           |
|    episodes         | 2984      |
|    fps              | 307       |
|    time_elapsed     | 542       |
|    total_timesteps  | 166519    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 31629     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.3     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.617    |
| time/               |          |
|    episodes         | 2988     |
|    fps              | 307      |
|    time_elapsed     | 542      |
|    total_timesteps  | 166661   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 94.4     |
|    n_updates        | 31665    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.616     |
| time/               |           |
|    episodes         | 2992      |
|    fps              | 307       |
|    time_elapsed     | 542       |
|    total_timesteps  | 166824    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 31705     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.616     |
| time/               |           |
|    episodes         | 2996      |
|    fps              | 307       |
|    time_elapsed     | 542       |
|    total_timesteps  | 166952    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.3      |
|    n_updates        | 31737     |
-----------------------------------
Eval num_timesteps=167000, episode_reward=-3124.53 +/- 927.43
Episode length: 35.72 +/- 7.84
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -3.12e+03 |
| rollout/            |           |
|    exploration_rate | 0.615     |
| time/               |           |
|    total_timesteps  | 167000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 50.3      |
|    n_updates        | 31749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.615     |
| time/               |           |
|    episodes         | 3000      |
|    fps              | 307       |
|    time_elapsed     | 544       |
|    total_timesteps  | 167085    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.3      |
|    n_updates        | 31771     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.615     |
| time/               |           |
|    episodes         | 3004      |
|    fps              | 307       |
|    time_elapsed     | 544       |
|    total_timesteps  | 167199    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 31799     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.614     |
| time/               |           |
|    episodes         | 3008      |
|    fps              | 307       |
|    time_elapsed     | 544       |
|    total_timesteps  | 167323    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 174       |
|    n_updates        | 31830     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.613     |
| time/               |           |
|    episodes         | 3012      |
|    fps              | 307       |
|    time_elapsed     | 544       |
|    total_timesteps  | 167433    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 211       |
|    n_updates        | 31858     |
-----------------------------------
Eval num_timesteps=167500, episode_reward=-2723.75 +/- 1150.02
Episode length: 35.84 +/- 5.87
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.613     |
| time/               |           |
|    total_timesteps  | 167500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 255       |
|    n_updates        | 31874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.613     |
| time/               |           |
|    episodes         | 3016      |
|    fps              | 307       |
|    time_elapsed     | 545       |
|    total_timesteps  | 167579    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 31894     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.612     |
| time/               |           |
|    episodes         | 3020      |
|    fps              | 307       |
|    time_elapsed     | 545       |
|    total_timesteps  | 167746    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.8      |
|    n_updates        | 31936     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.612     |
| time/               |           |
|    episodes         | 3024      |
|    fps              | 307       |
|    time_elapsed     | 545       |
|    total_timesteps  | 167866    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 277       |
|    n_updates        | 31966     |
-----------------------------------
Eval num_timesteps=168000, episode_reward=-3076.38 +/- 1028.85
Episode length: 35.24 +/- 6.29
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.611     |
| time/               |           |
|    total_timesteps  | 168000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.1      |
|    n_updates        | 31999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.611     |
| time/               |           |
|    episodes         | 3028      |
|    fps              | 307       |
|    time_elapsed     | 547       |
|    total_timesteps  | 168040    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92        |
|    n_updates        | 32009     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.61      |
| time/               |           |
|    episodes         | 3032      |
|    fps              | 307       |
|    time_elapsed     | 547       |
|    total_timesteps  | 168171    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.3      |
|    n_updates        | 32042     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.61      |
| time/               |           |
|    episodes         | 3036      |
|    fps              | 307       |
|    time_elapsed     | 547       |
|    total_timesteps  | 168291    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 32072     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.1     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.609    |
| time/               |          |
|    episodes         | 3040     |
|    fps              | 307      |
|    time_elapsed     | 547      |
|    total_timesteps  | 168388   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 171      |
|    n_updates        | 32096    |
----------------------------------
Eval num_timesteps=168500, episode_reward=-3109.85 +/- 897.51
Episode length: 35.66 +/- 5.64
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.609     |
| time/               |           |
|    total_timesteps  | 168500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 32124     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.1     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.608    |
| time/               |          |
|    episodes         | 3044     |
|    fps              | 307      |
|    time_elapsed     | 549      |
|    total_timesteps  | 168553   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 101      |
|    n_updates        | 32138    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.608     |
| time/               |           |
|    episodes         | 3048      |
|    fps              | 307       |
|    time_elapsed     | 549       |
|    total_timesteps  | 168717    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 32179     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.607     |
| time/               |           |
|    episodes         | 3052      |
|    fps              | 307       |
|    time_elapsed     | 549       |
|    total_timesteps  | 168915    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 32228     |
-----------------------------------
Eval num_timesteps=169000, episode_reward=-2934.84 +/- 1167.10
Episode length: 36.40 +/- 5.32
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.606     |
| time/               |           |
|    total_timesteps  | 169000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99        |
|    n_updates        | 32249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.606     |
| time/               |           |
|    episodes         | 3056      |
|    fps              | 306       |
|    time_elapsed     | 550       |
|    total_timesteps  | 169037    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 34.7      |
|    n_updates        | 32259     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.606     |
| time/               |           |
|    episodes         | 3060      |
|    fps              | 307       |
|    time_elapsed     | 550       |
|    total_timesteps  | 169153    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95        |
|    n_updates        | 32288     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.605     |
| time/               |           |
|    episodes         | 3064      |
|    fps              | 307       |
|    time_elapsed     | 550       |
|    total_timesteps  | 169272    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 32317     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.605     |
| time/               |           |
|    episodes         | 3068      |
|    fps              | 307       |
|    time_elapsed     | 550       |
|    total_timesteps  | 169373    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 32343     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.604     |
| time/               |           |
|    episodes         | 3072      |
|    fps              | 307       |
|    time_elapsed     | 551       |
|    total_timesteps  | 169497    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 39        |
|    n_updates        | 32374     |
-----------------------------------
Eval num_timesteps=169500, episode_reward=-2975.25 +/- 867.04
Episode length: 35.42 +/- 7.18
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.604     |
| time/               |           |
|    total_timesteps  | 169500    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.603     |
| time/               |           |
|    episodes         | 3076      |
|    fps              | 307       |
|    time_elapsed     | 552       |
|    total_timesteps  | 169643    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 32410     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.603     |
| time/               |           |
|    episodes         | 3080      |
|    fps              | 307       |
|    time_elapsed     | 552       |
|    total_timesteps  | 169804    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 32450     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.602     |
| time/               |           |
|    episodes         | 3084      |
|    fps              | 307       |
|    time_elapsed     | 552       |
|    total_timesteps  | 169915    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.5      |
|    n_updates        | 32478     |
-----------------------------------
Eval num_timesteps=170000, episode_reward=-2625.45 +/- 1373.63
Episode length: 35.14 +/- 7.32
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.63e+03 |
| rollout/            |           |
|    exploration_rate | 0.602     |
| time/               |           |
|    total_timesteps  | 170000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 214       |
|    n_updates        | 32499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.602     |
| time/               |           |
|    episodes         | 3088      |
|    fps              | 306       |
|    time_elapsed     | 554       |
|    total_timesteps  | 170036    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 32508     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.601     |
| time/               |           |
|    episodes         | 3092      |
|    fps              | 307       |
|    time_elapsed     | 554       |
|    total_timesteps  | 170191    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 32547     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.6       |
| time/               |           |
|    episodes         | 3096      |
|    fps              | 307       |
|    time_elapsed     | 554       |
|    total_timesteps  | 170361    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.9      |
|    n_updates        | 32590     |
-----------------------------------
Eval num_timesteps=170500, episode_reward=-2656.25 +/- 1318.82
Episode length: 36.82 +/- 6.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.66e+03 |
| rollout/            |           |
|    exploration_rate | 0.599     |
| time/               |           |
|    total_timesteps  | 170500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 32624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.599     |
| time/               |           |
|    episodes         | 3100      |
|    fps              | 306       |
|    time_elapsed     | 555       |
|    total_timesteps  | 170506    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.6      |
|    n_updates        | 32626     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.599     |
| time/               |           |
|    episodes         | 3104      |
|    fps              | 306       |
|    time_elapsed     | 555       |
|    total_timesteps  | 170630    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 36.1      |
|    n_updates        | 32657     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.598     |
| time/               |           |
|    episodes         | 3108      |
|    fps              | 307       |
|    time_elapsed     | 555       |
|    total_timesteps  | 170765    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.6      |
|    n_updates        | 32691     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.598     |
| time/               |           |
|    episodes         | 3112      |
|    fps              | 307       |
|    time_elapsed     | 556       |
|    total_timesteps  | 170897    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 32724     |
-----------------------------------
Eval num_timesteps=171000, episode_reward=-2848.12 +/- 1105.04
Episode length: 36.04 +/- 6.06
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.597     |
| time/               |           |
|    total_timesteps  | 171000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 32749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.597     |
| time/               |           |
|    episodes         | 3116      |
|    fps              | 306       |
|    time_elapsed     | 557       |
|    total_timesteps  | 171023    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 32755     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.596     |
| time/               |           |
|    episodes         | 3120      |
|    fps              | 306       |
|    time_elapsed     | 557       |
|    total_timesteps  | 171166    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 32791     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.596     |
| time/               |           |
|    episodes         | 3124      |
|    fps              | 307       |
|    time_elapsed     | 557       |
|    total_timesteps  | 171308    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 32826     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.595     |
| time/               |           |
|    episodes         | 3128      |
|    fps              | 307       |
|    time_elapsed     | 557       |
|    total_timesteps  | 171449    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 32862     |
-----------------------------------
Eval num_timesteps=171500, episode_reward=-2921.78 +/- 1020.01
Episode length: 35.32 +/- 7.31
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.595     |
| time/               |           |
|    total_timesteps  | 171500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 32874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.594     |
| time/               |           |
|    episodes         | 3132      |
|    fps              | 306       |
|    time_elapsed     | 559       |
|    total_timesteps  | 171591    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 32897     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.594     |
| time/               |           |
|    episodes         | 3136      |
|    fps              | 307       |
|    time_elapsed     | 559       |
|    total_timesteps  | 171757    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92        |
|    n_updates        | 32939     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.593     |
| time/               |           |
|    episodes         | 3140      |
|    fps              | 307       |
|    time_elapsed     | 559       |
|    total_timesteps  | 171878    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 32969     |
-----------------------------------
Eval num_timesteps=172000, episode_reward=-2902.04 +/- 1056.16
Episode length: 34.64 +/- 5.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.6     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.593    |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 114      |
|    n_updates        | 32999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.592     |
| time/               |           |
|    episodes         | 3144      |
|    fps              | 306       |
|    time_elapsed     | 560       |
|    total_timesteps  | 172018    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 33004     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.592     |
| time/               |           |
|    episodes         | 3148      |
|    fps              | 306       |
|    time_elapsed     | 560       |
|    total_timesteps  | 172183    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 33045     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.591     |
| time/               |           |
|    episodes         | 3152      |
|    fps              | 307       |
|    time_elapsed     | 561       |
|    total_timesteps  | 172312    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 33077     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.591     |
| time/               |           |
|    episodes         | 3156      |
|    fps              | 307       |
|    time_elapsed     | 561       |
|    total_timesteps  | 172431    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.2      |
|    n_updates        | 33107     |
-----------------------------------
Eval num_timesteps=172500, episode_reward=-2684.84 +/- 1254.75
Episode length: 36.40 +/- 6.61
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.59      |
| time/               |           |
|    total_timesteps  | 172500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 252       |
|    n_updates        | 33124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.59      |
| time/               |           |
|    episodes         | 3160      |
|    fps              | 306       |
|    time_elapsed     | 562       |
|    total_timesteps  | 172537    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.6      |
|    n_updates        | 33134     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.589     |
| time/               |           |
|    episodes         | 3164      |
|    fps              | 306       |
|    time_elapsed     | 562       |
|    total_timesteps  | 172660    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.1      |
|    n_updates        | 33164     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.589     |
| time/               |           |
|    episodes         | 3168      |
|    fps              | 307       |
|    time_elapsed     | 562       |
|    total_timesteps  | 172799    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 33199     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.588     |
| time/               |           |
|    episodes         | 3172      |
|    fps              | 307       |
|    time_elapsed     | 562       |
|    total_timesteps  | 172901    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 186       |
|    n_updates        | 33225     |
-----------------------------------
Eval num_timesteps=173000, episode_reward=-3060.03 +/- 901.49
Episode length: 34.86 +/- 5.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -3.06e+03 |
| rollout/            |           |
|    exploration_rate | 0.588     |
| time/               |           |
|    total_timesteps  | 173000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 33249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.588     |
| time/               |           |
|    episodes         | 3176      |
|    fps              | 306       |
|    time_elapsed     | 564       |
|    total_timesteps  | 173028    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 207       |
|    n_updates        | 33256     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.587     |
| time/               |           |
|    episodes         | 3180      |
|    fps              | 306       |
|    time_elapsed     | 564       |
|    total_timesteps  | 173175    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.4      |
|    n_updates        | 33293     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.6     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.587    |
| time/               |          |
|    episodes         | 3184     |
|    fps              | 307      |
|    time_elapsed     | 564      |
|    total_timesteps  | 173276   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 225      |
|    n_updates        | 33318    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.586     |
| time/               |           |
|    episodes         | 3188      |
|    fps              | 307       |
|    time_elapsed     | 564       |
|    total_timesteps  | 173490    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62.9      |
|    n_updates        | 33372     |
-----------------------------------
Eval num_timesteps=173500, episode_reward=-2813.95 +/- 1083.88
Episode length: 35.98 +/- 6.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.586     |
| time/               |           |
|    total_timesteps  | 173500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 29.7      |
|    n_updates        | 33374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.585     |
| time/               |           |
|    episodes         | 3192      |
|    fps              | 306       |
|    time_elapsed     | 565       |
|    total_timesteps  | 173593    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.4      |
|    n_updates        | 33398     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.585     |
| time/               |           |
|    episodes         | 3196      |
|    fps              | 306       |
|    time_elapsed     | 566       |
|    total_timesteps  | 173714    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 33428     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.5     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.584    |
| time/               |          |
|    episodes         | 3200     |
|    fps              | 307      |
|    time_elapsed     | 566      |
|    total_timesteps  | 173859   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 51.2     |
|    n_updates        | 33464    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.583     |
| time/               |           |
|    episodes         | 3204      |
|    fps              | 307       |
|    time_elapsed     | 566       |
|    total_timesteps  | 173996    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 33498     |
-----------------------------------
Eval num_timesteps=174000, episode_reward=-2879.12 +/- 1300.21
Episode length: 34.52 +/- 6.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.583     |
| time/               |           |
|    total_timesteps  | 174000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 33499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.583     |
| time/               |           |
|    episodes         | 3208      |
|    fps              | 306       |
|    time_elapsed     | 567       |
|    total_timesteps  | 174128    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47.6      |
|    n_updates        | 33531     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.582     |
| time/               |           |
|    episodes         | 3212      |
|    fps              | 306       |
|    time_elapsed     | 567       |
|    total_timesteps  | 174245    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 33561     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.582     |
| time/               |           |
|    episodes         | 3216      |
|    fps              | 307       |
|    time_elapsed     | 567       |
|    total_timesteps  | 174367    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.6      |
|    n_updates        | 33591     |
-----------------------------------
Eval num_timesteps=174500, episode_reward=-2550.32 +/- 1377.98
Episode length: 35.42 +/- 6.83
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.55e+03 |
| rollout/            |           |
|    exploration_rate | 0.581     |
| time/               |           |
|    total_timesteps  | 174500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 39.2      |
|    n_updates        | 33624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.581     |
| time/               |           |
|    episodes         | 3220      |
|    fps              | 306       |
|    time_elapsed     | 569       |
|    total_timesteps  | 174514    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 174       |
|    n_updates        | 33628     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.58      |
| time/               |           |
|    episodes         | 3224      |
|    fps              | 306       |
|    time_elapsed     | 569       |
|    total_timesteps  | 174634    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 33658     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.579     |
| time/               |           |
|    episodes         | 3228      |
|    fps              | 306       |
|    time_elapsed     | 569       |
|    total_timesteps  | 174809    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 33702     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.6     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.579    |
| time/               |          |
|    episodes         | 3232     |
|    fps              | 307      |
|    time_elapsed     | 569      |
|    total_timesteps  | 174952   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 36       |
|    n_updates        | 33737    |
----------------------------------
Eval num_timesteps=175000, episode_reward=-2786.81 +/- 1053.97
Episode length: 35.56 +/- 7.08
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.579     |
| time/               |           |
|    total_timesteps  | 175000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.6      |
|    n_updates        | 33749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.578     |
| time/               |           |
|    episodes         | 3236      |
|    fps              | 306       |
|    time_elapsed     | 570       |
|    total_timesteps  | 175102    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.3      |
|    n_updates        | 33775     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.578     |
| time/               |           |
|    episodes         | 3240      |
|    fps              | 306       |
|    time_elapsed     | 570       |
|    total_timesteps  | 175201    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 33800     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.577     |
| time/               |           |
|    episodes         | 3244      |
|    fps              | 307       |
|    time_elapsed     | 571       |
|    total_timesteps  | 175353    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 196       |
|    n_updates        | 33838     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.576     |
| time/               |           |
|    episodes         | 3248      |
|    fps              | 307       |
|    time_elapsed     | 571       |
|    total_timesteps  | 175477    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94        |
|    n_updates        | 33869     |
-----------------------------------
Eval num_timesteps=175500, episode_reward=-2822.53 +/- 1283.73
Episode length: 34.86 +/- 6.45
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.576     |
| time/               |           |
|    total_timesteps  | 175500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 33874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.575     |
| time/               |           |
|    episodes         | 3252      |
|    fps              | 306       |
|    time_elapsed     | 572       |
|    total_timesteps  | 175658    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 33914     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.575     |
| time/               |           |
|    episodes         | 3256      |
|    fps              | 306       |
|    time_elapsed     | 572       |
|    total_timesteps  | 175762    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 49.6      |
|    n_updates        | 33940     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.9     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.574    |
| time/               |          |
|    episodes         | 3260     |
|    fps              | 307      |
|    time_elapsed     | 572      |
|    total_timesteps  | 175930   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 108      |
|    n_updates        | 33982    |
----------------------------------
Eval num_timesteps=176000, episode_reward=-2788.69 +/- 1359.69
Episode length: 35.52 +/- 5.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.574     |
| time/               |           |
|    total_timesteps  | 176000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 33999     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.2     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.573    |
| time/               |          |
|    episodes         | 3264     |
|    fps              | 306      |
|    time_elapsed     | 574      |
|    total_timesteps  | 176077   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 68.9     |
|    n_updates        | 34019    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.573     |
| time/               |           |
|    episodes         | 3268      |
|    fps              | 306       |
|    time_elapsed     | 574       |
|    total_timesteps  | 176210    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 34052     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.572     |
| time/               |           |
|    episodes         | 3272      |
|    fps              | 306       |
|    time_elapsed     | 574       |
|    total_timesteps  | 176335    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 34083     |
-----------------------------------
Eval num_timesteps=176500, episode_reward=-2672.35 +/- 1319.69
Episode length: 35.14 +/- 7.04
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.572     |
| time/               |           |
|    total_timesteps  | 176500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.2      |
|    n_updates        | 34124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.571     |
| time/               |           |
|    episodes         | 3276      |
|    fps              | 306       |
|    time_elapsed     | 575       |
|    total_timesteps  | 176505    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 34126     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.571     |
| time/               |           |
|    episodes         | 3280      |
|    fps              | 306       |
|    time_elapsed     | 575       |
|    total_timesteps  | 176608    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.2      |
|    n_updates        | 34151     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.57     |
| time/               |          |
|    episodes         | 3284     |
|    fps              | 306      |
|    time_elapsed     | 576      |
|    total_timesteps  | 176780   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 20.2     |
|    n_updates        | 34194    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.57      |
| time/               |           |
|    episodes         | 3288      |
|    fps              | 307       |
|    time_elapsed     | 576       |
|    total_timesteps  | 176893    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 34223     |
-----------------------------------
Eval num_timesteps=177000, episode_reward=-2971.71 +/- 1308.68
Episode length: 35.16 +/- 5.83
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.569     |
| time/               |           |
|    total_timesteps  | 177000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 34249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.569     |
| time/               |           |
|    episodes         | 3292      |
|    fps              | 306       |
|    time_elapsed     | 577       |
|    total_timesteps  | 177012    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 34252     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.568     |
| time/               |           |
|    episodes         | 3296      |
|    fps              | 306       |
|    time_elapsed     | 577       |
|    total_timesteps  | 177140    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 34284     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.568     |
| time/               |           |
|    episodes         | 3300      |
|    fps              | 306       |
|    time_elapsed     | 577       |
|    total_timesteps  | 177251    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 34312     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.567     |
| time/               |           |
|    episodes         | 3304      |
|    fps              | 306       |
|    time_elapsed     | 577       |
|    total_timesteps  | 177367    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 34341     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.59e+03 |
|    exploration_rate | 0.567     |
| time/               |           |
|    episodes         | 3308      |
|    fps              | 307       |
|    time_elapsed     | 577       |
|    total_timesteps  | 177488    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 34371     |
-----------------------------------
Eval num_timesteps=177500, episode_reward=-3077.35 +/- 994.51
Episode length: 35.12 +/- 6.30
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.567     |
| time/               |           |
|    total_timesteps  | 177500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 34374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.566     |
| time/               |           |
|    episodes         | 3312      |
|    fps              | 306       |
|    time_elapsed     | 579       |
|    total_timesteps  | 177602    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 36.8      |
|    n_updates        | 34400     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.566     |
| time/               |           |
|    episodes         | 3316      |
|    fps              | 306       |
|    time_elapsed     | 579       |
|    total_timesteps  | 177758    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.4      |
|    n_updates        | 34439     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.565     |
| time/               |           |
|    episodes         | 3320      |
|    fps              | 306       |
|    time_elapsed     | 579       |
|    total_timesteps  | 177861    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.2      |
|    n_updates        | 34465     |
-----------------------------------
Eval num_timesteps=178000, episode_reward=-3117.70 +/- 1033.33
Episode length: 34.00 +/- 6.97
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -3.12e+03 |
| rollout/            |           |
|    exploration_rate | 0.564     |
| time/               |           |
|    total_timesteps  | 178000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 34499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.564     |
| time/               |           |
|    episodes         | 3324      |
|    fps              | 306       |
|    time_elapsed     | 580       |
|    total_timesteps  | 178010    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 34502     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.564     |
| time/               |           |
|    episodes         | 3328      |
|    fps              | 306       |
|    time_elapsed     | 580       |
|    total_timesteps  | 178102    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.8      |
|    n_updates        | 34525     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.564     |
| time/               |           |
|    episodes         | 3332      |
|    fps              | 306       |
|    time_elapsed     | 580       |
|    total_timesteps  | 178182    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 34545     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.563     |
| time/               |           |
|    episodes         | 3336      |
|    fps              | 306       |
|    time_elapsed     | 581       |
|    total_timesteps  | 178291    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 34572     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.563    |
| time/               |          |
|    episodes         | 3340     |
|    fps              | 306      |
|    time_elapsed     | 581      |
|    total_timesteps  | 178391   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 28.5     |
|    n_updates        | 34597    |
----------------------------------
Eval num_timesteps=178500, episode_reward=-3180.92 +/- 1020.85
Episode length: 34.14 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -3.18e+03 |
| rollout/            |           |
|    exploration_rate | 0.562     |
| time/               |           |
|    total_timesteps  | 178500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 34624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.562     |
| time/               |           |
|    episodes         | 3344      |
|    fps              | 306       |
|    time_elapsed     | 582       |
|    total_timesteps  | 178542    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 241       |
|    n_updates        | 34635     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.561     |
| time/               |           |
|    episodes         | 3348      |
|    fps              | 306       |
|    time_elapsed     | 582       |
|    total_timesteps  | 178675    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.6      |
|    n_updates        | 34668     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.1     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.56     |
| time/               |          |
|    episodes         | 3352     |
|    fps              | 306      |
|    time_elapsed     | 582      |
|    total_timesteps  | 178864   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 163      |
|    n_updates        | 34715    |
----------------------------------
Eval num_timesteps=179000, episode_reward=-2715.75 +/- 1225.23
Episode length: 36.02 +/- 5.41
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.56      |
| time/               |           |
|    total_timesteps  | 179000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.3      |
|    n_updates        | 34749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.56      |
| time/               |           |
|    episodes         | 3356      |
|    fps              | 306       |
|    time_elapsed     | 584       |
|    total_timesteps  | 179014    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 34753     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.559     |
| time/               |           |
|    episodes         | 3360      |
|    fps              | 306       |
|    time_elapsed     | 584       |
|    total_timesteps  | 179134    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 34783     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.558     |
| time/               |           |
|    episodes         | 3364      |
|    fps              | 306       |
|    time_elapsed     | 584       |
|    total_timesteps  | 179305    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 34826     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.557     |
| time/               |           |
|    episodes         | 3368      |
|    fps              | 306       |
|    time_elapsed     | 584       |
|    total_timesteps  | 179465    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 34866     |
-----------------------------------
Eval num_timesteps=179500, episode_reward=-2906.35 +/- 1037.75
Episode length: 34.16 +/- 7.13
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.557     |
| time/               |           |
|    total_timesteps  | 179500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 34874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.557     |
| time/               |           |
|    episodes         | 3372      |
|    fps              | 306       |
|    time_elapsed     | 585       |
|    total_timesteps  | 179627    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 34906     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.556     |
| time/               |           |
|    episodes         | 3376      |
|    fps              | 306       |
|    time_elapsed     | 586       |
|    total_timesteps  | 179797    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 34949     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.555     |
| time/               |           |
|    episodes         | 3380      |
|    fps              | 306       |
|    time_elapsed     | 586       |
|    total_timesteps  | 179908    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 34976     |
-----------------------------------
Eval num_timesteps=180000, episode_reward=-2868.79 +/- 1109.66
Episode length: 34.28 +/- 7.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.555     |
| time/               |           |
|    total_timesteps  | 180000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 34999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.58e+03 |
|    exploration_rate | 0.555     |
| time/               |           |
|    episodes         | 3384      |
|    fps              | 306       |
|    time_elapsed     | 587       |
|    total_timesteps  | 180019    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.4      |
|    n_updates        | 35004     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.554     |
| time/               |           |
|    episodes         | 3388      |
|    fps              | 306       |
|    time_elapsed     | 587       |
|    total_timesteps  | 180149    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 35037     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.59e+03 |
|    exploration_rate | 0.554     |
| time/               |           |
|    episodes         | 3392      |
|    fps              | 306       |
|    time_elapsed     | 587       |
|    total_timesteps  | 180260    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 35064     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.553     |
| time/               |           |
|    episodes         | 3396      |
|    fps              | 306       |
|    time_elapsed     | 587       |
|    total_timesteps  | 180416    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 35103     |
-----------------------------------
Eval num_timesteps=180500, episode_reward=-2918.55 +/- 1210.20
Episode length: 33.16 +/- 7.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.2      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.553     |
| time/               |           |
|    total_timesteps  | 180500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 35124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.552     |
| time/               |           |
|    episodes         | 3400      |
|    fps              | 306       |
|    time_elapsed     | 589       |
|    total_timesteps  | 180523    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 35130     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.552     |
| time/               |           |
|    episodes         | 3404      |
|    fps              | 306       |
|    time_elapsed     | 589       |
|    total_timesteps  | 180629    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 35157     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.551     |
| time/               |           |
|    episodes         | 3408      |
|    fps              | 306       |
|    time_elapsed     | 589       |
|    total_timesteps  | 180755    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.7      |
|    n_updates        | 35188     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.551     |
| time/               |           |
|    episodes         | 3412      |
|    fps              | 306       |
|    time_elapsed     | 589       |
|    total_timesteps  | 180867    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 35216     |
-----------------------------------
Eval num_timesteps=181000, episode_reward=-2920.93 +/- 795.82
Episode length: 34.48 +/- 7.09
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.55      |
| time/               |           |
|    total_timesteps  | 181000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 35249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.55      |
| time/               |           |
|    episodes         | 3416      |
|    fps              | 306       |
|    time_elapsed     | 590       |
|    total_timesteps  | 181038    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.9      |
|    n_updates        | 35259     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.549     |
| time/               |           |
|    episodes         | 3420      |
|    fps              | 306       |
|    time_elapsed     | 590       |
|    total_timesteps  | 181198    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 35299     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.549     |
| time/               |           |
|    episodes         | 3424      |
|    fps              | 306       |
|    time_elapsed     | 591       |
|    total_timesteps  | 181313    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.9      |
|    n_updates        | 35328     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.548     |
| time/               |           |
|    episodes         | 3428      |
|    fps              | 306       |
|    time_elapsed     | 591       |
|    total_timesteps  | 181436    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 35358     |
-----------------------------------
Eval num_timesteps=181500, episode_reward=-2824.72 +/- 1023.48
Episode length: 36.18 +/- 5.88
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.548     |
| time/               |           |
|    total_timesteps  | 181500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 35374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.547     |
| time/               |           |
|    episodes         | 3432      |
|    fps              | 306       |
|    time_elapsed     | 592       |
|    total_timesteps  | 181561    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 35390     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.547     |
| time/               |           |
|    episodes         | 3436      |
|    fps              | 306       |
|    time_elapsed     | 592       |
|    total_timesteps  | 181706    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 35426     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.546     |
| time/               |           |
|    episodes         | 3440      |
|    fps              | 306       |
|    time_elapsed     | 592       |
|    total_timesteps  | 181821    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.5      |
|    n_updates        | 35455     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.545     |
| time/               |           |
|    episodes         | 3444      |
|    fps              | 306       |
|    time_elapsed     | 592       |
|    total_timesteps  | 181969    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 31.4      |
|    n_updates        | 35492     |
-----------------------------------
Eval num_timesteps=182000, episode_reward=-2912.54 +/- 1180.02
Episode length: 33.68 +/- 6.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.545     |
| time/               |           |
|    total_timesteps  | 182000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 35499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.545     |
| time/               |           |
|    episodes         | 3448      |
|    fps              | 306       |
|    time_elapsed     | 594       |
|    total_timesteps  | 182154    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 35538     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.544     |
| time/               |           |
|    episodes         | 3452      |
|    fps              | 306       |
|    time_elapsed     | 594       |
|    total_timesteps  | 182302    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 35575     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.543     |
| time/               |           |
|    episodes         | 3456      |
|    fps              | 306       |
|    time_elapsed     | 594       |
|    total_timesteps  | 182463    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 35615     |
-----------------------------------
Eval num_timesteps=182500, episode_reward=-2726.99 +/- 1163.73
Episode length: 35.62 +/- 6.52
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.543     |
| time/               |           |
|    total_timesteps  | 182500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 35624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.543     |
| time/               |           |
|    episodes         | 3460      |
|    fps              | 306       |
|    time_elapsed     | 595       |
|    total_timesteps  | 182591    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 35647     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.542     |
| time/               |           |
|    episodes         | 3464      |
|    fps              | 306       |
|    time_elapsed     | 596       |
|    total_timesteps  | 182738    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.4      |
|    n_updates        | 35684     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.541     |
| time/               |           |
|    episodes         | 3468      |
|    fps              | 306       |
|    time_elapsed     | 596       |
|    total_timesteps  | 182901    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.3      |
|    n_updates        | 35725     |
-----------------------------------
Eval num_timesteps=183000, episode_reward=-2890.88 +/- 956.11
Episode length: 34.64 +/- 6.87
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.541     |
| time/               |           |
|    total_timesteps  | 183000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28.7      |
|    n_updates        | 35749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.54      |
| time/               |           |
|    episodes         | 3472      |
|    fps              | 306       |
|    time_elapsed     | 597       |
|    total_timesteps  | 183051    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 35762     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.54      |
| time/               |           |
|    episodes         | 3476      |
|    fps              | 306       |
|    time_elapsed     | 597       |
|    total_timesteps  | 183169    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 35792     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.539     |
| time/               |           |
|    episodes         | 3480      |
|    fps              | 306       |
|    time_elapsed     | 597       |
|    total_timesteps  | 183302    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 35825     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.538     |
| time/               |           |
|    episodes         | 3484      |
|    fps              | 306       |
|    time_elapsed     | 597       |
|    total_timesteps  | 183447    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 35861     |
-----------------------------------
Eval num_timesteps=183500, episode_reward=-2892.65 +/- 1067.25
Episode length: 34.40 +/- 7.38
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.538     |
| time/               |           |
|    total_timesteps  | 183500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 186       |
|    n_updates        | 35874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.538     |
| time/               |           |
|    episodes         | 3488      |
|    fps              | 306       |
|    time_elapsed     | 599       |
|    total_timesteps  | 183560    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 35889     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.537     |
| time/               |           |
|    episodes         | 3492      |
|    fps              | 306       |
|    time_elapsed     | 599       |
|    total_timesteps  | 183669    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 224       |
|    n_updates        | 35917     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.536     |
| time/               |           |
|    episodes         | 3496      |
|    fps              | 306       |
|    time_elapsed     | 599       |
|    total_timesteps  | 183837    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 35959     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.536     |
| time/               |           |
|    episodes         | 3500      |
|    fps              | 306       |
|    time_elapsed     | 599       |
|    total_timesteps  | 183948    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 35986     |
-----------------------------------
Eval num_timesteps=184000, episode_reward=-2654.21 +/- 1284.16
Episode length: 36.40 +/- 6.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.536     |
| time/               |           |
|    total_timesteps  | 184000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.2      |
|    n_updates        | 35999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.535     |
| time/               |           |
|    episodes         | 3504      |
|    fps              | 306       |
|    time_elapsed     | 600       |
|    total_timesteps  | 184056    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 286       |
|    n_updates        | 36013     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.535     |
| time/               |           |
|    episodes         | 3508      |
|    fps              | 306       |
|    time_elapsed     | 600       |
|    total_timesteps  | 184210    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 36052     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.534     |
| time/               |           |
|    episodes         | 3512      |
|    fps              | 306       |
|    time_elapsed     | 601       |
|    total_timesteps  | 184359    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 36089     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.533     |
| time/               |           |
|    episodes         | 3516      |
|    fps              | 306       |
|    time_elapsed     | 601       |
|    total_timesteps  | 184495    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 36123     |
-----------------------------------
Eval num_timesteps=184500, episode_reward=-2733.14 +/- 1155.62
Episode length: 35.62 +/- 6.54
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.533     |
| time/               |           |
|    total_timesteps  | 184500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 231       |
|    n_updates        | 36124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.532     |
| time/               |           |
|    episodes         | 3520      |
|    fps              | 306       |
|    time_elapsed     | 602       |
|    total_timesteps  | 184675    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 36168     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.532     |
| time/               |           |
|    episodes         | 3524      |
|    fps              | 306       |
|    time_elapsed     | 602       |
|    total_timesteps  | 184777    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 36194     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.531     |
| time/               |           |
|    episodes         | 3528      |
|    fps              | 306       |
|    time_elapsed     | 602       |
|    total_timesteps  | 184912    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 251       |
|    n_updates        | 36227     |
-----------------------------------
Eval num_timesteps=185000, episode_reward=-2975.93 +/- 1014.06
Episode length: 34.24 +/- 6.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.531     |
| time/               |           |
|    total_timesteps  | 185000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 36249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.531     |
| time/               |           |
|    episodes         | 3532      |
|    fps              | 306       |
|    time_elapsed     | 604       |
|    total_timesteps  | 185038    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 36259     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.53      |
| time/               |           |
|    episodes         | 3536      |
|    fps              | 306       |
|    time_elapsed     | 604       |
|    total_timesteps  | 185162    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 36290     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.53      |
| time/               |           |
|    episodes         | 3540      |
|    fps              | 306       |
|    time_elapsed     | 604       |
|    total_timesteps  | 185271    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 36317     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.529     |
| time/               |           |
|    episodes         | 3544      |
|    fps              | 306       |
|    time_elapsed     | 604       |
|    total_timesteps  | 185397    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88.9      |
|    n_updates        | 36349     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.528     |
| time/               |           |
|    episodes         | 3548      |
|    fps              | 306       |
|    time_elapsed     | 604       |
|    total_timesteps  | 185498    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 36374     |
-----------------------------------
Eval num_timesteps=185500, episode_reward=-3105.99 +/- 1018.09
Episode length: 34.60 +/- 5.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.528     |
| time/               |           |
|    total_timesteps  | 185500    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.528     |
| time/               |           |
|    episodes         | 3552      |
|    fps              | 306       |
|    time_elapsed     | 605       |
|    total_timesteps  | 185613    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 36403     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.527     |
| time/               |           |
|    episodes         | 3556      |
|    fps              | 306       |
|    time_elapsed     | 606       |
|    total_timesteps  | 185768    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 36441     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.527     |
| time/               |           |
|    episodes         | 3560      |
|    fps              | 306       |
|    time_elapsed     | 606       |
|    total_timesteps  | 185871    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 36467     |
-----------------------------------
Eval num_timesteps=186000, episode_reward=-2734.33 +/- 1430.30
Episode length: 35.16 +/- 6.49
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.526     |
| time/               |           |
|    total_timesteps  | 186000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.4      |
|    n_updates        | 36499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.526     |
| time/               |           |
|    episodes         | 3564      |
|    fps              | 306       |
|    time_elapsed     | 607       |
|    total_timesteps  | 186040    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.8      |
|    n_updates        | 36509     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.6     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.525    |
| time/               |          |
|    episodes         | 3568     |
|    fps              | 306      |
|    time_elapsed     | 607      |
|    total_timesteps  | 186165   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 134      |
|    n_updates        | 36541    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.525     |
| time/               |           |
|    episodes         | 3572      |
|    fps              | 306       |
|    time_elapsed     | 607       |
|    total_timesteps  | 186277    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 36569     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.524     |
| time/               |           |
|    episodes         | 3576      |
|    fps              | 306       |
|    time_elapsed     | 607       |
|    total_timesteps  | 186413    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 36603     |
-----------------------------------
Eval num_timesteps=186500, episode_reward=-2807.64 +/- 1005.99
Episode length: 36.02 +/- 5.45
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.524     |
| time/               |           |
|    total_timesteps  | 186500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 215       |
|    n_updates        | 36624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.523     |
| time/               |           |
|    episodes         | 3580      |
|    fps              | 306       |
|    time_elapsed     | 609       |
|    total_timesteps  | 186552    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 36637     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.523     |
| time/               |           |
|    episodes         | 3584      |
|    fps              | 306       |
|    time_elapsed     | 609       |
|    total_timesteps  | 186689    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 236       |
|    n_updates        | 36672     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.522     |
| time/               |           |
|    episodes         | 3588      |
|    fps              | 306       |
|    time_elapsed     | 609       |
|    total_timesteps  | 186857    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 36714     |
-----------------------------------
Eval num_timesteps=187000, episode_reward=-2455.79 +/- 1593.37
Episode length: 35.36 +/- 6.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.46e+03 |
| rollout/            |           |
|    exploration_rate | 0.521     |
| time/               |           |
|    total_timesteps  | 187000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 36749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.521     |
| time/               |           |
|    episodes         | 3592      |
|    fps              | 306       |
|    time_elapsed     | 610       |
|    total_timesteps  | 187070    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 36767     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.52      |
| time/               |           |
|    episodes         | 3596      |
|    fps              | 306       |
|    time_elapsed     | 611       |
|    total_timesteps  | 187265    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 36816     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.519     |
| time/               |           |
|    episodes         | 3600      |
|    fps              | 306       |
|    time_elapsed     | 611       |
|    total_timesteps  | 187432    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 56.6      |
|    n_updates        | 36857     |
-----------------------------------
Eval num_timesteps=187500, episode_reward=-2541.29 +/- 1213.21
Episode length: 36.52 +/- 6.89
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -2.54e+03 |
| rollout/            |           |
|    exploration_rate | 0.519     |
| time/               |           |
|    total_timesteps  | 187500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 36874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.518     |
| time/               |           |
|    episodes         | 3604      |
|    fps              | 306       |
|    time_elapsed     | 612       |
|    total_timesteps  | 187567    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 36891     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.518     |
| time/               |           |
|    episodes         | 3608      |
|    fps              | 306       |
|    time_elapsed     | 612       |
|    total_timesteps  | 187706    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 36926     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.517     |
| time/               |           |
|    episodes         | 3612      |
|    fps              | 306       |
|    time_elapsed     | 612       |
|    total_timesteps  | 187835    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 36958     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.516     |
| time/               |           |
|    episodes         | 3616      |
|    fps              | 306       |
|    time_elapsed     | 613       |
|    total_timesteps  | 187960    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 36989     |
-----------------------------------
Eval num_timesteps=188000, episode_reward=-2490.63 +/- 1352.38
Episode length: 36.22 +/- 6.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.49e+03 |
| rollout/            |           |
|    exploration_rate | 0.516     |
| time/               |           |
|    total_timesteps  | 188000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 36999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.516     |
| time/               |           |
|    episodes         | 3620      |
|    fps              | 306       |
|    time_elapsed     | 614       |
|    total_timesteps  | 188060    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 203       |
|    n_updates        | 37014     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.515     |
| time/               |           |
|    episodes         | 3624      |
|    fps              | 306       |
|    time_elapsed     | 614       |
|    total_timesteps  | 188156    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 37038     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.515     |
| time/               |           |
|    episodes         | 3628      |
|    fps              | 306       |
|    time_elapsed     | 614       |
|    total_timesteps  | 188260    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 267       |
|    n_updates        | 37064     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.514     |
| time/               |           |
|    episodes         | 3632      |
|    fps              | 306       |
|    time_elapsed     | 614       |
|    total_timesteps  | 188380    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.8      |
|    n_updates        | 37094     |
-----------------------------------
Eval num_timesteps=188500, episode_reward=-2607.18 +/- 1165.20
Episode length: 35.80 +/- 6.36
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.61e+03 |
| rollout/            |           |
|    exploration_rate | 0.514     |
| time/               |           |
|    total_timesteps  | 188500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 37124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.514     |
| time/               |           |
|    episodes         | 3636      |
|    fps              | 306       |
|    time_elapsed     | 616       |
|    total_timesteps  | 188514    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 37128     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.513     |
| time/               |           |
|    episodes         | 3640      |
|    fps              | 306       |
|    time_elapsed     | 616       |
|    total_timesteps  | 188624    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.1      |
|    n_updates        | 37155     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.92e+03 |
|    exploration_rate | 0.513     |
| time/               |           |
|    episodes         | 3644      |
|    fps              | 306       |
|    time_elapsed     | 616       |
|    total_timesteps  | 188739    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 266       |
|    n_updates        | 37184     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.93e+03 |
|    exploration_rate | 0.512     |
| time/               |           |
|    episodes         | 3648      |
|    fps              | 306       |
|    time_elapsed     | 616       |
|    total_timesteps  | 188958    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66        |
|    n_updates        | 37239     |
-----------------------------------
Eval num_timesteps=189000, episode_reward=-2890.49 +/- 1188.99
Episode length: 34.84 +/- 7.17
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.511     |
| time/               |           |
|    total_timesteps  | 189000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 37249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.511     |
| time/               |           |
|    episodes         | 3652      |
|    fps              | 306       |
|    time_elapsed     | 617       |
|    total_timesteps  | 189068    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 37266     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.51      |
| time/               |           |
|    episodes         | 3656      |
|    fps              | 306       |
|    time_elapsed     | 617       |
|    total_timesteps  | 189221    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 38.3      |
|    n_updates        | 37305     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.509     |
| time/               |           |
|    episodes         | 3660      |
|    fps              | 306       |
|    time_elapsed     | 618       |
|    total_timesteps  | 189370    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99        |
|    n_updates        | 37342     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.509     |
| time/               |           |
|    episodes         | 3664      |
|    fps              | 306       |
|    time_elapsed     | 618       |
|    total_timesteps  | 189472    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 37367     |
-----------------------------------
Eval num_timesteps=189500, episode_reward=-2748.91 +/- 1220.30
Episode length: 35.84 +/- 6.60
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.509     |
| time/               |           |
|    total_timesteps  | 189500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 37374     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.4     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.508    |
| time/               |          |
|    episodes         | 3668     |
|    fps              | 306      |
|    time_elapsed     | 619      |
|    total_timesteps  | 189601   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 174      |
|    n_updates        | 37400    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.508     |
| time/               |           |
|    episodes         | 3672      |
|    fps              | 306       |
|    time_elapsed     | 619       |
|    total_timesteps  | 189744    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 206       |
|    n_updates        | 37435     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.507     |
| time/               |           |
|    episodes         | 3676      |
|    fps              | 306       |
|    time_elapsed     | 619       |
|    total_timesteps  | 189929    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 37482     |
-----------------------------------
Eval num_timesteps=190000, episode_reward=-2817.53 +/- 1298.44
Episode length: 34.04 +/- 7.56
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.506     |
| time/               |           |
|    total_timesteps  | 190000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 37499     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.506    |
| time/               |          |
|    episodes         | 3680     |
|    fps              | 306      |
|    time_elapsed     | 621      |
|    total_timesteps  | 190086   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 95       |
|    n_updates        | 37521    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.505     |
| time/               |           |
|    episodes         | 3684      |
|    fps              | 306       |
|    time_elapsed     | 621       |
|    total_timesteps  | 190204    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 246       |
|    n_updates        | 37550     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.505     |
| time/               |           |
|    episodes         | 3688      |
|    fps              | 306       |
|    time_elapsed     | 621       |
|    total_timesteps  | 190321    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.4      |
|    n_updates        | 37580     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.504     |
| time/               |           |
|    episodes         | 3692      |
|    fps              | 306       |
|    time_elapsed     | 621       |
|    total_timesteps  | 190454    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.9      |
|    n_updates        | 37613     |
-----------------------------------
Eval num_timesteps=190500, episode_reward=-2607.77 +/- 1269.11
Episode length: 36.32 +/- 5.74
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.61e+03 |
| rollout/            |           |
|    exploration_rate | 0.504     |
| time/               |           |
|    total_timesteps  | 190500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 194       |
|    n_updates        | 37624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.503     |
| time/               |           |
|    episodes         | 3696      |
|    fps              | 305       |
|    time_elapsed     | 622       |
|    total_timesteps  | 190594    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 194       |
|    n_updates        | 37648     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.503     |
| time/               |           |
|    episodes         | 3700      |
|    fps              | 306       |
|    time_elapsed     | 623       |
|    total_timesteps  | 190724    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37.3      |
|    n_updates        | 37680     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.502     |
| time/               |           |
|    episodes         | 3704      |
|    fps              | 306       |
|    time_elapsed     | 623       |
|    total_timesteps  | 190878    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 37719     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.501     |
| time/               |           |
|    episodes         | 3708      |
|    fps              | 306       |
|    time_elapsed     | 623       |
|    total_timesteps  | 190991    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 37747     |
-----------------------------------
Eval num_timesteps=191000, episode_reward=-2510.98 +/- 1472.46
Episode length: 35.84 +/- 6.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.51e+03 |
| rollout/            |           |
|    exploration_rate | 0.501     |
| time/               |           |
|    total_timesteps  | 191000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 37749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.501     |
| time/               |           |
|    episodes         | 3712      |
|    fps              | 306       |
|    time_elapsed     | 624       |
|    total_timesteps  | 191164    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 37790     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.5       |
| time/               |           |
|    episodes         | 3716      |
|    fps              | 306       |
|    time_elapsed     | 624       |
|    total_timesteps  | 191285    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 37821     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.5       |
| time/               |           |
|    episodes         | 3720      |
|    fps              | 306       |
|    time_elapsed     | 624       |
|    total_timesteps  | 191390    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 37847     |
-----------------------------------
Eval num_timesteps=191500, episode_reward=-2838.41 +/- 1102.53
Episode length: 36.04 +/- 6.57
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.499     |
| time/               |           |
|    total_timesteps  | 191500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.2      |
|    n_updates        | 37874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.499     |
| time/               |           |
|    episodes         | 3724      |
|    fps              | 305       |
|    time_elapsed     | 626       |
|    total_timesteps  | 191521    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 37880     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.498     |
| time/               |           |
|    episodes         | 3728      |
|    fps              | 305       |
|    time_elapsed     | 626       |
|    total_timesteps  | 191676    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 37918     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.498     |
| time/               |           |
|    episodes         | 3732      |
|    fps              | 306       |
|    time_elapsed     | 626       |
|    total_timesteps  | 191792    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 37947     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.497     |
| time/               |           |
|    episodes         | 3736      |
|    fps              | 306       |
|    time_elapsed     | 626       |
|    total_timesteps  | 191934    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 37983     |
-----------------------------------
Eval num_timesteps=192000, episode_reward=-2841.78 +/- 1219.59
Episode length: 33.98 +/- 8.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.496     |
| time/               |           |
|    total_timesteps  | 192000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 37999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.496     |
| time/               |           |
|    episodes         | 3740      |
|    fps              | 305       |
|    time_elapsed     | 627       |
|    total_timesteps  | 192078    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 38019     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.495     |
| time/               |           |
|    episodes         | 3744      |
|    fps              | 306       |
|    time_elapsed     | 628       |
|    total_timesteps  | 192200    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.8      |
|    n_updates        | 38049     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.495     |
| time/               |           |
|    episodes         | 3748      |
|    fps              | 306       |
|    time_elapsed     | 628       |
|    total_timesteps  | 192358    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 38089     |
-----------------------------------
Eval num_timesteps=192500, episode_reward=-2861.28 +/- 1038.37
Episode length: 35.36 +/- 6.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.494     |
| time/               |           |
|    total_timesteps  | 192500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.3      |
|    n_updates        | 38124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.494     |
| time/               |           |
|    episodes         | 3752      |
|    fps              | 305       |
|    time_elapsed     | 629       |
|    total_timesteps  | 192537    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 31.2      |
|    n_updates        | 38134     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.493     |
| time/               |           |
|    episodes         | 3756      |
|    fps              | 305       |
|    time_elapsed     | 629       |
|    total_timesteps  | 192656    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.1      |
|    n_updates        | 38163     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.493     |
| time/               |           |
|    episodes         | 3760      |
|    fps              | 306       |
|    time_elapsed     | 629       |
|    total_timesteps  | 192785    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 217       |
|    n_updates        | 38196     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.492     |
| time/               |           |
|    episodes         | 3764      |
|    fps              | 306       |
|    time_elapsed     | 629       |
|    total_timesteps  | 192901    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62.9      |
|    n_updates        | 38225     |
-----------------------------------
Eval num_timesteps=193000, episode_reward=-2730.69 +/- 1261.58
Episode length: 34.78 +/- 7.04
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.492     |
| time/               |           |
|    total_timesteps  | 193000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 214       |
|    n_updates        | 38249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.491     |
| time/               |           |
|    episodes         | 3768      |
|    fps              | 305       |
|    time_elapsed     | 631       |
|    total_timesteps  | 193091    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.4      |
|    n_updates        | 38272     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.85e+03 |
|    exploration_rate | 0.49      |
| time/               |           |
|    episodes         | 3772      |
|    fps              | 305       |
|    time_elapsed     | 631       |
|    total_timesteps  | 193236    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 49.3      |
|    n_updates        | 38308     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.49      |
| time/               |           |
|    episodes         | 3776      |
|    fps              | 306       |
|    time_elapsed     | 631       |
|    total_timesteps  | 193396    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 38348     |
-----------------------------------
Eval num_timesteps=193500, episode_reward=-3154.40 +/- 875.77
Episode length: 34.18 +/- 6.64
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -3.15e+03 |
| rollout/            |           |
|    exploration_rate | 0.489     |
| time/               |           |
|    total_timesteps  | 193500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 38374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.489     |
| time/               |           |
|    episodes         | 3780      |
|    fps              | 305       |
|    time_elapsed     | 632       |
|    total_timesteps  | 193529    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91        |
|    n_updates        | 38382     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.488     |
| time/               |           |
|    episodes         | 3784      |
|    fps              | 305       |
|    time_elapsed     | 633       |
|    total_timesteps  | 193666    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 38416     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.488     |
| time/               |           |
|    episodes         | 3788      |
|    fps              | 306       |
|    time_elapsed     | 633       |
|    total_timesteps  | 193796    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.5      |
|    n_updates        | 38448     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.8     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.487    |
| time/               |          |
|    episodes         | 3792     |
|    fps              | 306      |
|    time_elapsed     | 633      |
|    total_timesteps  | 193937   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 27.7     |
|    n_updates        | 38484    |
----------------------------------
Eval num_timesteps=194000, episode_reward=-2975.08 +/- 1041.82
Episode length: 34.96 +/- 6.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.487     |
| time/               |           |
|    total_timesteps  | 194000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 38499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.91e+03 |
|    exploration_rate | 0.486     |
| time/               |           |
|    episodes         | 3796      |
|    fps              | 305       |
|    time_elapsed     | 634       |
|    total_timesteps  | 194067    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 38516     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.6     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.486    |
| time/               |          |
|    episodes         | 3800     |
|    fps              | 305      |
|    time_elapsed     | 634      |
|    total_timesteps  | 194188   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 136      |
|    n_updates        | 38546    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.485     |
| time/               |           |
|    episodes         | 3804      |
|    fps              | 306       |
|    time_elapsed     | 634       |
|    total_timesteps  | 194331    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 213       |
|    n_updates        | 38582     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.484     |
| time/               |           |
|    episodes         | 3808      |
|    fps              | 306       |
|    time_elapsed     | 635       |
|    total_timesteps  | 194491    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 38622     |
-----------------------------------
Eval num_timesteps=194500, episode_reward=-2865.77 +/- 1198.32
Episode length: 35.04 +/- 5.89
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.484     |
| time/               |           |
|    total_timesteps  | 194500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 241       |
|    n_updates        | 38624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.483     |
| time/               |           |
|    episodes         | 3812      |
|    fps              | 305       |
|    time_elapsed     | 636       |
|    total_timesteps  | 194609    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.6      |
|    n_updates        | 38652     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.483     |
| time/               |           |
|    episodes         | 3816      |
|    fps              | 305       |
|    time_elapsed     | 636       |
|    total_timesteps  | 194745    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 38686     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.482     |
| time/               |           |
|    episodes         | 3820      |
|    fps              | 306       |
|    time_elapsed     | 636       |
|    total_timesteps  | 194881    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.3      |
|    n_updates        | 38720     |
-----------------------------------
Eval num_timesteps=195000, episode_reward=-2663.48 +/- 1445.26
Episode length: 33.68 +/- 7.13
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -2.66e+03 |
| rollout/            |           |
|    exploration_rate | 0.482     |
| time/               |           |
|    total_timesteps  | 195000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 38749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.481     |
| time/               |           |
|    episodes         | 3824      |
|    fps              | 305       |
|    time_elapsed     | 637       |
|    total_timesteps  | 195020    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 209       |
|    n_updates        | 38754     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.481     |
| time/               |           |
|    episodes         | 3828      |
|    fps              | 305       |
|    time_elapsed     | 638       |
|    total_timesteps  | 195124    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 38780     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.48      |
| time/               |           |
|    episodes         | 3832      |
|    fps              | 305       |
|    time_elapsed     | 638       |
|    total_timesteps  | 195264    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.1      |
|    n_updates        | 38815     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.48      |
| time/               |           |
|    episodes         | 3836      |
|    fps              | 306       |
|    time_elapsed     | 638       |
|    total_timesteps  | 195382    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 276       |
|    n_updates        | 38845     |
-----------------------------------
Eval num_timesteps=195500, episode_reward=-2938.15 +/- 1054.96
Episode length: 35.00 +/- 6.45
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.479     |
| time/               |           |
|    total_timesteps  | 195500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 38874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.479     |
| time/               |           |
|    episodes         | 3840      |
|    fps              | 305       |
|    time_elapsed     | 639       |
|    total_timesteps  | 195512    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 38877     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.478     |
| time/               |           |
|    episodes         | 3844      |
|    fps              | 305       |
|    time_elapsed     | 639       |
|    total_timesteps  | 195639    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 38909     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.478     |
| time/               |           |
|    episodes         | 3848      |
|    fps              | 305       |
|    time_elapsed     | 639       |
|    total_timesteps  | 195782    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 38945     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.477     |
| time/               |           |
|    episodes         | 3852      |
|    fps              | 306       |
|    time_elapsed     | 640       |
|    total_timesteps  | 195916    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 228       |
|    n_updates        | 38978     |
-----------------------------------
Eval num_timesteps=196000, episode_reward=-2620.04 +/- 1332.02
Episode length: 36.42 +/- 6.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.62e+03 |
| rollout/            |           |
|    exploration_rate | 0.476     |
| time/               |           |
|    total_timesteps  | 196000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.6      |
|    n_updates        | 38999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.476     |
| time/               |           |
|    episodes         | 3856      |
|    fps              | 305       |
|    time_elapsed     | 641       |
|    total_timesteps  | 196042    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 39010     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.476     |
| time/               |           |
|    episodes         | 3860      |
|    fps              | 305       |
|    time_elapsed     | 641       |
|    total_timesteps  | 196162    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 39040     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.475     |
| time/               |           |
|    episodes         | 3864      |
|    fps              | 305       |
|    time_elapsed     | 641       |
|    total_timesteps  | 196299    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.9      |
|    n_updates        | 39074     |
-----------------------------------
Eval num_timesteps=196500, episode_reward=-2876.19 +/- 1211.83
Episode length: 35.20 +/- 6.09
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.474     |
| time/               |           |
|    total_timesteps  | 196500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 39124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.474     |
| time/               |           |
|    episodes         | 3868      |
|    fps              | 305       |
|    time_elapsed     | 643       |
|    total_timesteps  | 196503    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.7      |
|    n_updates        | 39125     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.473     |
| time/               |           |
|    episodes         | 3872      |
|    fps              | 305       |
|    time_elapsed     | 643       |
|    total_timesteps  | 196647    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 39161     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.473     |
| time/               |           |
|    episodes         | 3876      |
|    fps              | 305       |
|    time_elapsed     | 643       |
|    total_timesteps  | 196780    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.6      |
|    n_updates        | 39194     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.472     |
| time/               |           |
|    episodes         | 3880      |
|    fps              | 306       |
|    time_elapsed     | 643       |
|    total_timesteps  | 196921    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 195       |
|    n_updates        | 39230     |
-----------------------------------
Eval num_timesteps=197000, episode_reward=-2805.03 +/- 1244.54
Episode length: 35.82 +/- 6.26
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.471     |
| time/               |           |
|    total_timesteps  | 197000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 240       |
|    n_updates        | 39249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.471     |
| time/               |           |
|    episodes         | 3884      |
|    fps              | 305       |
|    time_elapsed     | 644       |
|    total_timesteps  | 197074    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 40        |
|    n_updates        | 39268     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.47      |
| time/               |           |
|    episodes         | 3888      |
|    fps              | 305       |
|    time_elapsed     | 644       |
|    total_timesteps  | 197235    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 39308     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.469     |
| time/               |           |
|    episodes         | 3892      |
|    fps              | 305       |
|    time_elapsed     | 645       |
|    total_timesteps  | 197393    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 39348     |
-----------------------------------
Eval num_timesteps=197500, episode_reward=-3008.77 +/- 915.79
Episode length: 37.68 +/- 6.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.7      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.469     |
| time/               |           |
|    total_timesteps  | 197500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 39374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.469     |
| time/               |           |
|    episodes         | 3896      |
|    fps              | 305       |
|    time_elapsed     | 646       |
|    total_timesteps  | 197504    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.5      |
|    n_updates        | 39375     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.468     |
| time/               |           |
|    episodes         | 3900      |
|    fps              | 305       |
|    time_elapsed     | 646       |
|    total_timesteps  | 197632    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 39407     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.468     |
| time/               |           |
|    episodes         | 3904      |
|    fps              | 305       |
|    time_elapsed     | 646       |
|    total_timesteps  | 197769    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 238       |
|    n_updates        | 39442     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.467     |
| time/               |           |
|    episodes         | 3908      |
|    fps              | 305       |
|    time_elapsed     | 646       |
|    total_timesteps  | 197887    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 39471     |
-----------------------------------
Eval num_timesteps=198000, episode_reward=-2608.52 +/- 1341.58
Episode length: 35.42 +/- 6.14
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.61e+03 |
| rollout/            |           |
|    exploration_rate | 0.466     |
| time/               |           |
|    total_timesteps  | 198000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 216       |
|    n_updates        | 39499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.466     |
| time/               |           |
|    episodes         | 3912      |
|    fps              | 305       |
|    time_elapsed     | 648       |
|    total_timesteps  | 198006    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37.5      |
|    n_updates        | 39501     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.466     |
| time/               |           |
|    episodes         | 3916      |
|    fps              | 305       |
|    time_elapsed     | 648       |
|    total_timesteps  | 198097    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 39524     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.465     |
| time/               |           |
|    episodes         | 3920      |
|    fps              | 305       |
|    time_elapsed     | 648       |
|    total_timesteps  | 198255    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 39563     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.464     |
| time/               |           |
|    episodes         | 3924      |
|    fps              | 305       |
|    time_elapsed     | 648       |
|    total_timesteps  | 198420    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 39604     |
-----------------------------------
Eval num_timesteps=198500, episode_reward=-2628.98 +/- 1236.54
Episode length: 35.90 +/- 7.11
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.63e+03 |
| rollout/            |           |
|    exploration_rate | 0.464     |
| time/               |           |
|    total_timesteps  | 198500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.5      |
|    n_updates        | 39624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.464     |
| time/               |           |
|    episodes         | 3928      |
|    fps              | 305       |
|    time_elapsed     | 649       |
|    total_timesteps  | 198559    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 39639     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.3     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.463    |
| time/               |          |
|    episodes         | 3932     |
|    fps              | 305      |
|    time_elapsed     | 650      |
|    total_timesteps  | 198693   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 141      |
|    n_updates        | 39673    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.462     |
| time/               |           |
|    episodes         | 3936      |
|    fps              | 305       |
|    time_elapsed     | 650       |
|    total_timesteps  | 198857    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 255       |
|    n_updates        | 39714     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.461     |
| time/               |           |
|    episodes         | 3940      |
|    fps              | 305       |
|    time_elapsed     | 650       |
|    total_timesteps  | 198981    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92        |
|    n_updates        | 39745     |
-----------------------------------
Eval num_timesteps=199000, episode_reward=-2921.51 +/- 1232.03
Episode length: 34.56 +/- 7.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.461     |
| time/               |           |
|    total_timesteps  | 199000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 39749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.461     |
| time/               |           |
|    episodes         | 3944      |
|    fps              | 305       |
|    time_elapsed     | 651       |
|    total_timesteps  | 199083    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 39770     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.46      |
| time/               |           |
|    episodes         | 3948      |
|    fps              | 305       |
|    time_elapsed     | 651       |
|    total_timesteps  | 199215    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.7      |
|    n_updates        | 39803     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.459     |
| time/               |           |
|    episodes         | 3952      |
|    fps              | 305       |
|    time_elapsed     | 651       |
|    total_timesteps  | 199371    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 39842     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.459     |
| time/               |           |
|    episodes         | 3956      |
|    fps              | 305       |
|    time_elapsed     | 652       |
|    total_timesteps  | 199485    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 39871     |
-----------------------------------
Eval num_timesteps=199500, episode_reward=-2784.54 +/- 1078.54
Episode length: 36.30 +/- 5.42
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.459     |
| time/               |           |
|    total_timesteps  | 199500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.3      |
|    n_updates        | 39874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.458     |
| time/               |           |
|    episodes         | 3960      |
|    fps              | 305       |
|    time_elapsed     | 653       |
|    total_timesteps  | 199613    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.6      |
|    n_updates        | 39903     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.457     |
| time/               |           |
|    episodes         | 3964      |
|    fps              | 305       |
|    time_elapsed     | 653       |
|    total_timesteps  | 199756    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.1      |
|    n_updates        | 39938     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.457     |
| time/               |           |
|    episodes         | 3968      |
|    fps              | 305       |
|    time_elapsed     | 653       |
|    total_timesteps  | 199920    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.2      |
|    n_updates        | 39979     |
-----------------------------------
Eval num_timesteps=200000, episode_reward=-2601.75 +/- 1386.81
Episode length: 35.76 +/- 7.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.8     |
|    mean_reward      | -2.6e+03 |
| rollout/            |          |
|    exploration_rate | 0.456    |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 125      |
|    n_updates        | 39999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.456     |
| time/               |           |
|    episodes         | 3972      |
|    fps              | 305       |
|    time_elapsed     | 655       |
|    total_timesteps  | 200056    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 40013     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.455     |
| time/               |           |
|    episodes         | 3976      |
|    fps              | 305       |
|    time_elapsed     | 655       |
|    total_timesteps  | 200165    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.1      |
|    n_updates        | 40041     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.455     |
| time/               |           |
|    episodes         | 3980      |
|    fps              | 305       |
|    time_elapsed     | 655       |
|    total_timesteps  | 200302    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.2      |
|    n_updates        | 40075     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.454     |
| time/               |           |
|    episodes         | 3984      |
|    fps              | 305       |
|    time_elapsed     | 655       |
|    total_timesteps  | 200448    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 40111     |
-----------------------------------
Eval num_timesteps=200500, episode_reward=-2977.27 +/- 959.26
Episode length: 34.86 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.454     |
| time/               |           |
|    total_timesteps  | 200500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 40124     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.4     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.453    |
| time/               |          |
|    episodes         | 3988     |
|    fps              | 305      |
|    time_elapsed     | 656      |
|    total_timesteps  | 200579   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 138      |
|    n_updates        | 40144    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.453     |
| time/               |           |
|    episodes         | 3992      |
|    fps              | 305       |
|    time_elapsed     | 656       |
|    total_timesteps  | 200716    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 301       |
|    n_updates        | 40178     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.5     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.452    |
| time/               |          |
|    episodes         | 3996     |
|    fps              | 305      |
|    time_elapsed     | 657      |
|    total_timesteps  | 200857   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 106      |
|    n_updates        | 40214    |
----------------------------------
Eval num_timesteps=201000, episode_reward=-2915.58 +/- 1009.37
Episode length: 35.56 +/- 6.11
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.451     |
| time/               |           |
|    total_timesteps  | 201000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 283       |
|    n_updates        | 40249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.451     |
| time/               |           |
|    episodes         | 4000      |
|    fps              | 305       |
|    time_elapsed     | 658       |
|    total_timesteps  | 201015    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.2      |
|    n_updates        | 40253     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.45      |
| time/               |           |
|    episodes         | 4004      |
|    fps              | 305       |
|    time_elapsed     | 658       |
|    total_timesteps  | 201182    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.6      |
|    n_updates        | 40295     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.449     |
| time/               |           |
|    episodes         | 4008      |
|    fps              | 305       |
|    time_elapsed     | 658       |
|    total_timesteps  | 201374    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 216       |
|    n_updates        | 40343     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.449     |
| time/               |           |
|    episodes         | 4012      |
|    fps              | 305       |
|    time_elapsed     | 658       |
|    total_timesteps  | 201486    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.4      |
|    n_updates        | 40371     |
-----------------------------------
Eval num_timesteps=201500, episode_reward=-3080.92 +/- 1129.30
Episode length: 32.56 +/- 6.91
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.6      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.449     |
| time/               |           |
|    total_timesteps  | 201500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 40374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.448     |
| time/               |           |
|    episodes         | 4016      |
|    fps              | 305       |
|    time_elapsed     | 660       |
|    total_timesteps  | 201661    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 267       |
|    n_updates        | 40415     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.447     |
| time/               |           |
|    episodes         | 4020      |
|    fps              | 305       |
|    time_elapsed     | 660       |
|    total_timesteps  | 201771    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 40442     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.447     |
| time/               |           |
|    episodes         | 4024      |
|    fps              | 305       |
|    time_elapsed     | 660       |
|    total_timesteps  | 201886    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 40471     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.446     |
| time/               |           |
|    episodes         | 4028      |
|    fps              | 305       |
|    time_elapsed     | 660       |
|    total_timesteps  | 201999    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 186       |
|    n_updates        | 40499     |
-----------------------------------
Eval num_timesteps=202000, episode_reward=-2971.32 +/- 1220.95
Episode length: 33.90 +/- 7.73
----------------------------------
| eval/              |           |
|    mean_ep_length  | 33.9      |
|    mean_reward     | -2.97e+03 |
| time/              |           |
|    total_timesteps | 202000    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.445     |
| time/               |           |
|    episodes         | 4032      |
|    fps              | 305       |
|    time_elapsed     | 661       |
|    total_timesteps  | 202132    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 40532     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.444     |
| time/               |           |
|    episodes         | 4036      |
|    fps              | 305       |
|    time_elapsed     | 662       |
|    total_timesteps  | 202300    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 40574     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.444     |
| time/               |           |
|    episodes         | 4040      |
|    fps              | 305       |
|    time_elapsed     | 662       |
|    total_timesteps  | 202457    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.4      |
|    n_updates        | 40614     |
-----------------------------------
Eval num_timesteps=202500, episode_reward=-2759.74 +/- 1190.57
Episode length: 37.30 +/- 6.01
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.3      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.443     |
| time/               |           |
|    total_timesteps  | 202500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 40624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.443     |
| time/               |           |
|    episodes         | 4044      |
|    fps              | 305       |
|    time_elapsed     | 663       |
|    total_timesteps  | 202592    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 40647     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.442     |
| time/               |           |
|    episodes         | 4048      |
|    fps              | 305       |
|    time_elapsed     | 663       |
|    total_timesteps  | 202746    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 210       |
|    n_updates        | 40686     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.441    |
| time/               |          |
|    episodes         | 4052     |
|    fps              | 305      |
|    time_elapsed     | 663      |
|    total_timesteps  | 202877   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 210      |
|    n_updates        | 40719    |
----------------------------------
Eval num_timesteps=203000, episode_reward=-2721.84 +/- 1014.82
Episode length: 35.02 +/- 7.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.441     |
| time/               |           |
|    total_timesteps  | 203000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.9      |
|    n_updates        | 40749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.441     |
| time/               |           |
|    episodes         | 4056      |
|    fps              | 305       |
|    time_elapsed     | 665       |
|    total_timesteps  | 203014    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 192       |
|    n_updates        | 40753     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.44      |
| time/               |           |
|    episodes         | 4060      |
|    fps              | 305       |
|    time_elapsed     | 665       |
|    total_timesteps  | 203109    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.8      |
|    n_updates        | 40777     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.44      |
| time/               |           |
|    episodes         | 4064      |
|    fps              | 305       |
|    time_elapsed     | 665       |
|    total_timesteps  | 203241    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 40810     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.439     |
| time/               |           |
|    episodes         | 4068      |
|    fps              | 305       |
|    time_elapsed     | 665       |
|    total_timesteps  | 203390    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.4      |
|    n_updates        | 40847     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.438     |
| time/               |           |
|    episodes         | 4072      |
|    fps              | 305       |
|    time_elapsed     | 665       |
|    total_timesteps  | 203499    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 225       |
|    n_updates        | 40874     |
-----------------------------------
Eval num_timesteps=203500, episode_reward=-2939.65 +/- 963.57
Episode length: 34.98 +/- 7.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 35        |
|    mean_reward     | -2.94e+03 |
| time/              |           |
|    total_timesteps | 203500    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.438     |
| time/               |           |
|    episodes         | 4076      |
|    fps              | 305       |
|    time_elapsed     | 667       |
|    total_timesteps  | 203637    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.2      |
|    n_updates        | 40909     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.8     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.437    |
| time/               |          |
|    episodes         | 4080     |
|    fps              | 305      |
|    time_elapsed     | 667      |
|    total_timesteps  | 203782   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 107      |
|    n_updates        | 40945    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.6     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.436    |
| time/               |          |
|    episodes         | 4084     |
|    fps              | 305      |
|    time_elapsed     | 667      |
|    total_timesteps  | 203913   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 118      |
|    n_updates        | 40978    |
----------------------------------
Eval num_timesteps=204000, episode_reward=-2640.96 +/- 1197.49
Episode length: 36.38 +/- 6.06
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.436     |
| time/               |           |
|    total_timesteps  | 204000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.6      |
|    n_updates        | 40999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.436     |
| time/               |           |
|    episodes         | 4088      |
|    fps              | 305       |
|    time_elapsed     | 668       |
|    total_timesteps  | 204026    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 41006     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.435     |
| time/               |           |
|    episodes         | 4092      |
|    fps              | 305       |
|    time_elapsed     | 668       |
|    total_timesteps  | 204159    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 41039     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.434     |
| time/               |           |
|    episodes         | 4096      |
|    fps              | 305       |
|    time_elapsed     | 668       |
|    total_timesteps  | 204297    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 41074     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.434     |
| time/               |           |
|    episodes         | 4100      |
|    fps              | 305       |
|    time_elapsed     | 669       |
|    total_timesteps  | 204394    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 41098     |
-----------------------------------
Eval num_timesteps=204500, episode_reward=-2896.30 +/- 1215.01
Episode length: 33.92 +/- 7.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.9     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.433    |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 80.4     |
|    n_updates        | 41124    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.433     |
| time/               |           |
|    episodes         | 4104      |
|    fps              | 305       |
|    time_elapsed     | 670       |
|    total_timesteps  | 204529    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 41132     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.432     |
| time/               |           |
|    episodes         | 4108      |
|    fps              | 305       |
|    time_elapsed     | 670       |
|    total_timesteps  | 204656    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 41163     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.432     |
| time/               |           |
|    episodes         | 4112      |
|    fps              | 305       |
|    time_elapsed     | 670       |
|    total_timesteps  | 204769    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 41192     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.431     |
| time/               |           |
|    episodes         | 4116      |
|    fps              | 305       |
|    time_elapsed     | 670       |
|    total_timesteps  | 204879    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 41219     |
-----------------------------------
Eval num_timesteps=205000, episode_reward=-2702.80 +/- 1411.39
Episode length: 36.02 +/- 6.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36       |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.431    |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 242      |
|    n_updates        | 41249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.43      |
| time/               |           |
|    episodes         | 4120      |
|    fps              | 305       |
|    time_elapsed     | 672       |
|    total_timesteps  | 205069    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 41267     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.429     |
| time/               |           |
|    episodes         | 4124      |
|    fps              | 305       |
|    time_elapsed     | 672       |
|    total_timesteps  | 205234    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 41308     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.429     |
| time/               |           |
|    episodes         | 4128      |
|    fps              | 305       |
|    time_elapsed     | 672       |
|    total_timesteps  | 205385    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 41346     |
-----------------------------------
Eval num_timesteps=205500, episode_reward=-2688.87 +/- 1306.85
Episode length: 35.84 +/- 7.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.428     |
| time/               |           |
|    total_timesteps  | 205500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 41374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.428     |
| time/               |           |
|    episodes         | 4132      |
|    fps              | 305       |
|    time_elapsed     | 673       |
|    total_timesteps  | 205550    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 41387     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.427     |
| time/               |           |
|    episodes         | 4136      |
|    fps              | 305       |
|    time_elapsed     | 673       |
|    total_timesteps  | 205708    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.9      |
|    n_updates        | 41426     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.426     |
| time/               |           |
|    episodes         | 4140      |
|    fps              | 305       |
|    time_elapsed     | 674       |
|    total_timesteps  | 205835    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 41458     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.6     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.426    |
| time/               |          |
|    episodes         | 4144     |
|    fps              | 305      |
|    time_elapsed     | 674      |
|    total_timesteps  | 205952   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 107      |
|    n_updates        | 41487    |
----------------------------------
Eval num_timesteps=206000, episode_reward=-2496.34 +/- 1409.94
Episode length: 36.00 +/- 7.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36       |
|    mean_reward      | -2.5e+03 |
| rollout/            |          |
|    exploration_rate | 0.425    |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 206      |
|    n_updates        | 41499    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.425     |
| time/               |           |
|    episodes         | 4148      |
|    fps              | 305       |
|    time_elapsed     | 675       |
|    total_timesteps  | 206101    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 30.6      |
|    n_updates        | 41525     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.424     |
| time/               |           |
|    episodes         | 4152      |
|    fps              | 305       |
|    time_elapsed     | 675       |
|    total_timesteps  | 206265    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 41566     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.423     |
| time/               |           |
|    episodes         | 4156      |
|    fps              | 305       |
|    time_elapsed     | 675       |
|    total_timesteps  | 206380    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.3      |
|    n_updates        | 41594     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.423     |
| time/               |           |
|    episodes         | 4160      |
|    fps              | 305       |
|    time_elapsed     | 675       |
|    total_timesteps  | 206476    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 167       |
|    n_updates        | 41618     |
-----------------------------------
Eval num_timesteps=206500, episode_reward=-2921.08 +/- 1274.96
Episode length: 34.62 +/- 7.18
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.423     |
| time/               |           |
|    total_timesteps  | 206500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 41624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.5     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.422    |
| time/               |          |
|    episodes         | 4164     |
|    fps              | 305      |
|    time_elapsed     | 677      |
|    total_timesteps  | 206586   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 50.9     |
|    n_updates        | 41646    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.2     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.422    |
| time/               |          |
|    episodes         | 4168     |
|    fps              | 305      |
|    time_elapsed     | 677      |
|    total_timesteps  | 206715   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 218      |
|    n_updates        | 41678    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.4     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.421    |
| time/               |          |
|    episodes         | 4172     |
|    fps              | 305      |
|    time_elapsed     | 677      |
|    total_timesteps  | 206834   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 92.2     |
|    n_updates        | 41708    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.4     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.42     |
| time/               |          |
|    episodes         | 4176     |
|    fps              | 305      |
|    time_elapsed     | 677      |
|    total_timesteps  | 206973   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 107      |
|    n_updates        | 41743    |
----------------------------------
Eval num_timesteps=207000, episode_reward=-2681.30 +/- 1121.74
Episode length: 35.96 +/- 7.00
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.42      |
| time/               |           |
|    total_timesteps  | 207000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 41749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.419     |
| time/               |           |
|    episodes         | 4180      |
|    fps              | 305       |
|    time_elapsed     | 679       |
|    total_timesteps  | 207143    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 41785     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.5     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.419    |
| time/               |          |
|    episodes         | 4184     |
|    fps              | 305      |
|    time_elapsed     | 679      |
|    total_timesteps  | 207265   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 176      |
|    n_updates        | 41816    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.418     |
| time/               |           |
|    episodes         | 4188      |
|    fps              | 305       |
|    time_elapsed     | 679       |
|    total_timesteps  | 207390    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65.4      |
|    n_updates        | 41847     |
-----------------------------------
Eval num_timesteps=207500, episode_reward=-2998.94 +/- 1263.75
Episode length: 34.50 +/- 6.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.5     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.418    |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 126      |
|    n_updates        | 41874    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.417     |
| time/               |           |
|    episodes         | 4192      |
|    fps              | 304       |
|    time_elapsed     | 680       |
|    total_timesteps  | 207534    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 41883     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.417     |
| time/               |           |
|    episodes         | 4196      |
|    fps              | 305       |
|    time_elapsed     | 680       |
|    total_timesteps  | 207668    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.1      |
|    n_updates        | 41916     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.416     |
| time/               |           |
|    episodes         | 4200      |
|    fps              | 305       |
|    time_elapsed     | 680       |
|    total_timesteps  | 207837    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.6      |
|    n_updates        | 41959     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.415     |
| time/               |           |
|    episodes         | 4204      |
|    fps              | 305       |
|    time_elapsed     | 681       |
|    total_timesteps  | 207986    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 41996     |
-----------------------------------
Eval num_timesteps=208000, episode_reward=-2824.47 +/- 1338.16
Episode length: 34.88 +/- 6.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.415     |
| time/               |           |
|    total_timesteps  | 208000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 41999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.414     |
| time/               |           |
|    episodes         | 4208      |
|    fps              | 304       |
|    time_elapsed     | 682       |
|    total_timesteps  | 208107    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 42026     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.414     |
| time/               |           |
|    episodes         | 4212      |
|    fps              | 305       |
|    time_elapsed     | 682       |
|    total_timesteps  | 208254    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 42063     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.413     |
| time/               |           |
|    episodes         | 4216      |
|    fps              | 305       |
|    time_elapsed     | 682       |
|    total_timesteps  | 208389    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.7      |
|    n_updates        | 42097     |
-----------------------------------
Eval num_timesteps=208500, episode_reward=-2887.47 +/- 1190.82
Episode length: 35.50 +/- 6.93
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.412     |
| time/               |           |
|    total_timesteps  | 208500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 42124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.412     |
| time/               |           |
|    episodes         | 4220      |
|    fps              | 304       |
|    time_elapsed     | 684       |
|    total_timesteps  | 208527    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 42131     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.86e+03 |
|    exploration_rate | 0.411     |
| time/               |           |
|    episodes         | 4224      |
|    fps              | 304       |
|    time_elapsed     | 684       |
|    total_timesteps  | 208666    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 42166     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.411     |
| time/               |           |
|    episodes         | 4228      |
|    fps              | 305       |
|    time_elapsed     | 684       |
|    total_timesteps  | 208795    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 42198     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.41      |
| time/               |           |
|    episodes         | 4232      |
|    fps              | 305       |
|    time_elapsed     | 684       |
|    total_timesteps  | 208952    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 42237     |
-----------------------------------
Eval num_timesteps=209000, episode_reward=-2752.48 +/- 1220.74
Episode length: 35.88 +/- 6.64
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.41      |
| time/               |           |
|    total_timesteps  | 209000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 42249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.409     |
| time/               |           |
|    episodes         | 4236      |
|    fps              | 304       |
|    time_elapsed     | 686       |
|    total_timesteps  | 209083    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 42270     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.409     |
| time/               |           |
|    episodes         | 4240      |
|    fps              | 304       |
|    time_elapsed     | 686       |
|    total_timesteps  | 209197    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 42299     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.408     |
| time/               |           |
|    episodes         | 4244      |
|    fps              | 305       |
|    time_elapsed     | 686       |
|    total_timesteps  | 209334    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64.2      |
|    n_updates        | 42333     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.8     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.407    |
| time/               |          |
|    episodes         | 4248     |
|    fps              | 305      |
|    time_elapsed     | 686      |
|    total_timesteps  | 209480   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 90.5     |
|    n_updates        | 42369    |
----------------------------------
Eval num_timesteps=209500, episode_reward=-2981.70 +/- 1073.13
Episode length: 35.52 +/- 6.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.407     |
| time/               |           |
|    total_timesteps  | 209500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.4      |
|    n_updates        | 42374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.407     |
| time/               |           |
|    episodes         | 4252      |
|    fps              | 304       |
|    time_elapsed     | 687       |
|    total_timesteps  | 209599    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 42399     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.406     |
| time/               |           |
|    episodes         | 4256      |
|    fps              | 304       |
|    time_elapsed     | 687       |
|    total_timesteps  | 209713    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 42428     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.6     |
|    ep_rew_mean      | -3.9e+03 |
|    exploration_rate | 0.405    |
| time/               |          |
|    episodes         | 4260     |
|    fps              | 304      |
|    time_elapsed     | 688      |
|    total_timesteps  | 209834   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 226      |
|    n_updates        | 42458    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.405     |
| time/               |           |
|    episodes         | 4264      |
|    fps              | 305       |
|    time_elapsed     | 688       |
|    total_timesteps  | 209959    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 42489     |
-----------------------------------
Eval num_timesteps=210000, episode_reward=-2568.91 +/- 1246.42
Episode length: 36.12 +/- 6.30
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.57e+03 |
| rollout/            |           |
|    exploration_rate | 0.404     |
| time/               |           |
|    total_timesteps  | 210000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 42499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.404     |
| time/               |           |
|    episodes         | 4268      |
|    fps              | 304       |
|    time_elapsed     | 689       |
|    total_timesteps  | 210083    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 42520     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.403     |
| time/               |           |
|    episodes         | 4272      |
|    fps              | 304       |
|    time_elapsed     | 689       |
|    total_timesteps  | 210215    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 42553     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.403     |
| time/               |           |
|    episodes         | 4276      |
|    fps              | 304       |
|    time_elapsed     | 689       |
|    total_timesteps  | 210354    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 42588     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.89e+03 |
|    exploration_rate | 0.402     |
| time/               |           |
|    episodes         | 4280      |
|    fps              | 305       |
|    time_elapsed     | 689       |
|    total_timesteps  | 210486    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.9      |
|    n_updates        | 42621     |
-----------------------------------
Eval num_timesteps=210500, episode_reward=-2925.49 +/- 1182.50
Episode length: 34.74 +/- 6.98
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.402     |
| time/               |           |
|    total_timesteps  | 210500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 42624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.88e+03 |
|    exploration_rate | 0.401     |
| time/               |           |
|    episodes         | 4284      |
|    fps              | 304       |
|    time_elapsed     | 691       |
|    total_timesteps  | 210627    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 221       |
|    n_updates        | 42656     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.87e+03 |
|    exploration_rate | 0.401     |
| time/               |           |
|    episodes         | 4288      |
|    fps              | 304       |
|    time_elapsed     | 691       |
|    total_timesteps  | 210734    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.2      |
|    n_updates        | 42683     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.4       |
| time/               |           |
|    episodes         | 4292      |
|    fps              | 304       |
|    time_elapsed     | 691       |
|    total_timesteps  | 210836    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 42708     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.399     |
| time/               |           |
|    episodes         | 4296      |
|    fps              | 305       |
|    time_elapsed     | 691       |
|    total_timesteps  | 210980    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77        |
|    n_updates        | 42744     |
-----------------------------------
Eval num_timesteps=211000, episode_reward=-3110.72 +/- 892.86
Episode length: 33.86 +/- 6.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.399     |
| time/               |           |
|    total_timesteps  | 211000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 226       |
|    n_updates        | 42749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.398     |
| time/               |           |
|    episodes         | 4300      |
|    fps              | 304       |
|    time_elapsed     | 692       |
|    total_timesteps  | 211160    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 42789     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.9     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.398    |
| time/               |          |
|    episodes         | 4304     |
|    fps              | 304      |
|    time_elapsed     | 693      |
|    total_timesteps  | 211274   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 121      |
|    n_updates        | 42818    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33       |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.397    |
| time/               |          |
|    episodes         | 4308     |
|    fps              | 304      |
|    time_elapsed     | 693      |
|    total_timesteps  | 211411   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 175      |
|    n_updates        | 42852    |
----------------------------------
Eval num_timesteps=211500, episode_reward=-3304.94 +/- 892.47
Episode length: 33.78 +/- 7.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.8     |
|    mean_reward      | -3.3e+03 |
| rollout/            |          |
|    exploration_rate | 0.397    |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 125      |
|    n_updates        | 42874    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.396     |
| time/               |           |
|    episodes         | 4312      |
|    fps              | 304       |
|    time_elapsed     | 694       |
|    total_timesteps  | 211565    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83        |
|    n_updates        | 42891     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.396     |
| time/               |           |
|    episodes         | 4316      |
|    fps              | 304       |
|    time_elapsed     | 694       |
|    total_timesteps  | 211693    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 49.6      |
|    n_updates        | 42923     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.395     |
| time/               |           |
|    episodes         | 4320      |
|    fps              | 304       |
|    time_elapsed     | 694       |
|    total_timesteps  | 211813    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.4      |
|    n_updates        | 42953     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.394     |
| time/               |           |
|    episodes         | 4324      |
|    fps              | 304       |
|    time_elapsed     | 694       |
|    total_timesteps  | 211930    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 42982     |
-----------------------------------
Eval num_timesteps=212000, episode_reward=-2857.60 +/- 1059.56
Episode length: 34.76 +/- 6.32
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.394     |
| time/               |           |
|    total_timesteps  | 212000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 42999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.393     |
| time/               |           |
|    episodes         | 4328      |
|    fps              | 304       |
|    time_elapsed     | 696       |
|    total_timesteps  | 212089    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 43022     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.393     |
| time/               |           |
|    episodes         | 4332      |
|    fps              | 304       |
|    time_elapsed     | 696       |
|    total_timesteps  | 212185    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.6      |
|    n_updates        | 43046     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32       |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.392    |
| time/               |          |
|    episodes         | 4336     |
|    fps              | 304      |
|    time_elapsed     | 696      |
|    total_timesteps  | 212280   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 174      |
|    n_updates        | 43069    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.392     |
| time/               |           |
|    episodes         | 4340      |
|    fps              | 304       |
|    time_elapsed     | 696       |
|    total_timesteps  | 212447    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 43111     |
-----------------------------------
Eval num_timesteps=212500, episode_reward=-2958.52 +/- 1170.28
Episode length: 34.76 +/- 6.51
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.391     |
| time/               |           |
|    total_timesteps  | 212500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 295       |
|    n_updates        | 43124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.391     |
| time/               |           |
|    episodes         | 4344      |
|    fps              | 304       |
|    time_elapsed     | 697       |
|    total_timesteps  | 212599    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.4      |
|    n_updates        | 43149     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.39      |
| time/               |           |
|    episodes         | 4348      |
|    fps              | 304       |
|    time_elapsed     | 698       |
|    total_timesteps  | 212733    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68        |
|    n_updates        | 43183     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.389     |
| time/               |           |
|    episodes         | 4352      |
|    fps              | 304       |
|    time_elapsed     | 698       |
|    total_timesteps  | 212876    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 43218     |
-----------------------------------
Eval num_timesteps=213000, episode_reward=-2867.24 +/- 1027.42
Episode length: 36.16 +/- 6.09
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.389     |
| time/               |           |
|    total_timesteps  | 213000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 43249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.389     |
| time/               |           |
|    episodes         | 4356      |
|    fps              | 304       |
|    time_elapsed     | 699       |
|    total_timesteps  | 213003    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 43250     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.388     |
| time/               |           |
|    episodes         | 4360      |
|    fps              | 304       |
|    time_elapsed     | 699       |
|    total_timesteps  | 213122    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 405       |
|    n_updates        | 43280     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.387     |
| time/               |           |
|    episodes         | 4364      |
|    fps              | 304       |
|    time_elapsed     | 699       |
|    total_timesteps  | 213261    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 43315     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.386     |
| time/               |           |
|    episodes         | 4368      |
|    fps              | 304       |
|    time_elapsed     | 699       |
|    total_timesteps  | 213442    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 214       |
|    n_updates        | 43360     |
-----------------------------------
Eval num_timesteps=213500, episode_reward=-2895.21 +/- 1047.76
Episode length: 34.06 +/- 6.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.1     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.386    |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 138      |
|    n_updates        | 43374    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.386     |
| time/               |           |
|    episodes         | 4372      |
|    fps              | 304       |
|    time_elapsed     | 701       |
|    total_timesteps  | 213565    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 240       |
|    n_updates        | 43391     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.385     |
| time/               |           |
|    episodes         | 4376      |
|    fps              | 304       |
|    time_elapsed     | 701       |
|    total_timesteps  | 213715    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 209       |
|    n_updates        | 43428     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.384     |
| time/               |           |
|    episodes         | 4380      |
|    fps              | 304       |
|    time_elapsed     | 701       |
|    total_timesteps  | 213832    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 277       |
|    n_updates        | 43457     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.383     |
| time/               |           |
|    episodes         | 4384      |
|    fps              | 304       |
|    time_elapsed     | 701       |
|    total_timesteps  | 213994    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 43498     |
-----------------------------------
Eval num_timesteps=214000, episode_reward=-2630.44 +/- 1251.71
Episode length: 35.86 +/- 5.85
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.63e+03 |
| rollout/            |           |
|    exploration_rate | 0.383     |
| time/               |           |
|    total_timesteps  | 214000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 43499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.383     |
| time/               |           |
|    episodes         | 4388      |
|    fps              | 304       |
|    time_elapsed     | 703       |
|    total_timesteps  | 214088    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 43521     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.382     |
| time/               |           |
|    episodes         | 4392      |
|    fps              | 304       |
|    time_elapsed     | 703       |
|    total_timesteps  | 214210    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 250       |
|    n_updates        | 43552     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.381     |
| time/               |           |
|    episodes         | 4396      |
|    fps              | 304       |
|    time_elapsed     | 703       |
|    total_timesteps  | 214359    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 43589     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.3     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.381    |
| time/               |          |
|    episodes         | 4400     |
|    fps              | 304      |
|    time_elapsed     | 703      |
|    total_timesteps  | 214488   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 131      |
|    n_updates        | 43621    |
----------------------------------
Eval num_timesteps=214500, episode_reward=-3129.61 +/- 1238.43
Episode length: 34.32 +/- 6.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -3.13e+03 |
| rollout/            |           |
|    exploration_rate | 0.381     |
| time/               |           |
|    total_timesteps  | 214500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 43624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.38      |
| time/               |           |
|    episodes         | 4404      |
|    fps              | 304       |
|    time_elapsed     | 704       |
|    total_timesteps  | 214634    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 212       |
|    n_updates        | 43658     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.379     |
| time/               |           |
|    episodes         | 4408      |
|    fps              | 304       |
|    time_elapsed     | 704       |
|    total_timesteps  | 214759    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 43689     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.378     |
| time/               |           |
|    episodes         | 4412      |
|    fps              | 304       |
|    time_elapsed     | 704       |
|    total_timesteps  | 214910    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 43727     |
-----------------------------------
Eval num_timesteps=215000, episode_reward=-2483.40 +/- 1458.19
Episode length: 36.80 +/- 7.00
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.48e+03 |
| rollout/            |           |
|    exploration_rate | 0.378     |
| time/               |           |
|    total_timesteps  | 215000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 43749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.378     |
| time/               |           |
|    episodes         | 4416      |
|    fps              | 304       |
|    time_elapsed     | 706       |
|    total_timesteps  | 215045    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 43761     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.377     |
| time/               |           |
|    episodes         | 4420      |
|    fps              | 304       |
|    time_elapsed     | 706       |
|    total_timesteps  | 215179    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 43794     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.376     |
| time/               |           |
|    episodes         | 4424      |
|    fps              | 304       |
|    time_elapsed     | 706       |
|    total_timesteps  | 215324    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 43830     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.376     |
| time/               |           |
|    episodes         | 4428      |
|    fps              | 304       |
|    time_elapsed     | 706       |
|    total_timesteps  | 215448    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.9      |
|    n_updates        | 43861     |
-----------------------------------
Eval num_timesteps=215500, episode_reward=-2643.41 +/- 1240.90
Episode length: 35.20 +/- 6.32
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.375     |
| time/               |           |
|    total_timesteps  | 215500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 41.3      |
|    n_updates        | 43874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.375     |
| time/               |           |
|    episodes         | 4432      |
|    fps              | 304       |
|    time_elapsed     | 708       |
|    total_timesteps  | 215605    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 205       |
|    n_updates        | 43901     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.374     |
| time/               |           |
|    episodes         | 4436      |
|    fps              | 304       |
|    time_elapsed     | 708       |
|    total_timesteps  | 215717    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 43929     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.374     |
| time/               |           |
|    episodes         | 4440      |
|    fps              | 304       |
|    time_elapsed     | 708       |
|    total_timesteps  | 215836    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 43958     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.373     |
| time/               |           |
|    episodes         | 4444      |
|    fps              | 304       |
|    time_elapsed     | 708       |
|    total_timesteps  | 215952    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 43987     |
-----------------------------------
Eval num_timesteps=216000, episode_reward=-2770.22 +/- 1269.80
Episode length: 33.94 +/- 7.06
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.373     |
| time/               |           |
|    total_timesteps  | 216000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 219       |
|    n_updates        | 43999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.372     |
| time/               |           |
|    episodes         | 4448      |
|    fps              | 304       |
|    time_elapsed     | 709       |
|    total_timesteps  | 216079    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 44019     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.372     |
| time/               |           |
|    episodes         | 4452      |
|    fps              | 304       |
|    time_elapsed     | 709       |
|    total_timesteps  | 216190    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 44047     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.371     |
| time/               |           |
|    episodes         | 4456      |
|    fps              | 304       |
|    time_elapsed     | 710       |
|    total_timesteps  | 216322    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66        |
|    n_updates        | 44080     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.37      |
| time/               |           |
|    episodes         | 4460      |
|    fps              | 304       |
|    time_elapsed     | 710       |
|    total_timesteps  | 216429    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 44107     |
-----------------------------------
Eval num_timesteps=216500, episode_reward=-3046.18 +/- 904.78
Episode length: 34.52 +/- 6.87
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -3.05e+03 |
| rollout/            |           |
|    exploration_rate | 0.37      |
| time/               |           |
|    total_timesteps  | 216500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 44124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.37      |
| time/               |           |
|    episodes         | 4464      |
|    fps              | 304       |
|    time_elapsed     | 711       |
|    total_timesteps  | 216544    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.2      |
|    n_updates        | 44135     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.369     |
| time/               |           |
|    episodes         | 4468      |
|    fps              | 304       |
|    time_elapsed     | 711       |
|    total_timesteps  | 216654    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.1      |
|    n_updates        | 44163     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.369     |
| time/               |           |
|    episodes         | 4472      |
|    fps              | 304       |
|    time_elapsed     | 711       |
|    total_timesteps  | 216768    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 44191     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.1     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.368    |
| time/               |          |
|    episodes         | 4476     |
|    fps              | 304      |
|    time_elapsed     | 711      |
|    total_timesteps  | 216921   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 230      |
|    n_updates        | 44230    |
----------------------------------
Eval num_timesteps=217000, episode_reward=-3042.50 +/- 937.70
Episode length: 34.76 +/- 6.73
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.367     |
| time/               |           |
|    total_timesteps  | 217000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 44249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.367     |
| time/               |           |
|    episodes         | 4480      |
|    fps              | 304       |
|    time_elapsed     | 713       |
|    total_timesteps  | 217054    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 44263     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.366     |
| time/               |           |
|    episodes         | 4484      |
|    fps              | 304       |
|    time_elapsed     | 713       |
|    total_timesteps  | 217213    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 44303     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.366     |
| time/               |           |
|    episodes         | 4488      |
|    fps              | 304       |
|    time_elapsed     | 713       |
|    total_timesteps  | 217323    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 44330     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.365     |
| time/               |           |
|    episodes         | 4492      |
|    fps              | 304       |
|    time_elapsed     | 713       |
|    total_timesteps  | 217458    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.6      |
|    n_updates        | 44364     |
-----------------------------------
Eval num_timesteps=217500, episode_reward=-3102.14 +/- 807.29
Episode length: 33.14 +/- 7.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.1     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.365    |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 198      |
|    n_updates        | 44374    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.364     |
| time/               |           |
|    episodes         | 4496      |
|    fps              | 304       |
|    time_elapsed     | 714       |
|    total_timesteps  | 217562    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 44390     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.364     |
| time/               |           |
|    episodes         | 4500      |
|    fps              | 304       |
|    time_elapsed     | 714       |
|    total_timesteps  | 217672    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 44417     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.5      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.363     |
| time/               |           |
|    episodes         | 4504      |
|    fps              | 304       |
|    time_elapsed     | 715       |
|    total_timesteps  | 217785    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.1      |
|    n_updates        | 44446     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.4      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.362     |
| time/               |           |
|    episodes         | 4508      |
|    fps              | 304       |
|    time_elapsed     | 715       |
|    total_timesteps  | 217898    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.8      |
|    n_updates        | 44474     |
-----------------------------------
Eval num_timesteps=218000, episode_reward=-2447.89 +/- 1166.56
Episode length: 36.50 +/- 5.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -2.45e+03 |
| rollout/            |           |
|    exploration_rate | 0.362     |
| time/               |           |
|    total_timesteps  | 218000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 44499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31        |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.362     |
| time/               |           |
|    episodes         | 4512      |
|    fps              | 304       |
|    time_elapsed     | 716       |
|    total_timesteps  | 218006    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.9      |
|    n_updates        | 44501     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.1      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.361     |
| time/               |           |
|    episodes         | 4516      |
|    fps              | 304       |
|    time_elapsed     | 716       |
|    total_timesteps  | 218159    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.6      |
|    n_updates        | 44539     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.2     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.36     |
| time/               |          |
|    episodes         | 4520     |
|    fps              | 304      |
|    time_elapsed     | 716      |
|    total_timesteps  | 218296   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 254      |
|    n_updates        | 44573    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31        |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.36      |
| time/               |           |
|    episodes         | 4524      |
|    fps              | 304       |
|    time_elapsed     | 716       |
|    total_timesteps  | 218424    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 44605     |
-----------------------------------
Eval num_timesteps=218500, episode_reward=-3064.65 +/- 1093.72
Episode length: 33.36 +/- 7.25
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.4      |
|    mean_reward      | -3.06e+03 |
| rollout/            |           |
|    exploration_rate | 0.359     |
| time/               |           |
|    total_timesteps  | 218500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 44624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.1      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.359     |
| time/               |           |
|    episodes         | 4528      |
|    fps              | 304       |
|    time_elapsed     | 718       |
|    total_timesteps  | 218559    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 44639     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.6      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.358     |
| time/               |           |
|    episodes         | 4532      |
|    fps              | 304       |
|    time_elapsed     | 718       |
|    total_timesteps  | 218660    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.5      |
|    n_updates        | 44664     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.5      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.358     |
| time/               |           |
|    episodes         | 4536      |
|    fps              | 304       |
|    time_elapsed     | 718       |
|    total_timesteps  | 218771    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 44692     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.5      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.357     |
| time/               |           |
|    episodes         | 4540      |
|    fps              | 304       |
|    time_elapsed     | 718       |
|    total_timesteps  | 218886    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 44721     |
-----------------------------------
Eval num_timesteps=219000, episode_reward=-2629.70 +/- 1236.24
Episode length: 37.08 +/- 5.54
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.1      |
|    mean_reward      | -2.63e+03 |
| rollout/            |           |
|    exploration_rate | 0.357     |
| time/               |           |
|    total_timesteps  | 219000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 44749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.6      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.356     |
| time/               |           |
|    episodes         | 4544      |
|    fps              | 304       |
|    time_elapsed     | 719       |
|    total_timesteps  | 219008    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.6      |
|    n_updates        | 44751     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.6      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.356     |
| time/               |           |
|    episodes         | 4548      |
|    fps              | 304       |
|    time_elapsed     | 720       |
|    total_timesteps  | 219142    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.3      |
|    n_updates        | 44785     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.1      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.355     |
| time/               |           |
|    episodes         | 4552      |
|    fps              | 304       |
|    time_elapsed     | 720       |
|    total_timesteps  | 219305    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 25.1      |
|    n_updates        | 44826     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.3      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.354     |
| time/               |           |
|    episodes         | 4556      |
|    fps              | 304       |
|    time_elapsed     | 720       |
|    total_timesteps  | 219449    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 44862     |
-----------------------------------
Eval num_timesteps=219500, episode_reward=-2780.18 +/- 1174.97
Episode length: 33.66 +/- 6.97
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.354     |
| time/               |           |
|    total_timesteps  | 219500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.8      |
|    n_updates        | 44874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.4      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.353     |
| time/               |           |
|    episodes         | 4560      |
|    fps              | 304       |
|    time_elapsed     | 721       |
|    total_timesteps  | 219568    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 44891     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.353     |
| time/               |           |
|    episodes         | 4564      |
|    fps              | 304       |
|    time_elapsed     | 721       |
|    total_timesteps  | 219703    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 44925     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.352     |
| time/               |           |
|    episodes         | 4568      |
|    fps              | 304       |
|    time_elapsed     | 721       |
|    total_timesteps  | 219850    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 44962     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.351     |
| time/               |           |
|    episodes         | 4572      |
|    fps              | 304       |
|    time_elapsed     | 722       |
|    total_timesteps  | 219989    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 44997     |
-----------------------------------
Eval num_timesteps=220000, episode_reward=-2876.81 +/- 977.35
Episode length: 35.16 +/- 6.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.351     |
| time/               |           |
|    total_timesteps  | 220000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 44999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.351     |
| time/               |           |
|    episodes         | 4576      |
|    fps              | 304       |
|    time_elapsed     | 723       |
|    total_timesteps  | 220103    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 45025     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.7      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.35      |
| time/               |           |
|    episodes         | 4580      |
|    fps              | 304       |
|    time_elapsed     | 723       |
|    total_timesteps  | 220220    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 45054     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.3      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.349     |
| time/               |           |
|    episodes         | 4584      |
|    fps              | 304       |
|    time_elapsed     | 723       |
|    total_timesteps  | 220345    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 45086     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.5      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.349     |
| time/               |           |
|    episodes         | 4588      |
|    fps              | 304       |
|    time_elapsed     | 723       |
|    total_timesteps  | 220476    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 45118     |
-----------------------------------
Eval num_timesteps=220500, episode_reward=-2563.40 +/- 1183.62
Episode length: 36.80 +/- 5.90
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.56e+03 |
| rollout/            |           |
|    exploration_rate | 0.348     |
| time/               |           |
|    total_timesteps  | 220500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 35.8      |
|    n_updates        | 45124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.3      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.348     |
| time/               |           |
|    episodes         | 4592      |
|    fps              | 304       |
|    time_elapsed     | 725       |
|    total_timesteps  | 220591    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.4      |
|    n_updates        | 45147     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.3      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.347     |
| time/               |           |
|    episodes         | 4596      |
|    fps              | 304       |
|    time_elapsed     | 725       |
|    total_timesteps  | 220691    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.2      |
|    n_updates        | 45172     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.5      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.347     |
| time/               |           |
|    episodes         | 4600      |
|    fps              | 304       |
|    time_elapsed     | 725       |
|    total_timesteps  | 220823    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 45205     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.346     |
| time/               |           |
|    episodes         | 4604      |
|    fps              | 304       |
|    time_elapsed     | 725       |
|    total_timesteps  | 220942    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 45235     |
-----------------------------------
Eval num_timesteps=221000, episode_reward=-2731.28 +/- 1180.70
Episode length: 35.82 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.346     |
| time/               |           |
|    total_timesteps  | 221000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 45249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.345     |
| time/               |           |
|    episodes         | 4608      |
|    fps              | 304       |
|    time_elapsed     | 726       |
|    total_timesteps  | 221103    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.4      |
|    n_updates        | 45275     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.344     |
| time/               |           |
|    episodes         | 4612      |
|    fps              | 304       |
|    time_elapsed     | 727       |
|    total_timesteps  | 221232    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 45307     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.344     |
| time/               |           |
|    episodes         | 4616      |
|    fps              | 304       |
|    time_elapsed     | 727       |
|    total_timesteps  | 221344    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 275       |
|    n_updates        | 45335     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.7      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.343     |
| time/               |           |
|    episodes         | 4620      |
|    fps              | 304       |
|    time_elapsed     | 727       |
|    total_timesteps  | 221464    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 45365     |
-----------------------------------
Eval num_timesteps=221500, episode_reward=-2743.04 +/- 1213.08
Episode length: 34.70 +/- 7.46
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.74e+03 |
| rollout/            |           |
|    exploration_rate | 0.343     |
| time/               |           |
|    total_timesteps  | 221500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 45374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.5      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.343     |
| time/               |           |
|    episodes         | 4624      |
|    fps              | 304       |
|    time_elapsed     | 728       |
|    total_timesteps  | 221571    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 45392     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.4      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.342     |
| time/               |           |
|    episodes         | 4628      |
|    fps              | 304       |
|    time_elapsed     | 728       |
|    total_timesteps  | 221697    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 180       |
|    n_updates        | 45424     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.5      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.341     |
| time/               |           |
|    episodes         | 4632      |
|    fps              | 304       |
|    time_elapsed     | 728       |
|    total_timesteps  | 221812    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 45452     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.7      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.341     |
| time/               |           |
|    episodes         | 4636      |
|    fps              | 304       |
|    time_elapsed     | 728       |
|    total_timesteps  | 221945    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 45486     |
-----------------------------------
Eval num_timesteps=222000, episode_reward=-2546.67 +/- 1404.17
Episode length: 35.50 +/- 6.39
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.55e+03 |
| rollout/            |           |
|    exploration_rate | 0.34      |
| time/               |           |
|    total_timesteps  | 222000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 45499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.34      |
| time/               |           |
|    episodes         | 4640      |
|    fps              | 304       |
|    time_elapsed     | 730       |
|    total_timesteps  | 222049    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 45512     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.339     |
| time/               |           |
|    episodes         | 4644      |
|    fps              | 304       |
|    time_elapsed     | 730       |
|    total_timesteps  | 222201    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52        |
|    n_updates        | 45550     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.8      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.339     |
| time/               |           |
|    episodes         | 4648      |
|    fps              | 304       |
|    time_elapsed     | 730       |
|    total_timesteps  | 222318    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.5      |
|    n_updates        | 45579     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.6     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.338    |
| time/               |          |
|    episodes         | 4652     |
|    fps              | 304      |
|    time_elapsed     | 730      |
|    total_timesteps  | 222463   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 162      |
|    n_updates        | 45615    |
----------------------------------
Eval num_timesteps=222500, episode_reward=-2808.36 +/- 1129.82
Episode length: 34.40 +/- 7.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.338     |
| time/               |           |
|    total_timesteps  | 222500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 45624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.4     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.337    |
| time/               |          |
|    episodes         | 4656     |
|    fps              | 304      |
|    time_elapsed     | 731      |
|    total_timesteps  | 222584   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 110      |
|    n_updates        | 45645    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.336     |
| time/               |           |
|    episodes         | 4660      |
|    fps              | 304       |
|    time_elapsed     | 732       |
|    total_timesteps  | 222732    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 45682     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.336     |
| time/               |           |
|    episodes         | 4664      |
|    fps              | 304       |
|    time_elapsed     | 732       |
|    total_timesteps  | 222862    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 261       |
|    n_updates        | 45715     |
-----------------------------------
Eval num_timesteps=223000, episode_reward=-2764.77 +/- 1175.32
Episode length: 35.40 +/- 6.28
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.335     |
| time/               |           |
|    total_timesteps  | 223000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.4      |
|    n_updates        | 45749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.5      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.335     |
| time/               |           |
|    episodes         | 4668      |
|    fps              | 303       |
|    time_elapsed     | 733       |
|    total_timesteps  | 223003    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 45750     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.3      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.334     |
| time/               |           |
|    episodes         | 4672      |
|    fps              | 304       |
|    time_elapsed     | 733       |
|    total_timesteps  | 223119    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 45779     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.7      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.333     |
| time/               |           |
|    episodes         | 4676      |
|    fps              | 304       |
|    time_elapsed     | 733       |
|    total_timesteps  | 223272    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 45817     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.5      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.333     |
| time/               |           |
|    episodes         | 4680      |
|    fps              | 304       |
|    time_elapsed     | 734       |
|    total_timesteps  | 223374    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 45843     |
-----------------------------------
Eval num_timesteps=223500, episode_reward=-2492.05 +/- 1394.82
Episode length: 36.46 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -2.49e+03 |
| rollout/            |           |
|    exploration_rate | 0.332     |
| time/               |           |
|    total_timesteps  | 223500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.1      |
|    n_updates        | 45874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.332     |
| time/               |           |
|    episodes         | 4684      |
|    fps              | 303       |
|    time_elapsed     | 735       |
|    total_timesteps  | 223500    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.5      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.331     |
| time/               |           |
|    episodes         | 4688      |
|    fps              | 304       |
|    time_elapsed     | 735       |
|    total_timesteps  | 223624    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81        |
|    n_updates        | 45905     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.331     |
| time/               |           |
|    episodes         | 4692      |
|    fps              | 304       |
|    time_elapsed     | 735       |
|    total_timesteps  | 223753    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 45938     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.33      |
| time/               |           |
|    episodes         | 4696      |
|    fps              | 304       |
|    time_elapsed     | 735       |
|    total_timesteps  | 223888    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 258       |
|    n_updates        | 45971     |
-----------------------------------
Eval num_timesteps=224000, episode_reward=-2849.99 +/- 1161.27
Episode length: 35.84 +/- 6.35
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.329     |
| time/               |           |
|    total_timesteps  | 224000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.5      |
|    n_updates        | 45999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.329     |
| time/               |           |
|    episodes         | 4700      |
|    fps              | 303       |
|    time_elapsed     | 737       |
|    total_timesteps  | 224020    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 46004     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.328     |
| time/               |           |
|    episodes         | 4704      |
|    fps              | 304       |
|    time_elapsed     | 737       |
|    total_timesteps  | 224171    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 46042     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.8      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.328     |
| time/               |           |
|    episodes         | 4708      |
|    fps              | 304       |
|    time_elapsed     | 737       |
|    total_timesteps  | 224281    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 46070     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.7      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.327     |
| time/               |           |
|    episodes         | 4712      |
|    fps              | 304       |
|    time_elapsed     | 737       |
|    total_timesteps  | 224402    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 46100     |
-----------------------------------
Eval num_timesteps=224500, episode_reward=-3012.02 +/- 1101.19
Episode length: 34.60 +/- 7.35
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.327     |
| time/               |           |
|    total_timesteps  | 224500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 46124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.326     |
| time/               |           |
|    episodes         | 4716      |
|    fps              | 304       |
|    time_elapsed     | 739       |
|    total_timesteps  | 224657    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 46164     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.325     |
| time/               |           |
|    episodes         | 4720      |
|    fps              | 304       |
|    time_elapsed     | 739       |
|    total_timesteps  | 224771    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 46192     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.324     |
| time/               |           |
|    episodes         | 4724      |
|    fps              | 304       |
|    time_elapsed     | 739       |
|    total_timesteps  | 224908    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 46226     |
-----------------------------------
Eval num_timesteps=225000, episode_reward=-2601.91 +/- 1304.41
Episode length: 35.30 +/- 6.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.3     |
|    mean_reward      | -2.6e+03 |
| rollout/            |          |
|    exploration_rate | 0.324    |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 180      |
|    n_updates        | 46249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.324     |
| time/               |           |
|    episodes         | 4728      |
|    fps              | 303       |
|    time_elapsed     | 740       |
|    total_timesteps  | 225035    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.7      |
|    n_updates        | 46258     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.323     |
| time/               |           |
|    episodes         | 4732      |
|    fps              | 303       |
|    time_elapsed     | 740       |
|    total_timesteps  | 225140    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.5      |
|    n_updates        | 46284     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.323     |
| time/               |           |
|    episodes         | 4736      |
|    fps              | 304       |
|    time_elapsed     | 740       |
|    total_timesteps  | 225254    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.6      |
|    n_updates        | 46313     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.3     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.322    |
| time/               |          |
|    episodes         | 4740     |
|    fps              | 304      |
|    time_elapsed     | 741      |
|    total_timesteps  | 225380   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 137      |
|    n_updates        | 46344    |
----------------------------------
Eval num_timesteps=225500, episode_reward=-2839.10 +/- 1135.22
Episode length: 35.62 +/- 7.30
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.321     |
| time/               |           |
|    total_timesteps  | 225500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 278       |
|    n_updates        | 46374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.321     |
| time/               |           |
|    episodes         | 4744      |
|    fps              | 303       |
|    time_elapsed     | 742       |
|    total_timesteps  | 225512    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 46377     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.32      |
| time/               |           |
|    episodes         | 4748      |
|    fps              | 303       |
|    time_elapsed     | 742       |
|    total_timesteps  | 225643    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 46410     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.32      |
| time/               |           |
|    episodes         | 4752      |
|    fps              | 303       |
|    time_elapsed     | 742       |
|    total_timesteps  | 225768    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 46441     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.319     |
| time/               |           |
|    episodes         | 4756      |
|    fps              | 304       |
|    time_elapsed     | 743       |
|    total_timesteps  | 225888    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 46471     |
-----------------------------------
Eval num_timesteps=226000, episode_reward=-3089.27 +/- 771.12
Episode length: 34.34 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -3.09e+03 |
| rollout/            |           |
|    exploration_rate | 0.318     |
| time/               |           |
|    total_timesteps  | 226000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 46499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.318     |
| time/               |           |
|    episodes         | 4760      |
|    fps              | 303       |
|    time_elapsed     | 744       |
|    total_timesteps  | 226035    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 46508     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.318     |
| time/               |           |
|    episodes         | 4764      |
|    fps              | 303       |
|    time_elapsed     | 744       |
|    total_timesteps  | 226155    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 46538     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.8     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.317    |
| time/               |          |
|    episodes         | 4768     |
|    fps              | 303      |
|    time_elapsed     | 744      |
|    total_timesteps  | 226287   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 155      |
|    n_updates        | 46571    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.316     |
| time/               |           |
|    episodes         | 4772      |
|    fps              | 303       |
|    time_elapsed     | 744       |
|    total_timesteps  | 226393    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 46598     |
-----------------------------------
Eval num_timesteps=226500, episode_reward=-2684.32 +/- 1440.24
Episode length: 34.00 +/- 7.02
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.316     |
| time/               |           |
|    total_timesteps  | 226500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 46624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.316     |
| time/               |           |
|    episodes         | 4776      |
|    fps              | 303       |
|    time_elapsed     | 746       |
|    total_timesteps  | 226511    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 311       |
|    n_updates        | 46627     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.315     |
| time/               |           |
|    episodes         | 4780      |
|    fps              | 303       |
|    time_elapsed     | 746       |
|    total_timesteps  | 226674    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 46668     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.314     |
| time/               |           |
|    episodes         | 4784      |
|    fps              | 303       |
|    time_elapsed     | 746       |
|    total_timesteps  | 226810    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 46702     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.313     |
| time/               |           |
|    episodes         | 4788      |
|    fps              | 303       |
|    time_elapsed     | 746       |
|    total_timesteps  | 226952    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 221       |
|    n_updates        | 46737     |
-----------------------------------
Eval num_timesteps=227000, episode_reward=-3116.35 +/- 1075.53
Episode length: 33.18 +/- 6.19
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.2      |
|    mean_reward      | -3.12e+03 |
| rollout/            |           |
|    exploration_rate | 0.313     |
| time/               |           |
|    total_timesteps  | 227000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 46749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.312     |
| time/               |           |
|    episodes         | 4792      |
|    fps              | 303       |
|    time_elapsed     | 747       |
|    total_timesteps  | 227076    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 46768     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.312     |
| time/               |           |
|    episodes         | 4796      |
|    fps              | 303       |
|    time_elapsed     | 748       |
|    total_timesteps  | 227204    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 46800     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.311     |
| time/               |           |
|    episodes         | 4800      |
|    fps              | 303       |
|    time_elapsed     | 748       |
|    total_timesteps  | 227318    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 46829     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.311     |
| time/               |           |
|    episodes         | 4804      |
|    fps              | 303       |
|    time_elapsed     | 748       |
|    total_timesteps  | 227435    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 198       |
|    n_updates        | 46858     |
-----------------------------------
Eval num_timesteps=227500, episode_reward=-2793.53 +/- 1328.95
Episode length: 32.82 +/- 8.08
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.8      |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.31      |
| time/               |           |
|    total_timesteps  | 227500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 46874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.31      |
| time/               |           |
|    episodes         | 4808      |
|    fps              | 303       |
|    time_elapsed     | 749       |
|    total_timesteps  | 227589    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 246       |
|    n_updates        | 46897     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.309     |
| time/               |           |
|    episodes         | 4812      |
|    fps              | 303       |
|    time_elapsed     | 749       |
|    total_timesteps  | 227718    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 46929     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.308     |
| time/               |           |
|    episodes         | 4816      |
|    fps              | 303       |
|    time_elapsed     | 749       |
|    total_timesteps  | 227851    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 27.3      |
|    n_updates        | 46962     |
-----------------------------------
Eval num_timesteps=228000, episode_reward=-2619.18 +/- 1308.78
Episode length: 36.28 +/- 6.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.62e+03 |
| rollout/            |           |
|    exploration_rate | 0.307     |
| time/               |           |
|    total_timesteps  | 228000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 299       |
|    n_updates        | 46999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.307     |
| time/               |           |
|    episodes         | 4820      |
|    fps              | 303       |
|    time_elapsed     | 751       |
|    total_timesteps  | 228014    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.5      |
|    n_updates        | 47003     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.306     |
| time/               |           |
|    episodes         | 4824      |
|    fps              | 303       |
|    time_elapsed     | 751       |
|    total_timesteps  | 228187    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 47046     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.306     |
| time/               |           |
|    episodes         | 4828      |
|    fps              | 303       |
|    time_elapsed     | 751       |
|    total_timesteps  | 228321    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 47080     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.2     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.305    |
| time/               |          |
|    episodes         | 4832     |
|    fps              | 303      |
|    time_elapsed     | 751      |
|    total_timesteps  | 228461   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 105      |
|    n_updates        | 47115    |
----------------------------------
Eval num_timesteps=228500, episode_reward=-2730.97 +/- 1228.19
Episode length: 36.68 +/- 6.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.7      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.305     |
| time/               |           |
|    total_timesteps  | 228500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 205       |
|    n_updates        | 47124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.304     |
| time/               |           |
|    episodes         | 4836      |
|    fps              | 303       |
|    time_elapsed     | 753       |
|    total_timesteps  | 228589    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 47147     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.304     |
| time/               |           |
|    episodes         | 4840      |
|    fps              | 303       |
|    time_elapsed     | 753       |
|    total_timesteps  | 228704    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.4      |
|    n_updates        | 47175     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.303     |
| time/               |           |
|    episodes         | 4844      |
|    fps              | 303       |
|    time_elapsed     | 753       |
|    total_timesteps  | 228848    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 47211     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.302     |
| time/               |           |
|    episodes         | 4848      |
|    fps              | 303       |
|    time_elapsed     | 753       |
|    total_timesteps  | 228932    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.3      |
|    n_updates        | 47232     |
-----------------------------------
Eval num_timesteps=229000, episode_reward=-2521.73 +/- 1314.29
Episode length: 35.80 +/- 6.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.52e+03 |
| rollout/            |           |
|    exploration_rate | 0.302     |
| time/               |           |
|    total_timesteps  | 229000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 47249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.302     |
| time/               |           |
|    episodes         | 4852      |
|    fps              | 303       |
|    time_elapsed     | 754       |
|    total_timesteps  | 229058    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.6      |
|    n_updates        | 47264     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.301     |
| time/               |           |
|    episodes         | 4856      |
|    fps              | 303       |
|    time_elapsed     | 755       |
|    total_timesteps  | 229180    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 337       |
|    n_updates        | 47294     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.3       |
| time/               |           |
|    episodes         | 4860      |
|    fps              | 303       |
|    time_elapsed     | 755       |
|    total_timesteps  | 229328    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 47331     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.299     |
| time/               |           |
|    episodes         | 4864      |
|    fps              | 303       |
|    time_elapsed     | 755       |
|    total_timesteps  | 229464    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.6      |
|    n_updates        | 47365     |
-----------------------------------
Eval num_timesteps=229500, episode_reward=-2589.81 +/- 1239.43
Episode length: 35.54 +/- 5.96
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.59e+03 |
| rollout/            |           |
|    exploration_rate | 0.299     |
| time/               |           |
|    total_timesteps  | 229500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.6      |
|    n_updates        | 47374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.299     |
| time/               |           |
|    episodes         | 4868      |
|    fps              | 303       |
|    time_elapsed     | 756       |
|    total_timesteps  | 229591    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.8      |
|    n_updates        | 47397     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.298     |
| time/               |           |
|    episodes         | 4872      |
|    fps              | 303       |
|    time_elapsed     | 756       |
|    total_timesteps  | 229685    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 47421     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.297     |
| time/               |           |
|    episodes         | 4876      |
|    fps              | 303       |
|    time_elapsed     | 756       |
|    total_timesteps  | 229823    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 180       |
|    n_updates        | 47455     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.296     |
| time/               |           |
|    episodes         | 4880      |
|    fps              | 303       |
|    time_elapsed     | 757       |
|    total_timesteps  | 229970    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 47492     |
-----------------------------------
Eval num_timesteps=230000, episode_reward=-2716.92 +/- 1112.86
Episode length: 35.96 +/- 6.45
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.296     |
| time/               |           |
|    total_timesteps  | 230000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 47499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.296     |
| time/               |           |
|    episodes         | 4884      |
|    fps              | 303       |
|    time_elapsed     | 758       |
|    total_timesteps  | 230093    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.8      |
|    n_updates        | 47523     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.295     |
| time/               |           |
|    episodes         | 4888      |
|    fps              | 303       |
|    time_elapsed     | 758       |
|    total_timesteps  | 230232    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94        |
|    n_updates        | 47557     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.294     |
| time/               |           |
|    episodes         | 4892      |
|    fps              | 303       |
|    time_elapsed     | 758       |
|    total_timesteps  | 230350    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88.8      |
|    n_updates        | 47587     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.294     |
| time/               |           |
|    episodes         | 4896      |
|    fps              | 303       |
|    time_elapsed     | 758       |
|    total_timesteps  | 230477    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52.3      |
|    n_updates        | 47619     |
-----------------------------------
Eval num_timesteps=230500, episode_reward=-2886.44 +/- 1052.00
Episode length: 35.68 +/- 6.22
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.294     |
| time/               |           |
|    total_timesteps  | 230500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86        |
|    n_updates        | 47624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.293     |
| time/               |           |
|    episodes         | 4900      |
|    fps              | 303       |
|    time_elapsed     | 760       |
|    total_timesteps  | 230595    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 259       |
|    n_updates        | 47648     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.292     |
| time/               |           |
|    episodes         | 4904      |
|    fps              | 303       |
|    time_elapsed     | 760       |
|    total_timesteps  | 230730    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 47682     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.84e+03 |
|    exploration_rate | 0.292     |
| time/               |           |
|    episodes         | 4908      |
|    fps              | 303       |
|    time_elapsed     | 760       |
|    total_timesteps  | 230864    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 47715     |
-----------------------------------
Eval num_timesteps=231000, episode_reward=-2812.57 +/- 1271.52
Episode length: 35.38 +/- 7.22
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.291     |
| time/               |           |
|    total_timesteps  | 231000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.1      |
|    n_updates        | 47749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.291     |
| time/               |           |
|    episodes         | 4912      |
|    fps              | 303       |
|    time_elapsed     | 761       |
|    total_timesteps  | 231000    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.6     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.29     |
| time/               |          |
|    episodes         | 4916     |
|    fps              | 303      |
|    time_elapsed     | 761      |
|    total_timesteps  | 231114   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 130      |
|    n_updates        | 47778    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.289     |
| time/               |           |
|    episodes         | 4920      |
|    fps              | 303       |
|    time_elapsed     | 762       |
|    total_timesteps  | 231248    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.5      |
|    n_updates        | 47811     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.288     |
| time/               |           |
|    episodes         | 4924      |
|    fps              | 303       |
|    time_elapsed     | 762       |
|    total_timesteps  | 231412    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 221       |
|    n_updates        | 47852     |
-----------------------------------
Eval num_timesteps=231500, episode_reward=-2467.82 +/- 1541.38
Episode length: 35.88 +/- 6.72
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.47e+03 |
| rollout/            |           |
|    exploration_rate | 0.288     |
| time/               |           |
|    total_timesteps  | 231500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 47874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.288     |
| time/               |           |
|    episodes         | 4928      |
|    fps              | 303       |
|    time_elapsed     | 763       |
|    total_timesteps  | 231532    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 47882     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.8      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.287     |
| time/               |           |
|    episodes         | 4932      |
|    fps              | 303       |
|    time_elapsed     | 763       |
|    total_timesteps  | 231644    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 47910     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.286     |
| time/               |           |
|    episodes         | 4936      |
|    fps              | 303       |
|    time_elapsed     | 763       |
|    total_timesteps  | 231781    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 219       |
|    n_updates        | 47945     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.286     |
| time/               |           |
|    episodes         | 4940      |
|    fps              | 303       |
|    time_elapsed     | 764       |
|    total_timesteps  | 231914    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 247       |
|    n_updates        | 47978     |
-----------------------------------
Eval num_timesteps=232000, episode_reward=-3098.21 +/- 1050.08
Episode length: 33.32 +/- 7.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.3     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.285    |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 81.4     |
|    n_updates        | 47999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.285     |
| time/               |           |
|    episodes         | 4944      |
|    fps              | 303       |
|    time_elapsed     | 765       |
|    total_timesteps  | 232078    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 213       |
|    n_updates        | 48019     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.284     |
| time/               |           |
|    episodes         | 4948      |
|    fps              | 303       |
|    time_elapsed     | 765       |
|    total_timesteps  | 232197    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 48049     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.283     |
| time/               |           |
|    episodes         | 4952      |
|    fps              | 303       |
|    time_elapsed     | 765       |
|    total_timesteps  | 232315    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62        |
|    n_updates        | 48078     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.283     |
| time/               |           |
|    episodes         | 4956      |
|    fps              | 303       |
|    time_elapsed     | 765       |
|    total_timesteps  | 232454    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 48113     |
-----------------------------------
Eval num_timesteps=232500, episode_reward=-2481.78 +/- 1268.24
Episode length: 35.90 +/- 7.51
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.48e+03 |
| rollout/            |           |
|    exploration_rate | 0.282     |
| time/               |           |
|    total_timesteps  | 232500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 301       |
|    n_updates        | 48124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.282     |
| time/               |           |
|    episodes         | 4960      |
|    fps              | 303       |
|    time_elapsed     | 767       |
|    total_timesteps  | 232616    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 48153     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.281     |
| time/               |           |
|    episodes         | 4964      |
|    fps              | 303       |
|    time_elapsed     | 767       |
|    total_timesteps  | 232775    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 48193     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.28      |
| time/               |           |
|    episodes         | 4968      |
|    fps              | 303       |
|    time_elapsed     | 767       |
|    total_timesteps  | 232926    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 48231     |
-----------------------------------
Eval num_timesteps=233000, episode_reward=-2771.18 +/- 1178.32
Episode length: 36.26 +/- 6.73
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.28      |
| time/               |           |
|    total_timesteps  | 233000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82        |
|    n_updates        | 48249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.279     |
| time/               |           |
|    episodes         | 4972      |
|    fps              | 303       |
|    time_elapsed     | 768       |
|    total_timesteps  | 233068    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 48266     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.278     |
| time/               |           |
|    episodes         | 4976      |
|    fps              | 303       |
|    time_elapsed     | 769       |
|    total_timesteps  | 233246    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 48311     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.277     |
| time/               |           |
|    episodes         | 4980      |
|    fps              | 303       |
|    time_elapsed     | 769       |
|    total_timesteps  | 233393    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65        |
|    n_updates        | 48348     |
-----------------------------------
Eval num_timesteps=233500, episode_reward=-2651.60 +/- 1192.77
Episode length: 35.76 +/- 6.31
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.277     |
| time/               |           |
|    total_timesteps  | 233500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 48374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.277     |
| time/               |           |
|    episodes         | 4984      |
|    fps              | 303       |
|    time_elapsed     | 770       |
|    total_timesteps  | 233540    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 48384     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.276     |
| time/               |           |
|    episodes         | 4988      |
|    fps              | 303       |
|    time_elapsed     | 770       |
|    total_timesteps  | 233642    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.6      |
|    n_updates        | 48410     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.275     |
| time/               |           |
|    episodes         | 4992      |
|    fps              | 303       |
|    time_elapsed     | 770       |
|    total_timesteps  | 233789    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 48447     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.274     |
| time/               |           |
|    episodes         | 4996      |
|    fps              | 303       |
|    time_elapsed     | 771       |
|    total_timesteps  | 233912    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 48477     |
-----------------------------------
Eval num_timesteps=234000, episode_reward=-2958.43 +/- 951.94
Episode length: 34.16 +/- 6.32
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.274     |
| time/               |           |
|    total_timesteps  | 234000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 48499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.274     |
| time/               |           |
|    episodes         | 5000      |
|    fps              | 303       |
|    time_elapsed     | 772       |
|    total_timesteps  | 234058    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 48514     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.273     |
| time/               |           |
|    episodes         | 5004      |
|    fps              | 303       |
|    time_elapsed     | 772       |
|    total_timesteps  | 234187    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 48546     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.272     |
| time/               |           |
|    episodes         | 5008      |
|    fps              | 303       |
|    time_elapsed     | 772       |
|    total_timesteps  | 234310    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 48577     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.271     |
| time/               |           |
|    episodes         | 5012      |
|    fps              | 303       |
|    time_elapsed     | 772       |
|    total_timesteps  | 234489    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 48622     |
-----------------------------------
Eval num_timesteps=234500, episode_reward=-2777.09 +/- 1100.84
Episode length: 35.44 +/- 6.53
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.271     |
| time/               |           |
|    total_timesteps  | 234500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 48624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.27      |
| time/               |           |
|    episodes         | 5016      |
|    fps              | 303       |
|    time_elapsed     | 774       |
|    total_timesteps  | 234619    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.6      |
|    n_updates        | 48654     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.27      |
| time/               |           |
|    episodes         | 5020      |
|    fps              | 303       |
|    time_elapsed     | 774       |
|    total_timesteps  | 234759    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.5      |
|    n_updates        | 48689     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.269     |
| time/               |           |
|    episodes         | 5024      |
|    fps              | 303       |
|    time_elapsed     | 774       |
|    total_timesteps  | 234893    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 218       |
|    n_updates        | 48723     |
-----------------------------------
Eval num_timesteps=235000, episode_reward=-2760.91 +/- 1276.42
Episode length: 34.52 +/- 7.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.268     |
| time/               |           |
|    total_timesteps  | 235000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 174       |
|    n_updates        | 48749     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.268    |
| time/               |          |
|    episodes         | 5028     |
|    fps              | 302      |
|    time_elapsed     | 775      |
|    total_timesteps  | 235029   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 158      |
|    n_updates        | 48757    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.268     |
| time/               |           |
|    episodes         | 5032      |
|    fps              | 303       |
|    time_elapsed     | 775       |
|    total_timesteps  | 235139    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 48784     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.267     |
| time/               |           |
|    episodes         | 5036      |
|    fps              | 303       |
|    time_elapsed     | 776       |
|    total_timesteps  | 235260    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 48814     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.8     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.266    |
| time/               |          |
|    episodes         | 5040     |
|    fps              | 303      |
|    time_elapsed     | 776      |
|    total_timesteps  | 235391   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 235      |
|    n_updates        | 48847    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.266     |
| time/               |           |
|    episodes         | 5044      |
|    fps              | 303       |
|    time_elapsed     | 776       |
|    total_timesteps  | 235497    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 225       |
|    n_updates        | 48874     |
-----------------------------------
Eval num_timesteps=235500, episode_reward=-3022.28 +/- 928.47
Episode length: 34.56 +/- 7.59
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -3.02e+03 |
| rollout/            |           |
|    exploration_rate | 0.266     |
| time/               |           |
|    total_timesteps  | 235500    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.265     |
| time/               |           |
|    episodes         | 5048      |
|    fps              | 303       |
|    time_elapsed     | 777       |
|    total_timesteps  | 235646    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.8      |
|    n_updates        | 48911     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.264     |
| time/               |           |
|    episodes         | 5052      |
|    fps              | 303       |
|    time_elapsed     | 777       |
|    total_timesteps  | 235779    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 218       |
|    n_updates        | 48944     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.6     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.263    |
| time/               |          |
|    episodes         | 5056     |
|    fps              | 303      |
|    time_elapsed     | 777      |
|    total_timesteps  | 235915   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 202      |
|    n_updates        | 48978    |
----------------------------------
Eval num_timesteps=236000, episode_reward=-2699.45 +/- 1287.64
Episode length: 34.56 +/- 7.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.6     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.263    |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 101      |
|    n_updates        | 48999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.4     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.262    |
| time/               |          |
|    episodes         | 5060     |
|    fps              | 302      |
|    time_elapsed     | 779      |
|    total_timesteps  | 236060   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 199      |
|    n_updates        | 49014    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.262     |
| time/               |           |
|    episodes         | 5064      |
|    fps              | 303       |
|    time_elapsed     | 779       |
|    total_timesteps  | 236168    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 49041     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.261     |
| time/               |           |
|    episodes         | 5068      |
|    fps              | 303       |
|    time_elapsed     | 779       |
|    total_timesteps  | 236280    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 49069     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.261     |
| time/               |           |
|    episodes         | 5072      |
|    fps              | 303       |
|    time_elapsed     | 779       |
|    total_timesteps  | 236383    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 49095     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.26      |
| time/               |           |
|    episodes         | 5076      |
|    fps              | 303       |
|    time_elapsed     | 779       |
|    total_timesteps  | 236493    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 49123     |
-----------------------------------
Eval num_timesteps=236500, episode_reward=-2966.16 +/- 1031.94
Episode length: 34.62 +/- 6.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.26      |
| time/               |           |
|    total_timesteps  | 236500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 49124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.259     |
| time/               |           |
|    episodes         | 5080      |
|    fps              | 302       |
|    time_elapsed     | 781       |
|    total_timesteps  | 236611    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 49152     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.259     |
| time/               |           |
|    episodes         | 5084      |
|    fps              | 303       |
|    time_elapsed     | 781       |
|    total_timesteps  | 236703    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 49175     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.258     |
| time/               |           |
|    episodes         | 5088      |
|    fps              | 303       |
|    time_elapsed     | 781       |
|    total_timesteps  | 236851    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 49212     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.257     |
| time/               |           |
|    episodes         | 5092      |
|    fps              | 303       |
|    time_elapsed     | 781       |
|    total_timesteps  | 236985    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 49246     |
-----------------------------------
Eval num_timesteps=237000, episode_reward=-2894.30 +/- 1016.08
Episode length: 36.04 +/- 6.53
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.257     |
| time/               |           |
|    total_timesteps  | 237000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 34.5      |
|    n_updates        | 49249     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.256    |
| time/               |          |
|    episodes         | 5096     |
|    fps              | 302      |
|    time_elapsed     | 782      |
|    total_timesteps  | 237103   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 37.5     |
|    n_updates        | 49275    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.256     |
| time/               |           |
|    episodes         | 5100      |
|    fps              | 302       |
|    time_elapsed     | 782       |
|    total_timesteps  | 237222    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 49305     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.8     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.255    |
| time/               |          |
|    episodes         | 5104     |
|    fps              | 303      |
|    time_elapsed     | 783      |
|    total_timesteps  | 237371   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 106      |
|    n_updates        | 49342    |
----------------------------------
Eval num_timesteps=237500, episode_reward=-2458.76 +/- 1364.38
Episode length: 36.50 +/- 5.86
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -2.46e+03 |
| rollout/            |           |
|    exploration_rate | 0.254     |
| time/               |           |
|    total_timesteps  | 237500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 49374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.254     |
| time/               |           |
|    episodes         | 5108      |
|    fps              | 302       |
|    time_elapsed     | 784       |
|    total_timesteps  | 237500    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.3      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.254     |
| time/               |           |
|    episodes         | 5112      |
|    fps              | 302       |
|    time_elapsed     | 784       |
|    total_timesteps  | 237622    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 227       |
|    n_updates        | 49405     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.2      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.253     |
| time/               |           |
|    episodes         | 5116      |
|    fps              | 302       |
|    time_elapsed     | 784       |
|    total_timesteps  | 237735    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 273       |
|    n_updates        | 49433     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31        |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.252     |
| time/               |           |
|    episodes         | 5120      |
|    fps              | 303       |
|    time_elapsed     | 784       |
|    total_timesteps  | 237856    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.7      |
|    n_updates        | 49463     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.8      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.252     |
| time/               |           |
|    episodes         | 5124      |
|    fps              | 303       |
|    time_elapsed     | 785       |
|    total_timesteps  | 237975    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 49493     |
-----------------------------------
Eval num_timesteps=238000, episode_reward=-3083.41 +/- 910.29
Episode length: 37.38 +/- 7.08
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.4      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.251     |
| time/               |           |
|    total_timesteps  | 238000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.8      |
|    n_updates        | 49499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.7      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.251     |
| time/               |           |
|    episodes         | 5128      |
|    fps              | 302       |
|    time_elapsed     | 786       |
|    total_timesteps  | 238096    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64.5      |
|    n_updates        | 49523     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.8      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.25      |
| time/               |           |
|    episodes         | 5132      |
|    fps              | 302       |
|    time_elapsed     | 786       |
|    total_timesteps  | 238223    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.2      |
|    n_updates        | 49555     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.7      |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.25      |
| time/               |           |
|    episodes         | 5136      |
|    fps              | 302       |
|    time_elapsed     | 786       |
|    total_timesteps  | 238328    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 49581     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.4     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.249    |
| time/               |          |
|    episodes         | 5140     |
|    fps              | 303      |
|    time_elapsed     | 786      |
|    total_timesteps  | 238432   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 121      |
|    n_updates        | 49607    |
----------------------------------
Eval num_timesteps=238500, episode_reward=-2883.80 +/- 1218.55
Episode length: 35.46 +/- 7.89
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.249     |
| time/               |           |
|    total_timesteps  | 238500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 49624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.4      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.248     |
| time/               |           |
|    episodes         | 5144      |
|    fps              | 302       |
|    time_elapsed     | 788       |
|    total_timesteps  | 238536    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.7      |
|    n_updates        | 49633     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30       |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.248    |
| time/               |          |
|    episodes         | 5148     |
|    fps              | 302      |
|    time_elapsed     | 788      |
|    total_timesteps  | 238645   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 231      |
|    n_updates        | 49661    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 29.9      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.247     |
| time/               |           |
|    episodes         | 5152      |
|    fps              | 302       |
|    time_elapsed     | 788       |
|    total_timesteps  | 238764    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 49690     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.5     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.246    |
| time/               |          |
|    episodes         | 5156     |
|    fps              | 302      |
|    time_elapsed     | 788      |
|    total_timesteps  | 238869   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 220      |
|    n_updates        | 49717    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 29.4      |
|    ep_rew_mean      | -3.79e+03 |
|    exploration_rate | 0.246     |
| time/               |           |
|    episodes         | 5160      |
|    fps              | 303       |
|    time_elapsed     | 788       |
|    total_timesteps  | 238999    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 271       |
|    n_updates        | 49749     |
-----------------------------------
Eval num_timesteps=239000, episode_reward=-2770.82 +/- 1243.80
Episode length: 35.22 +/- 7.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 35.2      |
|    mean_reward     | -2.77e+03 |
| time/              |           |
|    total_timesteps | 239000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.8     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.245    |
| time/               |          |
|    episodes         | 5164     |
|    fps              | 302      |
|    time_elapsed     | 790      |
|    total_timesteps  | 239148   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 135      |
|    n_updates        | 49786    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30        |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.244     |
| time/               |           |
|    episodes         | 5168      |
|    fps              | 302       |
|    time_elapsed     | 790       |
|    total_timesteps  | 239277    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75        |
|    n_updates        | 49819     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 29.9      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.244     |
| time/               |           |
|    episodes         | 5172      |
|    fps              | 302       |
|    time_elapsed     | 790       |
|    total_timesteps  | 239377    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 49844     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30        |
|    ep_rew_mean      | -3.82e+03 |
|    exploration_rate | 0.243     |
| time/               |           |
|    episodes         | 5176      |
|    fps              | 302       |
|    time_elapsed     | 790       |
|    total_timesteps  | 239495    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 49873     |
-----------------------------------
Eval num_timesteps=239500, episode_reward=-2686.87 +/- 1390.55
Episode length: 35.54 +/- 6.64
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.243     |
| time/               |           |
|    total_timesteps  | 239500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 49874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.1      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.242     |
| time/               |           |
|    episodes         | 5180      |
|    fps              | 302       |
|    time_elapsed     | 791       |
|    total_timesteps  | 239622    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.8      |
|    n_updates        | 49905     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.7      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.241     |
| time/               |           |
|    episodes         | 5184      |
|    fps              | 302       |
|    time_elapsed     | 792       |
|    total_timesteps  | 239776    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 226       |
|    n_updates        | 49943     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.4      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.241     |
| time/               |           |
|    episodes         | 5188      |
|    fps              | 302       |
|    time_elapsed     | 792       |
|    total_timesteps  | 239891    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 49972     |
-----------------------------------
Eval num_timesteps=240000, episode_reward=-2621.45 +/- 1466.96
Episode length: 36.74 +/- 7.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.7      |
|    mean_reward      | -2.62e+03 |
| rollout/            |           |
|    exploration_rate | 0.24      |
| time/               |           |
|    total_timesteps  | 240000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.5      |
|    n_updates        | 49999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.2      |
|    ep_rew_mean      | -3.81e+03 |
|    exploration_rate | 0.24      |
| time/               |           |
|    episodes         | 5192      |
|    fps              | 302       |
|    time_elapsed     | 793       |
|    total_timesteps  | 240003    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 260       |
|    n_updates        | 50000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.4      |
|    ep_rew_mean      | -3.83e+03 |
|    exploration_rate | 0.239     |
| time/               |           |
|    episodes         | 5196      |
|    fps              | 302       |
|    time_elapsed     | 793       |
|    total_timesteps  | 240138    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 50034     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.2     |
|    ep_rew_mean      | -3.8e+03 |
|    exploration_rate | 0.239    |
| time/               |          |
|    episodes         | 5200     |
|    fps              | 302      |
|    time_elapsed     | 793      |
|    total_timesteps  | 240245   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 179      |
|    n_updates        | 50061    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30        |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.238     |
| time/               |           |
|    episodes         | 5204      |
|    fps              | 302       |
|    time_elapsed     | 793       |
|    total_timesteps  | 240373    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 50093     |
-----------------------------------
Eval num_timesteps=240500, episode_reward=-2358.93 +/- 1475.95
Episode length: 35.86 +/- 7.61
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.36e+03 |
| rollout/            |           |
|    exploration_rate | 0.237     |
| time/               |           |
|    total_timesteps  | 240500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 192       |
|    n_updates        | 50124     |
-----------------------------------
New best mean reward!
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30        |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.237     |
| time/               |           |
|    episodes         | 5208      |
|    fps              | 302       |
|    time_elapsed     | 795       |
|    total_timesteps  | 240501    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 50125     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 29.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.237     |
| time/               |           |
|    episodes         | 5212      |
|    fps              | 302       |
|    time_elapsed     | 795       |
|    total_timesteps  | 240605    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 50151     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.1      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.236     |
| time/               |           |
|    episodes         | 5216      |
|    fps              | 302       |
|    time_elapsed     | 795       |
|    total_timesteps  | 240750    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 50187     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.6      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.235     |
| time/               |           |
|    episodes         | 5220      |
|    fps              | 302       |
|    time_elapsed     | 795       |
|    total_timesteps  | 240912    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 50227     |
-----------------------------------
Eval num_timesteps=241000, episode_reward=-2792.29 +/- 1183.81
Episode length: 36.10 +/- 5.68
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.234     |
| time/               |           |
|    total_timesteps  | 241000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 227       |
|    n_updates        | 50249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.4      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.234     |
| time/               |           |
|    episodes         | 5224      |
|    fps              | 302       |
|    time_elapsed     | 797       |
|    total_timesteps  | 241017    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 50254     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.5      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.233     |
| time/               |           |
|    episodes         | 5228      |
|    fps              | 302       |
|    time_elapsed     | 797       |
|    total_timesteps  | 241145    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 50286     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.4      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.233     |
| time/               |           |
|    episodes         | 5232      |
|    fps              | 302       |
|    time_elapsed     | 797       |
|    total_timesteps  | 241265    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 50316     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.232     |
| time/               |           |
|    episodes         | 5236      |
|    fps              | 302       |
|    time_elapsed     | 797       |
|    total_timesteps  | 241404    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.8      |
|    n_updates        | 50350     |
-----------------------------------
Eval num_timesteps=241500, episode_reward=-2945.13 +/- 1077.57
Episode length: 34.58 +/- 7.00
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.231     |
| time/               |           |
|    total_timesteps  | 241500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 245       |
|    n_updates        | 50374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.1      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.231     |
| time/               |           |
|    episodes         | 5240      |
|    fps              | 302       |
|    time_elapsed     | 798       |
|    total_timesteps  | 241543    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 50385     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.1      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.231     |
| time/               |           |
|    episodes         | 5244      |
|    fps              | 302       |
|    time_elapsed     | 798       |
|    total_timesteps  | 241647    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 50411     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.3      |
|    ep_rew_mean      | -3.78e+03 |
|    exploration_rate | 0.23      |
| time/               |           |
|    episodes         | 5248      |
|    fps              | 302       |
|    time_elapsed     | 799       |
|    total_timesteps  | 241772    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 50442     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.4      |
|    ep_rew_mean      | -3.77e+03 |
|    exploration_rate | 0.229     |
| time/               |           |
|    episodes         | 5252      |
|    fps              | 302       |
|    time_elapsed     | 799       |
|    total_timesteps  | 241909    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 237       |
|    n_updates        | 50477     |
-----------------------------------
Eval num_timesteps=242000, episode_reward=-3296.94 +/- 753.43
Episode length: 43.92 +/- 68.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | -3.3e+03 |
| rollout/            |          |
|    exploration_rate | 0.229    |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 98.6     |
|    n_updates        | 50499    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.8      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.228     |
| time/               |           |
|    episodes         | 5256      |
|    fps              | 302       |
|    time_elapsed     | 800       |
|    total_timesteps  | 242046    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 50511     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.228     |
| time/               |           |
|    episodes         | 5260      |
|    fps              | 302       |
|    time_elapsed     | 800       |
|    total_timesteps  | 242159    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.7      |
|    n_updates        | 50539     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.4      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.227     |
| time/               |           |
|    episodes         | 5264      |
|    fps              | 302       |
|    time_elapsed     | 801       |
|    total_timesteps  | 242286    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 50571     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.3      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.226     |
| time/               |           |
|    episodes         | 5268      |
|    fps              | 302       |
|    time_elapsed     | 801       |
|    total_timesteps  | 242409    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.9      |
|    n_updates        | 50602     |
-----------------------------------
Eval num_timesteps=242500, episode_reward=-2805.30 +/- 1236.74
Episode length: 36.30 +/- 6.82
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.226     |
| time/               |           |
|    total_timesteps  | 242500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 50624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.225     |
| time/               |           |
|    episodes         | 5272      |
|    fps              | 302       |
|    time_elapsed     | 802       |
|    total_timesteps  | 242564    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 50640     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.225     |
| time/               |           |
|    episodes         | 5276      |
|    fps              | 302       |
|    time_elapsed     | 802       |
|    total_timesteps  | 242683    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.4      |
|    n_updates        | 50670     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.224     |
| time/               |           |
|    episodes         | 5280      |
|    fps              | 302       |
|    time_elapsed     | 802       |
|    total_timesteps  | 242815    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.3      |
|    n_updates        | 50703     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.223     |
| time/               |           |
|    episodes         | 5284      |
|    fps              | 302       |
|    time_elapsed     | 803       |
|    total_timesteps  | 242935    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.2      |
|    n_updates        | 50733     |
-----------------------------------
Eval num_timesteps=243000, episode_reward=-2945.28 +/- 1197.16
Episode length: 34.88 +/- 7.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.223     |
| time/               |           |
|    total_timesteps  | 243000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 212       |
|    n_updates        | 50749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.222     |
| time/               |           |
|    episodes         | 5288      |
|    fps              | 302       |
|    time_elapsed     | 804       |
|    total_timesteps  | 243084    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.3      |
|    n_updates        | 50770     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.222     |
| time/               |           |
|    episodes         | 5292      |
|    fps              | 302       |
|    time_elapsed     | 804       |
|    total_timesteps  | 243207    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78        |
|    n_updates        | 50801     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.221     |
| time/               |           |
|    episodes         | 5296      |
|    fps              | 302       |
|    time_elapsed     | 804       |
|    total_timesteps  | 243347    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 50836     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.22      |
| time/               |           |
|    episodes         | 5300      |
|    fps              | 302       |
|    time_elapsed     | 804       |
|    total_timesteps  | 243467    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.9      |
|    n_updates        | 50866     |
-----------------------------------
Eval num_timesteps=243500, episode_reward=-2823.87 +/- 1157.55
Episode length: 37.06 +/- 32.74
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.1      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.22      |
| time/               |           |
|    total_timesteps  | 243500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 50874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.76e+03 |
|    exploration_rate | 0.219     |
| time/               |           |
|    episodes         | 5304      |
|    fps              | 302       |
|    time_elapsed     | 806       |
|    total_timesteps  | 243588    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 50896     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.75e+03 |
|    exploration_rate | 0.219     |
| time/               |           |
|    episodes         | 5308      |
|    fps              | 302       |
|    time_elapsed     | 806       |
|    total_timesteps  | 243715    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.4      |
|    n_updates        | 50928     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.73e+03 |
|    exploration_rate | 0.218     |
| time/               |           |
|    episodes         | 5312      |
|    fps              | 302       |
|    time_elapsed     | 806       |
|    total_timesteps  | 243876    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 50968     |
-----------------------------------
Eval num_timesteps=244000, episode_reward=-2861.07 +/- 943.88
Episode length: 35.52 +/- 6.53
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.217     |
| time/               |           |
|    total_timesteps  | 244000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 50999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.217     |
| time/               |           |
|    episodes         | 5316      |
|    fps              | 302       |
|    time_elapsed     | 807       |
|    total_timesteps  | 244018    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.1      |
|    n_updates        | 51004     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.74e+03 |
|    exploration_rate | 0.216     |
| time/               |           |
|    episodes         | 5320      |
|    fps              | 302       |
|    time_elapsed     | 807       |
|    total_timesteps  | 244142    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 51035     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.216     |
| time/               |           |
|    episodes         | 5324      |
|    fps              | 302       |
|    time_elapsed     | 808       |
|    total_timesteps  | 244256    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 51063     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.215     |
| time/               |           |
|    episodes         | 5328      |
|    fps              | 302       |
|    time_elapsed     | 808       |
|    total_timesteps  | 244388    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 51096     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.214     |
| time/               |           |
|    episodes         | 5332      |
|    fps              | 302       |
|    time_elapsed     | 808       |
|    total_timesteps  | 244499    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 51124     |
-----------------------------------
Eval num_timesteps=244500, episode_reward=-3071.42 +/- 1000.52
Episode length: 34.86 +/- 5.91
----------------------------------
| eval/              |           |
|    mean_ep_length  | 34.9      |
|    mean_reward     | -3.07e+03 |
| time/              |           |
|    total_timesteps | 244500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.3     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.213    |
| time/               |          |
|    episodes         | 5336     |
|    fps              | 302      |
|    time_elapsed     | 809      |
|    total_timesteps  | 244633   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 107      |
|    n_updates        | 51158    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.3     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.213    |
| time/               |          |
|    episodes         | 5340     |
|    fps              | 302      |
|    time_elapsed     | 809      |
|    total_timesteps  | 244775   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 107      |
|    n_updates        | 51193    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.212     |
| time/               |           |
|    episodes         | 5344      |
|    fps              | 302       |
|    time_elapsed     | 809       |
|    total_timesteps  | 244886    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 196       |
|    n_updates        | 51221     |
-----------------------------------
Eval num_timesteps=245000, episode_reward=-3065.90 +/- 1107.28
Episode length: 64.30 +/- 116.57
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 64.3      |
|    mean_reward      | -3.07e+03 |
| rollout/            |           |
|    exploration_rate | 0.211     |
| time/               |           |
|    total_timesteps  | 245000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 237       |
|    n_updates        | 51249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.211     |
| time/               |           |
|    episodes         | 5348      |
|    fps              | 301       |
|    time_elapsed     | 812       |
|    total_timesteps  | 245007    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 51251     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.211     |
| time/               |           |
|    episodes         | 5352      |
|    fps              | 301       |
|    time_elapsed     | 812       |
|    total_timesteps  | 245127    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 206       |
|    n_updates        | 51281     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.21     |
| time/               |          |
|    episodes         | 5356     |
|    fps              | 301      |
|    time_elapsed     | 812      |
|    total_timesteps  | 245240   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 86.2     |
|    n_updates        | 51309    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.209     |
| time/               |           |
|    episodes         | 5360      |
|    fps              | 301       |
|    time_elapsed     | 812       |
|    total_timesteps  | 245370    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.5      |
|    n_updates        | 51342     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.209     |
| time/               |           |
|    episodes         | 5364      |
|    fps              | 302       |
|    time_elapsed     | 812       |
|    total_timesteps  | 245485    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 51371     |
-----------------------------------
Eval num_timesteps=245500, episode_reward=-2885.21 +/- 945.60
Episode length: 47.02 +/- 68.54
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 47        |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.208     |
| time/               |           |
|    total_timesteps  | 245500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 51374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.208     |
| time/               |           |
|    episodes         | 5368      |
|    fps              | 301       |
|    time_elapsed     | 814       |
|    total_timesteps  | 245622    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 51405     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.207     |
| time/               |           |
|    episodes         | 5372      |
|    fps              | 301       |
|    time_elapsed     | 814       |
|    total_timesteps  | 245751    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 51437     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.206     |
| time/               |           |
|    episodes         | 5376      |
|    fps              | 301       |
|    time_elapsed     | 814       |
|    total_timesteps  | 245889    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 51472     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.8      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.206     |
| time/               |           |
|    episodes         | 5380      |
|    fps              | 301       |
|    time_elapsed     | 814       |
|    total_timesteps  | 245993    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 278       |
|    n_updates        | 51498     |
-----------------------------------
Eval num_timesteps=246000, episode_reward=-3066.18 +/- 1035.91
Episode length: 35.80 +/- 6.41
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -3.07e+03 |
| rollout/            |           |
|    exploration_rate | 0.206     |
| time/               |           |
|    total_timesteps  | 246000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 51499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.205     |
| time/               |           |
|    episodes         | 5384      |
|    fps              | 301       |
|    time_elapsed     | 816       |
|    total_timesteps  | 246137    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 51534     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.204    |
| time/               |          |
|    episodes         | 5388     |
|    fps              | 301      |
|    time_elapsed     | 816      |
|    total_timesteps  | 246269   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 223      |
|    n_updates        | 51567    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.8     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.203    |
| time/               |          |
|    episodes         | 5392     |
|    fps              | 301      |
|    time_elapsed     | 816      |
|    total_timesteps  | 246391   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 227      |
|    n_updates        | 51597    |
----------------------------------
Eval num_timesteps=246500, episode_reward=-2660.87 +/- 1181.17
Episode length: 36.14 +/- 7.14
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.66e+03 |
| rollout/            |           |
|    exploration_rate | 0.203     |
| time/               |           |
|    total_timesteps  | 246500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 51624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.202    |
| time/               |          |
|    episodes         | 5396     |
|    fps              | 301      |
|    time_elapsed     | 817      |
|    total_timesteps  | 246541   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 175      |
|    n_updates        | 51635    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.59e+03 |
|    exploration_rate | 0.202     |
| time/               |           |
|    episodes         | 5400      |
|    fps              | 301       |
|    time_elapsed     | 818       |
|    total_timesteps  | 246683    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 51670     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.59e+03 |
|    exploration_rate | 0.201     |
| time/               |           |
|    episodes         | 5404      |
|    fps              | 301       |
|    time_elapsed     | 818       |
|    total_timesteps  | 246800    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 234       |
|    n_updates        | 51699     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.2       |
| time/               |           |
|    episodes         | 5408      |
|    fps              | 301       |
|    time_elapsed     | 818       |
|    total_timesteps  | 246965    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.4      |
|    n_updates        | 51741     |
-----------------------------------
Eval num_timesteps=247000, episode_reward=-2687.64 +/- 1322.14
Episode length: 46.22 +/- 68.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 46.2      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.2       |
| time/               |           |
|    total_timesteps  | 247000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 51749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.199     |
| time/               |           |
|    episodes         | 5412      |
|    fps              | 301       |
|    time_elapsed     | 820       |
|    total_timesteps  | 247080    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 51769     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.198     |
| time/               |           |
|    episodes         | 5416      |
|    fps              | 301       |
|    time_elapsed     | 820       |
|    total_timesteps  | 247232    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.5      |
|    n_updates        | 51807     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.198     |
| time/               |           |
|    episodes         | 5420      |
|    fps              | 301       |
|    time_elapsed     | 820       |
|    total_timesteps  | 247345    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.6      |
|    n_updates        | 51836     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.197     |
| time/               |           |
|    episodes         | 5424      |
|    fps              | 301       |
|    time_elapsed     | 820       |
|    total_timesteps  | 247473    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94        |
|    n_updates        | 51868     |
-----------------------------------
Eval num_timesteps=247500, episode_reward=-2860.34 +/- 1146.89
Episode length: 45.64 +/- 68.85
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 45.6      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.197     |
| time/               |           |
|    total_timesteps  | 247500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 51874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.196     |
| time/               |           |
|    episodes         | 5428      |
|    fps              | 301       |
|    time_elapsed     | 822       |
|    total_timesteps  | 247593    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 51898     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.196     |
| time/               |           |
|    episodes         | 5432      |
|    fps              | 301       |
|    time_elapsed     | 822       |
|    total_timesteps  | 247728    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.7      |
|    n_updates        | 51931     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.195     |
| time/               |           |
|    episodes         | 5436      |
|    fps              | 301       |
|    time_elapsed     | 822       |
|    total_timesteps  | 247839    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.1      |
|    n_updates        | 51959     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.194     |
| time/               |           |
|    episodes         | 5440      |
|    fps              | 301       |
|    time_elapsed     | 822       |
|    total_timesteps  | 247961    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 51990     |
-----------------------------------
Eval num_timesteps=248000, episode_reward=-2814.49 +/- 1142.51
Episode length: 38.56 +/- 10.82
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 38.6      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.194     |
| time/               |           |
|    total_timesteps  | 248000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 51999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.193     |
| time/               |           |
|    episodes         | 5444      |
|    fps              | 301       |
|    time_elapsed     | 823       |
|    total_timesteps  | 248095    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53.8      |
|    n_updates        | 52023     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.193     |
| time/               |           |
|    episodes         | 5448      |
|    fps              | 301       |
|    time_elapsed     | 824       |
|    total_timesteps  | 248217    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 52054     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.192     |
| time/               |           |
|    episodes         | 5452      |
|    fps              | 301       |
|    time_elapsed     | 824       |
|    total_timesteps  | 248366    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 52091     |
-----------------------------------
Eval num_timesteps=248500, episode_reward=-2636.25 +/- 1458.57
Episode length: 37.22 +/- 6.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.2      |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.191     |
| time/               |           |
|    total_timesteps  | 248500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 52124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.191     |
| time/               |           |
|    episodes         | 5456      |
|    fps              | 300       |
|    time_elapsed     | 825       |
|    total_timesteps  | 248528    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.2      |
|    n_updates        | 52131     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.19      |
| time/               |           |
|    episodes         | 5460      |
|    fps              | 301       |
|    time_elapsed     | 825       |
|    total_timesteps  | 248645    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 221       |
|    n_updates        | 52161     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.189     |
| time/               |           |
|    episodes         | 5464      |
|    fps              | 301       |
|    time_elapsed     | 826       |
|    total_timesteps  | 248786    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 52196     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.189     |
| time/               |           |
|    episodes         | 5468      |
|    fps              | 301       |
|    time_elapsed     | 826       |
|    total_timesteps  | 248905    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 52226     |
-----------------------------------
Eval num_timesteps=249000, episode_reward=-2941.17 +/- 1172.61
Episode length: 35.36 +/- 7.39
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.188     |
| time/               |           |
|    total_timesteps  | 249000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 273       |
|    n_updates        | 52249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.188     |
| time/               |           |
|    episodes         | 5472      |
|    fps              | 300       |
|    time_elapsed     | 827       |
|    total_timesteps  | 249034    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 52258     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.187     |
| time/               |           |
|    episodes         | 5476      |
|    fps              | 301       |
|    time_elapsed     | 827       |
|    total_timesteps  | 249245    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 52311     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.9     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.186    |
| time/               |          |
|    episodes         | 5480     |
|    fps              | 301      |
|    time_elapsed     | 827      |
|    total_timesteps  | 249383   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 58.2     |
|    n_updates        | 52345    |
----------------------------------
Eval num_timesteps=249500, episode_reward=-2694.15 +/- 1249.98
Episode length: 37.06 +/- 6.51
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.1      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.185     |
| time/               |           |
|    total_timesteps  | 249500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 52374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.185     |
| time/               |           |
|    episodes         | 5484      |
|    fps              | 300       |
|    time_elapsed     | 829       |
|    total_timesteps  | 249513    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 52378     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.185     |
| time/               |           |
|    episodes         | 5488      |
|    fps              | 300       |
|    time_elapsed     | 829       |
|    total_timesteps  | 249615    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 52403     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.184     |
| time/               |           |
|    episodes         | 5492      |
|    fps              | 301       |
|    time_elapsed     | 829       |
|    total_timesteps  | 249730    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 52432     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.183     |
| time/               |           |
|    episodes         | 5496      |
|    fps              | 301       |
|    time_elapsed     | 829       |
|    total_timesteps  | 249872    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 52467     |
-----------------------------------
Eval num_timesteps=250000, episode_reward=-2887.99 +/- 1214.25
Episode length: 44.56 +/- 68.96
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 44.6      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.182     |
| time/               |           |
|    total_timesteps  | 250000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 245       |
|    n_updates        | 52499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.182     |
| time/               |           |
|    episodes         | 5500      |
|    fps              | 300       |
|    time_elapsed     | 831       |
|    total_timesteps  | 250004    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 234       |
|    n_updates        | 52500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.182     |
| time/               |           |
|    episodes         | 5504      |
|    fps              | 300       |
|    time_elapsed     | 831       |
|    total_timesteps  | 250127    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 50        |
|    n_updates        | 52531     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.181     |
| time/               |           |
|    episodes         | 5508      |
|    fps              | 300       |
|    time_elapsed     | 831       |
|    total_timesteps  | 250252    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 52562     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.18      |
| time/               |           |
|    episodes         | 5512      |
|    fps              | 301       |
|    time_elapsed     | 831       |
|    total_timesteps  | 250374    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 52593     |
-----------------------------------
Eval num_timesteps=250500, episode_reward=-2950.59 +/- 1114.77
Episode length: 44.22 +/- 69.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 44.2      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.179     |
| time/               |           |
|    total_timesteps  | 250500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 192       |
|    n_updates        | 52624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.179     |
| time/               |           |
|    episodes         | 5516      |
|    fps              | 300       |
|    time_elapsed     | 833       |
|    total_timesteps  | 250503    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 210       |
|    n_updates        | 52625     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.179     |
| time/               |           |
|    episodes         | 5520      |
|    fps              | 300       |
|    time_elapsed     | 833       |
|    total_timesteps  | 250627    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 213       |
|    n_updates        | 52656     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.178     |
| time/               |           |
|    episodes         | 5524      |
|    fps              | 300       |
|    time_elapsed     | 833       |
|    total_timesteps  | 250765    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 52691     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.177     |
| time/               |           |
|    episodes         | 5528      |
|    fps              | 300       |
|    time_elapsed     | 833       |
|    total_timesteps  | 250876    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 52718     |
-----------------------------------
Eval num_timesteps=251000, episode_reward=-2872.40 +/- 1149.98
Episode length: 45.18 +/- 68.96
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 45.2      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.176     |
| time/               |           |
|    total_timesteps  | 251000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 52749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.176     |
| time/               |           |
|    episodes         | 5532      |
|    fps              | 300       |
|    time_elapsed     | 835       |
|    total_timesteps  | 251047    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.5      |
|    n_updates        | 52761     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.175     |
| time/               |           |
|    episodes         | 5536      |
|    fps              | 300       |
|    time_elapsed     | 835       |
|    total_timesteps  | 251188    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 52796     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.175     |
| time/               |           |
|    episodes         | 5540      |
|    fps              | 300       |
|    time_elapsed     | 835       |
|    total_timesteps  | 251316    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 167       |
|    n_updates        | 52828     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.174     |
| time/               |           |
|    episodes         | 5544      |
|    fps              | 300       |
|    time_elapsed     | 835       |
|    total_timesteps  | 251428    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 52856     |
-----------------------------------
Eval num_timesteps=251500, episode_reward=-2932.67 +/- 996.85
Episode length: 36.36 +/- 8.23
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.174     |
| time/               |           |
|    total_timesteps  | 251500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 52874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.173     |
| time/               |           |
|    episodes         | 5548      |
|    fps              | 300       |
|    time_elapsed     | 837       |
|    total_timesteps  | 251568    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 52891     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.172     |
| time/               |           |
|    episodes         | 5552      |
|    fps              | 300       |
|    time_elapsed     | 837       |
|    total_timesteps  | 251714    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 52928     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.172     |
| time/               |           |
|    episodes         | 5556      |
|    fps              | 300       |
|    time_elapsed     | 837       |
|    total_timesteps  | 251841    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.3      |
|    n_updates        | 52960     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.171     |
| time/               |           |
|    episodes         | 5560      |
|    fps              | 300       |
|    time_elapsed     | 837       |
|    total_timesteps  | 251969    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 52992     |
-----------------------------------
Eval num_timesteps=252000, episode_reward=-3108.30 +/- 916.75
Episode length: 35.98 +/- 7.89
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.171     |
| time/               |           |
|    total_timesteps  | 252000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 52999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.17      |
| time/               |           |
|    episodes         | 5564      |
|    fps              | 300       |
|    time_elapsed     | 839       |
|    total_timesteps  | 252102    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 53025     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.169     |
| time/               |           |
|    episodes         | 5568      |
|    fps              | 300       |
|    time_elapsed     | 839       |
|    total_timesteps  | 252215    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 257       |
|    n_updates        | 53053     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.169     |
| time/               |           |
|    episodes         | 5572      |
|    fps              | 300       |
|    time_elapsed     | 839       |
|    total_timesteps  | 252345    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 53086     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.168     |
| time/               |           |
|    episodes         | 5576      |
|    fps              | 300       |
|    time_elapsed     | 839       |
|    total_timesteps  | 252468    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 53116     |
-----------------------------------
Eval num_timesteps=252500, episode_reward=-2947.81 +/- 1089.23
Episode length: 34.86 +/- 6.90
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.168     |
| time/               |           |
|    total_timesteps  | 252500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 53124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.72e+03 |
|    exploration_rate | 0.167     |
| time/               |           |
|    episodes         | 5580      |
|    fps              | 300       |
|    time_elapsed     | 840       |
|    total_timesteps  | 252601    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 53150     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.71e+03 |
|    exploration_rate | 0.166     |
| time/               |           |
|    episodes         | 5584      |
|    fps              | 300       |
|    time_elapsed     | 840       |
|    total_timesteps  | 252737    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 336       |
|    n_updates        | 53184     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.6     |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.165    |
| time/               |          |
|    episodes         | 5588     |
|    fps              | 300      |
|    time_elapsed     | 841      |
|    total_timesteps  | 252879   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 203      |
|    n_updates        | 53219    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.165     |
| time/               |           |
|    episodes         | 5592      |
|    fps              | 300       |
|    time_elapsed     | 841       |
|    total_timesteps  | 252981    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.3      |
|    n_updates        | 53245     |
-----------------------------------
Eval num_timesteps=253000, episode_reward=-2987.32 +/- 1020.79
Episode length: 36.58 +/- 5.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.6      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.165     |
| time/               |           |
|    total_timesteps  | 253000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 225       |
|    n_updates        | 53249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.164     |
| time/               |           |
|    episodes         | 5596      |
|    fps              | 300       |
|    time_elapsed     | 842       |
|    total_timesteps  | 253154    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.4      |
|    n_updates        | 53288     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.163     |
| time/               |           |
|    episodes         | 5600      |
|    fps              | 300       |
|    time_elapsed     | 842       |
|    total_timesteps  | 253278    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.3      |
|    n_updates        | 53319     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.68e+03 |
|    exploration_rate | 0.162     |
| time/               |           |
|    episodes         | 5604      |
|    fps              | 300       |
|    time_elapsed     | 842       |
|    total_timesteps  | 253399    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 53349     |
-----------------------------------
Eval num_timesteps=253500, episode_reward=-3055.39 +/- 941.18
Episode length: 34.72 +/- 6.13
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -3.06e+03 |
| rollout/            |           |
|    exploration_rate | 0.162     |
| time/               |           |
|    total_timesteps  | 253500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.8      |
|    n_updates        | 53374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.162     |
| time/               |           |
|    episodes         | 5608      |
|    fps              | 300       |
|    time_elapsed     | 844       |
|    total_timesteps  | 253540    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 364       |
|    n_updates        | 53384     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.161     |
| time/               |           |
|    episodes         | 5612      |
|    fps              | 300       |
|    time_elapsed     | 844       |
|    total_timesteps  | 253653    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 217       |
|    n_updates        | 53413     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.16      |
| time/               |           |
|    episodes         | 5616      |
|    fps              | 300       |
|    time_elapsed     | 844       |
|    total_timesteps  | 253781    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.7      |
|    n_updates        | 53445     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.159     |
| time/               |           |
|    episodes         | 5620      |
|    fps              | 300       |
|    time_elapsed     | 844       |
|    total_timesteps  | 253895    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 53473     |
-----------------------------------
Eval num_timesteps=254000, episode_reward=-2785.14 +/- 1190.05
Episode length: 36.26 +/- 5.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.159     |
| time/               |           |
|    total_timesteps  | 254000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 53499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.159     |
| time/               |           |
|    episodes         | 5624      |
|    fps              | 300       |
|    time_elapsed     | 846       |
|    total_timesteps  | 254032    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 53507     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.158     |
| time/               |           |
|    episodes         | 5628      |
|    fps              | 300       |
|    time_elapsed     | 846       |
|    total_timesteps  | 254163    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 53540     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.157     |
| time/               |           |
|    episodes         | 5632      |
|    fps              | 300       |
|    time_elapsed     | 846       |
|    total_timesteps  | 254280    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.6      |
|    n_updates        | 53569     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32       |
|    ep_rew_mean      | -3.7e+03 |
|    exploration_rate | 0.157    |
| time/               |          |
|    episodes         | 5636     |
|    fps              | 300      |
|    time_elapsed     | 846      |
|    total_timesteps  | 254391   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 207      |
|    n_updates        | 53597    |
----------------------------------
Eval num_timesteps=254500, episode_reward=-3307.00 +/- 851.73
Episode length: 34.32 +/- 8.01
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -3.31e+03 |
| rollout/            |           |
|    exploration_rate | 0.156     |
| time/               |           |
|    total_timesteps  | 254500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 167       |
|    n_updates        | 53624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.156     |
| time/               |           |
|    episodes         | 5640      |
|    fps              | 300       |
|    time_elapsed     | 847       |
|    total_timesteps  | 254540    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 247       |
|    n_updates        | 53634     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.69e+03 |
|    exploration_rate | 0.155     |
| time/               |           |
|    episodes         | 5644      |
|    fps              | 300       |
|    time_elapsed     | 848       |
|    total_timesteps  | 254658    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 53664     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.154     |
| time/               |           |
|    episodes         | 5648      |
|    fps              | 300       |
|    time_elapsed     | 848       |
|    total_timesteps  | 254785    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 207       |
|    n_updates        | 53696     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.153     |
| time/               |           |
|    episodes         | 5652      |
|    fps              | 300       |
|    time_elapsed     | 848       |
|    total_timesteps  | 254928    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 53731     |
-----------------------------------
Eval num_timesteps=255000, episode_reward=-2837.92 +/- 1272.06
Episode length: 35.70 +/- 5.79
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.153     |
| time/               |           |
|    total_timesteps  | 255000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 53749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.152     |
| time/               |           |
|    episodes         | 5656      |
|    fps              | 300       |
|    time_elapsed     | 849       |
|    total_timesteps  | 255094    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 53773     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.152     |
| time/               |           |
|    episodes         | 5660      |
|    fps              | 300       |
|    time_elapsed     | 849       |
|    total_timesteps  | 255207    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.5      |
|    n_updates        | 53801     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.151     |
| time/               |           |
|    episodes         | 5664      |
|    fps              | 300       |
|    time_elapsed     | 850       |
|    total_timesteps  | 255340    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.5      |
|    n_updates        | 53834     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.15      |
| time/               |           |
|    episodes         | 5668      |
|    fps              | 300       |
|    time_elapsed     | 850       |
|    total_timesteps  | 255468    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 53866     |
-----------------------------------
Eval num_timesteps=255500, episode_reward=-2853.00 +/- 1187.20
Episode length: 34.96 +/- 6.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.15      |
| time/               |           |
|    total_timesteps  | 255500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 223       |
|    n_updates        | 53874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.149     |
| time/               |           |
|    episodes         | 5672      |
|    fps              | 300       |
|    time_elapsed     | 851       |
|    total_timesteps  | 255616    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 235       |
|    n_updates        | 53903     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.149     |
| time/               |           |
|    episodes         | 5676      |
|    fps              | 300       |
|    time_elapsed     | 851       |
|    total_timesteps  | 255726    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 224       |
|    n_updates        | 53931     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.148     |
| time/               |           |
|    episodes         | 5680      |
|    fps              | 300       |
|    time_elapsed     | 851       |
|    total_timesteps  | 255847    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 53961     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.147     |
| time/               |           |
|    episodes         | 5684      |
|    fps              | 300       |
|    time_elapsed     | 851       |
|    total_timesteps  | 255968    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 53991     |
-----------------------------------
Eval num_timesteps=256000, episode_reward=-2884.28 +/- 1187.53
Episode length: 34.52 +/- 7.93
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.147     |
| time/               |           |
|    total_timesteps  | 256000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 13.5      |
|    n_updates        | 53999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.146     |
| time/               |           |
|    episodes         | 5688      |
|    fps              | 300       |
|    time_elapsed     | 853       |
|    total_timesteps  | 256101    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 242       |
|    n_updates        | 54025     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.146     |
| time/               |           |
|    episodes         | 5692      |
|    fps              | 300       |
|    time_elapsed     | 853       |
|    total_timesteps  | 256220    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 54054     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.145     |
| time/               |           |
|    episodes         | 5696      |
|    fps              | 300       |
|    time_elapsed     | 853       |
|    total_timesteps  | 256361    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 54090     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.144     |
| time/               |           |
|    episodes         | 5700      |
|    fps              | 300       |
|    time_elapsed     | 853       |
|    total_timesteps  | 256490    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.1      |
|    n_updates        | 54122     |
-----------------------------------
Eval num_timesteps=256500, episode_reward=-2764.35 +/- 1223.46
Episode length: 35.60 +/- 6.51
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.144     |
| time/               |           |
|    total_timesteps  | 256500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 54124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.143     |
| time/               |           |
|    episodes         | 5704      |
|    fps              | 300       |
|    time_elapsed     | 855       |
|    total_timesteps  | 256641    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 54160     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.61e+03 |
|    exploration_rate | 0.142     |
| time/               |           |
|    episodes         | 5708      |
|    fps              | 300       |
|    time_elapsed     | 855       |
|    total_timesteps  | 256783    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 54195     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.8     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.142    |
| time/               |          |
|    episodes         | 5712     |
|    fps              | 300      |
|    time_elapsed     | 855      |
|    total_timesteps  | 256930   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 163      |
|    n_updates        | 54232    |
----------------------------------
Eval num_timesteps=257000, episode_reward=-2808.25 +/- 1147.65
Episode length: 36.18 +/- 6.73
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.141     |
| time/               |           |
|    total_timesteps  | 257000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 54249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.58e+03 |
|    exploration_rate | 0.141     |
| time/               |           |
|    episodes         | 5716      |
|    fps              | 300       |
|    time_elapsed     | 856       |
|    total_timesteps  | 257055    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 54263     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.59e+03 |
|    exploration_rate | 0.14      |
| time/               |           |
|    episodes         | 5720      |
|    fps              | 300       |
|    time_elapsed     | 856       |
|    total_timesteps  | 257173    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 54293     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.55e+03 |
|    exploration_rate | 0.139     |
| time/               |           |
|    episodes         | 5724      |
|    fps              | 300       |
|    time_elapsed     | 857       |
|    total_timesteps  | 257343    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 250       |
|    n_updates        | 54335     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.51e+03 |
|    exploration_rate | 0.138     |
| time/               |           |
|    episodes         | 5728      |
|    fps              | 300       |
|    time_elapsed     | 857       |
|    total_timesteps  | 257479    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.6      |
|    n_updates        | 54369     |
-----------------------------------
Eval num_timesteps=257500, episode_reward=-2719.55 +/- 1287.69
Episode length: 35.90 +/- 6.82
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.138     |
| time/               |           |
|    total_timesteps  | 257500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.1      |
|    n_updates        | 54374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.46e+03 |
|    exploration_rate | 0.138     |
| time/               |           |
|    episodes         | 5732      |
|    fps              | 300       |
|    time_elapsed     | 858       |
|    total_timesteps  | 257600    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.1      |
|    n_updates        | 54399     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.43e+03 |
|    exploration_rate | 0.137     |
| time/               |           |
|    episodes         | 5736      |
|    fps              | 300       |
|    time_elapsed     | 858       |
|    total_timesteps  | 257734    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 54433     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.43e+03 |
|    exploration_rate | 0.136     |
| time/               |           |
|    episodes         | 5740      |
|    fps              | 300       |
|    time_elapsed     | 858       |
|    total_timesteps  | 257870    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 54467     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.4     |
|    ep_rew_mean      | -3.4e+03 |
|    exploration_rate | 0.135    |
| time/               |          |
|    episodes         | 5744     |
|    fps              | 300      |
|    time_elapsed     | 858      |
|    total_timesteps  | 257994   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 107      |
|    n_updates        | 54498    |
----------------------------------
Eval num_timesteps=258000, episode_reward=-2799.48 +/- 1053.16
Episode length: 36.40 +/- 7.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.4     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.135    |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 153      |
|    n_updates        | 54499    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.43e+03 |
|    exploration_rate | 0.134     |
| time/               |           |
|    episodes         | 5748      |
|    fps              | 300       |
|    time_elapsed     | 860       |
|    total_timesteps  | 258125    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 54531     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.44e+03 |
|    exploration_rate | 0.134     |
| time/               |           |
|    episodes         | 5752      |
|    fps              | 300       |
|    time_elapsed     | 860       |
|    total_timesteps  | 258253    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.4      |
|    n_updates        | 54563     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.45e+03 |
|    exploration_rate | 0.133     |
| time/               |           |
|    episodes         | 5756      |
|    fps              | 300       |
|    time_elapsed     | 860       |
|    total_timesteps  | 258366    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 54591     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.44e+03 |
|    exploration_rate | 0.132     |
| time/               |           |
|    episodes         | 5760      |
|    fps              | 300       |
|    time_elapsed     | 860       |
|    total_timesteps  | 258485    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 54621     |
-----------------------------------
Eval num_timesteps=258500, episode_reward=-2736.33 +/- 1226.79
Episode length: 36.66 +/- 6.26
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.7      |
|    mean_reward      | -2.74e+03 |
| rollout/            |           |
|    exploration_rate | 0.132     |
| time/               |           |
|    total_timesteps  | 258500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 54624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.43e+03 |
|    exploration_rate | 0.132     |
| time/               |           |
|    episodes         | 5764      |
|    fps              | 299       |
|    time_elapsed     | 862       |
|    total_timesteps  | 258606    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 54651     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.44e+03 |
|    exploration_rate | 0.131     |
| time/               |           |
|    episodes         | 5768      |
|    fps              | 300       |
|    time_elapsed     | 862       |
|    total_timesteps  | 258726    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 54681     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.45e+03 |
|    exploration_rate | 0.13      |
| time/               |           |
|    episodes         | 5772      |
|    fps              | 300       |
|    time_elapsed     | 862       |
|    total_timesteps  | 258848    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 297       |
|    n_updates        | 54711     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.44e+03 |
|    exploration_rate | 0.129     |
| time/               |           |
|    episodes         | 5776      |
|    fps              | 300       |
|    time_elapsed     | 862       |
|    total_timesteps  | 258982    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 212       |
|    n_updates        | 54745     |
-----------------------------------
Eval num_timesteps=259000, episode_reward=-3110.81 +/- 925.61
Episode length: 35.06 +/- 7.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.129     |
| time/               |           |
|    total_timesteps  | 259000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 54749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.48e+03 |
|    exploration_rate | 0.128     |
| time/               |           |
|    episodes         | 5780      |
|    fps              | 299       |
|    time_elapsed     | 863       |
|    total_timesteps  | 259123    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 54780     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.8      |
|    ep_rew_mean      | -3.48e+03 |
|    exploration_rate | 0.128     |
| time/               |           |
|    episodes         | 5784      |
|    fps              | 300       |
|    time_elapsed     | 863       |
|    total_timesteps  | 259247    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.1      |
|    n_updates        | 54811     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.47e+03 |
|    exploration_rate | 0.127     |
| time/               |           |
|    episodes         | 5788      |
|    fps              | 300       |
|    time_elapsed     | 864       |
|    total_timesteps  | 259348    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 54836     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.48e+03 |
|    exploration_rate | 0.126     |
| time/               |           |
|    episodes         | 5792      |
|    fps              | 300       |
|    time_elapsed     | 864       |
|    total_timesteps  | 259470    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 54867     |
-----------------------------------
Eval num_timesteps=259500, episode_reward=-3101.87 +/- 995.11
Episode length: 33.90 +/- 7.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.9     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.126    |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 117      |
|    n_updates        | 54874    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.3      |
|    ep_rew_mean      | -3.49e+03 |
|    exploration_rate | 0.126     |
| time/               |           |
|    episodes         | 5796      |
|    fps              | 299       |
|    time_elapsed     | 865       |
|    total_timesteps  | 259594    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 194       |
|    n_updates        | 54898     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.49e+03 |
|    exploration_rate | 0.125     |
| time/               |           |
|    episodes         | 5800      |
|    fps              | 300       |
|    time_elapsed     | 865       |
|    total_timesteps  | 259707    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 180       |
|    n_updates        | 54926     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32        |
|    ep_rew_mean      | -3.51e+03 |
|    exploration_rate | 0.124     |
| time/               |           |
|    episodes         | 5804      |
|    fps              | 300       |
|    time_elapsed     | 865       |
|    total_timesteps  | 259838    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.2      |
|    n_updates        | 54959     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.54e+03 |
|    exploration_rate | 0.124     |
| time/               |           |
|    episodes         | 5808      |
|    fps              | 300       |
|    time_elapsed     | 865       |
|    total_timesteps  | 259944    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 54985     |
-----------------------------------
Eval num_timesteps=260000, episode_reward=-2663.83 +/- 1196.46
Episode length: 36.60 +/- 5.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.6      |
|    mean_reward      | -2.66e+03 |
| rollout/            |           |
|    exploration_rate | 0.123     |
| time/               |           |
|    total_timesteps  | 260000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 54999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.4      |
|    ep_rew_mean      | -3.56e+03 |
|    exploration_rate | 0.123     |
| time/               |           |
|    episodes         | 5812      |
|    fps              | 299       |
|    time_elapsed     | 867       |
|    total_timesteps  | 260068    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 55016     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.5      |
|    ep_rew_mean      | -3.57e+03 |
|    exploration_rate | 0.122     |
| time/               |           |
|    episodes         | 5816      |
|    fps              | 299       |
|    time_elapsed     | 867       |
|    total_timesteps  | 260206    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 55051     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.56e+03 |
|    exploration_rate | 0.121     |
| time/               |           |
|    episodes         | 5820      |
|    fps              | 300       |
|    time_elapsed     | 867       |
|    total_timesteps  | 260328    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 55081     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.3      |
|    ep_rew_mean      | -3.57e+03 |
|    exploration_rate | 0.12      |
| time/               |           |
|    episodes         | 5824      |
|    fps              | 300       |
|    time_elapsed     | 867       |
|    total_timesteps  | 260471    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 259       |
|    n_updates        | 55117     |
-----------------------------------
Eval num_timesteps=260500, episode_reward=-2787.85 +/- 1135.40
Episode length: 37.02 +/- 6.28
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37        |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.12      |
| time/               |           |
|    total_timesteps  | 260500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 55124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.1      |
|    ep_rew_mean      | -3.58e+03 |
|    exploration_rate | 0.12      |
| time/               |           |
|    episodes         | 5828      |
|    fps              | 299       |
|    time_elapsed     | 869       |
|    total_timesteps  | 260592    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 216       |
|    n_updates        | 55147     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.2      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.119     |
| time/               |           |
|    episodes         | 5832      |
|    fps              | 299       |
|    time_elapsed     | 869       |
|    total_timesteps  | 260719    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 55179     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31        |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.118     |
| time/               |           |
|    episodes         | 5836      |
|    fps              | 300       |
|    time_elapsed     | 869       |
|    total_timesteps  | 260833    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 55208     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.9      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.117     |
| time/               |           |
|    episodes         | 5840      |
|    fps              | 300       |
|    time_elapsed     | 869       |
|    total_timesteps  | 260965    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 55241     |
-----------------------------------
Eval num_timesteps=261000, episode_reward=-2718.04 +/- 1220.66
Episode length: 35.12 +/- 7.19
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.117     |
| time/               |           |
|    total_timesteps  | 261000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 316       |
|    n_updates        | 55249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.9      |
|    ep_rew_mean      | -3.67e+03 |
|    exploration_rate | 0.117     |
| time/               |           |
|    episodes         | 5844      |
|    fps              | 299       |
|    time_elapsed     | 870       |
|    total_timesteps  | 261087    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 46.6      |
|    n_updates        | 55271     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.8      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.116     |
| time/               |           |
|    episodes         | 5848      |
|    fps              | 299       |
|    time_elapsed     | 870       |
|    total_timesteps  | 261208    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 55301     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.9      |
|    ep_rew_mean      | -3.66e+03 |
|    exploration_rate | 0.115     |
| time/               |           |
|    episodes         | 5852      |
|    fps              | 300       |
|    time_elapsed     | 871       |
|    total_timesteps  | 261345    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.3      |
|    n_updates        | 55336     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.2      |
|    ep_rew_mean      | -3.65e+03 |
|    exploration_rate | 0.114     |
| time/               |           |
|    episodes         | 5856      |
|    fps              | 300       |
|    time_elapsed     | 871       |
|    total_timesteps  | 261482    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 217       |
|    n_updates        | 55370     |
-----------------------------------
Eval num_timesteps=261500, episode_reward=-3146.21 +/- 809.48
Episode length: 33.26 +/- 7.29
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.3      |
|    mean_reward      | -3.15e+03 |
| rollout/            |           |
|    exploration_rate | 0.114     |
| time/               |           |
|    total_timesteps  | 261500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.1      |
|    n_updates        | 55374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.1      |
|    ep_rew_mean      | -3.64e+03 |
|    exploration_rate | 0.114     |
| time/               |           |
|    episodes         | 5860      |
|    fps              | 299       |
|    time_elapsed     | 872       |
|    total_timesteps  | 261599    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 55399     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.9      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.113     |
| time/               |           |
|    episodes         | 5864      |
|    fps              | 299       |
|    time_elapsed     | 872       |
|    total_timesteps  | 261698    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 55424     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 30.9      |
|    ep_rew_mean      | -3.63e+03 |
|    exploration_rate | 0.112     |
| time/               |           |
|    episodes         | 5868      |
|    fps              | 299       |
|    time_elapsed     | 872       |
|    total_timesteps  | 261815    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.9      |
|    n_updates        | 55453     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.1      |
|    ep_rew_mean      | -3.62e+03 |
|    exploration_rate | 0.111     |
| time/               |           |
|    episodes         | 5872      |
|    fps              | 300       |
|    time_elapsed     | 872       |
|    total_timesteps  | 261960    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 55489     |
-----------------------------------
Eval num_timesteps=262000, episode_reward=-2877.29 +/- 998.47
Episode length: 35.02 +/- 7.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.111     |
| time/               |           |
|    total_timesteps  | 262000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 207       |
|    n_updates        | 55499     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.3     |
|    ep_rew_mean      | -3.6e+03 |
|    exploration_rate | 0.111    |
| time/               |          |
|    episodes         | 5876     |
|    fps              | 299      |
|    time_elapsed     | 874      |
|    total_timesteps  | 262113   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 210      |
|    n_updates        | 55528    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.2      |
|    ep_rew_mean      | -3.59e+03 |
|    exploration_rate | 0.11      |
| time/               |           |
|    episodes         | 5880      |
|    fps              | 299       |
|    time_elapsed     | 874       |
|    total_timesteps  | 262242    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.1      |
|    n_updates        | 55560     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.3      |
|    ep_rew_mean      | -3.59e+03 |
|    exploration_rate | 0.109     |
| time/               |           |
|    episodes         | 5884      |
|    fps              | 300       |
|    time_elapsed     | 874       |
|    total_timesteps  | 262381    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 55595     |
-----------------------------------
Eval num_timesteps=262500, episode_reward=-2816.95 +/- 1078.98
Episode length: 35.82 +/- 6.52
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.108     |
| time/               |           |
|    total_timesteps  | 262500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 55624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.7      |
|    ep_rew_mean      | -3.58e+03 |
|    exploration_rate | 0.108     |
| time/               |           |
|    episodes         | 5888      |
|    fps              | 299       |
|    time_elapsed     | 875       |
|    total_timesteps  | 262514    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.3      |
|    n_updates        | 55628     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.6      |
|    ep_rew_mean      | -3.56e+03 |
|    exploration_rate | 0.107     |
| time/               |           |
|    episodes         | 5892      |
|    fps              | 299       |
|    time_elapsed     | 876       |
|    total_timesteps  | 262635    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 55658     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 31.9      |
|    ep_rew_mean      | -3.55e+03 |
|    exploration_rate | 0.107     |
| time/               |           |
|    episodes         | 5896      |
|    fps              | 299       |
|    time_elapsed     | 876       |
|    total_timesteps  | 262789    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 55697     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.1      |
|    ep_rew_mean      | -3.53e+03 |
|    exploration_rate | 0.106     |
| time/               |           |
|    episodes         | 5900      |
|    fps              | 300       |
|    time_elapsed     | 876       |
|    total_timesteps  | 262919    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 55729     |
-----------------------------------
Eval num_timesteps=263000, episode_reward=-2968.85 +/- 1152.65
Episode length: 34.64 +/- 7.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.105     |
| time/               |           |
|    total_timesteps  | 263000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 55749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.2      |
|    ep_rew_mean      | -3.52e+03 |
|    exploration_rate | 0.105     |
| time/               |           |
|    episodes         | 5904      |
|    fps              | 299       |
|    time_elapsed     | 877       |
|    total_timesteps  | 263062    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 55765     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.52e+03 |
|    exploration_rate | 0.104     |
| time/               |           |
|    episodes         | 5908      |
|    fps              | 299       |
|    time_elapsed     | 877       |
|    total_timesteps  | 263211    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 55802     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.51e+03 |
|    exploration_rate | 0.103     |
| time/               |           |
|    episodes         | 5912      |
|    fps              | 299       |
|    time_elapsed     | 877       |
|    total_timesteps  | 263334    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 55833     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.6      |
|    ep_rew_mean      | -3.51e+03 |
|    exploration_rate | 0.102     |
| time/               |           |
|    episodes         | 5916      |
|    fps              | 300       |
|    time_elapsed     | 878       |
|    total_timesteps  | 263468    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.2      |
|    n_updates        | 55866     |
-----------------------------------
Eval num_timesteps=263500, episode_reward=-2694.49 +/- 1193.60
Episode length: 36.12 +/- 7.61
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.102     |
| time/               |           |
|    total_timesteps  | 263500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 55874     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.7     |
|    ep_rew_mean      | -3.5e+03 |
|    exploration_rate | 0.102    |
| time/               |          |
|    episodes         | 5920     |
|    fps              | 299      |
|    time_elapsed     | 879      |
|    total_timesteps  | 263602   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 122      |
|    n_updates        | 55900    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.53e+03 |
|    exploration_rate | 0.101     |
| time/               |           |
|    episodes         | 5924      |
|    fps              | 299       |
|    time_elapsed     | 879       |
|    total_timesteps  | 263716    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 55928     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.53e+03 |
|    exploration_rate | 0.1       |
| time/               |           |
|    episodes         | 5928      |
|    fps              | 299       |
|    time_elapsed     | 879       |
|    total_timesteps  | 263840    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 55959     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.53e+03 |
|    exploration_rate | 0.0994    |
| time/               |           |
|    episodes         | 5932      |
|    fps              | 300       |
|    time_elapsed     | 879       |
|    total_timesteps  | 263963    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 254       |
|    n_updates        | 55990     |
-----------------------------------
Eval num_timesteps=264000, episode_reward=-2719.38 +/- 1277.36
Episode length: 35.88 +/- 7.33
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.0992    |
| time/               |           |
|    total_timesteps  | 264000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 55999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.5      |
|    ep_rew_mean      | -3.53e+03 |
|    exploration_rate | 0.0987    |
| time/               |           |
|    episodes         | 5936      |
|    fps              | 299       |
|    time_elapsed     | 881       |
|    total_timesteps  | 264082    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.3      |
|    n_updates        | 56020     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.4      |
|    ep_rew_mean      | -3.54e+03 |
|    exploration_rate | 0.098     |
| time/               |           |
|    episodes         | 5940      |
|    fps              | 299       |
|    time_elapsed     | 881       |
|    total_timesteps  | 264205    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 56051     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.7      |
|    ep_rew_mean      | -3.54e+03 |
|    exploration_rate | 0.097     |
| time/               |           |
|    episodes         | 5944      |
|    fps              | 299       |
|    time_elapsed     | 881       |
|    total_timesteps  | 264360    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 56089     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 32.9      |
|    ep_rew_mean      | -3.56e+03 |
|    exploration_rate | 0.0962    |
| time/               |           |
|    episodes         | 5948      |
|    fps              | 300       |
|    time_elapsed     | 881       |
|    total_timesteps  | 264495    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 56123     |
-----------------------------------
Eval num_timesteps=264500, episode_reward=-3072.14 +/- 1294.02
Episode length: 34.34 +/- 9.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -3.07e+03 |
| rollout/            |           |
|    exploration_rate | 0.0962    |
| time/               |           |
|    total_timesteps  | 264500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 56124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33        |
|    ep_rew_mean      | -3.55e+03 |
|    exploration_rate | 0.0953    |
| time/               |           |
|    episodes         | 5952      |
|    fps              | 299       |
|    time_elapsed     | 882       |
|    total_timesteps  | 264646    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 56161     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.1     |
|    ep_rew_mean      | -3.5e+03 |
|    exploration_rate | 0.0945   |
| time/               |          |
|    episodes         | 5956     |
|    fps              | 299      |
|    time_elapsed     | 883      |
|    total_timesteps  | 264789   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 236      |
|    n_updates        | 56197    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.51e+03 |
|    exploration_rate | 0.0937    |
| time/               |           |
|    episodes         | 5960      |
|    fps              | 299       |
|    time_elapsed     | 883       |
|    total_timesteps  | 264917    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 56229     |
-----------------------------------
Eval num_timesteps=265000, episode_reward=-2541.86 +/- 1235.56
Episode length: 35.40 +/- 7.75
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.54e+03 |
| rollout/            |           |
|    exploration_rate | 0.0932    |
| time/               |           |
|    total_timesteps  | 265000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79        |
|    n_updates        | 56249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.46e+03 |
|    exploration_rate | 0.0931    |
| time/               |           |
|    episodes         | 5964      |
|    fps              | 299       |
|    time_elapsed     | 884       |
|    total_timesteps  | 265020    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 56254     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.44e+03 |
|    exploration_rate | 0.0922    |
| time/               |           |
|    episodes         | 5968      |
|    fps              | 299       |
|    time_elapsed     | 884       |
|    total_timesteps  | 265157    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 56289     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.4      |
|    ep_rew_mean      | -3.42e+03 |
|    exploration_rate | 0.0914    |
| time/               |           |
|    episodes         | 5972      |
|    fps              | 299       |
|    time_elapsed     | 884       |
|    total_timesteps  | 265298    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 56324     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.3     |
|    ep_rew_mean      | -3.4e+03 |
|    exploration_rate | 0.0905   |
| time/               |          |
|    episodes         | 5976     |
|    fps              | 299      |
|    time_elapsed     | 885      |
|    total_timesteps  | 265447   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 210      |
|    n_updates        | 56361    |
----------------------------------
Eval num_timesteps=265500, episode_reward=-2731.78 +/- 985.82
Episode length: 34.82 +/- 8.26
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.0902    |
| time/               |           |
|    total_timesteps  | 265500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 56374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.41e+03 |
|    exploration_rate | 0.0896    |
| time/               |           |
|    episodes         | 5980      |
|    fps              | 299       |
|    time_elapsed     | 886       |
|    total_timesteps  | 265589    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 240       |
|    n_updates        | 56397     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.5     |
|    ep_rew_mean      | -3.4e+03 |
|    exploration_rate | 0.0887   |
| time/               |          |
|    episodes         | 5984     |
|    fps              | 299      |
|    time_elapsed     | 886      |
|    total_timesteps  | 265733   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 187      |
|    n_updates        | 56433    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -3.41e+03 |
|    exploration_rate | 0.0879    |
| time/               |           |
|    episodes         | 5988      |
|    fps              | 299       |
|    time_elapsed     | 886       |
|    total_timesteps  | 265874    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 252       |
|    n_updates        | 56468     |
-----------------------------------
Eval num_timesteps=266000, episode_reward=-2623.94 +/- 1422.53
Episode length: 34.18 +/- 7.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.62e+03 |
| rollout/            |           |
|    exploration_rate | 0.0871    |
| time/               |           |
|    total_timesteps  | 266000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.6      |
|    n_updates        | 56499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.41e+03 |
|    exploration_rate | 0.087     |
| time/               |           |
|    episodes         | 5992      |
|    fps              | 299       |
|    time_elapsed     | 888       |
|    total_timesteps  | 266019    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 56504     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.39e+03 |
|    exploration_rate | 0.0861    |
| time/               |           |
|    episodes         | 5996      |
|    fps              | 299       |
|    time_elapsed     | 888       |
|    total_timesteps  | 266170    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 211       |
|    n_updates        | 56542     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.41e+03 |
|    exploration_rate | 0.0852    |
| time/               |           |
|    episodes         | 6000      |
|    fps              | 299       |
|    time_elapsed     | 888       |
|    total_timesteps  | 266312    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 239       |
|    n_updates        | 56577     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.42e+03 |
|    exploration_rate | 0.0845    |
| time/               |           |
|    episodes         | 6004      |
|    fps              | 299       |
|    time_elapsed     | 888       |
|    total_timesteps  | 266435    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 56608     |
-----------------------------------
Eval num_timesteps=266500, episode_reward=-2864.34 +/- 1162.61
Episode length: 34.10 +/- 7.04
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.0841    |
| time/               |           |
|    total_timesteps  | 266500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.7      |
|    n_updates        | 56624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.39e+03 |
|    exploration_rate | 0.0838    |
| time/               |           |
|    episodes         | 6008      |
|    fps              | 299       |
|    time_elapsed     | 889       |
|    total_timesteps  | 266543    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 242       |
|    n_updates        | 56635     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.37e+03 |
|    exploration_rate | 0.083     |
| time/               |           |
|    episodes         | 6012      |
|    fps              | 299       |
|    time_elapsed     | 890       |
|    total_timesteps  | 266680    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 240       |
|    n_updates        | 56669     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.32e+03 |
|    exploration_rate | 0.082     |
| time/               |           |
|    episodes         | 6016      |
|    fps              | 299       |
|    time_elapsed     | 890       |
|    total_timesteps  | 266841    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 56710     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.32e+03 |
|    exploration_rate | 0.0812    |
| time/               |           |
|    episodes         | 6020      |
|    fps              | 299       |
|    time_elapsed     | 890       |
|    total_timesteps  | 266984    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 56745     |
-----------------------------------
Eval num_timesteps=267000, episode_reward=-3135.87 +/- 1000.78
Episode length: 33.92 +/- 7.36
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -3.14e+03 |
| rollout/            |           |
|    exploration_rate | 0.0811    |
| time/               |           |
|    total_timesteps  | 267000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 56749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.27e+03 |
|    exploration_rate | 0.0801    |
| time/               |           |
|    episodes         | 6024      |
|    fps              | 299       |
|    time_elapsed     | 891       |
|    total_timesteps  | 267154    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 56788     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.28e+03 |
|    exploration_rate | 0.0792    |
| time/               |           |
|    episodes         | 6028      |
|    fps              | 299       |
|    time_elapsed     | 891       |
|    total_timesteps  | 267313    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 56828     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.26e+03 |
|    exploration_rate | 0.0783    |
| time/               |           |
|    episodes         | 6032      |
|    fps              | 299       |
|    time_elapsed     | 892       |
|    total_timesteps  | 267453    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 214       |
|    n_updates        | 56863     |
-----------------------------------
Eval num_timesteps=267500, episode_reward=-2811.39 +/- 1052.68
Episode length: 35.58 +/- 6.67
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.078     |
| time/               |           |
|    total_timesteps  | 267500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 56874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -3.23e+03 |
|    exploration_rate | 0.0774    |
| time/               |           |
|    episodes         | 6036      |
|    fps              | 299       |
|    time_elapsed     | 893       |
|    total_timesteps  | 267605    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 56901     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -3.21e+03 |
|    exploration_rate | 0.0765    |
| time/               |           |
|    episodes         | 6040      |
|    fps              | 299       |
|    time_elapsed     | 893       |
|    total_timesteps  | 267753    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 56938     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.24e+03 |
|    exploration_rate | 0.0758    |
| time/               |           |
|    episodes         | 6044      |
|    fps              | 299       |
|    time_elapsed     | 893       |
|    total_timesteps  | 267872    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.4      |
|    n_updates        | 56967     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.24e+03 |
|    exploration_rate | 0.0751    |
| time/               |           |
|    episodes         | 6048      |
|    fps              | 299       |
|    time_elapsed     | 893       |
|    total_timesteps  | 267987    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 56996     |
-----------------------------------
Eval num_timesteps=268000, episode_reward=-2780.80 +/- 1185.09
Episode length: 37.42 +/- 6.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.4      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.075     |
| time/               |           |
|    total_timesteps  | 268000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.6      |
|    n_updates        | 56999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.23e+03 |
|    exploration_rate | 0.0742    |
| time/               |           |
|    episodes         | 6052      |
|    fps              | 299       |
|    time_elapsed     | 895       |
|    total_timesteps  | 268132    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 57032     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.29e+03 |
|    exploration_rate | 0.0735    |
| time/               |           |
|    episodes         | 6056      |
|    fps              | 299       |
|    time_elapsed     | 895       |
|    total_timesteps  | 268251    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 57062     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.26e+03 |
|    exploration_rate | 0.0726    |
| time/               |           |
|    episodes         | 6060      |
|    fps              | 299       |
|    time_elapsed     | 895       |
|    total_timesteps  | 268386    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 57096     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.31e+03 |
|    exploration_rate | 0.072     |
| time/               |           |
|    episodes         | 6064      |
|    fps              | 299       |
|    time_elapsed     | 895       |
|    total_timesteps  | 268488    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 57121     |
-----------------------------------
Eval num_timesteps=268500, episode_reward=-2902.05 +/- 1113.78
Episode length: 35.18 +/- 8.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.2     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.0719   |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 86.7     |
|    n_updates        | 57124    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.34e+03 |
|    exploration_rate | 0.0713    |
| time/               |           |
|    episodes         | 6068      |
|    fps              | 299       |
|    time_elapsed     | 897       |
|    total_timesteps  | 268603    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.9      |
|    n_updates        | 57150     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.34e+03 |
|    exploration_rate | 0.0704    |
| time/               |           |
|    episodes         | 6072      |
|    fps              | 299       |
|    time_elapsed     | 897       |
|    total_timesteps  | 268758    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 57189     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.35e+03 |
|    exploration_rate | 0.0694    |
| time/               |           |
|    episodes         | 6076      |
|    fps              | 299       |
|    time_elapsed     | 897       |
|    total_timesteps  | 268914    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 245       |
|    n_updates        | 57228     |
-----------------------------------
Eval num_timesteps=269000, episode_reward=-3108.98 +/- 1056.33
Episode length: 34.86 +/- 6.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.0689    |
| time/               |           |
|    total_timesteps  | 269000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 57249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.34e+03 |
|    exploration_rate | 0.0686    |
| time/               |           |
|    episodes         | 6080      |
|    fps              | 299       |
|    time_elapsed     | 898       |
|    total_timesteps  | 269050    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.9      |
|    n_updates        | 57262     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.34e+03 |
|    exploration_rate | 0.0678    |
| time/               |           |
|    episodes         | 6084      |
|    fps              | 299       |
|    time_elapsed     | 898       |
|    total_timesteps  | 269172    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 57292     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.34e+03 |
|    exploration_rate | 0.067     |
| time/               |           |
|    episodes         | 6088      |
|    fps              | 299       |
|    time_elapsed     | 899       |
|    total_timesteps  | 269305    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 57326     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.35e+03 |
|    exploration_rate | 0.0662    |
| time/               |           |
|    episodes         | 6092      |
|    fps              | 299       |
|    time_elapsed     | 899       |
|    total_timesteps  | 269438    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 57359     |
-----------------------------------
Eval num_timesteps=269500, episode_reward=-2943.90 +/- 858.01
Episode length: 36.00 +/- 6.23
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.0658    |
| time/               |           |
|    total_timesteps  | 269500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 57374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.36e+03 |
|    exploration_rate | 0.0654    |
| time/               |           |
|    episodes         | 6096      |
|    fps              | 299       |
|    time_elapsed     | 900       |
|    total_timesteps  | 269578    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 251       |
|    n_updates        | 57394     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.1     |
|    ep_rew_mean      | -3.3e+03 |
|    exploration_rate | 0.0645   |
| time/               |          |
|    episodes         | 6100     |
|    fps              | 299      |
|    time_elapsed     | 900      |
|    total_timesteps  | 269726   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 125      |
|    n_updates        | 57431    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.2     |
|    ep_rew_mean      | -3.3e+03 |
|    exploration_rate | 0.0637   |
| time/               |          |
|    episodes         | 6104     |
|    fps              | 299      |
|    time_elapsed     | 900      |
|    total_timesteps  | 269853   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 112      |
|    n_updates        | 57463    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.32e+03 |
|    exploration_rate | 0.0628    |
| time/               |           |
|    episodes         | 6108      |
|    fps              | 299       |
|    time_elapsed     | 901       |
|    total_timesteps  | 269993    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 57498     |
-----------------------------------
Eval num_timesteps=270000, episode_reward=-2723.91 +/- 1418.93
Episode length: 35.62 +/- 6.52
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.0628    |
| time/               |           |
|    total_timesteps  | 270000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.8      |
|    n_updates        | 57499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.32e+03 |
|    exploration_rate | 0.062     |
| time/               |           |
|    episodes         | 6112      |
|    fps              | 299       |
|    time_elapsed     | 902       |
|    total_timesteps  | 270129    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 314       |
|    n_updates        | 57532     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.36e+03 |
|    exploration_rate | 0.0613    |
| time/               |           |
|    episodes         | 6116      |
|    fps              | 299       |
|    time_elapsed     | 902       |
|    total_timesteps  | 270250    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 57562     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.33e+03 |
|    exploration_rate | 0.0604    |
| time/               |           |
|    episodes         | 6120      |
|    fps              | 299       |
|    time_elapsed     | 902       |
|    total_timesteps  | 270394    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75        |
|    n_updates        | 57598     |
-----------------------------------
Eval num_timesteps=270500, episode_reward=-2714.87 +/- 1201.44
Episode length: 36.12 +/- 6.08
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.71e+03 |
| rollout/            |           |
|    exploration_rate | 0.0597    |
| time/               |           |
|    total_timesteps  | 270500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 262       |
|    n_updates        | 57624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.38e+03 |
|    exploration_rate | 0.0596    |
| time/               |           |
|    episodes         | 6124      |
|    fps              | 299       |
|    time_elapsed     | 904       |
|    total_timesteps  | 270525    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 192       |
|    n_updates        | 57631     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.3      |
|    ep_rew_mean      | -3.39e+03 |
|    exploration_rate | 0.0588    |
| time/               |           |
|    episodes         | 6128      |
|    fps              | 299       |
|    time_elapsed     | 904       |
|    total_timesteps  | 270645    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 295       |
|    n_updates        | 57661     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.34e+03 |
|    exploration_rate | 0.0579    |
| time/               |           |
|    episodes         | 6132      |
|    fps              | 299       |
|    time_elapsed     | 904       |
|    total_timesteps  | 270800    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73        |
|    n_updates        | 57699     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.35e+03 |
|    exploration_rate | 0.0571    |
| time/               |           |
|    episodes         | 6136      |
|    fps              | 299       |
|    time_elapsed     | 904       |
|    total_timesteps  | 270928    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 57731     |
-----------------------------------
Eval num_timesteps=271000, episode_reward=-2564.00 +/- 1181.26
Episode length: 38.68 +/- 8.73
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 38.7      |
|    mean_reward      | -2.56e+03 |
| rollout/            |           |
|    exploration_rate | 0.0567    |
| time/               |           |
|    total_timesteps  | 271000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 57749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.1      |
|    ep_rew_mean      | -3.36e+03 |
|    exploration_rate | 0.0562    |
| time/               |           |
|    episodes         | 6140      |
|    fps              | 299       |
|    time_elapsed     | 906       |
|    total_timesteps  | 271068    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 361       |
|    n_updates        | 57766     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.2      |
|    ep_rew_mean      | -3.34e+03 |
|    exploration_rate | 0.0555    |
| time/               |           |
|    episodes         | 6144      |
|    fps              | 299       |
|    time_elapsed     | 906       |
|    total_timesteps  | 271193    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 57798     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.29e+03 |
|    exploration_rate | 0.0546    |
| time/               |           |
|    episodes         | 6148      |
|    fps              | 299       |
|    time_elapsed     | 906       |
|    total_timesteps  | 271334    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 221       |
|    n_updates        | 57833     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.5      |
|    ep_rew_mean      | -3.29e+03 |
|    exploration_rate | 0.0537    |
| time/               |           |
|    episodes         | 6152      |
|    fps              | 299       |
|    time_elapsed     | 906       |
|    total_timesteps  | 271481    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 57870     |
-----------------------------------
Eval num_timesteps=271500, episode_reward=-2580.32 +/- 1350.59
Episode length: 37.34 +/- 6.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.3      |
|    mean_reward      | -2.58e+03 |
| rollout/            |           |
|    exploration_rate | 0.0536    |
| time/               |           |
|    total_timesteps  | 271500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.6      |
|    n_updates        | 57874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.26e+03 |
|    exploration_rate | 0.0529    |
| time/               |           |
|    episodes         | 6156      |
|    fps              | 298       |
|    time_elapsed     | 908       |
|    total_timesteps  | 271621    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 57905     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.27e+03 |
|    exploration_rate | 0.052     |
| time/               |           |
|    episodes         | 6160      |
|    fps              | 299       |
|    time_elapsed     | 908       |
|    total_timesteps  | 271756    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 294       |
|    n_updates        | 57938     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.25e+03 |
|    exploration_rate | 0.0511    |
| time/               |           |
|    episodes         | 6164      |
|    fps              | 299       |
|    time_elapsed     | 908       |
|    total_timesteps  | 271901    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.8      |
|    n_updates        | 57975     |
-----------------------------------
Eval num_timesteps=272000, episode_reward=-2888.12 +/- 933.14
Episode length: 37.16 +/- 8.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.2      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.0505    |
| time/               |           |
|    total_timesteps  | 272000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 57999     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.6     |
|    ep_rew_mean      | -3.2e+03 |
|    exploration_rate | 0.0502   |
| time/               |          |
|    episodes         | 6168     |
|    fps              | 298      |
|    time_elapsed     | 910      |
|    total_timesteps  | 272059   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 210      |
|    n_updates        | 58014    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.6     |
|    ep_rew_mean      | -3.2e+03 |
|    exploration_rate | 0.0492   |
| time/               |          |
|    episodes         | 6172     |
|    fps              | 298      |
|    time_elapsed     | 910      |
|    total_timesteps  | 272218   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 85.8     |
|    n_updates        | 58054    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.23e+03 |
|    exploration_rate | 0.0484    |
| time/               |           |
|    episodes         | 6176      |
|    fps              | 299       |
|    time_elapsed     | 910       |
|    total_timesteps  | 272351    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 26        |
|    n_updates        | 58087     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.26e+03 |
|    exploration_rate | 0.0476    |
| time/               |           |
|    episodes         | 6180      |
|    fps              | 299       |
|    time_elapsed     | 910       |
|    total_timesteps  | 272471    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 58117     |
-----------------------------------
Eval num_timesteps=272500, episode_reward=-2995.27 +/- 963.45
Episode length: 35.10 +/- 6.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.1     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.0475   |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 68.5     |
|    n_updates        | 58124    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.24e+03 |
|    exploration_rate | 0.0468    |
| time/               |           |
|    episodes         | 6184      |
|    fps              | 298       |
|    time_elapsed     | 912       |
|    total_timesteps  | 272605    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 220       |
|    n_updates        | 58151     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.22e+03 |
|    exploration_rate | 0.0459    |
| time/               |           |
|    episodes         | 6188      |
|    fps              | 298       |
|    time_elapsed     | 912       |
|    total_timesteps  | 272753    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 216       |
|    n_updates        | 58188     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.19e+03 |
|    exploration_rate | 0.045     |
| time/               |           |
|    episodes         | 6192      |
|    fps              | 299       |
|    time_elapsed     | 912       |
|    total_timesteps  | 272893    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 58223     |
-----------------------------------
Eval num_timesteps=273000, episode_reward=-2730.52 +/- 1170.25
Episode length: 35.86 +/- 5.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.0444    |
| time/               |           |
|    total_timesteps  | 273000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.5      |
|    n_updates        | 58249     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.5     |
|    ep_rew_mean      | -3.2e+03 |
|    exploration_rate | 0.0442   |
| time/               |          |
|    episodes         | 6196     |
|    fps              | 298      |
|    time_elapsed     | 913      |
|    total_timesteps  | 273029   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 111      |
|    n_updates        | 58257    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.24e+03 |
|    exploration_rate | 0.0434    |
| time/               |           |
|    episodes         | 6200      |
|    fps              | 298       |
|    time_elapsed     | 913       |
|    total_timesteps  | 273166    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.4      |
|    n_updates        | 58291     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.21e+03 |
|    exploration_rate | 0.0424    |
| time/               |           |
|    episodes         | 6204      |
|    fps              | 298       |
|    time_elapsed     | 914       |
|    total_timesteps  | 273324    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 58330     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.19e+03 |
|    exploration_rate | 0.0415    |
| time/               |           |
|    episodes         | 6208      |
|    fps              | 299       |
|    time_elapsed     | 914       |
|    total_timesteps  | 273468    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 196       |
|    n_updates        | 58366     |
-----------------------------------
Eval num_timesteps=273500, episode_reward=-2966.48 +/- 1142.61
Episode length: 33.52 +/- 7.06
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.5      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.0413    |
| time/               |           |
|    total_timesteps  | 273500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 58374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.16e+03 |
|    exploration_rate | 0.0406    |
| time/               |           |
|    episodes         | 6212      |
|    fps              | 298       |
|    time_elapsed     | 915       |
|    total_timesteps  | 273614    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 58403     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.17e+03 |
|    exploration_rate | 0.0397    |
| time/               |           |
|    episodes         | 6216      |
|    fps              | 298       |
|    time_elapsed     | 915       |
|    total_timesteps  | 273753    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 244       |
|    n_updates        | 58438     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.22e+03 |
|    exploration_rate | 0.0389    |
| time/               |           |
|    episodes         | 6220      |
|    fps              | 299       |
|    time_elapsed     | 915       |
|    total_timesteps  | 273887    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 58471     |
-----------------------------------
Eval num_timesteps=274000, episode_reward=-2816.80 +/- 1181.87
Episode length: 35.88 +/- 6.29
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.0382    |
| time/               |           |
|    total_timesteps  | 274000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 58499     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -3.2e+03 |
|    exploration_rate | 0.0381   |
| time/               |          |
|    episodes         | 6224     |
|    fps              | 298      |
|    time_elapsed     | 917      |
|    total_timesteps  | 274026   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 146      |
|    n_updates        | 58506    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.19e+03 |
|    exploration_rate | 0.0373    |
| time/               |           |
|    episodes         | 6228      |
|    fps              | 298       |
|    time_elapsed     | 917       |
|    total_timesteps  | 274158    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 58539     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.25e+03 |
|    exploration_rate | 0.0365    |
| time/               |           |
|    episodes         | 6232      |
|    fps              | 298       |
|    time_elapsed     | 917       |
|    total_timesteps  | 274279    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 29.3      |
|    n_updates        | 58569     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.23e+03 |
|    exploration_rate | 0.0356    |
| time/               |           |
|    episodes         | 6236      |
|    fps              | 299       |
|    time_elapsed     | 917       |
|    total_timesteps  | 274421    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 58605     |
-----------------------------------
Eval num_timesteps=274500, episode_reward=-3162.56 +/- 926.50
Episode length: 35.00 +/- 7.36
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -3.16e+03 |
| rollout/            |           |
|    exploration_rate | 0.0351    |
| time/               |           |
|    total_timesteps  | 274500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 58624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.19e+03 |
|    exploration_rate | 0.0349    |
| time/               |           |
|    episodes         | 6240      |
|    fps              | 298       |
|    time_elapsed     | 919       |
|    total_timesteps  | 274547    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.7      |
|    n_updates        | 58636     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.19e+03 |
|    exploration_rate | 0.0341    |
| time/               |           |
|    episodes         | 6244      |
|    fps              | 298       |
|    time_elapsed     | 919       |
|    total_timesteps  | 274677    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 58669     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.24e+03 |
|    exploration_rate | 0.0332    |
| time/               |           |
|    episodes         | 6248      |
|    fps              | 298       |
|    time_elapsed     | 919       |
|    total_timesteps  | 274811    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 58702     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.19e+03 |
|    exploration_rate | 0.0323    |
| time/               |           |
|    episodes         | 6252      |
|    fps              | 299       |
|    time_elapsed     | 919       |
|    total_timesteps  | 274967    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 58741     |
-----------------------------------
Eval num_timesteps=275000, episode_reward=-2865.15 +/- 1145.95
Episode length: 34.58 +/- 6.53
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.0321    |
| time/               |           |
|    total_timesteps  | 275000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.9      |
|    n_updates        | 58749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.21e+03 |
|    exploration_rate | 0.0314    |
| time/               |           |
|    episodes         | 6256      |
|    fps              | 298       |
|    time_elapsed     | 920       |
|    total_timesteps  | 275099    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 246       |
|    n_updates        | 58774     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.21e+03 |
|    exploration_rate | 0.0305    |
| time/               |           |
|    episodes         | 6260      |
|    fps              | 298       |
|    time_elapsed     | 921       |
|    total_timesteps  | 275254    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 58813     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -3.18e+03 |
|    exploration_rate | 0.0294    |
| time/               |           |
|    episodes         | 6264      |
|    fps              | 298       |
|    time_elapsed     | 921       |
|    total_timesteps  | 275422    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.9      |
|    n_updates        | 58855     |
-----------------------------------
Eval num_timesteps=275500, episode_reward=-2446.59 +/- 1349.64
Episode length: 37.80 +/- 5.82
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.8      |
|    mean_reward      | -2.45e+03 |
| rollout/            |           |
|    exploration_rate | 0.029     |
| time/               |           |
|    total_timesteps  | 275500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 58874     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -3.2e+03 |
|    exploration_rate | 0.0286   |
| time/               |          |
|    episodes         | 6268     |
|    fps              | 298      |
|    time_elapsed     | 922      |
|    total_timesteps  | 275559   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 159      |
|    n_updates        | 58889    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.19e+03 |
|    exploration_rate | 0.0278    |
| time/               |           |
|    episodes         | 6272      |
|    fps              | 298       |
|    time_elapsed     | 922       |
|    total_timesteps  | 275688    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 254       |
|    n_updates        | 58921     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.17e+03 |
|    exploration_rate | 0.027     |
| time/               |           |
|    episodes         | 6276      |
|    fps              | 298       |
|    time_elapsed     | 923       |
|    total_timesteps  | 275824    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 214       |
|    n_updates        | 58955     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.11e+03 |
|    exploration_rate | 0.0261    |
| time/               |           |
|    episodes         | 6280      |
|    fps              | 298       |
|    time_elapsed     | 923       |
|    total_timesteps  | 275960    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 58989     |
-----------------------------------
Eval num_timesteps=276000, episode_reward=-3091.08 +/- 974.55
Episode length: 36.50 +/- 7.15
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -3.09e+03 |
| rollout/            |           |
|    exploration_rate | 0.0259    |
| time/               |           |
|    total_timesteps  | 276000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 40.4      |
|    n_updates        | 58999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.07e+03 |
|    exploration_rate | 0.0253    |
| time/               |           |
|    episodes         | 6284      |
|    fps              | 298       |
|    time_elapsed     | 924       |
|    total_timesteps  | 276091    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 167       |
|    n_updates        | 59022     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.04e+03 |
|    exploration_rate | 0.0243    |
| time/               |           |
|    episodes         | 6288      |
|    fps              | 298       |
|    time_elapsed     | 924       |
|    total_timesteps  | 276246    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 207       |
|    n_updates        | 59061     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.04e+03 |
|    exploration_rate | 0.0234    |
| time/               |           |
|    episodes         | 6292      |
|    fps              | 298       |
|    time_elapsed     | 924       |
|    total_timesteps  | 276399    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 59099     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.0228    |
| time/               |           |
|    episodes         | 6296      |
|    fps              | 298       |
|    time_elapsed     | 925       |
|    total_timesteps  | 276498    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 167       |
|    n_updates        | 59124     |
-----------------------------------
Eval num_timesteps=276500, episode_reward=-2485.04 +/- 1298.32
Episode length: 36.38 +/- 6.53
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.49e+03 |
| rollout/            |           |
|    exploration_rate | 0.0228    |
| time/               |           |
|    total_timesteps  | 276500    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.0218   |
| time/               |          |
|    episodes         | 6300     |
|    fps              | 298      |
|    time_elapsed     | 926      |
|    total_timesteps  | 276663   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 168      |
|    n_updates        | 59165    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.7     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.0209   |
| time/               |          |
|    episodes         | 6304     |
|    fps              | 298      |
|    time_elapsed     | 926      |
|    total_timesteps  | 276797   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 160      |
|    n_updates        | 59199    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.02      |
| time/               |           |
|    episodes         | 6308      |
|    fps              | 298       |
|    time_elapsed     | 926       |
|    total_timesteps  | 276955    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 59238     |
-----------------------------------
Eval num_timesteps=277000, episode_reward=-2999.81 +/- 1131.61
Episode length: 35.94 +/- 6.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.9     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.0197   |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 205      |
|    n_updates        | 59249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.0191    |
| time/               |           |
|    episodes         | 6312      |
|    fps              | 298       |
|    time_elapsed     | 928       |
|    total_timesteps  | 277092    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 235       |
|    n_updates        | 59272     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.0181    |
| time/               |           |
|    episodes         | 6316      |
|    fps              | 298       |
|    time_elapsed     | 928       |
|    total_timesteps  | 277252    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 254       |
|    n_updates        | 59312     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.0171   |
| time/               |          |
|    episodes         | 6320     |
|    fps              | 298      |
|    time_elapsed     | 928      |
|    total_timesteps  | 277415   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 160      |
|    n_updates        | 59353    |
----------------------------------
Eval num_timesteps=277500, episode_reward=-2779.20 +/- 1315.83
Episode length: 34.54 +/- 6.89
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.0166    |
| time/               |           |
|    total_timesteps  | 277500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 59374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.0161    |
| time/               |           |
|    episodes         | 6324      |
|    fps              | 298       |
|    time_elapsed     | 929       |
|    total_timesteps  | 277571    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 215       |
|    n_updates        | 59392     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.0152    |
| time/               |           |
|    episodes         | 6328      |
|    fps              | 298       |
|    time_elapsed     | 930       |
|    total_timesteps  | 277723    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 59430     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.0143    |
| time/               |           |
|    episodes         | 6332      |
|    fps              | 298       |
|    time_elapsed     | 930       |
|    total_timesteps  | 277863    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 59465     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.0136    |
| time/               |           |
|    episodes         | 6336      |
|    fps              | 298       |
|    time_elapsed     | 930       |
|    total_timesteps  | 277971    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 59492     |
-----------------------------------
Eval num_timesteps=278000, episode_reward=-2738.46 +/- 1065.52
Episode length: 33.00 +/- 6.87
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33        |
|    mean_reward      | -2.74e+03 |
| rollout/            |           |
|    exploration_rate | 0.0135    |
| time/               |           |
|    total_timesteps  | 278000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 249       |
|    n_updates        | 59499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.0126    |
| time/               |           |
|    episodes         | 6340      |
|    fps              | 298       |
|    time_elapsed     | 931       |
|    total_timesteps  | 278138    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 59534     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.0117    |
| time/               |           |
|    episodes         | 6344      |
|    fps              | 298       |
|    time_elapsed     | 931       |
|    total_timesteps  | 278279    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 59569     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.0108    |
| time/               |           |
|    episodes         | 6348      |
|    fps              | 298       |
|    time_elapsed     | 932       |
|    total_timesteps  | 278427    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.7      |
|    n_updates        | 59606     |
-----------------------------------
Eval num_timesteps=278500, episode_reward=-2821.93 +/- 1142.75
Episode length: 35.96 +/- 5.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.0104    |
| time/               |           |
|    total_timesteps  | 278500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 42.7      |
|    n_updates        | 59624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.00995  |
| time/               |          |
|    episodes         | 6352     |
|    fps              | 298      |
|    time_elapsed     | 933      |
|    total_timesteps  | 278565   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 104      |
|    n_updates        | 59641    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.00912   |
| time/               |           |
|    episodes         | 6356      |
|    fps              | 298       |
|    time_elapsed     | 933       |
|    total_timesteps  | 278698    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 346       |
|    n_updates        | 59674     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.00825   |
| time/               |           |
|    episodes         | 6360      |
|    fps              | 298       |
|    time_elapsed     | 933       |
|    total_timesteps  | 278837    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 59709     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.00741   |
| time/               |           |
|    episodes         | 6364      |
|    fps              | 298       |
|    time_elapsed     | 933       |
|    total_timesteps  | 278973    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 59743     |
-----------------------------------
Eval num_timesteps=279000, episode_reward=-2723.92 +/- 1230.83
Episode length: 35.26 +/- 7.08
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.00724   |
| time/               |           |
|    total_timesteps  | 279000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.8      |
|    n_updates        | 59749     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.5     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.00658  |
| time/               |          |
|    episodes         | 6368     |
|    fps              | 298      |
|    time_elapsed     | 935      |
|    total_timesteps  | 279106   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 141      |
|    n_updates        | 59776    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.00578   |
| time/               |           |
|    episodes         | 6372      |
|    fps              | 298       |
|    time_elapsed     | 935       |
|    total_timesteps  | 279234    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.2      |
|    n_updates        | 59808     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.00511   |
| time/               |           |
|    episodes         | 6376      |
|    fps              | 298       |
|    time_elapsed     | 935       |
|    total_timesteps  | 279342    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 196       |
|    n_updates        | 59835     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.00427   |
| time/               |           |
|    episodes         | 6380      |
|    fps              | 298       |
|    time_elapsed     | 935       |
|    total_timesteps  | 279476    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 235       |
|    n_updates        | 59868     |
-----------------------------------
Eval num_timesteps=279500, episode_reward=-3005.78 +/- 1095.10
Episode length: 32.76 +/- 7.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.8      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.00413   |
| time/               |           |
|    total_timesteps  | 279500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 229       |
|    n_updates        | 59874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.00326   |
| time/               |           |
|    episodes         | 6384      |
|    fps              | 298       |
|    time_elapsed     | 937       |
|    total_timesteps  | 279638    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 59909     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.00245   |
| time/               |           |
|    episodes         | 6388      |
|    fps              | 298       |
|    time_elapsed     | 937       |
|    total_timesteps  | 279768    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 59941     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.00151   |
| time/               |           |
|    episodes         | 6392      |
|    fps              | 298       |
|    time_elapsed     | 937       |
|    total_timesteps  | 279918    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.7      |
|    n_updates        | 59979     |
-----------------------------------
Eval num_timesteps=280000, episode_reward=-2623.67 +/- 1315.81
Episode length: 35.20 +/- 7.03
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.62e+03 |
| rollout/            |           |
|    exploration_rate | 0.00101   |
| time/               |           |
|    total_timesteps  | 280000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 27.1      |
|    n_updates        | 59999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6396      |
|    fps              | 298       |
|    time_elapsed     | 938       |
|    total_timesteps  | 280079    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 205       |
|    n_updates        | 60019     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6400      |
|    fps              | 298       |
|    time_elapsed     | 938       |
|    total_timesteps  | 280218    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 212       |
|    n_updates        | 60054     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6404      |
|    fps              | 298       |
|    time_elapsed     | 939       |
|    total_timesteps  | 280380    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 259       |
|    n_updates        | 60094     |
-----------------------------------
Eval num_timesteps=280500, episode_reward=-2825.19 +/- 1034.31
Episode length: 34.20 +/- 7.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 280500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.8      |
|    n_updates        | 60124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6408      |
|    fps              | 298       |
|    time_elapsed     | 940       |
|    total_timesteps  | 280516    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 60128     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6412      |
|    fps              | 298       |
|    time_elapsed     | 940       |
|    total_timesteps  | 280653    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 60163     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6416      |
|    fps              | 298       |
|    time_elapsed     | 940       |
|    total_timesteps  | 280776    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 60193     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6420      |
|    fps              | 298       |
|    time_elapsed     | 940       |
|    total_timesteps  | 280915    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 236       |
|    n_updates        | 60228     |
-----------------------------------
Eval num_timesteps=281000, episode_reward=-2937.22 +/- 1224.81
Episode length: 33.60 +/- 6.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.6      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 281000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 60249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6424      |
|    fps              | 298       |
|    time_elapsed     | 942       |
|    total_timesteps  | 281042    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91        |
|    n_updates        | 60260     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6428      |
|    fps              | 298       |
|    time_elapsed     | 942       |
|    total_timesteps  | 281176    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 60293     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6432      |
|    fps              | 298       |
|    time_elapsed     | 942       |
|    total_timesteps  | 281318    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.8      |
|    n_updates        | 60329     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6436      |
|    fps              | 298       |
|    time_elapsed     | 942       |
|    total_timesteps  | 281445    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 242       |
|    n_updates        | 60361     |
-----------------------------------
Eval num_timesteps=281500, episode_reward=-3047.99 +/- 924.04
Episode length: 33.86 +/- 6.97
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -3.05e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 281500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 60374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6440      |
|    fps              | 298       |
|    time_elapsed     | 944       |
|    total_timesteps  | 281585    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 60396     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6444      |
|    fps              | 298       |
|    time_elapsed     | 944       |
|    total_timesteps  | 281722    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.2      |
|    n_updates        | 60430     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6448      |
|    fps              | 298       |
|    time_elapsed     | 944       |
|    total_timesteps  | 281870    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 324       |
|    n_updates        | 60467     |
-----------------------------------
Eval num_timesteps=282000, episode_reward=-2717.88 +/- 1055.40
Episode length: 37.24 +/- 5.72
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.2      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 282000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 60499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6452      |
|    fps              | 298       |
|    time_elapsed     | 945       |
|    total_timesteps  | 282005    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 60501     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6456      |
|    fps              | 298       |
|    time_elapsed     | 945       |
|    total_timesteps  | 282125    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 60531     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6460      |
|    fps              | 298       |
|    time_elapsed     | 946       |
|    total_timesteps  | 282283    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 60570     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6464      |
|    fps              | 298       |
|    time_elapsed     | 946       |
|    total_timesteps  | 282401    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 60600     |
-----------------------------------
Eval num_timesteps=282500, episode_reward=-3020.26 +/- 1009.41
Episode length: 35.00 +/- 6.09
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -3.02e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 282500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 199       |
|    n_updates        | 60624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.4     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6468     |
|    fps              | 298      |
|    time_elapsed     | 947      |
|    total_timesteps  | 282542   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 76       |
|    n_updates        | 60635    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6472      |
|    fps              | 298       |
|    time_elapsed     | 947       |
|    total_timesteps  | 282676    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 211       |
|    n_updates        | 60668     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6476      |
|    fps              | 298       |
|    time_elapsed     | 947       |
|    total_timesteps  | 282816    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 60703     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6480      |
|    fps              | 298       |
|    time_elapsed     | 948       |
|    total_timesteps  | 282974    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 253       |
|    n_updates        | 60743     |
-----------------------------------
Eval num_timesteps=283000, episode_reward=-2753.57 +/- 1178.87
Episode length: 36.78 +/- 5.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 283000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 60749     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.7     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6484     |
|    fps              | 298      |
|    time_elapsed     | 949      |
|    total_timesteps  | 283110   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 163      |
|    n_updates        | 60777    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6488      |
|    fps              | 298       |
|    time_elapsed     | 949       |
|    total_timesteps  | 283225    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 60806     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6492      |
|    fps              | 298       |
|    time_elapsed     | 949       |
|    total_timesteps  | 283373    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 60843     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6496      |
|    fps              | 298       |
|    time_elapsed     | 949       |
|    total_timesteps  | 283495    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.6      |
|    n_updates        | 60873     |
-----------------------------------
Eval num_timesteps=283500, episode_reward=-3194.99 +/- 761.77
Episode length: 33.82 +/- 7.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -3.19e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 283500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 60874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6500      |
|    fps              | 298       |
|    time_elapsed     | 951       |
|    total_timesteps  | 283637    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 60909     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6504      |
|    fps              | 298       |
|    time_elapsed     | 951       |
|    total_timesteps  | 283773    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72        |
|    n_updates        | 60943     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34       |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6508     |
|    fps              | 298      |
|    time_elapsed     | 951      |
|    total_timesteps  | 283911   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 195      |
|    n_updates        | 60977    |
----------------------------------
Eval num_timesteps=284000, episode_reward=-2669.11 +/- 1294.67
Episode length: 35.38 +/- 6.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 284000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.3      |
|    n_updates        | 60999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6512      |
|    fps              | 298       |
|    time_elapsed     | 953       |
|    total_timesteps  | 284040    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.5      |
|    n_updates        | 61009     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6516      |
|    fps              | 298       |
|    time_elapsed     | 953       |
|    total_timesteps  | 284195    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 61048     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6520      |
|    fps              | 298       |
|    time_elapsed     | 953       |
|    total_timesteps  | 284325    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.1      |
|    n_updates        | 61081     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6524      |
|    fps              | 298       |
|    time_elapsed     | 953       |
|    total_timesteps  | 284480    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 61119     |
-----------------------------------
Eval num_timesteps=284500, episode_reward=-2560.18 +/- 1404.69
Episode length: 37.08 +/- 6.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.1      |
|    mean_reward      | -2.56e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 284500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 61124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6528      |
|    fps              | 298       |
|    time_elapsed     | 954       |
|    total_timesteps  | 284614    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 55.2      |
|    n_updates        | 61153     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6532      |
|    fps              | 298       |
|    time_elapsed     | 955       |
|    total_timesteps  | 284769    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.9      |
|    n_updates        | 61192     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.62e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6536      |
|    fps              | 298       |
|    time_elapsed     | 955       |
|    total_timesteps  | 284918    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.6      |
|    n_updates        | 61229     |
-----------------------------------
Eval num_timesteps=285000, episode_reward=-3028.65 +/- 1152.07
Episode length: 35.08 +/- 6.10
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -3.03e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 285000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 61249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6540      |
|    fps              | 297       |
|    time_elapsed     | 956       |
|    total_timesteps  | 285061    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 217       |
|    n_updates        | 61265     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6544      |
|    fps              | 298       |
|    time_elapsed     | 956       |
|    total_timesteps  | 285224    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 61305     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6548     |
|    fps              | 298      |
|    time_elapsed     | 957      |
|    total_timesteps  | 285385   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 108      |
|    n_updates        | 61346    |
----------------------------------
Eval num_timesteps=285500, episode_reward=-3038.42 +/- 957.34
Episode length: 33.94 +/- 7.32
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 285500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53.1      |
|    n_updates        | 61374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6552      |
|    fps              | 297       |
|    time_elapsed     | 958       |
|    total_timesteps  | 285520    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.5      |
|    n_updates        | 61379     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6556      |
|    fps              | 298       |
|    time_elapsed     | 958       |
|    total_timesteps  | 285660    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 61414     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6560      |
|    fps              | 298       |
|    time_elapsed     | 958       |
|    total_timesteps  | 285812    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.4      |
|    n_updates        | 61452     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6564      |
|    fps              | 298       |
|    time_elapsed     | 958       |
|    total_timesteps  | 285941    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.6      |
|    n_updates        | 61485     |
-----------------------------------
Eval num_timesteps=286000, episode_reward=-2607.32 +/- 1374.29
Episode length: 36.78 +/- 6.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.61e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 286000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 61499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6568      |
|    fps              | 297       |
|    time_elapsed     | 960       |
|    total_timesteps  | 286080    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 196       |
|    n_updates        | 61519     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6572      |
|    fps              | 298       |
|    time_elapsed     | 960       |
|    total_timesteps  | 286208    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 61551     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6576      |
|    fps              | 298       |
|    time_elapsed     | 960       |
|    total_timesteps  | 286347    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 61586     |
-----------------------------------
Eval num_timesteps=286500, episode_reward=-2838.72 +/- 1096.20
Episode length: 35.82 +/- 7.34
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 286500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 61624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6580      |
|    fps              | 297       |
|    time_elapsed     | 961       |
|    total_timesteps  | 286503    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 61625     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6584      |
|    fps              | 297       |
|    time_elapsed     | 962       |
|    total_timesteps  | 286670    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 61667     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6588      |
|    fps              | 298       |
|    time_elapsed     | 962       |
|    total_timesteps  | 286821    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 61705     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6592      |
|    fps              | 298       |
|    time_elapsed     | 962       |
|    total_timesteps  | 286958    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.3      |
|    n_updates        | 61739     |
-----------------------------------
Eval num_timesteps=287000, episode_reward=-2905.25 +/- 1092.71
Episode length: 36.16 +/- 6.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 287000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62.7      |
|    n_updates        | 61749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6596      |
|    fps              | 297       |
|    time_elapsed     | 963       |
|    total_timesteps  | 287104    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 217       |
|    n_updates        | 61775     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6600      |
|    fps              | 297       |
|    time_elapsed     | 963       |
|    total_timesteps  | 287230    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 61807     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6604      |
|    fps              | 298       |
|    time_elapsed     | 964       |
|    total_timesteps  | 287368    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 61841     |
-----------------------------------
Eval num_timesteps=287500, episode_reward=-2520.12 +/- 1411.52
Episode length: 36.86 +/- 6.25
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.9      |
|    mean_reward      | -2.52e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 287500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 61874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6608      |
|    fps              | 297       |
|    time_elapsed     | 965       |
|    total_timesteps  | 287521    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 61880     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6612      |
|    fps              | 297       |
|    time_elapsed     | 965       |
|    total_timesteps  | 287657    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 61914     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6616      |
|    fps              | 297       |
|    time_elapsed     | 965       |
|    total_timesteps  | 287804    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.2      |
|    n_updates        | 61950     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6620      |
|    fps              | 298       |
|    time_elapsed     | 966       |
|    total_timesteps  | 287957    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.3      |
|    n_updates        | 61989     |
-----------------------------------
Eval num_timesteps=288000, episode_reward=-2785.88 +/- 1224.52
Episode length: 34.96 +/- 7.93
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 288000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 198       |
|    n_updates        | 61999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6624      |
|    fps              | 297       |
|    time_elapsed     | 967       |
|    total_timesteps  | 288112    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 62027     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6628      |
|    fps              | 297       |
|    time_elapsed     | 967       |
|    total_timesteps  | 288229    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 62057     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6632      |
|    fps              | 297       |
|    time_elapsed     | 967       |
|    total_timesteps  | 288371    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 62092     |
-----------------------------------
Eval num_timesteps=288500, episode_reward=-2844.90 +/- 1173.68
Episode length: 34.36 +/- 7.14
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 288500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 62124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6636      |
|    fps              | 297       |
|    time_elapsed     | 969       |
|    total_timesteps  | 288501    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.7      |
|    n_updates        | 62125     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6640      |
|    fps              | 297       |
|    time_elapsed     | 969       |
|    total_timesteps  | 288648    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 55.6      |
|    n_updates        | 62161     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6644      |
|    fps              | 297       |
|    time_elapsed     | 969       |
|    total_timesteps  | 288790    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 62197     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6648      |
|    fps              | 298       |
|    time_elapsed     | 969       |
|    total_timesteps  | 288936    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 62233     |
-----------------------------------
Eval num_timesteps=289000, episode_reward=-2971.53 +/- 1215.46
Episode length: 34.12 +/- 6.83
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 289000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 240       |
|    n_updates        | 62249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6652      |
|    fps              | 297       |
|    time_elapsed     | 970       |
|    total_timesteps  | 289069    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.1      |
|    n_updates        | 62267     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6656      |
|    fps              | 297       |
|    time_elapsed     | 971       |
|    total_timesteps  | 289189    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 62297     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6660      |
|    fps              | 297       |
|    time_elapsed     | 971       |
|    total_timesteps  | 289315    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.2      |
|    n_updates        | 62328     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6664      |
|    fps              | 298       |
|    time_elapsed     | 971       |
|    total_timesteps  | 289463    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.9      |
|    n_updates        | 62365     |
-----------------------------------
Eval num_timesteps=289500, episode_reward=-2938.67 +/- 1172.03
Episode length: 34.82 +/- 6.72
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 289500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.5      |
|    n_updates        | 62374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6668      |
|    fps              | 297       |
|    time_elapsed     | 972       |
|    total_timesteps  | 289604    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 62400     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6672     |
|    fps              | 297      |
|    time_elapsed     | 972      |
|    total_timesteps  | 289741   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 44.4     |
|    n_updates        | 62435    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6676     |
|    fps              | 297      |
|    time_elapsed     | 973      |
|    total_timesteps  | 289874   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 121      |
|    n_updates        | 62468    |
----------------------------------
Eval num_timesteps=290000, episode_reward=-2899.04 +/- 1228.16
Episode length: 33.08 +/- 7.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.1     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 93.9     |
|    n_updates        | 62499    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6680      |
|    fps              | 297       |
|    time_elapsed     | 974       |
|    total_timesteps  | 290023    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 62505     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6684      |
|    fps              | 297       |
|    time_elapsed     | 974       |
|    total_timesteps  | 290175    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 62543     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6688      |
|    fps              | 297       |
|    time_elapsed     | 974       |
|    total_timesteps  | 290321    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.2      |
|    n_updates        | 62580     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6692      |
|    fps              | 297       |
|    time_elapsed     | 974       |
|    total_timesteps  | 290465    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 62616     |
-----------------------------------
Eval num_timesteps=290500, episode_reward=-3015.80 +/- 1060.29
Episode length: 33.60 +/- 7.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.6      |
|    mean_reward      | -3.02e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 290500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 62624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6696      |
|    fps              | 297       |
|    time_elapsed     | 976       |
|    total_timesteps  | 290612    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 62652     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6700      |
|    fps              | 297       |
|    time_elapsed     | 976       |
|    total_timesteps  | 290730    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.4      |
|    n_updates        | 62682     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6704      |
|    fps              | 297       |
|    time_elapsed     | 976       |
|    total_timesteps  | 290865    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 62716     |
-----------------------------------
Eval num_timesteps=291000, episode_reward=-2804.38 +/- 1380.61
Episode length: 34.88 +/- 6.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.9     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 291000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 86.5     |
|    n_updates        | 62749    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6708      |
|    fps              | 297       |
|    time_elapsed     | 977       |
|    total_timesteps  | 291015    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 62753     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6712      |
|    fps              | 297       |
|    time_elapsed     | 978       |
|    total_timesteps  | 291152    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.9      |
|    n_updates        | 62787     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6716     |
|    fps              | 297      |
|    time_elapsed     | 978      |
|    total_timesteps  | 291318   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 172      |
|    n_updates        | 62829    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6720      |
|    fps              | 297       |
|    time_elapsed     | 978       |
|    total_timesteps  | 291455    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 62863     |
-----------------------------------
Eval num_timesteps=291500, episode_reward=-2845.29 +/- 1020.15
Episode length: 36.94 +/- 5.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.9      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 291500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 174       |
|    n_updates        | 62874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6724      |
|    fps              | 297       |
|    time_elapsed     | 979       |
|    total_timesteps  | 291596    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 181       |
|    n_updates        | 62898     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6728      |
|    fps              | 297       |
|    time_elapsed     | 979       |
|    total_timesteps  | 291722    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 62930     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6732      |
|    fps              | 297       |
|    time_elapsed     | 980       |
|    total_timesteps  | 291872    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 62967     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6736      |
|    fps              | 297       |
|    time_elapsed     | 980       |
|    total_timesteps  | 291980    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.5      |
|    n_updates        | 62994     |
-----------------------------------
Eval num_timesteps=292000, episode_reward=-2952.32 +/- 983.35
Episode length: 34.16 +/- 7.00
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 292000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 62999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6740      |
|    fps              | 297       |
|    time_elapsed     | 981       |
|    total_timesteps  | 292105    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 33.8      |
|    n_updates        | 63026     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6744      |
|    fps              | 297       |
|    time_elapsed     | 981       |
|    total_timesteps  | 292270    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 63067     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6748     |
|    fps              | 297      |
|    time_elapsed     | 981      |
|    total_timesteps  | 292431   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 121      |
|    n_updates        | 63107    |
----------------------------------
Eval num_timesteps=292500, episode_reward=-3068.34 +/- 1021.11
Episode length: 35.66 +/- 7.52
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -3.07e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 292500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.2      |
|    n_updates        | 63124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6752      |
|    fps              | 297       |
|    time_elapsed     | 983       |
|    total_timesteps  | 292561    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 63140     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6756      |
|    fps              | 297       |
|    time_elapsed     | 983       |
|    total_timesteps  | 292709    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 63177     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6760      |
|    fps              | 297       |
|    time_elapsed     | 983       |
|    total_timesteps  | 292831    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 63207     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6764      |
|    fps              | 297       |
|    time_elapsed     | 983       |
|    total_timesteps  | 292978    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 198       |
|    n_updates        | 63244     |
-----------------------------------
Eval num_timesteps=293000, episode_reward=-3097.92 +/- 1090.50
Episode length: 35.26 +/- 6.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.3     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 124      |
|    n_updates        | 63249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6768      |
|    fps              | 297       |
|    time_elapsed     | 985       |
|    total_timesteps  | 293103    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 63275     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6772      |
|    fps              | 297       |
|    time_elapsed     | 985       |
|    total_timesteps  | 293236    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 63308     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6776      |
|    fps              | 297       |
|    time_elapsed     | 985       |
|    total_timesteps  | 293360    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.1      |
|    n_updates        | 63339     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6780      |
|    fps              | 297       |
|    time_elapsed     | 985       |
|    total_timesteps  | 293490    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 63372     |
-----------------------------------
Eval num_timesteps=293500, episode_reward=-2701.38 +/- 1162.22
Episode length: 36.14 +/- 6.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.1     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 218      |
|    n_updates        | 63374    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6784      |
|    fps              | 297       |
|    time_elapsed     | 986       |
|    total_timesteps  | 293647    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 63411     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6788      |
|    fps              | 297       |
|    time_elapsed     | 987       |
|    total_timesteps  | 293803    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 63450     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6792      |
|    fps              | 297       |
|    time_elapsed     | 987       |
|    total_timesteps  | 293958    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 63489     |
-----------------------------------
Eval num_timesteps=294000, episode_reward=-2672.43 +/- 1233.94
Episode length: 35.42 +/- 7.14
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 294000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 63499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6796      |
|    fps              | 297       |
|    time_elapsed     | 988       |
|    total_timesteps  | 294093    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.4      |
|    n_updates        | 63523     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6800      |
|    fps              | 297       |
|    time_elapsed     | 988       |
|    total_timesteps  | 294240    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 251       |
|    n_updates        | 63559     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6804      |
|    fps              | 297       |
|    time_elapsed     | 989       |
|    total_timesteps  | 294393    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 63598     |
-----------------------------------
Eval num_timesteps=294500, episode_reward=-3010.83 +/- 1239.17
Episode length: 34.36 +/- 6.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 294500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.8      |
|    n_updates        | 63624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6808      |
|    fps              | 297       |
|    time_elapsed     | 990       |
|    total_timesteps  | 294550    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88.7      |
|    n_updates        | 63637     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6812      |
|    fps              | 297       |
|    time_elapsed     | 990       |
|    total_timesteps  | 294685    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85        |
|    n_updates        | 63671     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6816      |
|    fps              | 297       |
|    time_elapsed     | 990       |
|    total_timesteps  | 294832    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 63707     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6820      |
|    fps              | 297       |
|    time_elapsed     | 990       |
|    total_timesteps  | 294982    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 63745     |
-----------------------------------
Eval num_timesteps=295000, episode_reward=-3040.55 +/- 1178.31
Episode length: 33.12 +/- 6.72
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.1      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 295000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 63749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6824      |
|    fps              | 297       |
|    time_elapsed     | 992       |
|    total_timesteps  | 295097    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 221       |
|    n_updates        | 63774     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.9     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6828     |
|    fps              | 297      |
|    time_elapsed     | 992      |
|    total_timesteps  | 295212   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 177      |
|    n_updates        | 63802    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6832      |
|    fps              | 297       |
|    time_elapsed     | 992       |
|    total_timesteps  | 295356    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 63838     |
-----------------------------------
Eval num_timesteps=295500, episode_reward=-2974.26 +/- 994.06
Episode length: 34.96 +/- 6.08
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 295500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 324       |
|    n_updates        | 63874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6836      |
|    fps              | 297       |
|    time_elapsed     | 993       |
|    total_timesteps  | 295503    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 63875     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.6     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6840     |
|    fps              | 297      |
|    time_elapsed     | 993      |
|    total_timesteps  | 295662   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 94.8     |
|    n_updates        | 63915    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6844      |
|    fps              | 297       |
|    time_elapsed     | 994       |
|    total_timesteps  | 295787    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.5      |
|    n_updates        | 63946     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6848      |
|    fps              | 297       |
|    time_elapsed     | 994       |
|    total_timesteps  | 295934    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.5      |
|    n_updates        | 63983     |
-----------------------------------
Eval num_timesteps=296000, episode_reward=-3191.79 +/- 947.69
Episode length: 35.76 +/- 7.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -3.19e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 296000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91        |
|    n_updates        | 63999     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6852     |
|    fps              | 297      |
|    time_elapsed     | 995      |
|    total_timesteps  | 296068   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 191      |
|    n_updates        | 64016    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6856      |
|    fps              | 297       |
|    time_elapsed     | 995       |
|    total_timesteps  | 296200    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 64049     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6860      |
|    fps              | 297       |
|    time_elapsed     | 996       |
|    total_timesteps  | 296345    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 64086     |
-----------------------------------
Eval num_timesteps=296500, episode_reward=-2923.13 +/- 1094.43
Episode length: 36.22 +/- 7.54
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 296500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 64124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6864      |
|    fps              | 297       |
|    time_elapsed     | 997       |
|    total_timesteps  | 296504    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 64125     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6868      |
|    fps              | 297       |
|    time_elapsed     | 997       |
|    total_timesteps  | 296639    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 64159     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6872      |
|    fps              | 297       |
|    time_elapsed     | 997       |
|    total_timesteps  | 296777    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.2      |
|    n_updates        | 64194     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6876      |
|    fps              | 297       |
|    time_elapsed     | 997       |
|    total_timesteps  | 296931    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 64232     |
-----------------------------------
Eval num_timesteps=297000, episode_reward=-3110.06 +/- 878.53
Episode length: 33.22 +/- 6.77
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.2      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 297000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 64249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6880      |
|    fps              | 297       |
|    time_elapsed     | 999       |
|    total_timesteps  | 297055    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.5      |
|    n_updates        | 64263     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6884      |
|    fps              | 297       |
|    time_elapsed     | 999       |
|    total_timesteps  | 297184    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 25.1      |
|    n_updates        | 64295     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6888      |
|    fps              | 297       |
|    time_elapsed     | 999       |
|    total_timesteps  | 297328    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.2      |
|    n_updates        | 64331     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6892      |
|    fps              | 297       |
|    time_elapsed     | 999       |
|    total_timesteps  | 297467    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 64366     |
-----------------------------------
Eval num_timesteps=297500, episode_reward=-2792.90 +/- 1156.54
Episode length: 34.76 +/- 6.79
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 297500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 226       |
|    n_updates        | 64374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6896      |
|    fps              | 297       |
|    time_elapsed     | 1001      |
|    total_timesteps  | 297598    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.4      |
|    n_updates        | 64399     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6900      |
|    fps              | 297       |
|    time_elapsed     | 1001      |
|    total_timesteps  | 297763    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 64440     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6904      |
|    fps              | 297       |
|    time_elapsed     | 1001      |
|    total_timesteps  | 297915    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.8      |
|    n_updates        | 64478     |
-----------------------------------
Eval num_timesteps=298000, episode_reward=-2992.19 +/- 1043.37
Episode length: 34.46 +/- 6.59
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 298000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.1      |
|    n_updates        | 64499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6908      |
|    fps              | 297       |
|    time_elapsed     | 1002      |
|    total_timesteps  | 298050    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 64512     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6912      |
|    fps              | 297       |
|    time_elapsed     | 1002      |
|    total_timesteps  | 298190    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 64547     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6916      |
|    fps              | 297       |
|    time_elapsed     | 1003      |
|    total_timesteps  | 298322    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.1      |
|    n_updates        | 64580     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6920      |
|    fps              | 297       |
|    time_elapsed     | 1003      |
|    total_timesteps  | 298439    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 64609     |
-----------------------------------
Eval num_timesteps=298500, episode_reward=-2682.09 +/- 1204.16
Episode length: 36.36 +/- 6.41
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 298500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.5      |
|    n_updates        | 64624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6924      |
|    fps              | 297       |
|    time_elapsed     | 1004      |
|    total_timesteps  | 298586    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.5      |
|    n_updates        | 64646     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6928      |
|    fps              | 297       |
|    time_elapsed     | 1004      |
|    total_timesteps  | 298716    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 64678     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6932      |
|    fps              | 297       |
|    time_elapsed     | 1004      |
|    total_timesteps  | 298855    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.5      |
|    n_updates        | 64713     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.6     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6936     |
|    fps              | 297      |
|    time_elapsed     | 1005     |
|    total_timesteps  | 298960   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 64.7     |
|    n_updates        | 64739    |
----------------------------------
Eval num_timesteps=299000, episode_reward=-3278.63 +/- 865.41
Episode length: 34.46 +/- 7.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -3.28e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 299000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.5      |
|    n_updates        | 64749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.03e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6940      |
|    fps              | 297       |
|    time_elapsed     | 1006      |
|    total_timesteps  | 299105    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.5      |
|    n_updates        | 64776     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.6     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6944     |
|    fps              | 297      |
|    time_elapsed     | 1006     |
|    total_timesteps  | 299243   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 89.3     |
|    n_updates        | 64810    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6948      |
|    fps              | 297       |
|    time_elapsed     | 1006      |
|    total_timesteps  | 299388    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 64846     |
-----------------------------------
Eval num_timesteps=299500, episode_reward=-3137.25 +/- 932.50
Episode length: 33.54 +/- 7.53
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.5      |
|    mean_reward      | -3.14e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 299500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 64874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6952      |
|    fps              | 297       |
|    time_elapsed     | 1008      |
|    total_timesteps  | 299519    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 64879     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6956      |
|    fps              | 297       |
|    time_elapsed     | 1008      |
|    total_timesteps  | 299657    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 194       |
|    n_updates        | 64914     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6960      |
|    fps              | 297       |
|    time_elapsed     | 1008      |
|    total_timesteps  | 299808    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 64951     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6964      |
|    fps              | 297       |
|    time_elapsed     | 1008      |
|    total_timesteps  | 299965    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 263       |
|    n_updates        | 64991     |
-----------------------------------
Eval num_timesteps=300000, episode_reward=-2952.80 +/- 1133.30
Episode length: 34.74 +/- 7.46
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 300000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 64999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6968      |
|    fps              | 297       |
|    time_elapsed     | 1009      |
|    total_timesteps  | 300114    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.4      |
|    n_updates        | 65028     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6972      |
|    fps              | 297       |
|    time_elapsed     | 1010      |
|    total_timesteps  | 300276    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52.5      |
|    n_updates        | 65068     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6976      |
|    fps              | 297       |
|    time_elapsed     | 1010      |
|    total_timesteps  | 300429    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 65107     |
-----------------------------------
Eval num_timesteps=300500, episode_reward=-2903.28 +/- 1145.25
Episode length: 35.70 +/- 6.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.7     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 300500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 142      |
|    n_updates        | 65124    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6980      |
|    fps              | 297       |
|    time_elapsed     | 1011      |
|    total_timesteps  | 300567    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 65141     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6984      |
|    fps              | 297       |
|    time_elapsed     | 1011      |
|    total_timesteps  | 300710    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 65177     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.5     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6988     |
|    fps              | 297      |
|    time_elapsed     | 1012     |
|    total_timesteps  | 300875   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 64.5     |
|    n_updates        | 65218    |
----------------------------------
Eval num_timesteps=301000, episode_reward=-2944.28 +/- 910.14
Episode length: 36.74 +/- 5.34
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.7      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 301000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 23.8      |
|    n_updates        | 65249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6992      |
|    fps              | 297       |
|    time_elapsed     | 1013      |
|    total_timesteps  | 301011    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 65252     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 6996      |
|    fps              | 297       |
|    time_elapsed     | 1013      |
|    total_timesteps  | 301116    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 65278     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7000      |
|    fps              | 297       |
|    time_elapsed     | 1013      |
|    total_timesteps  | 301264    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 65315     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7004      |
|    fps              | 297       |
|    time_elapsed     | 1013      |
|    total_timesteps  | 301386    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 65346     |
-----------------------------------
Eval num_timesteps=301500, episode_reward=-2685.04 +/- 1383.88
Episode length: 34.36 +/- 7.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 301500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 65374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7008      |
|    fps              | 297       |
|    time_elapsed     | 1015      |
|    total_timesteps  | 301535    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 65383     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7012      |
|    fps              | 297       |
|    time_elapsed     | 1015      |
|    total_timesteps  | 301667    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 231       |
|    n_updates        | 65416     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7016      |
|    fps              | 297       |
|    time_elapsed     | 1015      |
|    total_timesteps  | 301835    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 222       |
|    n_updates        | 65458     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7020      |
|    fps              | 297       |
|    time_elapsed     | 1015      |
|    total_timesteps  | 301961    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 65490     |
-----------------------------------
Eval num_timesteps=302000, episode_reward=-3034.52 +/- 976.86
Episode length: 33.42 +/- 7.06
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.4      |
|    mean_reward      | -3.03e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 302000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 65499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7024      |
|    fps              | 297       |
|    time_elapsed     | 1017      |
|    total_timesteps  | 302097    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64.5      |
|    n_updates        | 65524     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7028     |
|    fps              | 297      |
|    time_elapsed     | 1017     |
|    total_timesteps  | 302231   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 180      |
|    n_updates        | 65557    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7032      |
|    fps              | 297       |
|    time_elapsed     | 1017      |
|    total_timesteps  | 302361    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 65590     |
-----------------------------------
Eval num_timesteps=302500, episode_reward=-2957.01 +/- 1054.77
Episode length: 35.28 +/- 6.19
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 302500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 34.5      |
|    n_updates        | 65624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7036      |
|    fps              | 296       |
|    time_elapsed     | 1018      |
|    total_timesteps  | 302512    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 65627     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7040     |
|    fps              | 297      |
|    time_elapsed     | 1018     |
|    total_timesteps  | 302673   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 81.9     |
|    n_updates        | 65668    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7044      |
|    fps              | 297       |
|    time_elapsed     | 1019      |
|    total_timesteps  | 302814    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 65703     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7048      |
|    fps              | 297       |
|    time_elapsed     | 1019      |
|    total_timesteps  | 302952    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 65737     |
-----------------------------------
Eval num_timesteps=303000, episode_reward=-3101.59 +/- 985.37
Episode length: 34.16 +/- 6.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.2     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 303000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 182      |
|    n_updates        | 65749    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7052      |
|    fps              | 296       |
|    time_elapsed     | 1020      |
|    total_timesteps  | 303104    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.4      |
|    n_updates        | 65775     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7056      |
|    fps              | 297       |
|    time_elapsed     | 1020      |
|    total_timesteps  | 303241    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 65810     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7060      |
|    fps              | 297       |
|    time_elapsed     | 1020      |
|    total_timesteps  | 303407    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 65851     |
-----------------------------------
Eval num_timesteps=303500, episode_reward=-2880.04 +/- 970.89
Episode length: 35.38 +/- 6.44
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 303500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 65874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7064      |
|    fps              | 296       |
|    time_elapsed     | 1022      |
|    total_timesteps  | 303569    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.9      |
|    n_updates        | 65892     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7068      |
|    fps              | 296       |
|    time_elapsed     | 1022      |
|    total_timesteps  | 303690    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.5      |
|    n_updates        | 65922     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7072      |
|    fps              | 297       |
|    time_elapsed     | 1022      |
|    total_timesteps  | 303853    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 65963     |
-----------------------------------
Eval num_timesteps=304000, episode_reward=-2848.95 +/- 1071.71
Episode length: 35.20 +/- 7.34
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 304000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 252       |
|    n_updates        | 65999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7076      |
|    fps              | 296       |
|    time_elapsed     | 1024      |
|    total_timesteps  | 304000    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7080      |
|    fps              | 296       |
|    time_elapsed     | 1024      |
|    total_timesteps  | 304157    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 66039     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7084      |
|    fps              | 297       |
|    time_elapsed     | 1024      |
|    total_timesteps  | 304316    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.3      |
|    n_updates        | 66078     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7088      |
|    fps              | 297       |
|    time_elapsed     | 1024      |
|    total_timesteps  | 304453    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 66113     |
-----------------------------------
Eval num_timesteps=304500, episode_reward=-2525.68 +/- 1334.97
Episode length: 35.74 +/- 6.42
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.53e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 304500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 66124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7092      |
|    fps              | 296       |
|    time_elapsed     | 1026      |
|    total_timesteps  | 304574    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.3      |
|    n_updates        | 66143     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7096      |
|    fps              | 296       |
|    time_elapsed     | 1026      |
|    total_timesteps  | 304689    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 66172     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.6     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7100     |
|    fps              | 297      |
|    time_elapsed     | 1026     |
|    total_timesteps  | 304825   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 48       |
|    n_updates        | 66206    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7104      |
|    fps              | 297       |
|    time_elapsed     | 1026      |
|    total_timesteps  | 304955    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.2      |
|    n_updates        | 66238     |
-----------------------------------
Eval num_timesteps=305000, episode_reward=-2839.70 +/- 1215.06
Episode length: 35.14 +/- 5.98
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 305000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 66249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7108      |
|    fps              | 296       |
|    time_elapsed     | 1027      |
|    total_timesteps  | 305097    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 66274     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7112      |
|    fps              | 296       |
|    time_elapsed     | 1027      |
|    total_timesteps  | 305246    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.8      |
|    n_updates        | 66311     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7116      |
|    fps              | 297       |
|    time_elapsed     | 1028      |
|    total_timesteps  | 305369    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 66342     |
-----------------------------------
Eval num_timesteps=305500, episode_reward=-3177.57 +/- 1168.45
Episode length: 34.60 +/- 8.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -3.18e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 305500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 66374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7120      |
|    fps              | 296       |
|    time_elapsed     | 1029      |
|    total_timesteps  | 305525    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 66381     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7124      |
|    fps              | 296       |
|    time_elapsed     | 1029      |
|    total_timesteps  | 305670    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 66417     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7128     |
|    fps              | 296      |
|    time_elapsed     | 1029     |
|    total_timesteps  | 305826   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 106      |
|    n_updates        | 66456    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7132      |
|    fps              | 297       |
|    time_elapsed     | 1029      |
|    total_timesteps  | 305957    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 66489     |
-----------------------------------
Eval num_timesteps=306000, episode_reward=-3060.75 +/- 1108.30
Episode length: 34.20 +/- 6.83
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -3.06e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 306000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.8      |
|    n_updates        | 66499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7136      |
|    fps              | 296       |
|    time_elapsed     | 1031      |
|    total_timesteps  | 306091    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 66522     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7140      |
|    fps              | 296       |
|    time_elapsed     | 1031      |
|    total_timesteps  | 306236    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 66558     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7144      |
|    fps              | 296       |
|    time_elapsed     | 1031      |
|    total_timesteps  | 306381    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 66595     |
-----------------------------------
Eval num_timesteps=306500, episode_reward=-2890.65 +/- 1196.95
Episode length: 33.72 +/- 6.80
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 306500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 66624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7148      |
|    fps              | 296       |
|    time_elapsed     | 1032      |
|    total_timesteps  | 306510    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.8      |
|    n_updates        | 66627     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7152      |
|    fps              | 296       |
|    time_elapsed     | 1033      |
|    total_timesteps  | 306635    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 66658     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7156      |
|    fps              | 296       |
|    time_elapsed     | 1033      |
|    total_timesteps  | 306780    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.3      |
|    n_updates        | 66694     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7160      |
|    fps              | 297       |
|    time_elapsed     | 1033      |
|    total_timesteps  | 306936    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65.7      |
|    n_updates        | 66733     |
-----------------------------------
Eval num_timesteps=307000, episode_reward=-2881.15 +/- 1115.69
Episode length: 34.32 +/- 6.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 307000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 66749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7164      |
|    fps              | 296       |
|    time_elapsed     | 1034      |
|    total_timesteps  | 307099    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.7      |
|    n_updates        | 66774     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7168      |
|    fps              | 296       |
|    time_elapsed     | 1034      |
|    total_timesteps  | 307232    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 66807     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7172      |
|    fps              | 296       |
|    time_elapsed     | 1035      |
|    total_timesteps  | 307378    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 66844     |
-----------------------------------
Eval num_timesteps=307500, episode_reward=-2871.93 +/- 974.10
Episode length: 33.98 +/- 7.01
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 307500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 66874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7176      |
|    fps              | 296       |
|    time_elapsed     | 1036      |
|    total_timesteps  | 307513    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.5      |
|    n_updates        | 66878     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7180      |
|    fps              | 296       |
|    time_elapsed     | 1036      |
|    total_timesteps  | 307676    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 66918     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7184      |
|    fps              | 296       |
|    time_elapsed     | 1036      |
|    total_timesteps  | 307814    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52        |
|    n_updates        | 66953     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7188      |
|    fps              | 296       |
|    time_elapsed     | 1036      |
|    total_timesteps  | 307956    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 66988     |
-----------------------------------
Eval num_timesteps=308000, episode_reward=-2500.34 +/- 1398.21
Episode length: 35.38 +/- 7.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.4     |
|    mean_reward      | -2.5e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 308000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 121      |
|    n_updates        | 66999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7192     |
|    fps              | 296      |
|    time_elapsed     | 1038     |
|    total_timesteps  | 308104   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 156      |
|    n_updates        | 67025    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7196      |
|    fps              | 296       |
|    time_elapsed     | 1038      |
|    total_timesteps  | 308256    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 67063     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7200     |
|    fps              | 296      |
|    time_elapsed     | 1038     |
|    total_timesteps  | 308399   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 190      |
|    n_updates        | 67099    |
----------------------------------
Eval num_timesteps=308500, episode_reward=-2867.68 +/- 1125.89
Episode length: 35.16 +/- 6.86
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 308500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.8      |
|    n_updates        | 67124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7204      |
|    fps              | 296       |
|    time_elapsed     | 1040      |
|    total_timesteps  | 308555    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 67138     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7208      |
|    fps              | 296       |
|    time_elapsed     | 1040      |
|    total_timesteps  | 308676    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.8      |
|    n_updates        | 67168     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7212      |
|    fps              | 296       |
|    time_elapsed     | 1040      |
|    total_timesteps  | 308829    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.2      |
|    n_updates        | 67207     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7216      |
|    fps              | 296       |
|    time_elapsed     | 1040      |
|    total_timesteps  | 308999    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.3      |
|    n_updates        | 67249     |
-----------------------------------
Eval num_timesteps=309000, episode_reward=-3441.48 +/- 931.33
Episode length: 32.12 +/- 6.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 32.1      |
|    mean_reward     | -3.44e+03 |
| time/              |           |
|    total_timesteps | 309000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.1     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7220     |
|    fps              | 296      |
|    time_elapsed     | 1041     |
|    total_timesteps  | 309136   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 93.9     |
|    n_updates        | 67283    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.1     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7224     |
|    fps              | 296      |
|    time_elapsed     | 1042     |
|    total_timesteps  | 309280   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 73.8     |
|    n_updates        | 67319    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7228      |
|    fps              | 296       |
|    time_elapsed     | 1042      |
|    total_timesteps  | 309426    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.8      |
|    n_updates        | 67356     |
-----------------------------------
Eval num_timesteps=309500, episode_reward=-2810.26 +/- 1179.12
Episode length: 34.86 +/- 6.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 309500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.4      |
|    n_updates        | 67374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7232      |
|    fps              | 296       |
|    time_elapsed     | 1043      |
|    total_timesteps  | 309581    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28.1      |
|    n_updates        | 67395     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7236      |
|    fps              | 296       |
|    time_elapsed     | 1043      |
|    total_timesteps  | 309716    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 67428     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7240      |
|    fps              | 296       |
|    time_elapsed     | 1043      |
|    total_timesteps  | 309855    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 67463     |
-----------------------------------
Eval num_timesteps=310000, episode_reward=-3002.82 +/- 822.67
Episode length: 34.96 +/- 6.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35       |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 310000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 122      |
|    n_updates        | 67499    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7244      |
|    fps              | 296       |
|    time_elapsed     | 1045      |
|    total_timesteps  | 310004    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 67500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7248      |
|    fps              | 296       |
|    time_elapsed     | 1045      |
|    total_timesteps  | 310115    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 67528     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.2     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7252     |
|    fps              | 296      |
|    time_elapsed     | 1045     |
|    total_timesteps  | 310252   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 148      |
|    n_updates        | 67562    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7256      |
|    fps              | 296       |
|    time_elapsed     | 1045      |
|    total_timesteps  | 310396    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 67598     |
-----------------------------------
Eval num_timesteps=310500, episode_reward=-3004.48 +/- 1057.35
Episode length: 35.62 +/- 5.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.6     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 310500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 41.2     |
|    n_updates        | 67624    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7260      |
|    fps              | 296       |
|    time_elapsed     | 1047      |
|    total_timesteps  | 310546    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 67636     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7264      |
|    fps              | 296       |
|    time_elapsed     | 1047      |
|    total_timesteps  | 310693    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.8      |
|    n_updates        | 67673     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7268      |
|    fps              | 296       |
|    time_elapsed     | 1047      |
|    total_timesteps  | 310850    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 67712     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7272      |
|    fps              | 296       |
|    time_elapsed     | 1047      |
|    total_timesteps  | 310987    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 207       |
|    n_updates        | 67746     |
-----------------------------------
Eval num_timesteps=311000, episode_reward=-3039.54 +/- 1162.95
Episode length: 33.74 +/- 6.72
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 311000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 67749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7276      |
|    fps              | 296       |
|    time_elapsed     | 1049      |
|    total_timesteps  | 311152    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 67787     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7280      |
|    fps              | 296       |
|    time_elapsed     | 1049      |
|    total_timesteps  | 311303    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 181       |
|    n_updates        | 67825     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7284      |
|    fps              | 296       |
|    time_elapsed     | 1049      |
|    total_timesteps  | 311438    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 67859     |
-----------------------------------
Eval num_timesteps=311500, episode_reward=-2769.80 +/- 1250.70
Episode length: 35.70 +/- 6.56
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 311500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 67874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7288      |
|    fps              | 296       |
|    time_elapsed     | 1050      |
|    total_timesteps  | 311569    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.1      |
|    n_updates        | 67892     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7292      |
|    fps              | 296       |
|    time_elapsed     | 1050      |
|    total_timesteps  | 311687    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 272       |
|    n_updates        | 67921     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7296      |
|    fps              | 296       |
|    time_elapsed     | 1051      |
|    total_timesteps  | 311835    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 67958     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7300      |
|    fps              | 296       |
|    time_elapsed     | 1051      |
|    total_timesteps  | 311995    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.4      |
|    n_updates        | 67998     |
-----------------------------------
Eval num_timesteps=312000, episode_reward=-2950.80 +/- 1103.02
Episode length: 33.58 +/- 6.01
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.6      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 312000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 205       |
|    n_updates        | 67999     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.8     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7304     |
|    fps              | 296      |
|    time_elapsed     | 1052     |
|    total_timesteps  | 312136   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 114      |
|    n_updates        | 68033    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7308      |
|    fps              | 296       |
|    time_elapsed     | 1052      |
|    total_timesteps  | 312272    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 167       |
|    n_updates        | 68067     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7312      |
|    fps              | 296       |
|    time_elapsed     | 1052      |
|    total_timesteps  | 312413    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 68103     |
-----------------------------------
Eval num_timesteps=312500, episode_reward=-2898.89 +/- 1217.35
Episode length: 35.18 +/- 5.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.2     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 312500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 126      |
|    n_updates        | 68124    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7316      |
|    fps              | 296       |
|    time_elapsed     | 1054      |
|    total_timesteps  | 312567    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.9      |
|    n_updates        | 68141     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7320      |
|    fps              | 296       |
|    time_elapsed     | 1054      |
|    total_timesteps  | 312700    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 68174     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7324      |
|    fps              | 296       |
|    time_elapsed     | 1054      |
|    total_timesteps  | 312833    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 68208     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7328      |
|    fps              | 296       |
|    time_elapsed     | 1054      |
|    total_timesteps  | 312985    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 68246     |
-----------------------------------
Eval num_timesteps=313000, episode_reward=-3013.82 +/- 922.98
Episode length: 35.46 +/- 7.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 313000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 68249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7332      |
|    fps              | 296       |
|    time_elapsed     | 1056      |
|    total_timesteps  | 313121    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.8      |
|    n_updates        | 68280     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7336      |
|    fps              | 296       |
|    time_elapsed     | 1056      |
|    total_timesteps  | 313249    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 68312     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7340      |
|    fps              | 296       |
|    time_elapsed     | 1056      |
|    total_timesteps  | 313407    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.7      |
|    n_updates        | 68351     |
-----------------------------------
Eval num_timesteps=313500, episode_reward=-3091.66 +/- 1100.07
Episode length: 33.66 +/- 6.26
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -3.09e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 313500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 68374     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.5     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7344     |
|    fps              | 296      |
|    time_elapsed     | 1058     |
|    total_timesteps  | 313550   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 146      |
|    n_updates        | 68387    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7348      |
|    fps              | 296       |
|    time_elapsed     | 1058      |
|    total_timesteps  | 313696    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.9      |
|    n_updates        | 68423     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7352      |
|    fps              | 296       |
|    time_elapsed     | 1058      |
|    total_timesteps  | 313840    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.3      |
|    n_updates        | 68459     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7356      |
|    fps              | 296       |
|    time_elapsed     | 1058      |
|    total_timesteps  | 313997    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.9      |
|    n_updates        | 68499     |
-----------------------------------
Eval num_timesteps=314000, episode_reward=-2950.63 +/- 1140.77
Episode length: 34.12 +/- 6.33
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 314000    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7360      |
|    fps              | 296       |
|    time_elapsed     | 1059      |
|    total_timesteps  | 314138    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98        |
|    n_updates        | 68534     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7364      |
|    fps              | 296       |
|    time_elapsed     | 1060      |
|    total_timesteps  | 314294    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 46.7      |
|    n_updates        | 68573     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7368      |
|    fps              | 296       |
|    time_elapsed     | 1060      |
|    total_timesteps  | 314427    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.4      |
|    n_updates        | 68606     |
-----------------------------------
Eval num_timesteps=314500, episode_reward=-2828.39 +/- 1295.15
Episode length: 35.80 +/- 6.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 314500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 68624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7372      |
|    fps              | 296       |
|    time_elapsed     | 1061      |
|    total_timesteps  | 314514    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 68628     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7376      |
|    fps              | 296       |
|    time_elapsed     | 1061      |
|    total_timesteps  | 314633    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 68658     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7380      |
|    fps              | 296       |
|    time_elapsed     | 1061      |
|    total_timesteps  | 314770    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 68692     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7384      |
|    fps              | 296       |
|    time_elapsed     | 1061      |
|    total_timesteps  | 314914    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 68728     |
-----------------------------------
Eval num_timesteps=315000, episode_reward=-2817.53 +/- 1176.78
Episode length: 35.48 +/- 5.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 315000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 243       |
|    n_updates        | 68749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7388      |
|    fps              | 296       |
|    time_elapsed     | 1063      |
|    total_timesteps  | 315057    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 68764     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7392      |
|    fps              | 296       |
|    time_elapsed     | 1063      |
|    total_timesteps  | 315195    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 68798     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7396      |
|    fps              | 296       |
|    time_elapsed     | 1063      |
|    total_timesteps  | 315329    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 261       |
|    n_updates        | 68832     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7400      |
|    fps              | 296       |
|    time_elapsed     | 1063      |
|    total_timesteps  | 315489    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 68872     |
-----------------------------------
Eval num_timesteps=315500, episode_reward=-2964.98 +/- 893.27
Episode length: 35.74 +/- 5.44
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 315500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.9      |
|    n_updates        | 68874     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7404     |
|    fps              | 296      |
|    time_elapsed     | 1065     |
|    total_timesteps  | 315634   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 62.6     |
|    n_updates        | 68908    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7408      |
|    fps              | 296       |
|    time_elapsed     | 1065      |
|    total_timesteps  | 315771    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 68942     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7412      |
|    fps              | 296       |
|    time_elapsed     | 1065      |
|    total_timesteps  | 315916    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 68978     |
-----------------------------------
Eval num_timesteps=316000, episode_reward=-2821.31 +/- 1173.13
Episode length: 35.20 +/- 6.85
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 316000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 68999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7416      |
|    fps              | 296       |
|    time_elapsed     | 1066      |
|    total_timesteps  | 316085    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 69021     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7420     |
|    fps              | 296      |
|    time_elapsed     | 1067     |
|    total_timesteps  | 316213   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 103      |
|    n_updates        | 69053    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7424      |
|    fps              | 296       |
|    time_elapsed     | 1067      |
|    total_timesteps  | 316349    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 69087     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7428      |
|    fps              | 296       |
|    time_elapsed     | 1067      |
|    total_timesteps  | 316488    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 69121     |
-----------------------------------
Eval num_timesteps=316500, episode_reward=-2680.09 +/- 1397.46
Episode length: 35.38 +/- 6.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 316500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 69124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7432      |
|    fps              | 296       |
|    time_elapsed     | 1068      |
|    total_timesteps  | 316619    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 69154     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7436      |
|    fps              | 296       |
|    time_elapsed     | 1068      |
|    total_timesteps  | 316775    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 69193     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7440      |
|    fps              | 296       |
|    time_elapsed     | 1069      |
|    total_timesteps  | 316916    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 69228     |
-----------------------------------
Eval num_timesteps=317000, episode_reward=-2789.14 +/- 1115.05
Episode length: 35.20 +/- 6.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 317000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 255       |
|    n_updates        | 69249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7444      |
|    fps              | 296       |
|    time_elapsed     | 1070      |
|    total_timesteps  | 317055    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 264       |
|    n_updates        | 69263     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7448      |
|    fps              | 296       |
|    time_elapsed     | 1070      |
|    total_timesteps  | 317188    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 69296     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7452      |
|    fps              | 296       |
|    time_elapsed     | 1070      |
|    total_timesteps  | 317335    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 69333     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7456      |
|    fps              | 296       |
|    time_elapsed     | 1070      |
|    total_timesteps  | 317491    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.8      |
|    n_updates        | 69372     |
-----------------------------------
Eval num_timesteps=317500, episode_reward=-3007.84 +/- 1088.35
Episode length: 34.24 +/- 6.45
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 317500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.1      |
|    n_updates        | 69374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7460      |
|    fps              | 296       |
|    time_elapsed     | 1072      |
|    total_timesteps  | 317646    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 222       |
|    n_updates        | 69411     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7464      |
|    fps              | 296       |
|    time_elapsed     | 1072      |
|    total_timesteps  | 317776    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 69443     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7468      |
|    fps              | 296       |
|    time_elapsed     | 1072      |
|    total_timesteps  | 317929    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 69482     |
-----------------------------------
Eval num_timesteps=318000, episode_reward=-3008.11 +/- 1174.52
Episode length: 34.56 +/- 6.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 318000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.6      |
|    n_updates        | 69499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7472      |
|    fps              | 296       |
|    time_elapsed     | 1073      |
|    total_timesteps  | 318063    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 69515     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7476      |
|    fps              | 296       |
|    time_elapsed     | 1074      |
|    total_timesteps  | 318216    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 69553     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7480      |
|    fps              | 296       |
|    time_elapsed     | 1074      |
|    total_timesteps  | 318341    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 69585     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7484      |
|    fps              | 296       |
|    time_elapsed     | 1074      |
|    total_timesteps  | 318482    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76        |
|    n_updates        | 69620     |
-----------------------------------
Eval num_timesteps=318500, episode_reward=-3032.32 +/- 1053.35
Episode length: 35.06 +/- 6.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -3.03e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 318500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 69624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.6     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7488     |
|    fps              | 296      |
|    time_elapsed     | 1075     |
|    total_timesteps  | 318622   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 184      |
|    n_updates        | 69655    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7492      |
|    fps              | 296       |
|    time_elapsed     | 1075      |
|    total_timesteps  | 318740    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 69684     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7496      |
|    fps              | 296       |
|    time_elapsed     | 1075      |
|    total_timesteps  | 318842    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 69710     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7500      |
|    fps              | 296       |
|    time_elapsed     | 1076      |
|    total_timesteps  | 318997    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 10.9      |
|    n_updates        | 69749     |
-----------------------------------
Eval num_timesteps=319000, episode_reward=-2975.82 +/- 994.75
Episode length: 35.64 +/- 5.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 319000    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7504      |
|    fps              | 296       |
|    time_elapsed     | 1077      |
|    total_timesteps  | 319149    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.7      |
|    n_updates        | 69787     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7508      |
|    fps              | 296       |
|    time_elapsed     | 1077      |
|    total_timesteps  | 319293    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 69823     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7512      |
|    fps              | 296       |
|    time_elapsed     | 1077      |
|    total_timesteps  | 319449    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 69862     |
-----------------------------------
Eval num_timesteps=319500, episode_reward=-2726.50 +/- 1114.13
Episode length: 35.20 +/- 6.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 319500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74        |
|    n_updates        | 69874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7516      |
|    fps              | 296       |
|    time_elapsed     | 1079      |
|    total_timesteps  | 319588    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 210       |
|    n_updates        | 69896     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7520      |
|    fps              | 296       |
|    time_elapsed     | 1079      |
|    total_timesteps  | 319732    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 69932     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7524      |
|    fps              | 296       |
|    time_elapsed     | 1079      |
|    total_timesteps  | 319856    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 211       |
|    n_updates        | 69963     |
-----------------------------------
Eval num_timesteps=320000, episode_reward=-2908.23 +/- 1012.39
Episode length: 35.70 +/- 6.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 320000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 213       |
|    n_updates        | 69999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7528      |
|    fps              | 296       |
|    time_elapsed     | 1080      |
|    total_timesteps  | 320007    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47.5      |
|    n_updates        | 70001     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7532      |
|    fps              | 296       |
|    time_elapsed     | 1081      |
|    total_timesteps  | 320141    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.3      |
|    n_updates        | 70035     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7536      |
|    fps              | 296       |
|    time_elapsed     | 1081      |
|    total_timesteps  | 320267    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 70066     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7540      |
|    fps              | 296       |
|    time_elapsed     | 1081      |
|    total_timesteps  | 320410    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 70102     |
-----------------------------------
Eval num_timesteps=320500, episode_reward=-3004.43 +/- 1159.97
Episode length: 35.34 +/- 6.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.3     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 320500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 92.1     |
|    n_updates        | 70124    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7544      |
|    fps              | 296       |
|    time_elapsed     | 1082      |
|    total_timesteps  | 320567    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 56        |
|    n_updates        | 70141     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7548      |
|    fps              | 296       |
|    time_elapsed     | 1082      |
|    total_timesteps  | 320700    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 70174     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7552      |
|    fps              | 296       |
|    time_elapsed     | 1083      |
|    total_timesteps  | 320850    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 70212     |
-----------------------------------
Eval num_timesteps=321000, episode_reward=-2798.78 +/- 1083.53
Episode length: 35.24 +/- 5.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.2     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 321000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 97.7     |
|    n_updates        | 70249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7556      |
|    fps              | 295       |
|    time_elapsed     | 1084      |
|    total_timesteps  | 321002    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 70250     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7560      |
|    fps              | 296       |
|    time_elapsed     | 1084      |
|    total_timesteps  | 321129    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 70282     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7564      |
|    fps              | 296       |
|    time_elapsed     | 1084      |
|    total_timesteps  | 321283    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 70320     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7568      |
|    fps              | 296       |
|    time_elapsed     | 1084      |
|    total_timesteps  | 321422    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 70355     |
-----------------------------------
Eval num_timesteps=321500, episode_reward=-2924.22 +/- 952.07
Episode length: 34.34 +/- 7.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 321500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 70374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7572      |
|    fps              | 296       |
|    time_elapsed     | 1086      |
|    total_timesteps  | 321548    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.3      |
|    n_updates        | 70386     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7576      |
|    fps              | 296       |
|    time_elapsed     | 1086      |
|    total_timesteps  | 321692    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 174       |
|    n_updates        | 70422     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7580      |
|    fps              | 296       |
|    time_elapsed     | 1086      |
|    total_timesteps  | 321849    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.4      |
|    n_updates        | 70462     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7584      |
|    fps              | 296       |
|    time_elapsed     | 1086      |
|    total_timesteps  | 321962    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.4      |
|    n_updates        | 70490     |
-----------------------------------
Eval num_timesteps=322000, episode_reward=-2912.62 +/- 946.34
Episode length: 36.26 +/- 5.22
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 322000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 70499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7588      |
|    fps              | 296       |
|    time_elapsed     | 1088      |
|    total_timesteps  | 322099    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65.4      |
|    n_updates        | 70524     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7592      |
|    fps              | 296       |
|    time_elapsed     | 1088      |
|    total_timesteps  | 322233    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 70558     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7596      |
|    fps              | 296       |
|    time_elapsed     | 1088      |
|    total_timesteps  | 322375    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 70593     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7600      |
|    fps              | 296       |
|    time_elapsed     | 1088      |
|    total_timesteps  | 322495    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.8      |
|    n_updates        | 70623     |
-----------------------------------
Eval num_timesteps=322500, episode_reward=-2814.11 +/- 1164.37
Episode length: 34.66 +/- 7.29
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 322500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 70624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7604      |
|    fps              | 296       |
|    time_elapsed     | 1089      |
|    total_timesteps  | 322633    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 319       |
|    n_updates        | 70658     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7608      |
|    fps              | 296       |
|    time_elapsed     | 1090      |
|    total_timesteps  | 322785    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 70696     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7612      |
|    fps              | 296       |
|    time_elapsed     | 1090      |
|    total_timesteps  | 322915    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 70728     |
-----------------------------------
Eval num_timesteps=323000, episode_reward=-2776.28 +/- 1313.98
Episode length: 36.10 +/- 6.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 323000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.8      |
|    n_updates        | 70749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7616      |
|    fps              | 295       |
|    time_elapsed     | 1091      |
|    total_timesteps  | 323071    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 70767     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7620      |
|    fps              | 296       |
|    time_elapsed     | 1091      |
|    total_timesteps  | 323215    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 70803     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7624      |
|    fps              | 296       |
|    time_elapsed     | 1091      |
|    total_timesteps  | 323334    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 70833     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7628      |
|    fps              | 296       |
|    time_elapsed     | 1092      |
|    total_timesteps  | 323489    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 70872     |
-----------------------------------
Eval num_timesteps=323500, episode_reward=-2649.98 +/- 1308.97
Episode length: 36.14 +/- 6.35
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 323500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 70874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7632      |
|    fps              | 295       |
|    time_elapsed     | 1093      |
|    total_timesteps  | 323629    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 70907     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7636      |
|    fps              | 296       |
|    time_elapsed     | 1093      |
|    total_timesteps  | 323777    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 226       |
|    n_updates        | 70944     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7640      |
|    fps              | 296       |
|    time_elapsed     | 1093      |
|    total_timesteps  | 323926    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 70981     |
-----------------------------------
Eval num_timesteps=324000, episode_reward=-2742.26 +/- 1259.24
Episode length: 35.56 +/- 6.30
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.74e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 324000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 70999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7644      |
|    fps              | 295       |
|    time_elapsed     | 1095      |
|    total_timesteps  | 324064    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.4      |
|    n_updates        | 71015     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7648      |
|    fps              | 295       |
|    time_elapsed     | 1095      |
|    total_timesteps  | 324194    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 71048     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7652      |
|    fps              | 296       |
|    time_elapsed     | 1095      |
|    total_timesteps  | 324327    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89        |
|    n_updates        | 71081     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7656      |
|    fps              | 296       |
|    time_elapsed     | 1095      |
|    total_timesteps  | 324483    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 71120     |
-----------------------------------
Eval num_timesteps=324500, episode_reward=-2928.05 +/- 1074.18
Episode length: 35.66 +/- 5.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 324500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.3      |
|    n_updates        | 71124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7660      |
|    fps              | 295       |
|    time_elapsed     | 1097      |
|    total_timesteps  | 324646    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.1      |
|    n_updates        | 71161     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7664      |
|    fps              | 296       |
|    time_elapsed     | 1097      |
|    total_timesteps  | 324812    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.6      |
|    n_updates        | 71202     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7668      |
|    fps              | 296       |
|    time_elapsed     | 1097      |
|    total_timesteps  | 324944    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 71235     |
-----------------------------------
Eval num_timesteps=325000, episode_reward=-2752.89 +/- 1170.62
Episode length: 34.94 +/- 7.34
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 325000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.1      |
|    n_updates        | 71249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7672      |
|    fps              | 295       |
|    time_elapsed     | 1098      |
|    total_timesteps  | 325075    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 280       |
|    n_updates        | 71268     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7676      |
|    fps              | 295       |
|    time_elapsed     | 1098      |
|    total_timesteps  | 325227    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95        |
|    n_updates        | 71306     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7680      |
|    fps              | 296       |
|    time_elapsed     | 1099      |
|    total_timesteps  | 325379    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 71344     |
-----------------------------------
Eval num_timesteps=325500, episode_reward=-2586.97 +/- 1478.11
Episode length: 36.80 +/- 6.34
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.59e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 325500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 71374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7684      |
|    fps              | 295       |
|    time_elapsed     | 1100      |
|    total_timesteps  | 325517    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 71379     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7688      |
|    fps              | 295       |
|    time_elapsed     | 1100      |
|    total_timesteps  | 325665    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.7      |
|    n_updates        | 71416     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.57e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7692      |
|    fps              | 295       |
|    time_elapsed     | 1100      |
|    total_timesteps  | 325829    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 71457     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7696      |
|    fps              | 296       |
|    time_elapsed     | 1100      |
|    total_timesteps  | 325965    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 71491     |
-----------------------------------
Eval num_timesteps=326000, episode_reward=-2903.25 +/- 935.24
Episode length: 36.54 +/- 5.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.5     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 326000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 110      |
|    n_updates        | 71499    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.54e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7700      |
|    fps              | 295       |
|    time_elapsed     | 1102      |
|    total_timesteps  | 326100    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 71524     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7704      |
|    fps              | 295       |
|    time_elapsed     | 1102      |
|    total_timesteps  | 326233    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.5      |
|    n_updates        | 71558     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.56e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7708      |
|    fps              | 295       |
|    time_elapsed     | 1102      |
|    total_timesteps  | 326380    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.8      |
|    n_updates        | 71594     |
-----------------------------------
Eval num_timesteps=326500, episode_reward=-2655.43 +/- 1218.58
Episode length: 36.06 +/- 6.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.66e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 326500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 71624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.53e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7712      |
|    fps              | 295       |
|    time_elapsed     | 1104      |
|    total_timesteps  | 326540    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.3      |
|    n_updates        | 71634     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.52e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7716      |
|    fps              | 295       |
|    time_elapsed     | 1104      |
|    total_timesteps  | 326693    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.1      |
|    n_updates        | 71673     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.52e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7720      |
|    fps              | 295       |
|    time_elapsed     | 1104      |
|    total_timesteps  | 326847    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.8      |
|    n_updates        | 71711     |
-----------------------------------
Eval num_timesteps=327000, episode_reward=-2557.32 +/- 1520.87
Episode length: 35.68 +/- 7.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.56e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 327000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 71749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -2.45e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7724      |
|    fps              | 295       |
|    time_elapsed     | 1105      |
|    total_timesteps  | 327002    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 71750     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.49e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7728      |
|    fps              | 295       |
|    time_elapsed     | 1106      |
|    total_timesteps  | 327153    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 71788     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -2.46e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7732      |
|    fps              | 295       |
|    time_elapsed     | 1106      |
|    total_timesteps  | 327308    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 71826     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.7     |
|    ep_rew_mean      | -2.5e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7736     |
|    fps              | 295      |
|    time_elapsed     | 1106     |
|    total_timesteps  | 327450   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 81.1     |
|    n_updates        | 71862    |
----------------------------------
Eval num_timesteps=327500, episode_reward=-2800.15 +/- 1079.75
Episode length: 34.46 +/- 8.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.5     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 327500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 103      |
|    n_updates        | 71874    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.54e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7740      |
|    fps              | 295       |
|    time_elapsed     | 1107      |
|    total_timesteps  | 327571    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75        |
|    n_updates        | 71892     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.52e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7744      |
|    fps              | 295       |
|    time_elapsed     | 1107      |
|    total_timesteps  | 327720    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.8      |
|    n_updates        | 71929     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.57e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7748      |
|    fps              | 295       |
|    time_elapsed     | 1108      |
|    total_timesteps  | 327845    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 71961     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7752      |
|    fps              | 295       |
|    time_elapsed     | 1108      |
|    total_timesteps  | 327967    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 71991     |
-----------------------------------
Eval num_timesteps=328000, episode_reward=-3036.61 +/- 959.64
Episode length: 35.12 +/- 6.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 328000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 71999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7756      |
|    fps              | 295       |
|    time_elapsed     | 1109      |
|    total_timesteps  | 328106    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 72026     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7760      |
|    fps              | 295       |
|    time_elapsed     | 1109      |
|    total_timesteps  | 328228    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 72056     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7764      |
|    fps              | 295       |
|    time_elapsed     | 1109      |
|    total_timesteps  | 328358    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 72089     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7768      |
|    fps              | 295       |
|    time_elapsed     | 1109      |
|    total_timesteps  | 328491    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 72122     |
-----------------------------------
Eval num_timesteps=328500, episode_reward=-2939.22 +/- 1127.08
Episode length: 34.76 +/- 6.98
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 328500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.8      |
|    n_updates        | 72124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7772      |
|    fps              | 295       |
|    time_elapsed     | 1111      |
|    total_timesteps  | 328639    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95        |
|    n_updates        | 72159     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7776      |
|    fps              | 295       |
|    time_elapsed     | 1111      |
|    total_timesteps  | 328782    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.1      |
|    n_updates        | 72195     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7780      |
|    fps              | 295       |
|    time_elapsed     | 1111      |
|    total_timesteps  | 328917    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 238       |
|    n_updates        | 72229     |
-----------------------------------
Eval num_timesteps=329000, episode_reward=-2578.71 +/- 1400.40
Episode length: 36.84 +/- 6.35
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.58e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 329000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 72249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7784      |
|    fps              | 295       |
|    time_elapsed     | 1113      |
|    total_timesteps  | 329051    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 221       |
|    n_updates        | 72262     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7788      |
|    fps              | 295       |
|    time_elapsed     | 1113      |
|    total_timesteps  | 329175    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.9      |
|    n_updates        | 72293     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7792      |
|    fps              | 295       |
|    time_elapsed     | 1113      |
|    total_timesteps  | 329312    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 229       |
|    n_updates        | 72327     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7796      |
|    fps              | 295       |
|    time_elapsed     | 1113      |
|    total_timesteps  | 329450    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.1      |
|    n_updates        | 72362     |
-----------------------------------
Eval num_timesteps=329500, episode_reward=-2996.48 +/- 1068.76
Episode length: 33.14 +/- 6.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.1     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 329500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 207      |
|    n_updates        | 72374    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7800      |
|    fps              | 295       |
|    time_elapsed     | 1114      |
|    total_timesteps  | 329578    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 72394     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.8     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7804     |
|    fps              | 295      |
|    time_elapsed     | 1114     |
|    total_timesteps  | 329713   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 207      |
|    n_updates        | 72428    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7808      |
|    fps              | 295       |
|    time_elapsed     | 1115      |
|    total_timesteps  | 329821    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 72455     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7812      |
|    fps              | 295       |
|    time_elapsed     | 1115      |
|    total_timesteps  | 329971    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 247       |
|    n_updates        | 72492     |
-----------------------------------
Eval num_timesteps=330000, episode_reward=-2822.94 +/- 1496.29
Episode length: 34.76 +/- 6.22
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 330000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.4      |
|    n_updates        | 72499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7816      |
|    fps              | 295       |
|    time_elapsed     | 1116      |
|    total_timesteps  | 330119    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 72529     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.1     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7820     |
|    fps              | 295      |
|    time_elapsed     | 1116     |
|    total_timesteps  | 330259   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 73.5     |
|    n_updates        | 72564    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7824      |
|    fps              | 295       |
|    time_elapsed     | 1116      |
|    total_timesteps  | 330396    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88.9      |
|    n_updates        | 72598     |
-----------------------------------
Eval num_timesteps=330500, episode_reward=-3133.69 +/- 879.50
Episode length: 34.74 +/- 6.23
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -3.13e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 330500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 186       |
|    n_updates        | 72624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7828      |
|    fps              | 295       |
|    time_elapsed     | 1118      |
|    total_timesteps  | 330555    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.4      |
|    n_updates        | 72638     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7832      |
|    fps              | 295       |
|    time_elapsed     | 1118      |
|    total_timesteps  | 330696    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 72673     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7836      |
|    fps              | 295       |
|    time_elapsed     | 1118      |
|    total_timesteps  | 330831    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 72707     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7840      |
|    fps              | 295       |
|    time_elapsed     | 1118      |
|    total_timesteps  | 330954    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 72738     |
-----------------------------------
Eval num_timesteps=331000, episode_reward=-2902.38 +/- 1282.97
Episode length: 35.06 +/- 7.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.1     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 331000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 73.5     |
|    n_updates        | 72749    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7844      |
|    fps              | 295       |
|    time_elapsed     | 1120      |
|    total_timesteps  | 331113    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 290       |
|    n_updates        | 72778     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7848      |
|    fps              | 295       |
|    time_elapsed     | 1120      |
|    total_timesteps  | 331258    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 72814     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7852      |
|    fps              | 295       |
|    time_elapsed     | 1120      |
|    total_timesteps  | 331393    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.1      |
|    n_updates        | 72848     |
-----------------------------------
Eval num_timesteps=331500, episode_reward=-2801.55 +/- 1299.24
Episode length: 34.90 +/- 6.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.9     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 331500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 155      |
|    n_updates        | 72874    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7856      |
|    fps              | 295       |
|    time_elapsed     | 1121      |
|    total_timesteps  | 331524    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 72880     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7860      |
|    fps              | 295       |
|    time_elapsed     | 1121      |
|    total_timesteps  | 331633    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93        |
|    n_updates        | 72908     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7864      |
|    fps              | 295       |
|    time_elapsed     | 1122      |
|    total_timesteps  | 331771    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 72942     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7868      |
|    fps              | 295       |
|    time_elapsed     | 1122      |
|    total_timesteps  | 331909    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 72977     |
-----------------------------------
Eval num_timesteps=332000, episode_reward=-3213.37 +/- 804.55
Episode length: 32.90 +/- 6.50
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.9      |
|    mean_reward      | -3.21e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 332000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.7      |
|    n_updates        | 72999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7872      |
|    fps              | 295       |
|    time_elapsed     | 1123      |
|    total_timesteps  | 332032    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 73007     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7876      |
|    fps              | 295       |
|    time_elapsed     | 1123      |
|    total_timesteps  | 332168    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 73041     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7880      |
|    fps              | 295       |
|    time_elapsed     | 1123      |
|    total_timesteps  | 332298    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 73074     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7884      |
|    fps              | 295       |
|    time_elapsed     | 1123      |
|    total_timesteps  | 332437    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 73109     |
-----------------------------------
Eval num_timesteps=332500, episode_reward=-2523.57 +/- 1111.81
Episode length: 36.20 +/- 6.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.52e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 332500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.6      |
|    n_updates        | 73124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7888      |
|    fps              | 295       |
|    time_elapsed     | 1125      |
|    total_timesteps  | 332583    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 73145     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7892      |
|    fps              | 295       |
|    time_elapsed     | 1125      |
|    total_timesteps  | 332738    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 73184     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.4     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7896     |
|    fps              | 295      |
|    time_elapsed     | 1125     |
|    total_timesteps  | 332889   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 59.6     |
|    n_updates        | 73222    |
----------------------------------
Eval num_timesteps=333000, episode_reward=-2811.12 +/- 1002.20
Episode length: 36.00 +/- 6.56
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 333000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 224       |
|    n_updates        | 73249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7900      |
|    fps              | 295       |
|    time_elapsed     | 1127      |
|    total_timesteps  | 333038    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 196       |
|    n_updates        | 73259     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7904      |
|    fps              | 295       |
|    time_elapsed     | 1127      |
|    total_timesteps  | 333170    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 73292     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7908      |
|    fps              | 295       |
|    time_elapsed     | 1127      |
|    total_timesteps  | 333298    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 73324     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7912      |
|    fps              | 295       |
|    time_elapsed     | 1127      |
|    total_timesteps  | 333412    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 259       |
|    n_updates        | 73352     |
-----------------------------------
Eval num_timesteps=333500, episode_reward=-2821.82 +/- 1120.29
Episode length: 35.34 +/- 5.91
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 333500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.8      |
|    n_updates        | 73374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7916      |
|    fps              | 295       |
|    time_elapsed     | 1128      |
|    total_timesteps  | 333546    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 73386     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7920      |
|    fps              | 295       |
|    time_elapsed     | 1128      |
|    total_timesteps  | 333681    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 73420     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7924      |
|    fps              | 295       |
|    time_elapsed     | 1129      |
|    total_timesteps  | 333814    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 73453     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7928      |
|    fps              | 295       |
|    time_elapsed     | 1129      |
|    total_timesteps  | 333932    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.8      |
|    n_updates        | 73482     |
-----------------------------------
Eval num_timesteps=334000, episode_reward=-2795.07 +/- 1171.83
Episode length: 35.08 +/- 7.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.1     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 334000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 137      |
|    n_updates        | 73499    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7932      |
|    fps              | 295       |
|    time_elapsed     | 1130      |
|    total_timesteps  | 334089    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.3      |
|    n_updates        | 73522     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.1     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7936     |
|    fps              | 295      |
|    time_elapsed     | 1130     |
|    total_timesteps  | 334239   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 212      |
|    n_updates        | 73559    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7940      |
|    fps              | 295       |
|    time_elapsed     | 1130      |
|    total_timesteps  | 334385    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 73596     |
-----------------------------------
Eval num_timesteps=334500, episode_reward=-2874.20 +/- 1195.90
Episode length: 34.94 +/- 7.03
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 334500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 73624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.9     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7944     |
|    fps              | 295      |
|    time_elapsed     | 1132     |
|    total_timesteps  | 334502   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 140      |
|    n_updates        | 73625    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34       |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7948     |
|    fps              | 295      |
|    time_elapsed     | 1132     |
|    total_timesteps  | 334653   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 112      |
|    n_updates        | 73663    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.8     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7952     |
|    fps              | 295      |
|    time_elapsed     | 1132     |
|    total_timesteps  | 334776   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 142      |
|    n_updates        | 73693    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.9     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7956     |
|    fps              | 295      |
|    time_elapsed     | 1132     |
|    total_timesteps  | 334915   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 169      |
|    n_updates        | 73728    |
----------------------------------
Eval num_timesteps=335000, episode_reward=-2590.50 +/- 1268.14
Episode length: 35.90 +/- 7.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.59e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 335000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 73749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7960      |
|    fps              | 295       |
|    time_elapsed     | 1134      |
|    total_timesteps  | 335049    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 13.6      |
|    n_updates        | 73762     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7964      |
|    fps              | 295       |
|    time_elapsed     | 1134      |
|    total_timesteps  | 335164    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 207       |
|    n_updates        | 73790     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7968      |
|    fps              | 295       |
|    time_elapsed     | 1134      |
|    total_timesteps  | 335308    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 73826     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7972      |
|    fps              | 295       |
|    time_elapsed     | 1134      |
|    total_timesteps  | 335440    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 73859     |
-----------------------------------
Eval num_timesteps=335500, episode_reward=-2772.80 +/- 1191.02
Episode length: 35.08 +/- 6.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 335500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37.6      |
|    n_updates        | 73874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7976      |
|    fps              | 295       |
|    time_elapsed     | 1135      |
|    total_timesteps  | 335583    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 73895     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7980      |
|    fps              | 295       |
|    time_elapsed     | 1136      |
|    total_timesteps  | 335739    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 73934     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7984      |
|    fps              | 295       |
|    time_elapsed     | 1136      |
|    total_timesteps  | 335896    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 73973     |
-----------------------------------
Eval num_timesteps=336000, episode_reward=-2997.08 +/- 970.34
Episode length: 35.88 +/- 5.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.9     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 336000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 220      |
|    n_updates        | 73999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7988      |
|    fps              | 295       |
|    time_elapsed     | 1137      |
|    total_timesteps  | 336013    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 200       |
|    n_updates        | 74003     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7992      |
|    fps              | 295       |
|    time_elapsed     | 1137      |
|    total_timesteps  | 336167    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.8      |
|    n_updates        | 74041     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 7996      |
|    fps              | 295       |
|    time_elapsed     | 1138      |
|    total_timesteps  | 336302    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 74075     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8000      |
|    fps              | 295       |
|    time_elapsed     | 1138      |
|    total_timesteps  | 336450    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 74112     |
-----------------------------------
Eval num_timesteps=336500, episode_reward=-3075.38 +/- 1161.75
Episode length: 34.40 +/- 7.12
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 336500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.6      |
|    n_updates        | 74124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8004      |
|    fps              | 295       |
|    time_elapsed     | 1139      |
|    total_timesteps  | 336596    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.5      |
|    n_updates        | 74148     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.2     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8008     |
|    fps              | 295      |
|    time_elapsed     | 1139     |
|    total_timesteps  | 336722   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 218      |
|    n_updates        | 74180    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8012      |
|    fps              | 295       |
|    time_elapsed     | 1139      |
|    total_timesteps  | 336868    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 74216     |
-----------------------------------
Eval num_timesteps=337000, episode_reward=-2944.22 +/- 1165.34
Episode length: 34.50 +/- 6.91
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 337000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 74249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8016      |
|    fps              | 295       |
|    time_elapsed     | 1141      |
|    total_timesteps  | 337006    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.1      |
|    n_updates        | 74251     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8020      |
|    fps              | 295       |
|    time_elapsed     | 1141      |
|    total_timesteps  | 337154    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 74288     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8024      |
|    fps              | 295       |
|    time_elapsed     | 1141      |
|    total_timesteps  | 337299    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.9      |
|    n_updates        | 74324     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.9     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8028     |
|    fps              | 295      |
|    time_elapsed     | 1141     |
|    total_timesteps  | 337421   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 185      |
|    n_updates        | 74355    |
----------------------------------
Eval num_timesteps=337500, episode_reward=-2990.50 +/- 1082.72
Episode length: 35.18 +/- 5.72
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 337500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 74374     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8032     |
|    fps              | 295      |
|    time_elapsed     | 1143     |
|    total_timesteps  | 337584   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 116      |
|    n_updates        | 74395    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8036      |
|    fps              | 295       |
|    time_elapsed     | 1143      |
|    total_timesteps  | 337704    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95        |
|    n_updates        | 74425     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8040      |
|    fps              | 295       |
|    time_elapsed     | 1143      |
|    total_timesteps  | 337851    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 74462     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8044      |
|    fps              | 295       |
|    time_elapsed     | 1143      |
|    total_timesteps  | 337956    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 74488     |
-----------------------------------
Eval num_timesteps=338000, episode_reward=-2616.88 +/- 1349.09
Episode length: 35.34 +/- 7.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.62e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 338000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 74499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8048      |
|    fps              | 295       |
|    time_elapsed     | 1144      |
|    total_timesteps  | 338102    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 74525     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8052      |
|    fps              | 295       |
|    time_elapsed     | 1144      |
|    total_timesteps  | 338236    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.2      |
|    n_updates        | 74558     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8056      |
|    fps              | 295       |
|    time_elapsed     | 1145      |
|    total_timesteps  | 338354    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.4      |
|    n_updates        | 74588     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.4     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8060     |
|    fps              | 295      |
|    time_elapsed     | 1145     |
|    total_timesteps  | 338491   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 126      |
|    n_updates        | 74622    |
----------------------------------
Eval num_timesteps=338500, episode_reward=-2590.02 +/- 1225.42
Episode length: 36.84 +/- 6.01
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.59e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 338500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 74624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8064      |
|    fps              | 295       |
|    time_elapsed     | 1146      |
|    total_timesteps  | 338632    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 230       |
|    n_updates        | 74657     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8068      |
|    fps              | 295       |
|    time_elapsed     | 1146      |
|    total_timesteps  | 338782    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 289       |
|    n_updates        | 74695     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.03e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8072      |
|    fps              | 295       |
|    time_elapsed     | 1146      |
|    total_timesteps  | 338886    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 246       |
|    n_updates        | 74721     |
-----------------------------------
Eval num_timesteps=339000, episode_reward=-2961.51 +/- 894.31
Episode length: 35.68 +/- 7.33
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 339000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 74749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8076      |
|    fps              | 295       |
|    time_elapsed     | 1148      |
|    total_timesteps  | 339028    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.4      |
|    n_updates        | 74756     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8080      |
|    fps              | 295       |
|    time_elapsed     | 1148      |
|    total_timesteps  | 339184    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 74795     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8084      |
|    fps              | 295       |
|    time_elapsed     | 1148      |
|    total_timesteps  | 339328    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 213       |
|    n_updates        | 74831     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8088      |
|    fps              | 295       |
|    time_elapsed     | 1148      |
|    total_timesteps  | 339460    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 74864     |
-----------------------------------
Eval num_timesteps=339500, episode_reward=-3144.25 +/- 953.18
Episode length: 33.46 +/- 7.29
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.5      |
|    mean_reward      | -3.14e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 339500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77        |
|    n_updates        | 74874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8092      |
|    fps              | 295       |
|    time_elapsed     | 1150      |
|    total_timesteps  | 339605    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.2      |
|    n_updates        | 74901     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8096      |
|    fps              | 295       |
|    time_elapsed     | 1150      |
|    total_timesteps  | 339746    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 74936     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8100      |
|    fps              | 295       |
|    time_elapsed     | 1150      |
|    total_timesteps  | 339886    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 196       |
|    n_updates        | 74971     |
-----------------------------------
Eval num_timesteps=340000, episode_reward=-2830.90 +/- 1411.91
Episode length: 35.40 +/- 6.49
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 340000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 74999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8104      |
|    fps              | 295       |
|    time_elapsed     | 1151      |
|    total_timesteps  | 340036    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 75008     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8108      |
|    fps              | 295       |
|    time_elapsed     | 1151      |
|    total_timesteps  | 340171    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.9      |
|    n_updates        | 75042     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8112      |
|    fps              | 295       |
|    time_elapsed     | 1152      |
|    total_timesteps  | 340319    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 181       |
|    n_updates        | 75079     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8116      |
|    fps              | 295       |
|    time_elapsed     | 1152      |
|    total_timesteps  | 340448    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 75111     |
-----------------------------------
Eval num_timesteps=340500, episode_reward=-2953.06 +/- 975.18
Episode length: 35.18 +/- 5.74
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 340500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 75124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8120      |
|    fps              | 295       |
|    time_elapsed     | 1153      |
|    total_timesteps  | 340563    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 75140     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8124      |
|    fps              | 295       |
|    time_elapsed     | 1153      |
|    total_timesteps  | 340689    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 75172     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8128      |
|    fps              | 295       |
|    time_elapsed     | 1153      |
|    total_timesteps  | 340825    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 231       |
|    n_updates        | 75206     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8132      |
|    fps              | 295       |
|    time_elapsed     | 1154      |
|    total_timesteps  | 340970    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 75242     |
-----------------------------------
Eval num_timesteps=341000, episode_reward=-2912.87 +/- 1254.91
Episode length: 33.20 +/- 7.35
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.2      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 341000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 75249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8136      |
|    fps              | 295       |
|    time_elapsed     | 1155      |
|    total_timesteps  | 341116    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 326       |
|    n_updates        | 75278     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.2     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8140     |
|    fps              | 295      |
|    time_elapsed     | 1155     |
|    total_timesteps  | 341270   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 127      |
|    n_updates        | 75317    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8144      |
|    fps              | 295       |
|    time_elapsed     | 1155      |
|    total_timesteps  | 341413    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 75353     |
-----------------------------------
Eval num_timesteps=341500, episode_reward=-2914.61 +/- 976.39
Episode length: 35.22 +/- 6.38
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 341500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.2      |
|    n_updates        | 75374     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.5     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8148     |
|    fps              | 295      |
|    time_elapsed     | 1157     |
|    total_timesteps  | 341557   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 165      |
|    n_updates        | 75389    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.05e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8152      |
|    fps              | 295       |
|    time_elapsed     | 1157      |
|    total_timesteps  | 341679    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 32.5      |
|    n_updates        | 75419     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8156      |
|    fps              | 295       |
|    time_elapsed     | 1157      |
|    total_timesteps  | 341827    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 75456     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8160      |
|    fps              | 295       |
|    time_elapsed     | 1157      |
|    total_timesteps  | 341968    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 245       |
|    n_updates        | 75491     |
-----------------------------------
Eval num_timesteps=342000, episode_reward=-2925.11 +/- 1273.28
Episode length: 34.42 +/- 6.53
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 342000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 75499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8164      |
|    fps              | 295       |
|    time_elapsed     | 1158      |
|    total_timesteps  | 342126    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 75531     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8168      |
|    fps              | 295       |
|    time_elapsed     | 1159      |
|    total_timesteps  | 342269    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.6      |
|    n_updates        | 75567     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8172      |
|    fps              | 295       |
|    time_elapsed     | 1159      |
|    total_timesteps  | 342411    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 75602     |
-----------------------------------
Eval num_timesteps=342500, episode_reward=-2632.87 +/- 1269.59
Episode length: 36.34 +/- 6.18
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.63e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 342500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.3      |
|    n_updates        | 75624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8176      |
|    fps              | 295       |
|    time_elapsed     | 1160      |
|    total_timesteps  | 342578    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 75644     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8180      |
|    fps              | 295       |
|    time_elapsed     | 1160      |
|    total_timesteps  | 342706    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 256       |
|    n_updates        | 75676     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8184      |
|    fps              | 295       |
|    time_elapsed     | 1160      |
|    total_timesteps  | 342847    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 75711     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8188      |
|    fps              | 295       |
|    time_elapsed     | 1161      |
|    total_timesteps  | 342992    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 219       |
|    n_updates        | 75747     |
-----------------------------------
Eval num_timesteps=343000, episode_reward=-3050.65 +/- 1091.17
Episode length: 34.46 +/- 7.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -3.05e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 343000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 253       |
|    n_updates        | 75749     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8192     |
|    fps              | 295      |
|    time_elapsed     | 1162     |
|    total_timesteps  | 343132   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 65.8     |
|    n_updates        | 75782    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8196      |
|    fps              | 295       |
|    time_elapsed     | 1162      |
|    total_timesteps  | 343270    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 196       |
|    n_updates        | 75817     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8200      |
|    fps              | 295       |
|    time_elapsed     | 1162      |
|    total_timesteps  | 343399    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 243       |
|    n_updates        | 75849     |
-----------------------------------
Eval num_timesteps=343500, episode_reward=-3221.58 +/- 848.82
Episode length: 35.10 +/- 8.05
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -3.22e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 343500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 75874     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8204     |
|    fps              | 295      |
|    time_elapsed     | 1164     |
|    total_timesteps  | 343535   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 145      |
|    n_updates        | 75883    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8208      |
|    fps              | 295       |
|    time_elapsed     | 1164      |
|    total_timesteps  | 343679    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 75919     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -3.03e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8212      |
|    fps              | 295       |
|    time_elapsed     | 1164      |
|    total_timesteps  | 343843    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 233       |
|    n_updates        | 75960     |
-----------------------------------
Eval num_timesteps=344000, episode_reward=-2897.41 +/- 1068.09
Episode length: 35.48 +/- 6.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.5     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 344000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 120      |
|    n_updates        | 75999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8216      |
|    fps              | 295       |
|    time_elapsed     | 1165      |
|    total_timesteps  | 344008    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 76001     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8220      |
|    fps              | 295       |
|    time_elapsed     | 1165      |
|    total_timesteps  | 344151    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 76037     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8224      |
|    fps              | 295       |
|    time_elapsed     | 1166      |
|    total_timesteps  | 344303    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 76075     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8228      |
|    fps              | 295       |
|    time_elapsed     | 1166      |
|    total_timesteps  | 344440    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.9      |
|    n_updates        | 76109     |
-----------------------------------
Eval num_timesteps=344500, episode_reward=-2727.84 +/- 1300.76
Episode length: 36.42 +/- 6.22
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 344500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 76124     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.2     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8232     |
|    fps              | 295      |
|    time_elapsed     | 1167     |
|    total_timesteps  | 344586   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 160      |
|    n_updates        | 76146    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8236      |
|    fps              | 295       |
|    time_elapsed     | 1167      |
|    total_timesteps  | 344724    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 76180     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8240      |
|    fps              | 295       |
|    time_elapsed     | 1168      |
|    total_timesteps  | 344880    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 55.4      |
|    n_updates        | 76219     |
-----------------------------------
Eval num_timesteps=345000, episode_reward=-2680.85 +/- 1205.95
Episode length: 36.54 +/- 5.78
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 345000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.7      |
|    n_updates        | 76249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8244      |
|    fps              | 295       |
|    time_elapsed     | 1169      |
|    total_timesteps  | 345042    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.1      |
|    n_updates        | 76260     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8248      |
|    fps              | 295       |
|    time_elapsed     | 1169      |
|    total_timesteps  | 345193    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 76298     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8252      |
|    fps              | 295       |
|    time_elapsed     | 1169      |
|    total_timesteps  | 345333    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 76333     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8256      |
|    fps              | 295       |
|    time_elapsed     | 1169      |
|    total_timesteps  | 345490    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 226       |
|    n_updates        | 76372     |
-----------------------------------
Eval num_timesteps=345500, episode_reward=-2903.73 +/- 1200.30
Episode length: 35.06 +/- 6.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.1     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 345500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 71.2     |
|    n_updates        | 76374    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8260      |
|    fps              | 295       |
|    time_elapsed     | 1171      |
|    total_timesteps  | 345645    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88.8      |
|    n_updates        | 76411     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8264      |
|    fps              | 295       |
|    time_elapsed     | 1171      |
|    total_timesteps  | 345793    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 76448     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8268      |
|    fps              | 295       |
|    time_elapsed     | 1171      |
|    total_timesteps  | 345947    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 275       |
|    n_updates        | 76486     |
-----------------------------------
Eval num_timesteps=346000, episode_reward=-2906.43 +/- 1187.53
Episode length: 34.54 +/- 6.28
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 346000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 76499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8272      |
|    fps              | 295       |
|    time_elapsed     | 1173      |
|    total_timesteps  | 346073    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 238       |
|    n_updates        | 76518     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8276      |
|    fps              | 295       |
|    time_elapsed     | 1173      |
|    total_timesteps  | 346219    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 76554     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8280      |
|    fps              | 295       |
|    time_elapsed     | 1173      |
|    total_timesteps  | 346347    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.6      |
|    n_updates        | 76586     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.3     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8284     |
|    fps              | 295      |
|    time_elapsed     | 1173     |
|    total_timesteps  | 346481   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 204      |
|    n_updates        | 76620    |
----------------------------------
Eval num_timesteps=346500, episode_reward=-2568.11 +/- 1268.19
Episode length: 37.02 +/- 5.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37        |
|    mean_reward      | -2.57e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 346500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 76624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8288      |
|    fps              | 295       |
|    time_elapsed     | 1174      |
|    total_timesteps  | 346619    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.3      |
|    n_updates        | 76654     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8292      |
|    fps              | 295       |
|    time_elapsed     | 1175      |
|    total_timesteps  | 346754    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 76688     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8296      |
|    fps              | 295       |
|    time_elapsed     | 1175      |
|    total_timesteps  | 346885    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 76721     |
-----------------------------------
Eval num_timesteps=347000, episode_reward=-2969.55 +/- 946.52
Episode length: 37.34 +/- 6.20
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.3      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 347000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 76749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8300      |
|    fps              | 294       |
|    time_elapsed     | 1176      |
|    total_timesteps  | 347047    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 195       |
|    n_updates        | 76761     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8304      |
|    fps              | 295       |
|    time_elapsed     | 1176      |
|    total_timesteps  | 347186    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 254       |
|    n_updates        | 76796     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8308      |
|    fps              | 295       |
|    time_elapsed     | 1176      |
|    total_timesteps  | 347311    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 76827     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8312      |
|    fps              | 295       |
|    time_elapsed     | 1177      |
|    total_timesteps  | 347478    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.8      |
|    n_updates        | 76869     |
-----------------------------------
Eval num_timesteps=347500, episode_reward=-2643.37 +/- 1173.14
Episode length: 36.22 +/- 5.55
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 347500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.3      |
|    n_updates        | 76874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8316      |
|    fps              | 294       |
|    time_elapsed     | 1178      |
|    total_timesteps  | 347628    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.3      |
|    n_updates        | 76906     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8320      |
|    fps              | 295       |
|    time_elapsed     | 1178      |
|    total_timesteps  | 347776    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 220       |
|    n_updates        | 76943     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8324      |
|    fps              | 295       |
|    time_elapsed     | 1178      |
|    total_timesteps  | 347926    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 76981     |
-----------------------------------
Eval num_timesteps=348000, episode_reward=-2951.38 +/- 1049.10
Episode length: 35.02 +/- 7.23
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 348000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.6      |
|    n_updates        | 76999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8328      |
|    fps              | 294       |
|    time_elapsed     | 1180      |
|    total_timesteps  | 348086    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 77021     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8332      |
|    fps              | 294       |
|    time_elapsed     | 1180      |
|    total_timesteps  | 348221    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52.5      |
|    n_updates        | 77055     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8336      |
|    fps              | 295       |
|    time_elapsed     | 1180      |
|    total_timesteps  | 348358    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 216       |
|    n_updates        | 77089     |
-----------------------------------
Eval num_timesteps=348500, episode_reward=-2710.74 +/- 1446.54
Episode length: 36.28 +/- 6.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.71e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 348500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 77124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8340      |
|    fps              | 294       |
|    time_elapsed     | 1181      |
|    total_timesteps  | 348504    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 77125     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8344      |
|    fps              | 294       |
|    time_elapsed     | 1182      |
|    total_timesteps  | 348637    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 77159     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8348      |
|    fps              | 295       |
|    time_elapsed     | 1182      |
|    total_timesteps  | 348775    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 77193     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8352      |
|    fps              | 295       |
|    time_elapsed     | 1182      |
|    total_timesteps  | 348918    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 77229     |
-----------------------------------
Eval num_timesteps=349000, episode_reward=-2641.79 +/- 1212.79
Episode length: 34.90 +/- 7.97
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 349000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 77249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8356      |
|    fps              | 294       |
|    time_elapsed     | 1183      |
|    total_timesteps  | 349065    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 77266     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8360      |
|    fps              | 294       |
|    time_elapsed     | 1183      |
|    total_timesteps  | 349216    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 77303     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8364      |
|    fps              | 295       |
|    time_elapsed     | 1184      |
|    total_timesteps  | 349355    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 77338     |
-----------------------------------
Eval num_timesteps=349500, episode_reward=-3084.51 +/- 847.12
Episode length: 36.28 +/- 5.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 349500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.1      |
|    n_updates        | 77374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8368      |
|    fps              | 294       |
|    time_elapsed     | 1185      |
|    total_timesteps  | 349501    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 196       |
|    n_updates        | 77375     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8372      |
|    fps              | 294       |
|    time_elapsed     | 1185      |
|    total_timesteps  | 349640    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44        |
|    n_updates        | 77409     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8376      |
|    fps              | 294       |
|    time_elapsed     | 1185      |
|    total_timesteps  | 349801    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 77450     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8380      |
|    fps              | 295       |
|    time_elapsed     | 1186      |
|    total_timesteps  | 349948    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.6      |
|    n_updates        | 77486     |
-----------------------------------
Eval num_timesteps=350000, episode_reward=-2943.10 +/- 1109.32
Episode length: 34.42 +/- 7.25
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 350000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 77499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8384      |
|    fps              | 294       |
|    time_elapsed     | 1187      |
|    total_timesteps  | 350095    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 77523     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8388      |
|    fps              | 294       |
|    time_elapsed     | 1187      |
|    total_timesteps  | 350255    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 77563     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8392      |
|    fps              | 295       |
|    time_elapsed     | 1187      |
|    total_timesteps  | 350402    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 77600     |
-----------------------------------
Eval num_timesteps=350500, episode_reward=-2975.35 +/- 1001.11
Episode length: 35.58 +/- 6.74
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 350500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88.4      |
|    n_updates        | 77624     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.8     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8396     |
|    fps              | 294      |
|    time_elapsed     | 1189     |
|    total_timesteps  | 350560   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 82.4     |
|    n_updates        | 77639    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8400      |
|    fps              | 294       |
|    time_elapsed     | 1189      |
|    total_timesteps  | 350715    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 77678     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.7     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8404     |
|    fps              | 294      |
|    time_elapsed     | 1189     |
|    total_timesteps  | 350857   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 128      |
|    n_updates        | 77714    |
----------------------------------
Eval num_timesteps=351000, episode_reward=-2954.48 +/- 1008.72
Episode length: 35.56 +/- 5.35
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 351000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 77749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8408      |
|    fps              | 294       |
|    time_elapsed     | 1190      |
|    total_timesteps  | 351005    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 77751     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8412      |
|    fps              | 294       |
|    time_elapsed     | 1190      |
|    total_timesteps  | 351125    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 77781     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8416      |
|    fps              | 294       |
|    time_elapsed     | 1191      |
|    total_timesteps  | 351278    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 77819     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8420      |
|    fps              | 294       |
|    time_elapsed     | 1191      |
|    total_timesteps  | 351427    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 77856     |
-----------------------------------
Eval num_timesteps=351500, episode_reward=-2952.58 +/- 1138.56
Episode length: 34.62 +/- 5.93
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 351500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 77874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8424      |
|    fps              | 294       |
|    time_elapsed     | 1192      |
|    total_timesteps  | 351542    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 77885     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8428      |
|    fps              | 294       |
|    time_elapsed     | 1192      |
|    total_timesteps  | 351682    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 77920     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8432      |
|    fps              | 294       |
|    time_elapsed     | 1192      |
|    total_timesteps  | 351813    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 56.1      |
|    n_updates        | 77953     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8436      |
|    fps              | 294       |
|    time_elapsed     | 1193      |
|    total_timesteps  | 351961    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 261       |
|    n_updates        | 77990     |
-----------------------------------
Eval num_timesteps=352000, episode_reward=-2606.24 +/- 1259.60
Episode length: 36.86 +/- 5.64
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.9      |
|    mean_reward      | -2.61e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 352000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 77999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8440      |
|    fps              | 294       |
|    time_elapsed     | 1194      |
|    total_timesteps  | 352115    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 335       |
|    n_updates        | 78028     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8444      |
|    fps              | 294       |
|    time_elapsed     | 1194      |
|    total_timesteps  | 352238    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 78059     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8448      |
|    fps              | 294       |
|    time_elapsed     | 1194      |
|    total_timesteps  | 352376    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 181       |
|    n_updates        | 78093     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.8     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8452     |
|    fps              | 294      |
|    time_elapsed     | 1195     |
|    total_timesteps  | 352498   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 142      |
|    n_updates        | 78124    |
----------------------------------
Eval num_timesteps=352500, episode_reward=-2654.43 +/- 1436.49
Episode length: 36.38 +/- 6.57
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 352500    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8456      |
|    fps              | 294       |
|    time_elapsed     | 1196      |
|    total_timesteps  | 352646    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 78161     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8460     |
|    fps              | 294      |
|    time_elapsed     | 1196     |
|    total_timesteps  | 352819   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 183      |
|    n_updates        | 78204    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8464      |
|    fps              | 294       |
|    time_elapsed     | 1196      |
|    total_timesteps  | 352948    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 78236     |
-----------------------------------
Eval num_timesteps=353000, episode_reward=-2956.72 +/- 1260.32
Episode length: 33.58 +/- 7.59
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.6      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 353000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 78249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8468      |
|    fps              | 294       |
|    time_elapsed     | 1198      |
|    total_timesteps  | 353101    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.7      |
|    n_updates        | 78275     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8472      |
|    fps              | 294       |
|    time_elapsed     | 1198      |
|    total_timesteps  | 353262    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.1      |
|    n_updates        | 78315     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8476      |
|    fps              | 294       |
|    time_elapsed     | 1198      |
|    total_timesteps  | 353404    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.5      |
|    n_updates        | 78350     |
-----------------------------------
Eval num_timesteps=353500, episode_reward=-2878.10 +/- 1174.24
Episode length: 34.78 +/- 7.06
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 353500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.2      |
|    n_updates        | 78374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8480      |
|    fps              | 294       |
|    time_elapsed     | 1199      |
|    total_timesteps  | 353544    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 78385     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8484      |
|    fps              | 294       |
|    time_elapsed     | 1200      |
|    total_timesteps  | 353680    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 274       |
|    n_updates        | 78419     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.8     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8488     |
|    fps              | 294      |
|    time_elapsed     | 1200     |
|    total_timesteps  | 353833   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 151      |
|    n_updates        | 78458    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8492      |
|    fps              | 294       |
|    time_elapsed     | 1200      |
|    total_timesteps  | 353951    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 78487     |
-----------------------------------
Eval num_timesteps=354000, episode_reward=-2782.41 +/- 1150.21
Episode length: 37.04 +/- 6.77
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37        |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 354000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.4      |
|    n_updates        | 78499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8496      |
|    fps              | 294       |
|    time_elapsed     | 1201      |
|    total_timesteps  | 354107    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 78526     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8500      |
|    fps              | 294       |
|    time_elapsed     | 1201      |
|    total_timesteps  | 354248    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 78561     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8504     |
|    fps              | 294      |
|    time_elapsed     | 1202     |
|    total_timesteps  | 354366   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 216      |
|    n_updates        | 78591    |
----------------------------------
Eval num_timesteps=354500, episode_reward=-3077.71 +/- 863.89
Episode length: 34.66 +/- 8.19
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 354500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.4      |
|    n_updates        | 78624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8508      |
|    fps              | 294       |
|    time_elapsed     | 1203      |
|    total_timesteps  | 354511    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 40.8      |
|    n_updates        | 78627     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8512      |
|    fps              | 294       |
|    time_elapsed     | 1203      |
|    total_timesteps  | 354662    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 209       |
|    n_updates        | 78665     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8516     |
|    fps              | 294      |
|    time_elapsed     | 1203     |
|    total_timesteps  | 354811   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 99.6     |
|    n_updates        | 78702    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8520      |
|    fps              | 294       |
|    time_elapsed     | 1203      |
|    total_timesteps  | 354937    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.4      |
|    n_updates        | 78734     |
-----------------------------------
Eval num_timesteps=355000, episode_reward=-2716.10 +/- 1126.96
Episode length: 35.66 +/- 7.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 355000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 78749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8524      |
|    fps              | 294       |
|    time_elapsed     | 1205      |
|    total_timesteps  | 355081    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.4      |
|    n_updates        | 78770     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8528      |
|    fps              | 294       |
|    time_elapsed     | 1205      |
|    total_timesteps  | 355216    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.5      |
|    n_updates        | 78803     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8532      |
|    fps              | 294       |
|    time_elapsed     | 1205      |
|    total_timesteps  | 355343    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 78835     |
-----------------------------------
Eval num_timesteps=355500, episode_reward=-2926.86 +/- 1108.90
Episode length: 34.76 +/- 8.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 355500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 186       |
|    n_updates        | 78874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8536      |
|    fps              | 294       |
|    time_elapsed     | 1207      |
|    total_timesteps  | 355505    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.1      |
|    n_updates        | 78876     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8540      |
|    fps              | 294       |
|    time_elapsed     | 1207      |
|    total_timesteps  | 355645    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 78911     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8544      |
|    fps              | 294       |
|    time_elapsed     | 1207      |
|    total_timesteps  | 355786    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 211       |
|    n_updates        | 78946     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8548      |
|    fps              | 294       |
|    time_elapsed     | 1207      |
|    total_timesteps  | 355942    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 78985     |
-----------------------------------
Eval num_timesteps=356000, episode_reward=-2672.29 +/- 1062.18
Episode length: 36.26 +/- 6.73
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 356000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 78999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8552      |
|    fps              | 294       |
|    time_elapsed     | 1208      |
|    total_timesteps  | 356085    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 212       |
|    n_updates        | 79021     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8556      |
|    fps              | 294       |
|    time_elapsed     | 1209      |
|    total_timesteps  | 356242    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 79060     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8560      |
|    fps              | 294       |
|    time_elapsed     | 1209      |
|    total_timesteps  | 356391    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 6.29      |
|    n_updates        | 79097     |
-----------------------------------
Eval num_timesteps=356500, episode_reward=-2910.70 +/- 1098.60
Episode length: 35.54 +/- 7.23
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 356500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.9      |
|    n_updates        | 79124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8564      |
|    fps              | 294       |
|    time_elapsed     | 1210      |
|    total_timesteps  | 356551    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 79137     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8568      |
|    fps              | 294       |
|    time_elapsed     | 1210      |
|    total_timesteps  | 356695    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 79173     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8572      |
|    fps              | 294       |
|    time_elapsed     | 1210      |
|    total_timesteps  | 356832    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 79207     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8576     |
|    fps              | 294      |
|    time_elapsed     | 1211     |
|    total_timesteps  | 356974   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 103      |
|    n_updates        | 79243    |
----------------------------------
Eval num_timesteps=357000, episode_reward=-2941.27 +/- 1086.22
Episode length: 35.90 +/- 7.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 357000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.8      |
|    n_updates        | 79249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8580      |
|    fps              | 294       |
|    time_elapsed     | 1212      |
|    total_timesteps  | 357136    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 79283     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8584      |
|    fps              | 294       |
|    time_elapsed     | 1212      |
|    total_timesteps  | 357281    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 79320     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8588      |
|    fps              | 294       |
|    time_elapsed     | 1212      |
|    total_timesteps  | 357438    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.4      |
|    n_updates        | 79359     |
-----------------------------------
Eval num_timesteps=357500, episode_reward=-2682.61 +/- 1257.46
Episode length: 36.32 +/- 6.57
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 357500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76        |
|    n_updates        | 79374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8592      |
|    fps              | 294       |
|    time_elapsed     | 1214      |
|    total_timesteps  | 357580    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 79394     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8596      |
|    fps              | 294       |
|    time_elapsed     | 1214      |
|    total_timesteps  | 357724    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 79430     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8600      |
|    fps              | 294       |
|    time_elapsed     | 1214      |
|    total_timesteps  | 357858    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 79464     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8604      |
|    fps              | 294       |
|    time_elapsed     | 1214      |
|    total_timesteps  | 357961    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.4      |
|    n_updates        | 79490     |
-----------------------------------
Eval num_timesteps=358000, episode_reward=-2850.00 +/- 1136.92
Episode length: 35.94 +/- 5.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 358000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 79499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8608      |
|    fps              | 294       |
|    time_elapsed     | 1216      |
|    total_timesteps  | 358110    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 79527     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8612      |
|    fps              | 294       |
|    time_elapsed     | 1216      |
|    total_timesteps  | 358243    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 220       |
|    n_updates        | 79560     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.8     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8616     |
|    fps              | 294      |
|    time_elapsed     | 1216     |
|    total_timesteps  | 358390   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 177      |
|    n_updates        | 79597    |
----------------------------------
Eval num_timesteps=358500, episode_reward=-2701.92 +/- 1156.26
Episode length: 36.32 +/- 6.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.3     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 358500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 145      |
|    n_updates        | 79624    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8620      |
|    fps              | 294       |
|    time_elapsed     | 1217      |
|    total_timesteps  | 358545    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.5      |
|    n_updates        | 79636     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8624      |
|    fps              | 294       |
|    time_elapsed     | 1218      |
|    total_timesteps  | 358708    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 79676     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8628      |
|    fps              | 294       |
|    time_elapsed     | 1218      |
|    total_timesteps  | 358871    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 212       |
|    n_updates        | 79717     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.5     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8632     |
|    fps              | 294      |
|    time_elapsed     | 1218     |
|    total_timesteps  | 358992   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 69.6     |
|    n_updates        | 79747    |
----------------------------------
Eval num_timesteps=359000, episode_reward=-2910.58 +/- 1199.03
Episode length: 34.60 +/- 6.87
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 359000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 79749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8636      |
|    fps              | 294       |
|    time_elapsed     | 1219      |
|    total_timesteps  | 359105    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.3      |
|    n_updates        | 79776     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8640      |
|    fps              | 294       |
|    time_elapsed     | 1219      |
|    total_timesteps  | 359278    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 79819     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8644      |
|    fps              | 294       |
|    time_elapsed     | 1220      |
|    total_timesteps  | 359411    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61        |
|    n_updates        | 79852     |
-----------------------------------
Eval num_timesteps=359500, episode_reward=-2945.90 +/- 998.87
Episode length: 34.24 +/- 6.23
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 359500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.2      |
|    n_updates        | 79874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8648      |
|    fps              | 294       |
|    time_elapsed     | 1221      |
|    total_timesteps  | 359562    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53.8      |
|    n_updates        | 79890     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8652      |
|    fps              | 294       |
|    time_elapsed     | 1221      |
|    total_timesteps  | 359709    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 79927     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8656      |
|    fps              | 294       |
|    time_elapsed     | 1221      |
|    total_timesteps  | 359819    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 79954     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8660      |
|    fps              | 294       |
|    time_elapsed     | 1221      |
|    total_timesteps  | 359963    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.1      |
|    n_updates        | 79990     |
-----------------------------------
Eval num_timesteps=360000, episode_reward=-3173.76 +/- 844.32
Episode length: 33.08 +/- 6.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.1      |
|    mean_reward      | -3.17e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 360000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 79999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8664      |
|    fps              | 294       |
|    time_elapsed     | 1223      |
|    total_timesteps  | 360114    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 242       |
|    n_updates        | 80028     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8668      |
|    fps              | 294       |
|    time_elapsed     | 1223      |
|    total_timesteps  | 360267    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 249       |
|    n_updates        | 80066     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8672      |
|    fps              | 294       |
|    time_elapsed     | 1223      |
|    total_timesteps  | 360413    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 220       |
|    n_updates        | 80103     |
-----------------------------------
Eval num_timesteps=360500, episode_reward=-2554.98 +/- 1488.69
Episode length: 36.56 +/- 6.51
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.6      |
|    mean_reward      | -2.55e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 360500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 80124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8676      |
|    fps              | 294       |
|    time_elapsed     | 1224      |
|    total_timesteps  | 360576    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.3      |
|    n_updates        | 80143     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8680      |
|    fps              | 294       |
|    time_elapsed     | 1225      |
|    total_timesteps  | 360733    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 80183     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8684      |
|    fps              | 294       |
|    time_elapsed     | 1225      |
|    total_timesteps  | 360895    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53.5      |
|    n_updates        | 80223     |
-----------------------------------
Eval num_timesteps=361000, episode_reward=-2824.16 +/- 1200.44
Episode length: 35.66 +/- 6.41
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 361000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 80249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8688      |
|    fps              | 294       |
|    time_elapsed     | 1226      |
|    total_timesteps  | 361027    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 42.1      |
|    n_updates        | 80256     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8692      |
|    fps              | 294       |
|    time_elapsed     | 1226      |
|    total_timesteps  | 361171    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.8      |
|    n_updates        | 80292     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.9     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8696     |
|    fps              | 294      |
|    time_elapsed     | 1226     |
|    total_timesteps  | 361318   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 135      |
|    n_updates        | 80329    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8700      |
|    fps              | 294       |
|    time_elapsed     | 1227      |
|    total_timesteps  | 361464    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 80365     |
-----------------------------------
Eval num_timesteps=361500, episode_reward=-2988.87 +/- 785.57
Episode length: 36.24 +/- 5.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 361500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 80374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8704      |
|    fps              | 294       |
|    time_elapsed     | 1228      |
|    total_timesteps  | 361622    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 80405     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.58e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8708      |
|    fps              | 294       |
|    time_elapsed     | 1228      |
|    total_timesteps  | 361764    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 30.2      |
|    n_updates        | 80440     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.58e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8712      |
|    fps              | 294       |
|    time_elapsed     | 1228      |
|    total_timesteps  | 361887    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 80471     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8716      |
|    fps              | 294       |
|    time_elapsed     | 1228      |
|    total_timesteps  | 361986    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 80496     |
-----------------------------------
Eval num_timesteps=362000, episode_reward=-2955.38 +/- 1082.54
Episode length: 35.12 +/- 6.40
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 362000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.5      |
|    n_updates        | 80499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.58e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8720      |
|    fps              | 294       |
|    time_elapsed     | 1230      |
|    total_timesteps  | 362127    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 80531     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.56e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8724      |
|    fps              | 294       |
|    time_elapsed     | 1230      |
|    total_timesteps  | 362280    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53.8      |
|    n_updates        | 80569     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8728      |
|    fps              | 294       |
|    time_elapsed     | 1230      |
|    total_timesteps  | 362434    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 80608     |
-----------------------------------
Eval num_timesteps=362500, episode_reward=-3120.21 +/- 971.79
Episode length: 35.44 +/- 6.61
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -3.12e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 362500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 80624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.53e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8732      |
|    fps              | 294       |
|    time_elapsed     | 1232      |
|    total_timesteps  | 362597    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 80649     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.52e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8736      |
|    fps              | 294       |
|    time_elapsed     | 1232      |
|    total_timesteps  | 362727    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 80681     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8740      |
|    fps              | 294       |
|    time_elapsed     | 1232      |
|    total_timesteps  | 362860    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74        |
|    n_updates        | 80714     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.56e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8744      |
|    fps              | 294       |
|    time_elapsed     | 1232      |
|    total_timesteps  | 362998    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 80749     |
-----------------------------------
Eval num_timesteps=363000, episode_reward=-2820.57 +/- 1161.10
Episode length: 34.56 +/- 7.06
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 363000    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.62e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8748      |
|    fps              | 294       |
|    time_elapsed     | 1233      |
|    total_timesteps  | 363140    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.9      |
|    n_updates        | 80784     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8752      |
|    fps              | 294       |
|    time_elapsed     | 1234      |
|    total_timesteps  | 363278    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.1      |
|    n_updates        | 80819     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8756      |
|    fps              | 294       |
|    time_elapsed     | 1234      |
|    total_timesteps  | 363416    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 80853     |
-----------------------------------
Eval num_timesteps=363500, episode_reward=-3084.78 +/- 1115.77
Episode length: 33.66 +/- 7.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.7      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 363500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.3      |
|    n_updates        | 80874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8760      |
|    fps              | 294       |
|    time_elapsed     | 1235      |
|    total_timesteps  | 363570    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 80892     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8764      |
|    fps              | 294       |
|    time_elapsed     | 1235      |
|    total_timesteps  | 363708    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.9      |
|    n_updates        | 80926     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8768      |
|    fps              | 294       |
|    time_elapsed     | 1235      |
|    total_timesteps  | 363850    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 80962     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8772      |
|    fps              | 294       |
|    time_elapsed     | 1236      |
|    total_timesteps  | 363997    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 80999     |
-----------------------------------
Eval num_timesteps=364000, episode_reward=-2931.66 +/- 1001.98
Episode length: 34.36 +/- 6.18
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 364000    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8776      |
|    fps              | 294       |
|    time_elapsed     | 1237      |
|    total_timesteps  | 364160    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 81039     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8780      |
|    fps              | 294       |
|    time_elapsed     | 1237      |
|    total_timesteps  | 364294    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 81073     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8784      |
|    fps              | 294       |
|    time_elapsed     | 1237      |
|    total_timesteps  | 364418    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 81104     |
-----------------------------------
Eval num_timesteps=364500, episode_reward=-3145.24 +/- 872.84
Episode length: 33.82 +/- 7.28
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -3.15e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 364500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 81124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8788      |
|    fps              | 294       |
|    time_elapsed     | 1239      |
|    total_timesteps  | 364569    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 290       |
|    n_updates        | 81142     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8792      |
|    fps              | 294       |
|    time_elapsed     | 1239      |
|    total_timesteps  | 364679    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.2      |
|    n_updates        | 81169     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8796      |
|    fps              | 294       |
|    time_elapsed     | 1239      |
|    total_timesteps  | 364810    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 81202     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8800      |
|    fps              | 294       |
|    time_elapsed     | 1239      |
|    total_timesteps  | 364940    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 81234     |
-----------------------------------
Eval num_timesteps=365000, episode_reward=-2823.15 +/- 1240.99
Episode length: 34.82 +/- 6.52
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 365000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.2      |
|    n_updates        | 81249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8804      |
|    fps              | 294       |
|    time_elapsed     | 1240      |
|    total_timesteps  | 365081    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 81270     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8808      |
|    fps              | 294       |
|    time_elapsed     | 1240      |
|    total_timesteps  | 365227    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.7      |
|    n_updates        | 81306     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8812      |
|    fps              | 294       |
|    time_elapsed     | 1241      |
|    total_timesteps  | 365374    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 81343     |
-----------------------------------
Eval num_timesteps=365500, episode_reward=-2571.26 +/- 1444.32
Episode length: 35.48 +/- 6.79
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.57e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 365500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 71.7      |
|    n_updates        | 81374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8816      |
|    fps              | 294       |
|    time_elapsed     | 1242      |
|    total_timesteps  | 365528    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.2      |
|    n_updates        | 81381     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8820      |
|    fps              | 294       |
|    time_elapsed     | 1242      |
|    total_timesteps  | 365681    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 81420     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8824      |
|    fps              | 294       |
|    time_elapsed     | 1242      |
|    total_timesteps  | 365835    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.3      |
|    n_updates        | 81458     |
-----------------------------------
Eval num_timesteps=366000, episode_reward=-2879.37 +/- 1007.52
Episode length: 34.92 +/- 7.40
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 366000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 81499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8828      |
|    fps              | 294       |
|    time_elapsed     | 1244      |
|    total_timesteps  | 366004    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 81500     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8832      |
|    fps              | 294       |
|    time_elapsed     | 1244      |
|    total_timesteps  | 366168    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 294       |
|    n_updates        | 81541     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8836      |
|    fps              | 294       |
|    time_elapsed     | 1244      |
|    total_timesteps  | 366320    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 326       |
|    n_updates        | 81579     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8840      |
|    fps              | 294       |
|    time_elapsed     | 1244      |
|    total_timesteps  | 366485    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 81621     |
-----------------------------------
Eval num_timesteps=366500, episode_reward=-2782.59 +/- 1248.50
Episode length: 34.82 +/- 7.22
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 366500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.9      |
|    n_updates        | 81624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8844      |
|    fps              | 294       |
|    time_elapsed     | 1246      |
|    total_timesteps  | 366605    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 81651     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8848      |
|    fps              | 294       |
|    time_elapsed     | 1246      |
|    total_timesteps  | 366777    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 81694     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8852      |
|    fps              | 294       |
|    time_elapsed     | 1246      |
|    total_timesteps  | 366938    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 81734     |
-----------------------------------
Eval num_timesteps=367000, episode_reward=-2984.80 +/- 961.59
Episode length: 34.20 +/- 6.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 367000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.4      |
|    n_updates        | 81749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.62e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8856      |
|    fps              | 294       |
|    time_elapsed     | 1247      |
|    total_timesteps  | 367072    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.1      |
|    n_updates        | 81767     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8860      |
|    fps              | 294       |
|    time_elapsed     | 1248      |
|    total_timesteps  | 367226    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 81806     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8864      |
|    fps              | 294       |
|    time_elapsed     | 1248      |
|    total_timesteps  | 367372    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 81842     |
-----------------------------------
Eval num_timesteps=367500, episode_reward=-2930.91 +/- 1058.54
Episode length: 35.04 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 367500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 11.7      |
|    n_updates        | 81874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8868      |
|    fps              | 294       |
|    time_elapsed     | 1249      |
|    total_timesteps  | 367510    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.3      |
|    n_updates        | 81877     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.57e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8872      |
|    fps              | 294       |
|    time_elapsed     | 1249      |
|    total_timesteps  | 367661    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 81915     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.55e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8876      |
|    fps              | 294       |
|    time_elapsed     | 1249      |
|    total_timesteps  | 367805    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 81951     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.55e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8880      |
|    fps              | 294       |
|    time_elapsed     | 1250      |
|    total_timesteps  | 367911    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 195       |
|    n_updates        | 81977     |
-----------------------------------
Eval num_timesteps=368000, episode_reward=-2852.20 +/- 1046.58
Episode length: 35.00 +/- 6.68
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 368000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 81999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.51e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8884      |
|    fps              | 294       |
|    time_elapsed     | 1251      |
|    total_timesteps  | 368037    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84        |
|    n_updates        | 82009     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.49e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8888      |
|    fps              | 294       |
|    time_elapsed     | 1251      |
|    total_timesteps  | 368185    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 82046     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.48e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8892      |
|    fps              | 294       |
|    time_elapsed     | 1251      |
|    total_timesteps  | 368303    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 82075     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.47e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8896      |
|    fps              | 294       |
|    time_elapsed     | 1251      |
|    total_timesteps  | 368470    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.8      |
|    n_updates        | 82117     |
-----------------------------------
Eval num_timesteps=368500, episode_reward=-3276.29 +/- 789.68
Episode length: 32.24 +/- 7.44
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.2      |
|    mean_reward      | -3.28e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 368500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 82124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.45e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8900      |
|    fps              | 294       |
|    time_elapsed     | 1253      |
|    total_timesteps  | 368596    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64.2      |
|    n_updates        | 82148     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -2.45e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8904      |
|    fps              | 294       |
|    time_elapsed     | 1253      |
|    total_timesteps  | 368755    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 82188     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -2.45e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8908      |
|    fps              | 294       |
|    time_elapsed     | 1253      |
|    total_timesteps  | 368920    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 211       |
|    n_updates        | 82229     |
-----------------------------------
Eval num_timesteps=369000, episode_reward=-2689.08 +/- 1208.16
Episode length: 36.08 +/- 7.78
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 369000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 82249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -2.48e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8912      |
|    fps              | 294       |
|    time_elapsed     | 1254      |
|    total_timesteps  | 369067    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.9      |
|    n_updates        | 82266     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.8     |
|    ep_rew_mean      | -2.5e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8916     |
|    fps              | 294      |
|    time_elapsed     | 1255     |
|    total_timesteps  | 369205   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 81.9     |
|    n_updates        | 82301    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -2.47e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8920      |
|    fps              | 294       |
|    time_elapsed     | 1255      |
|    total_timesteps  | 369369    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 82342     |
-----------------------------------
Eval num_timesteps=369500, episode_reward=-2787.51 +/- 1257.25
Episode length: 34.76 +/- 7.11
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 369500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 82374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -2.49e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8924      |
|    fps              | 294       |
|    time_elapsed     | 1256      |
|    total_timesteps  | 369504    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 82375     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.56e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8928      |
|    fps              | 294       |
|    time_elapsed     | 1256      |
|    total_timesteps  | 369653    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 232       |
|    n_updates        | 82413     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.55e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8932      |
|    fps              | 294       |
|    time_elapsed     | 1257      |
|    total_timesteps  | 369820    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 249       |
|    n_updates        | 82454     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8936      |
|    fps              | 294       |
|    time_elapsed     | 1257      |
|    total_timesteps  | 369971    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 230       |
|    n_updates        | 82492     |
-----------------------------------
Eval num_timesteps=370000, episode_reward=-2987.82 +/- 885.82
Episode length: 36.54 +/- 5.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 370000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 82499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8940      |
|    fps              | 294       |
|    time_elapsed     | 1258      |
|    total_timesteps  | 370127    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 82531     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8944      |
|    fps              | 294       |
|    time_elapsed     | 1258      |
|    total_timesteps  | 370272    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 82567     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8948      |
|    fps              | 294       |
|    time_elapsed     | 1259      |
|    total_timesteps  | 370414    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.9      |
|    n_updates        | 82603     |
-----------------------------------
Eval num_timesteps=370500, episode_reward=-2845.22 +/- 1133.37
Episode length: 34.54 +/- 6.59
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 370500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 56.1      |
|    n_updates        | 82624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8952      |
|    fps              | 294       |
|    time_elapsed     | 1260      |
|    total_timesteps  | 370565    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 203       |
|    n_updates        | 82641     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8956      |
|    fps              | 294       |
|    time_elapsed     | 1260      |
|    total_timesteps  | 370676    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 82668     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8960      |
|    fps              | 294       |
|    time_elapsed     | 1260      |
|    total_timesteps  | 370815    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 50.3      |
|    n_updates        | 82703     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8964     |
|    fps              | 294      |
|    time_elapsed     | 1260     |
|    total_timesteps  | 370945   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 197      |
|    n_updates        | 82736    |
----------------------------------
Eval num_timesteps=371000, episode_reward=-2833.22 +/- 1167.54
Episode length: 34.58 +/- 6.33
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 371000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 82749     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8968     |
|    fps              | 294      |
|    time_elapsed     | 1262     |
|    total_timesteps  | 371110   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 124      |
|    n_updates        | 82777    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8972      |
|    fps              | 294       |
|    time_elapsed     | 1262      |
|    total_timesteps  | 371254    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 82813     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8976      |
|    fps              | 294       |
|    time_elapsed     | 1262      |
|    total_timesteps  | 371395    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 181       |
|    n_updates        | 82848     |
-----------------------------------
Eval num_timesteps=371500, episode_reward=-3122.72 +/- 913.97
Episode length: 36.44 +/- 7.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -3.12e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 371500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 82874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8980      |
|    fps              | 293       |
|    time_elapsed     | 1264      |
|    total_timesteps  | 371525    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 82881     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8984      |
|    fps              | 293       |
|    time_elapsed     | 1264      |
|    total_timesteps  | 371661    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 82915     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8988      |
|    fps              | 294       |
|    time_elapsed     | 1264      |
|    total_timesteps  | 371821    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 82955     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8992      |
|    fps              | 294       |
|    time_elapsed     | 1264      |
|    total_timesteps  | 371945    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.6      |
|    n_updates        | 82986     |
-----------------------------------
Eval num_timesteps=372000, episode_reward=-3178.22 +/- 1048.42
Episode length: 34.16 +/- 6.64
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -3.18e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 372000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.9      |
|    n_updates        | 82999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 8996      |
|    fps              | 293       |
|    time_elapsed     | 1265      |
|    total_timesteps  | 372074    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 83018     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9000      |
|    fps              | 293       |
|    time_elapsed     | 1266      |
|    total_timesteps  | 372203    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 83050     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9004      |
|    fps              | 294       |
|    time_elapsed     | 1266      |
|    total_timesteps  | 372370    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 83092     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9008      |
|    fps              | 294       |
|    time_elapsed     | 1266      |
|    total_timesteps  | 372497    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.2      |
|    n_updates        | 83124     |
-----------------------------------
Eval num_timesteps=372500, episode_reward=-2982.84 +/- 1054.45
Episode length: 33.78 +/- 7.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 372500    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9012      |
|    fps              | 293       |
|    time_elapsed     | 1267      |
|    total_timesteps  | 372643    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.1      |
|    n_updates        | 83160     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9016      |
|    fps              | 293       |
|    time_elapsed     | 1267      |
|    total_timesteps  | 372757    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.4      |
|    n_updates        | 83189     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9020      |
|    fps              | 294       |
|    time_elapsed     | 1268      |
|    total_timesteps  | 372916    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 83228     |
-----------------------------------
Eval num_timesteps=373000, episode_reward=-2932.79 +/- 1124.57
Episode length: 35.10 +/- 5.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 373000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 83249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9024      |
|    fps              | 293       |
|    time_elapsed     | 1269      |
|    total_timesteps  | 373059    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 83264     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9028      |
|    fps              | 293       |
|    time_elapsed     | 1269      |
|    total_timesteps  | 373203    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 221       |
|    n_updates        | 83300     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9032      |
|    fps              | 293       |
|    time_elapsed     | 1269      |
|    total_timesteps  | 373340    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 83334     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9036      |
|    fps              | 294       |
|    time_elapsed     | 1270      |
|    total_timesteps  | 373465    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 83366     |
-----------------------------------
Eval num_timesteps=373500, episode_reward=-2992.25 +/- 1097.90
Episode length: 34.54 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 373500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 220       |
|    n_updates        | 83374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9040      |
|    fps              | 293       |
|    time_elapsed     | 1271      |
|    total_timesteps  | 373565    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 83391     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9044      |
|    fps              | 293       |
|    time_elapsed     | 1271      |
|    total_timesteps  | 373696    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 205       |
|    n_updates        | 83423     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9048      |
|    fps              | 293       |
|    time_elapsed     | 1271      |
|    total_timesteps  | 373827    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 83456     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.7      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9052      |
|    fps              | 294       |
|    time_elapsed     | 1271      |
|    total_timesteps  | 373934    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 83483     |
-----------------------------------
Eval num_timesteps=374000, episode_reward=-2954.58 +/- 1064.11
Episode length: 33.82 +/- 6.08
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 374000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 83499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9056      |
|    fps              | 293       |
|    time_elapsed     | 1273      |
|    total_timesteps  | 374076    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 42.5      |
|    n_updates        | 83518     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9060      |
|    fps              | 293       |
|    time_elapsed     | 1273      |
|    total_timesteps  | 374221    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 83555     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9064      |
|    fps              | 293       |
|    time_elapsed     | 1273      |
|    total_timesteps  | 374382    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 83595     |
-----------------------------------
Eval num_timesteps=374500, episode_reward=-2893.93 +/- 1232.55
Episode length: 34.76 +/- 6.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 374500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 83624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9068      |
|    fps              | 293       |
|    time_elapsed     | 1274      |
|    total_timesteps  | 374524    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.6      |
|    n_updates        | 83630     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.2     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9072     |
|    fps              | 293      |
|    time_elapsed     | 1275     |
|    total_timesteps  | 374671   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 97.5     |
|    n_updates        | 83667    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9076      |
|    fps              | 293       |
|    time_elapsed     | 1275      |
|    total_timesteps  | 374828    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 83706     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9080      |
|    fps              | 294       |
|    time_elapsed     | 1275      |
|    total_timesteps  | 374956    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 83738     |
-----------------------------------
Eval num_timesteps=375000, episode_reward=-2877.93 +/- 1303.55
Episode length: 34.80 +/- 6.77
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 375000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.2      |
|    n_updates        | 83749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9084      |
|    fps              | 293       |
|    time_elapsed     | 1276      |
|    total_timesteps  | 375093    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 83773     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.05e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9088      |
|    fps              | 293       |
|    time_elapsed     | 1276      |
|    total_timesteps  | 375229    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 83807     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9092      |
|    fps              | 293       |
|    time_elapsed     | 1277      |
|    total_timesteps  | 375378    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 83844     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.1     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9096     |
|    fps              | 294      |
|    time_elapsed     | 1277     |
|    total_timesteps  | 375488   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 56.6     |
|    n_updates        | 83871    |
----------------------------------
Eval num_timesteps=375500, episode_reward=-2832.37 +/- 1056.25
Episode length: 35.50 +/- 7.03
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 375500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 83874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9100      |
|    fps              | 293       |
|    time_elapsed     | 1278      |
|    total_timesteps  | 375650    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 83912     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9104      |
|    fps              | 293       |
|    time_elapsed     | 1278      |
|    total_timesteps  | 375799    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 83949     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9108      |
|    fps              | 293       |
|    time_elapsed     | 1278      |
|    total_timesteps  | 375952    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.8      |
|    n_updates        | 83987     |
-----------------------------------
Eval num_timesteps=376000, episode_reward=-2869.88 +/- 1283.20
Episode length: 33.60 +/- 7.57
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.6      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 376000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81        |
|    n_updates        | 83999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9112      |
|    fps              | 293       |
|    time_elapsed     | 1280      |
|    total_timesteps  | 376088    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 84021     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9116      |
|    fps              | 293       |
|    time_elapsed     | 1280      |
|    total_timesteps  | 376227    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.5      |
|    n_updates        | 84056     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9120      |
|    fps              | 293       |
|    time_elapsed     | 1280      |
|    total_timesteps  | 376384    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 273       |
|    n_updates        | 84095     |
-----------------------------------
Eval num_timesteps=376500, episode_reward=-2924.85 +/- 975.59
Episode length: 36.00 +/- 6.02
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 376500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.4      |
|    n_updates        | 84124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9124      |
|    fps              | 293       |
|    time_elapsed     | 1281      |
|    total_timesteps  | 376533    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67.1      |
|    n_updates        | 84133     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9128      |
|    fps              | 293       |
|    time_elapsed     | 1282      |
|    total_timesteps  | 376658    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 84164     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9132      |
|    fps              | 293       |
|    time_elapsed     | 1282      |
|    total_timesteps  | 376817    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 84204     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.8     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9136     |
|    fps              | 293      |
|    time_elapsed     | 1282     |
|    total_timesteps  | 376945   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 85.5     |
|    n_updates        | 84236    |
----------------------------------
Eval num_timesteps=377000, episode_reward=-2404.03 +/- 1302.75
Episode length: 37.52 +/- 6.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.5     |
|    mean_reward      | -2.4e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 377000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 114      |
|    n_updates        | 84249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9140      |
|    fps              | 293       |
|    time_elapsed     | 1283      |
|    total_timesteps  | 377101    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 84275     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9144      |
|    fps              | 293       |
|    time_elapsed     | 1284      |
|    total_timesteps  | 377230    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.3      |
|    n_updates        | 84307     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9148      |
|    fps              | 293       |
|    time_elapsed     | 1284      |
|    total_timesteps  | 377362    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 84340     |
-----------------------------------
Eval num_timesteps=377500, episode_reward=-2350.35 +/- 1234.05
Episode length: 37.54 +/- 5.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.5      |
|    mean_reward      | -2.35e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 377500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.5      |
|    n_updates        | 84374     |
-----------------------------------
New best mean reward!
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9152      |
|    fps              | 293       |
|    time_elapsed     | 1285      |
|    total_timesteps  | 377527    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 212       |
|    n_updates        | 84381     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.9     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9156     |
|    fps              | 293      |
|    time_elapsed     | 1285     |
|    total_timesteps  | 377666   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 95.6     |
|    n_updates        | 84416    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9160      |
|    fps              | 293       |
|    time_elapsed     | 1286      |
|    total_timesteps  | 377822    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 84455     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9164      |
|    fps              | 293       |
|    time_elapsed     | 1286      |
|    total_timesteps  | 377953    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 84488     |
-----------------------------------
Eval num_timesteps=378000, episode_reward=-2810.02 +/- 1096.26
Episode length: 35.40 +/- 6.84
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 378000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.1      |
|    n_updates        | 84499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9168      |
|    fps              | 293       |
|    time_elapsed     | 1287      |
|    total_timesteps  | 378082    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 84520     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9172      |
|    fps              | 293       |
|    time_elapsed     | 1287      |
|    total_timesteps  | 378212    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 84552     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9176      |
|    fps              | 293       |
|    time_elapsed     | 1287      |
|    total_timesteps  | 378354    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 194       |
|    n_updates        | 84588     |
-----------------------------------
Eval num_timesteps=378500, episode_reward=-2806.19 +/- 963.78
Episode length: 35.64 +/- 6.42
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 378500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 84624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9180      |
|    fps              | 293       |
|    time_elapsed     | 1289      |
|    total_timesteps  | 378509    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.8      |
|    n_updates        | 84627     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9184      |
|    fps              | 293       |
|    time_elapsed     | 1289      |
|    total_timesteps  | 378662    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 287       |
|    n_updates        | 84665     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9188      |
|    fps              | 293       |
|    time_elapsed     | 1289      |
|    total_timesteps  | 378804    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 84700     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9192      |
|    fps              | 293       |
|    time_elapsed     | 1289      |
|    total_timesteps  | 378962    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 84740     |
-----------------------------------
Eval num_timesteps=379000, episode_reward=-2831.93 +/- 1207.89
Episode length: 36.84 +/- 6.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 379000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 84749     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9196     |
|    fps              | 293      |
|    time_elapsed     | 1291     |
|    total_timesteps  | 379087   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 83.8     |
|    n_updates        | 84771    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9200      |
|    fps              | 293       |
|    time_elapsed     | 1291      |
|    total_timesteps  | 379226    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.8      |
|    n_updates        | 84806     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9204      |
|    fps              | 293       |
|    time_elapsed     | 1291      |
|    total_timesteps  | 379356    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 84838     |
-----------------------------------
Eval num_timesteps=379500, episode_reward=-3108.36 +/- 878.80
Episode length: 33.92 +/- 6.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 379500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 203       |
|    n_updates        | 84874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9208      |
|    fps              | 293       |
|    time_elapsed     | 1292      |
|    total_timesteps  | 379524    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 84880     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9212      |
|    fps              | 293       |
|    time_elapsed     | 1292      |
|    total_timesteps  | 379681    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 84920     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9216      |
|    fps              | 293       |
|    time_elapsed     | 1293      |
|    total_timesteps  | 379819    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 84954     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9220      |
|    fps              | 293       |
|    time_elapsed     | 1293      |
|    total_timesteps  | 379972    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 84992     |
-----------------------------------
Eval num_timesteps=380000, episode_reward=-3045.31 +/- 1063.86
Episode length: 34.54 +/- 6.90
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -3.05e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 380000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 84999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9224      |
|    fps              | 293       |
|    time_elapsed     | 1294      |
|    total_timesteps  | 380096    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 85023     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9228      |
|    fps              | 293       |
|    time_elapsed     | 1294      |
|    total_timesteps  | 380231    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 85057     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.4     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9232     |
|    fps              | 293      |
|    time_elapsed     | 1294     |
|    total_timesteps  | 380356   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 163      |
|    n_updates        | 85088    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.5     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9236     |
|    fps              | 293      |
|    time_elapsed     | 1295     |
|    total_timesteps  | 380496   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 205      |
|    n_updates        | 85123    |
----------------------------------
Eval num_timesteps=380500, episode_reward=-2324.54 +/- 1350.35
Episode length: 37.46 +/- 5.79
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.5      |
|    mean_reward      | -2.32e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 380500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.8      |
|    n_updates        | 85124     |
-----------------------------------
New best mean reward!
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9240      |
|    fps              | 293       |
|    time_elapsed     | 1296      |
|    total_timesteps  | 380630    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.6      |
|    n_updates        | 85157     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9244      |
|    fps              | 293       |
|    time_elapsed     | 1296      |
|    total_timesteps  | 380780    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 85194     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9248      |
|    fps              | 293       |
|    time_elapsed     | 1296      |
|    total_timesteps  | 380925    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 85231     |
-----------------------------------
Eval num_timesteps=381000, episode_reward=-2585.60 +/- 1161.98
Episode length: 37.56 +/- 4.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.6      |
|    mean_reward      | -2.59e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 381000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.8      |
|    n_updates        | 85249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9252      |
|    fps              | 293       |
|    time_elapsed     | 1298      |
|    total_timesteps  | 381068    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65.7      |
|    n_updates        | 85266     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9256      |
|    fps              | 293       |
|    time_elapsed     | 1298      |
|    total_timesteps  | 381208    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 85301     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9260      |
|    fps              | 293       |
|    time_elapsed     | 1298      |
|    total_timesteps  | 381349    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 85337     |
-----------------------------------
Eval num_timesteps=381500, episode_reward=-2538.09 +/- 1369.64
Episode length: 37.14 +/- 5.20
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.1      |
|    mean_reward      | -2.54e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 381500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.4      |
|    n_updates        | 85374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9264      |
|    fps              | 293       |
|    time_elapsed     | 1300      |
|    total_timesteps  | 381505    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.1      |
|    n_updates        | 85376     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9268      |
|    fps              | 293       |
|    time_elapsed     | 1300      |
|    total_timesteps  | 381668    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 23.1      |
|    n_updates        | 85416     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9272      |
|    fps              | 293       |
|    time_elapsed     | 1300      |
|    total_timesteps  | 381822    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.2      |
|    n_updates        | 85455     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9276      |
|    fps              | 293       |
|    time_elapsed     | 1300      |
|    total_timesteps  | 381937    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.1      |
|    n_updates        | 85484     |
-----------------------------------
Eval num_timesteps=382000, episode_reward=-2755.34 +/- 1180.35
Episode length: 35.48 +/- 6.00
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 382000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 85499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9280      |
|    fps              | 293       |
|    time_elapsed     | 1301      |
|    total_timesteps  | 382094    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 85523     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9284      |
|    fps              | 293       |
|    time_elapsed     | 1302      |
|    total_timesteps  | 382233    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 85558     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9288     |
|    fps              | 293      |
|    time_elapsed     | 1302     |
|    total_timesteps  | 382375   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 114      |
|    n_updates        | 85593    |
----------------------------------
Eval num_timesteps=382500, episode_reward=-2905.47 +/- 1120.35
Episode length: 35.92 +/- 5.89
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 382500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 85624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9292      |
|    fps              | 293       |
|    time_elapsed     | 1303      |
|    total_timesteps  | 382520    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 85629     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9296      |
|    fps              | 293       |
|    time_elapsed     | 1303      |
|    total_timesteps  | 382664    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.2      |
|    n_updates        | 85665     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9300      |
|    fps              | 293       |
|    time_elapsed     | 1304      |
|    total_timesteps  | 382810    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 85702     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9304      |
|    fps              | 293       |
|    time_elapsed     | 1304      |
|    total_timesteps  | 382957    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 180       |
|    n_updates        | 85739     |
-----------------------------------
Eval num_timesteps=383000, episode_reward=-2886.85 +/- 1084.87
Episode length: 36.08 +/- 6.75
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 383000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 85749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9308      |
|    fps              | 293       |
|    time_elapsed     | 1305      |
|    total_timesteps  | 383087    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 85771     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9312      |
|    fps              | 293       |
|    time_elapsed     | 1305      |
|    total_timesteps  | 383224    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 85805     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9316      |
|    fps              | 293       |
|    time_elapsed     | 1305      |
|    total_timesteps  | 383349    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 85837     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9320     |
|    fps              | 293      |
|    time_elapsed     | 1306     |
|    total_timesteps  | 383476   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 27.2     |
|    n_updates        | 85868    |
----------------------------------
Eval num_timesteps=383500, episode_reward=-2540.88 +/- 1472.31
Episode length: 35.60 +/- 5.90
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.54e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 383500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 85874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9324      |
|    fps              | 293       |
|    time_elapsed     | 1307      |
|    total_timesteps  | 383612    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 85902     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9328      |
|    fps              | 293       |
|    time_elapsed     | 1307      |
|    total_timesteps  | 383758    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 85939     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9332      |
|    fps              | 293       |
|    time_elapsed     | 1307      |
|    total_timesteps  | 383903    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.3      |
|    n_updates        | 85975     |
-----------------------------------
Eval num_timesteps=384000, episode_reward=-2914.18 +/- 967.09
Episode length: 36.00 +/- 6.34
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 384000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 255       |
|    n_updates        | 85999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9336      |
|    fps              | 293       |
|    time_elapsed     | 1309      |
|    total_timesteps  | 384027    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 86006     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9340      |
|    fps              | 293       |
|    time_elapsed     | 1309      |
|    total_timesteps  | 384149    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 86037     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9344      |
|    fps              | 293       |
|    time_elapsed     | 1309      |
|    total_timesteps  | 384294    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98        |
|    n_updates        | 86073     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9348      |
|    fps              | 293       |
|    time_elapsed     | 1309      |
|    total_timesteps  | 384452    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 86112     |
-----------------------------------
Eval num_timesteps=384500, episode_reward=-2831.16 +/- 1064.33
Episode length: 35.50 +/- 5.88
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 384500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.6      |
|    n_updates        | 86124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9352      |
|    fps              | 293       |
|    time_elapsed     | 1310      |
|    total_timesteps  | 384591    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 180       |
|    n_updates        | 86147     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9356      |
|    fps              | 293       |
|    time_elapsed     | 1311      |
|    total_timesteps  | 384738    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 86184     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9360     |
|    fps              | 293      |
|    time_elapsed     | 1311     |
|    total_timesteps  | 384876   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 135      |
|    n_updates        | 86218    |
----------------------------------
Eval num_timesteps=385000, episode_reward=-2609.45 +/- 1138.16
Episode length: 35.76 +/- 6.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.61e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 385000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.5      |
|    n_updates        | 86249     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9364     |
|    fps              | 293      |
|    time_elapsed     | 1312     |
|    total_timesteps  | 385018   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 62.6     |
|    n_updates        | 86254    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9368      |
|    fps              | 293       |
|    time_elapsed     | 1312      |
|    total_timesteps  | 385162    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 86290     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9372      |
|    fps              | 293       |
|    time_elapsed     | 1313      |
|    total_timesteps  | 385313    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98        |
|    n_updates        | 86328     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9376      |
|    fps              | 293       |
|    time_elapsed     | 1313      |
|    total_timesteps  | 385424    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 86355     |
-----------------------------------
Eval num_timesteps=385500, episode_reward=-2917.23 +/- 1055.50
Episode length: 35.38 +/- 6.40
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 385500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.3      |
|    n_updates        | 86374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9380      |
|    fps              | 293       |
|    time_elapsed     | 1314      |
|    total_timesteps  | 385565    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 86391     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9384      |
|    fps              | 293       |
|    time_elapsed     | 1314      |
|    total_timesteps  | 385709    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 86427     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9388      |
|    fps              | 293       |
|    time_elapsed     | 1314      |
|    total_timesteps  | 385839    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.9      |
|    n_updates        | 86459     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9392      |
|    fps              | 293       |
|    time_elapsed     | 1315      |
|    total_timesteps  | 385977    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 86494     |
-----------------------------------
Eval num_timesteps=386000, episode_reward=-2997.25 +/- 928.46
Episode length: 34.14 +/- 7.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.1     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 386000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 137      |
|    n_updates        | 86499    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9396      |
|    fps              | 293       |
|    time_elapsed     | 1316      |
|    total_timesteps  | 386123    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 86530     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9400      |
|    fps              | 293       |
|    time_elapsed     | 1316      |
|    total_timesteps  | 386279    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 86569     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9404      |
|    fps              | 293       |
|    time_elapsed     | 1316      |
|    total_timesteps  | 386411    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53.8      |
|    n_updates        | 86602     |
-----------------------------------
Eval num_timesteps=386500, episode_reward=-2769.90 +/- 1234.54
Episode length: 35.16 +/- 7.74
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 386500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 86624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9408      |
|    fps              | 293       |
|    time_elapsed     | 1318      |
|    total_timesteps  | 386571    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 56.7      |
|    n_updates        | 86642     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9412      |
|    fps              | 293       |
|    time_elapsed     | 1318      |
|    total_timesteps  | 386696    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.5      |
|    n_updates        | 86673     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9416      |
|    fps              | 293       |
|    time_elapsed     | 1318      |
|    total_timesteps  | 386817    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 86704     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9420      |
|    fps              | 293       |
|    time_elapsed     | 1318      |
|    total_timesteps  | 386959    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.8      |
|    n_updates        | 86739     |
-----------------------------------
Eval num_timesteps=387000, episode_reward=-2568.29 +/- 1389.74
Episode length: 36.72 +/- 6.14
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.7      |
|    mean_reward      | -2.57e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 387000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.8      |
|    n_updates        | 86749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9424      |
|    fps              | 293       |
|    time_elapsed     | 1319      |
|    total_timesteps  | 387083    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 86770     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9428      |
|    fps              | 293       |
|    time_elapsed     | 1320      |
|    total_timesteps  | 387222    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 86805     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9432      |
|    fps              | 293       |
|    time_elapsed     | 1320      |
|    total_timesteps  | 387363    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 299       |
|    n_updates        | 86840     |
-----------------------------------
Eval num_timesteps=387500, episode_reward=-3039.44 +/- 943.01
Episode length: 34.34 +/- 7.01
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 387500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 86874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9436      |
|    fps              | 293       |
|    time_elapsed     | 1321      |
|    total_timesteps  | 387504    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 86875     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9440      |
|    fps              | 293       |
|    time_elapsed     | 1321      |
|    total_timesteps  | 387662    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.1      |
|    n_updates        | 86915     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35       |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9444     |
|    fps              | 293      |
|    time_elapsed     | 1321     |
|    total_timesteps  | 387796   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 211      |
|    n_updates        | 86948    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9448      |
|    fps              | 293       |
|    time_elapsed     | 1322      |
|    total_timesteps  | 387912    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70        |
|    n_updates        | 86977     |
-----------------------------------
Eval num_timesteps=388000, episode_reward=-2604.54 +/- 1145.52
Episode length: 36.18 +/- 6.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.2     |
|    mean_reward      | -2.6e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 388000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 145      |
|    n_updates        | 86999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9452      |
|    fps              | 293       |
|    time_elapsed     | 1323      |
|    total_timesteps  | 388056    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 87013     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9456      |
|    fps              | 293       |
|    time_elapsed     | 1323      |
|    total_timesteps  | 388182    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 87045     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.5     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9460     |
|    fps              | 293      |
|    time_elapsed     | 1323     |
|    total_timesteps  | 388322   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 189      |
|    n_updates        | 87080    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9464      |
|    fps              | 293       |
|    time_elapsed     | 1323      |
|    total_timesteps  | 388478    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 87119     |
-----------------------------------
Eval num_timesteps=388500, episode_reward=-3070.02 +/- 839.09
Episode length: 34.68 +/- 6.44
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -3.07e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 388500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 87124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9468      |
|    fps              | 293       |
|    time_elapsed     | 1325      |
|    total_timesteps  | 388623    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.7      |
|    n_updates        | 87155     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9472      |
|    fps              | 293       |
|    time_elapsed     | 1325      |
|    total_timesteps  | 388760    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 223       |
|    n_updates        | 87189     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9476      |
|    fps              | 293       |
|    time_elapsed     | 1325      |
|    total_timesteps  | 388909    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 87227     |
-----------------------------------
Eval num_timesteps=389000, episode_reward=-2343.52 +/- 1455.19
Episode length: 35.90 +/- 6.98
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.34e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 389000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 87249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9480      |
|    fps              | 293       |
|    time_elapsed     | 1326      |
|    total_timesteps  | 389037    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 228       |
|    n_updates        | 87259     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.6     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9484     |
|    fps              | 293      |
|    time_elapsed     | 1327     |
|    total_timesteps  | 389165   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 102      |
|    n_updates        | 87291    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.7     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9488     |
|    fps              | 293      |
|    time_elapsed     | 1327     |
|    total_timesteps  | 389310   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 75       |
|    n_updates        | 87327    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9492      |
|    fps              | 293       |
|    time_elapsed     | 1327      |
|    total_timesteps  | 389449    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 87362     |
-----------------------------------
Eval num_timesteps=389500, episode_reward=-2618.09 +/- 1123.09
Episode length: 36.34 +/- 5.74
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.62e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 389500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.3      |
|    n_updates        | 87374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9496      |
|    fps              | 293       |
|    time_elapsed     | 1328      |
|    total_timesteps  | 389584    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 87395     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9500      |
|    fps              | 293       |
|    time_elapsed     | 1329      |
|    total_timesteps  | 389726    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 87431     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9504      |
|    fps              | 293       |
|    time_elapsed     | 1329      |
|    total_timesteps  | 389863    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.6      |
|    n_updates        | 87465     |
-----------------------------------
Eval num_timesteps=390000, episode_reward=-2940.52 +/- 1193.71
Episode length: 36.32 +/- 5.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 390000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 87499     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.3     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9508     |
|    fps              | 293      |
|    time_elapsed     | 1330     |
|    total_timesteps  | 390002   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 75.1     |
|    n_updates        | 87500    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9512      |
|    fps              | 293       |
|    time_elapsed     | 1330      |
|    total_timesteps  | 390140    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.9      |
|    n_updates        | 87534     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9516      |
|    fps              | 293       |
|    time_elapsed     | 1330      |
|    total_timesteps  | 390302    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 87575     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9520      |
|    fps              | 293       |
|    time_elapsed     | 1331      |
|    total_timesteps  | 390446    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.9      |
|    n_updates        | 87611     |
-----------------------------------
Eval num_timesteps=390500, episode_reward=-2828.96 +/- 1096.43
Episode length: 36.42 +/- 6.14
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 390500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 292       |
|    n_updates        | 87624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9524      |
|    fps              | 293       |
|    time_elapsed     | 1332      |
|    total_timesteps  | 390589    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 87647     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9528     |
|    fps              | 293      |
|    time_elapsed     | 1332     |
|    total_timesteps  | 390729   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 146      |
|    n_updates        | 87682    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9532      |
|    fps              | 293       |
|    time_elapsed     | 1332      |
|    total_timesteps  | 390883    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 241       |
|    n_updates        | 87720     |
-----------------------------------
Eval num_timesteps=391000, episode_reward=-2999.47 +/- 1088.68
Episode length: 34.00 +/- 6.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34       |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 391000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 101      |
|    n_updates        | 87749    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9536      |
|    fps              | 293       |
|    time_elapsed     | 1334      |
|    total_timesteps  | 391026    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.3      |
|    n_updates        | 87756     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9540      |
|    fps              | 293       |
|    time_elapsed     | 1334      |
|    total_timesteps  | 391172    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47.2      |
|    n_updates        | 87792     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9544      |
|    fps              | 293       |
|    time_elapsed     | 1334      |
|    total_timesteps  | 391312    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.7      |
|    n_updates        | 87827     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9548      |
|    fps              | 293       |
|    time_elapsed     | 1334      |
|    total_timesteps  | 391472    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 87867     |
-----------------------------------
Eval num_timesteps=391500, episode_reward=-2462.73 +/- 1503.43
Episode length: 36.34 +/- 6.84
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.46e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 391500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 24.9      |
|    n_updates        | 87874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9552      |
|    fps              | 293       |
|    time_elapsed     | 1336      |
|    total_timesteps  | 391627    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 87906     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9556     |
|    fps              | 293      |
|    time_elapsed     | 1336     |
|    total_timesteps  | 391748   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 149      |
|    n_updates        | 87936    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.5     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9560     |
|    fps              | 293      |
|    time_elapsed     | 1336     |
|    total_timesteps  | 391877   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 82.7     |
|    n_updates        | 87969    |
----------------------------------
Eval num_timesteps=392000, episode_reward=-3118.37 +/- 826.46
Episode length: 35.42 +/- 6.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -3.12e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 392000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 249       |
|    n_updates        | 87999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9564      |
|    fps              | 293       |
|    time_elapsed     | 1337      |
|    total_timesteps  | 392030    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.8      |
|    n_updates        | 88007     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9568      |
|    fps              | 293       |
|    time_elapsed     | 1337      |
|    total_timesteps  | 392191    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 88047     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9572      |
|    fps              | 293       |
|    time_elapsed     | 1338      |
|    total_timesteps  | 392332    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 218       |
|    n_updates        | 88082     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9576      |
|    fps              | 293       |
|    time_elapsed     | 1338      |
|    total_timesteps  | 392499    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 88124     |
-----------------------------------
Eval num_timesteps=392500, episode_reward=-2902.32 +/- 800.40
Episode length: 34.44 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | -2.9e+03 |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9580      |
|    fps              | 293       |
|    time_elapsed     | 1339      |
|    total_timesteps  | 392625    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.6      |
|    n_updates        | 88156     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9584      |
|    fps              | 293       |
|    time_elapsed     | 1339      |
|    total_timesteps  | 392769    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 88192     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9588      |
|    fps              | 293       |
|    time_elapsed     | 1339      |
|    total_timesteps  | 392887    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 28.6      |
|    n_updates        | 88221     |
-----------------------------------
Eval num_timesteps=393000, episode_reward=-3074.55 +/- 899.49
Episode length: 35.34 +/- 6.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -3.07e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 393000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 88249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9592      |
|    fps              | 293       |
|    time_elapsed     | 1341      |
|    total_timesteps  | 393008    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 88251     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9596      |
|    fps              | 293       |
|    time_elapsed     | 1341      |
|    total_timesteps  | 393162    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 88290     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9600      |
|    fps              | 293       |
|    time_elapsed     | 1341      |
|    total_timesteps  | 393304    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 88325     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9604      |
|    fps              | 293       |
|    time_elapsed     | 1341      |
|    total_timesteps  | 393441    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51        |
|    n_updates        | 88360     |
-----------------------------------
Eval num_timesteps=393500, episode_reward=-2910.80 +/- 1060.97
Episode length: 34.64 +/- 6.77
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 393500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65.2      |
|    n_updates        | 88374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9608      |
|    fps              | 293       |
|    time_elapsed     | 1343      |
|    total_timesteps  | 393581    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 284       |
|    n_updates        | 88395     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9612      |
|    fps              | 293       |
|    time_elapsed     | 1343      |
|    total_timesteps  | 393720    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 88429     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9616     |
|    fps              | 293      |
|    time_elapsed     | 1343     |
|    total_timesteps  | 393870   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 189      |
|    n_updates        | 88467    |
----------------------------------
Eval num_timesteps=394000, episode_reward=-3068.29 +/- 972.72
Episode length: 35.32 +/- 6.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -3.07e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 394000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 88499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9620      |
|    fps              | 292       |
|    time_elapsed     | 1344      |
|    total_timesteps  | 394011    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.6      |
|    n_updates        | 88502     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9624      |
|    fps              | 293       |
|    time_elapsed     | 1344      |
|    total_timesteps  | 394162    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 181       |
|    n_updates        | 88540     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.8     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9628     |
|    fps              | 293      |
|    time_elapsed     | 1345     |
|    total_timesteps  | 394310   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 273      |
|    n_updates        | 88577    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9632      |
|    fps              | 293       |
|    time_elapsed     | 1345      |
|    total_timesteps  | 394437    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 205       |
|    n_updates        | 88609     |
-----------------------------------
Eval num_timesteps=394500, episode_reward=-3044.46 +/- 1032.51
Episode length: 34.34 +/- 6.34
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 394500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 88624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9636      |
|    fps              | 293       |
|    time_elapsed     | 1346      |
|    total_timesteps  | 394574    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.4      |
|    n_updates        | 88643     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9640      |
|    fps              | 293       |
|    time_elapsed     | 1346      |
|    total_timesteps  | 394722    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 88680     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9644      |
|    fps              | 293       |
|    time_elapsed     | 1346      |
|    total_timesteps  | 394861    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 88715     |
-----------------------------------
Eval num_timesteps=395000, episode_reward=-2466.02 +/- 1238.76
Episode length: 37.20 +/- 6.02
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.2      |
|    mean_reward      | -2.47e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 395000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 222       |
|    n_updates        | 88749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9648      |
|    fps              | 292       |
|    time_elapsed     | 1348      |
|    total_timesteps  | 395024    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.4      |
|    n_updates        | 88755     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9652      |
|    fps              | 293       |
|    time_elapsed     | 1348      |
|    total_timesteps  | 395169    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 88792     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9656      |
|    fps              | 293       |
|    time_elapsed     | 1348      |
|    total_timesteps  | 395306    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 88826     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9660      |
|    fps              | 293       |
|    time_elapsed     | 1348      |
|    total_timesteps  | 395467    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.5      |
|    n_updates        | 88866     |
-----------------------------------
Eval num_timesteps=395500, episode_reward=-2880.49 +/- 1106.71
Episode length: 35.06 +/- 6.25
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 395500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 88874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9664      |
|    fps              | 292       |
|    time_elapsed     | 1350      |
|    total_timesteps  | 395606    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.6      |
|    n_updates        | 88901     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9668      |
|    fps              | 293       |
|    time_elapsed     | 1350      |
|    total_timesteps  | 395751    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.8      |
|    n_updates        | 88937     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.62e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9672      |
|    fps              | 293       |
|    time_elapsed     | 1350      |
|    total_timesteps  | 395908    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 88976     |
-----------------------------------
Eval num_timesteps=396000, episode_reward=-3012.62 +/- 949.46
Episode length: 34.94 +/- 6.88
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 396000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 88999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9676      |
|    fps              | 292       |
|    time_elapsed     | 1351      |
|    total_timesteps  | 396065    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88.8      |
|    n_updates        | 89016     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9680      |
|    fps              | 293       |
|    time_elapsed     | 1352      |
|    total_timesteps  | 396204    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.1      |
|    n_updates        | 89050     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9684      |
|    fps              | 293       |
|    time_elapsed     | 1352      |
|    total_timesteps  | 396349    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 89087     |
-----------------------------------
Eval num_timesteps=396500, episode_reward=-2886.00 +/- 933.99
Episode length: 37.08 +/- 5.41
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.1      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 396500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.6      |
|    n_updates        | 89124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9688      |
|    fps              | 292       |
|    time_elapsed     | 1353      |
|    total_timesteps  | 396504    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 89125     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9692      |
|    fps              | 292       |
|    time_elapsed     | 1353      |
|    total_timesteps  | 396629    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.2      |
|    n_updates        | 89157     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9696      |
|    fps              | 293       |
|    time_elapsed     | 1354      |
|    total_timesteps  | 396761    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 89190     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9700      |
|    fps              | 293       |
|    time_elapsed     | 1354      |
|    total_timesteps  | 396911    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 53        |
|    n_updates        | 89227     |
-----------------------------------
Eval num_timesteps=397000, episode_reward=-2894.32 +/- 1355.21
Episode length: 32.92 +/- 8.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.9      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 397000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 167       |
|    n_updates        | 89249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9704      |
|    fps              | 292       |
|    time_elapsed     | 1355      |
|    total_timesteps  | 397046    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 89261     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9708      |
|    fps              | 292       |
|    time_elapsed     | 1355      |
|    total_timesteps  | 397214    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 354       |
|    n_updates        | 89303     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9712      |
|    fps              | 293       |
|    time_elapsed     | 1355      |
|    total_timesteps  | 397336    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 89333     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9716      |
|    fps              | 293       |
|    time_elapsed     | 1355      |
|    total_timesteps  | 397475    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 89368     |
-----------------------------------
Eval num_timesteps=397500, episode_reward=-2967.31 +/- 969.94
Episode length: 33.48 +/- 7.41
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.5      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 397500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 210       |
|    n_updates        | 89374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9720      |
|    fps              | 292       |
|    time_elapsed     | 1357      |
|    total_timesteps  | 397607    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 89401     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9724      |
|    fps              | 293       |
|    time_elapsed     | 1357      |
|    total_timesteps  | 397751    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 42.4      |
|    n_updates        | 89437     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9728      |
|    fps              | 293       |
|    time_elapsed     | 1357      |
|    total_timesteps  | 397908    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 89476     |
-----------------------------------
Eval num_timesteps=398000, episode_reward=-2761.54 +/- 1293.44
Episode length: 36.58 +/- 6.01
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.6      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 398000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 212       |
|    n_updates        | 89499     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9732     |
|    fps              | 292      |
|    time_elapsed     | 1359     |
|    total_timesteps  | 398037   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 70.7     |
|    n_updates        | 89509    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9736      |
|    fps              | 292       |
|    time_elapsed     | 1359      |
|    total_timesteps  | 398178    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 89544     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.3     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9740     |
|    fps              | 293      |
|    time_elapsed     | 1359     |
|    total_timesteps  | 398350   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 168      |
|    n_updates        | 89587    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.1     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9744     |
|    fps              | 293      |
|    time_elapsed     | 1359     |
|    total_timesteps  | 398471   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 60       |
|    n_updates        | 89617    |
----------------------------------
Eval num_timesteps=398500, episode_reward=-3143.42 +/- 884.84
Episode length: 34.68 +/- 6.13
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -3.14e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 398500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.8      |
|    n_updates        | 89624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9748      |
|    fps              | 292       |
|    time_elapsed     | 1360      |
|    total_timesteps  | 398617    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.7      |
|    n_updates        | 89654     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9752      |
|    fps              | 292       |
|    time_elapsed     | 1361      |
|    total_timesteps  | 398752    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 89687     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.8     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9756     |
|    fps              | 293      |
|    time_elapsed     | 1361     |
|    total_timesteps  | 398890   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 187      |
|    n_updates        | 89722    |
----------------------------------
Eval num_timesteps=399000, episode_reward=-2861.00 +/- 1146.35
Episode length: 36.02 +/- 6.18
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 399000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.5      |
|    n_updates        | 89749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9760      |
|    fps              | 292       |
|    time_elapsed     | 1362      |
|    total_timesteps  | 399037    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 89759     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9764      |
|    fps              | 292       |
|    time_elapsed     | 1362      |
|    total_timesteps  | 399174    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 89793     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9768      |
|    fps              | 292       |
|    time_elapsed     | 1362      |
|    total_timesteps  | 399310    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 89827     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9772      |
|    fps              | 293       |
|    time_elapsed     | 1363      |
|    total_timesteps  | 399471    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 89867     |
-----------------------------------
Eval num_timesteps=399500, episode_reward=-2929.83 +/- 1065.23
Episode length: 35.54 +/- 6.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 399500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 195       |
|    n_updates        | 89874     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.5     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9776     |
|    fps              | 292      |
|    time_elapsed     | 1364     |
|    total_timesteps  | 399619   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 74.5     |
|    n_updates        | 89904    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9780      |
|    fps              | 292       |
|    time_elapsed     | 1364      |
|    total_timesteps  | 399771    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 89942     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9784      |
|    fps              | 293       |
|    time_elapsed     | 1364      |
|    total_timesteps  | 399892    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 89972     |
-----------------------------------
Eval num_timesteps=400000, episode_reward=-2970.86 +/- 1045.11
Episode length: 35.20 +/- 5.76
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 400000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76        |
|    n_updates        | 89999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9788      |
|    fps              | 292       |
|    time_elapsed     | 1366      |
|    total_timesteps  | 400048    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62.4      |
|    n_updates        | 90011     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9792      |
|    fps              | 292       |
|    time_elapsed     | 1366      |
|    total_timesteps  | 400182    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 90045     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9796     |
|    fps              | 292      |
|    time_elapsed     | 1366     |
|    total_timesteps  | 400333   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 188      |
|    n_updates        | 90083    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9800      |
|    fps              | 293       |
|    time_elapsed     | 1366      |
|    total_timesteps  | 400470    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 196       |
|    n_updates        | 90117     |
-----------------------------------
Eval num_timesteps=400500, episode_reward=-2667.42 +/- 1068.92
Episode length: 35.64 +/- 6.90
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 400500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 181       |
|    n_updates        | 90124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9804      |
|    fps              | 292       |
|    time_elapsed     | 1368      |
|    total_timesteps  | 400618    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.8      |
|    n_updates        | 90154     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9808      |
|    fps              | 292       |
|    time_elapsed     | 1368      |
|    total_timesteps  | 400768    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 90191     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9812      |
|    fps              | 293       |
|    time_elapsed     | 1368      |
|    total_timesteps  | 400928    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 90231     |
-----------------------------------
Eval num_timesteps=401000, episode_reward=-2881.56 +/- 1032.84
Episode length: 34.68 +/- 6.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.88e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 401000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 247       |
|    n_updates        | 90249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9816      |
|    fps              | 292       |
|    time_elapsed     | 1369      |
|    total_timesteps  | 401074    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 90268     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9820      |
|    fps              | 292       |
|    time_elapsed     | 1369      |
|    total_timesteps  | 401219    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 90304     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9824      |
|    fps              | 292       |
|    time_elapsed     | 1370      |
|    total_timesteps  | 401352    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 195       |
|    n_updates        | 90337     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9828      |
|    fps              | 293       |
|    time_elapsed     | 1370      |
|    total_timesteps  | 401471    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 90367     |
-----------------------------------
Eval num_timesteps=401500, episode_reward=-2799.48 +/- 1119.87
Episode length: 36.84 +/- 6.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.8     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 401500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 180      |
|    n_updates        | 90374    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9832      |
|    fps              | 292       |
|    time_elapsed     | 1371      |
|    total_timesteps  | 401609    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 192       |
|    n_updates        | 90402     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9836      |
|    fps              | 292       |
|    time_elapsed     | 1371      |
|    total_timesteps  | 401751    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 174       |
|    n_updates        | 90437     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9840      |
|    fps              | 292       |
|    time_elapsed     | 1371      |
|    total_timesteps  | 401889    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 90472     |
-----------------------------------
Eval num_timesteps=402000, episode_reward=-3078.95 +/- 1097.52
Episode length: 34.86 +/- 7.31
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 402000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 267       |
|    n_updates        | 90499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9844      |
|    fps              | 292       |
|    time_elapsed     | 1373      |
|    total_timesteps  | 402016    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 207       |
|    n_updates        | 90503     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9848      |
|    fps              | 292       |
|    time_elapsed     | 1373      |
|    total_timesteps  | 402188    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.3      |
|    n_updates        | 90546     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9852      |
|    fps              | 292       |
|    time_elapsed     | 1373      |
|    total_timesteps  | 402327    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.3      |
|    n_updates        | 90581     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9856      |
|    fps              | 292       |
|    time_elapsed     | 1373      |
|    total_timesteps  | 402464    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 90615     |
-----------------------------------
Eval num_timesteps=402500, episode_reward=-2721.52 +/- 1214.20
Episode length: 36.14 +/- 6.84
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 402500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 90624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9860      |
|    fps              | 292       |
|    time_elapsed     | 1375      |
|    total_timesteps  | 402579    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.7      |
|    n_updates        | 90644     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9864      |
|    fps              | 292       |
|    time_elapsed     | 1375      |
|    total_timesteps  | 402712    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.4      |
|    n_updates        | 90677     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9868      |
|    fps              | 292       |
|    time_elapsed     | 1375      |
|    total_timesteps  | 402858    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 90714     |
-----------------------------------
Eval num_timesteps=403000, episode_reward=-3088.82 +/- 1049.78
Episode length: 34.94 +/- 5.85
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -3.09e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 403000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.2      |
|    n_updates        | 90749     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.4     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9872     |
|    fps              | 292      |
|    time_elapsed     | 1376     |
|    total_timesteps  | 403009   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 116      |
|    n_updates        | 90752    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9876      |
|    fps              | 292       |
|    time_elapsed     | 1376      |
|    total_timesteps  | 403136    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 90783     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9880      |
|    fps              | 292       |
|    time_elapsed     | 1377      |
|    total_timesteps  | 403261    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64.9      |
|    n_updates        | 90815     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9884      |
|    fps              | 292       |
|    time_elapsed     | 1377      |
|    total_timesteps  | 403391    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 90847     |
-----------------------------------
Eval num_timesteps=403500, episode_reward=-2952.15 +/- 1084.51
Episode length: 35.22 +/- 6.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 403500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.1      |
|    n_updates        | 90874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9888      |
|    fps              | 292       |
|    time_elapsed     | 1378      |
|    total_timesteps  | 403526    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 227       |
|    n_updates        | 90881     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9892      |
|    fps              | 292       |
|    time_elapsed     | 1378      |
|    total_timesteps  | 403685    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 90921     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9896      |
|    fps              | 292       |
|    time_elapsed     | 1378      |
|    total_timesteps  | 403831    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 225       |
|    n_updates        | 90957     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9900      |
|    fps              | 292       |
|    time_elapsed     | 1379      |
|    total_timesteps  | 403949    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 90987     |
-----------------------------------
Eval num_timesteps=404000, episode_reward=-2702.68 +/- 1194.15
Episode length: 36.68 +/- 6.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.7     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 404000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 52.8     |
|    n_updates        | 90999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9904      |
|    fps              | 292       |
|    time_elapsed     | 1380      |
|    total_timesteps  | 404099    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 219       |
|    n_updates        | 91024     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9908      |
|    fps              | 292       |
|    time_elapsed     | 1380      |
|    total_timesteps  | 404243    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 91060     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9912      |
|    fps              | 292       |
|    time_elapsed     | 1380      |
|    total_timesteps  | 404386    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51        |
|    n_updates        | 91096     |
-----------------------------------
Eval num_timesteps=404500, episode_reward=-2950.01 +/- 1006.01
Episode length: 35.24 +/- 7.18
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 404500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 91124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9916      |
|    fps              | 292       |
|    time_elapsed     | 1382      |
|    total_timesteps  | 404539    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 221       |
|    n_updates        | 91134     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9920      |
|    fps              | 292       |
|    time_elapsed     | 1382      |
|    total_timesteps  | 404696    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 91173     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9924      |
|    fps              | 292       |
|    time_elapsed     | 1382      |
|    total_timesteps  | 404838    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 91209     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9928      |
|    fps              | 292       |
|    time_elapsed     | 1382      |
|    total_timesteps  | 404987    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.2      |
|    n_updates        | 91246     |
-----------------------------------
Eval num_timesteps=405000, episode_reward=-2777.76 +/- 1254.00
Episode length: 35.46 +/- 6.13
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 405000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 91249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9932      |
|    fps              | 292       |
|    time_elapsed     | 1384      |
|    total_timesteps  | 405121    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 43.1      |
|    n_updates        | 91280     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9936      |
|    fps              | 292       |
|    time_elapsed     | 1384      |
|    total_timesteps  | 405277    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47        |
|    n_updates        | 91319     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.2     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9940     |
|    fps              | 292      |
|    time_elapsed     | 1384     |
|    total_timesteps  | 405412   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 171      |
|    n_updates        | 91352    |
----------------------------------
Eval num_timesteps=405500, episode_reward=-2702.36 +/- 1158.76
Episode length: 36.36 +/- 6.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.4     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 405500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 122      |
|    n_updates        | 91374    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9944      |
|    fps              | 292       |
|    time_elapsed     | 1385      |
|    total_timesteps  | 405542    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.2      |
|    n_updates        | 91385     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9948      |
|    fps              | 292       |
|    time_elapsed     | 1385      |
|    total_timesteps  | 405688    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 91421     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9952      |
|    fps              | 292       |
|    time_elapsed     | 1386      |
|    total_timesteps  | 405835    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 91458     |
-----------------------------------
Eval num_timesteps=406000, episode_reward=-2696.52 +/- 1272.78
Episode length: 35.96 +/- 5.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36       |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 406000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 83       |
|    n_updates        | 91499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.4     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 9956     |
|    fps              | 292      |
|    time_elapsed     | 1387     |
|    total_timesteps  | 406003   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 60.4     |
|    n_updates        | 91500    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9960      |
|    fps              | 292       |
|    time_elapsed     | 1387      |
|    total_timesteps  | 406159    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 91539     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9964      |
|    fps              | 292       |
|    time_elapsed     | 1387      |
|    total_timesteps  | 406304    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 91575     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9968      |
|    fps              | 292       |
|    time_elapsed     | 1388      |
|    total_timesteps  | 406468    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 200       |
|    n_updates        | 91616     |
-----------------------------------
Eval num_timesteps=406500, episode_reward=-2829.60 +/- 1080.09
Episode length: 34.00 +/- 7.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 406500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 91624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9972      |
|    fps              | 292       |
|    time_elapsed     | 1389      |
|    total_timesteps  | 406611    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 91652     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9976      |
|    fps              | 292       |
|    time_elapsed     | 1389      |
|    total_timesteps  | 406772    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 49.9      |
|    n_updates        | 91692     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9980      |
|    fps              | 292       |
|    time_elapsed     | 1389      |
|    total_timesteps  | 406911    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97        |
|    n_updates        | 91727     |
-----------------------------------
Eval num_timesteps=407000, episode_reward=-3023.81 +/- 972.35
Episode length: 34.00 +/- 6.46
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -3.02e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 407000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 91749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9984      |
|    fps              | 292       |
|    time_elapsed     | 1390      |
|    total_timesteps  | 407010    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37.8      |
|    n_updates        | 91752     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9988      |
|    fps              | 292       |
|    time_elapsed     | 1391      |
|    total_timesteps  | 407171    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 186       |
|    n_updates        | 91792     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9992      |
|    fps              | 292       |
|    time_elapsed     | 1391      |
|    total_timesteps  | 407327    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 198       |
|    n_updates        | 91831     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 9996      |
|    fps              | 292       |
|    time_elapsed     | 1391      |
|    total_timesteps  | 407490    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 91872     |
-----------------------------------
Eval num_timesteps=407500, episode_reward=-2455.01 +/- 1220.63
Episode length: 36.80 +/- 5.91
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.46e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 407500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.1      |
|    n_updates        | 91874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10000     |
|    fps              | 292       |
|    time_elapsed     | 1392      |
|    total_timesteps  | 407646    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 121       |
|    n_updates        | 91911     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10004     |
|    fps              | 292       |
|    time_elapsed     | 1393      |
|    total_timesteps  | 407770    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.7      |
|    n_updates        | 91942     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10008     |
|    fps              | 292       |
|    time_elapsed     | 1393      |
|    total_timesteps  | 407883    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92        |
|    n_updates        | 91970     |
-----------------------------------
Eval num_timesteps=408000, episode_reward=-2646.22 +/- 1458.02
Episode length: 35.82 +/- 6.64
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 408000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 91999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10012     |
|    fps              | 292       |
|    time_elapsed     | 1394      |
|    total_timesteps  | 408031    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.9      |
|    n_updates        | 92007     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10016     |
|    fps              | 292       |
|    time_elapsed     | 1394      |
|    total_timesteps  | 408158    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 92039     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10020     |
|    fps              | 292       |
|    time_elapsed     | 1394      |
|    total_timesteps  | 408304    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.4      |
|    n_updates        | 92075     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10024     |
|    fps              | 292       |
|    time_elapsed     | 1395      |
|    total_timesteps  | 408462    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 92115     |
-----------------------------------
Eval num_timesteps=408500, episode_reward=-2763.19 +/- 987.88
Episode length: 35.40 +/- 5.93
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 408500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 206       |
|    n_updates        | 92124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10028     |
|    fps              | 292       |
|    time_elapsed     | 1396      |
|    total_timesteps  | 408606    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93        |
|    n_updates        | 92151     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10032     |
|    fps              | 292       |
|    time_elapsed     | 1396      |
|    total_timesteps  | 408747    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 92186     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10036     |
|    fps              | 292       |
|    time_elapsed     | 1396      |
|    total_timesteps  | 408885    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.5      |
|    n_updates        | 92221     |
-----------------------------------
Eval num_timesteps=409000, episode_reward=-2678.34 +/- 1247.54
Episode length: 35.64 +/- 7.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 409000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 92249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10040     |
|    fps              | 292       |
|    time_elapsed     | 1398      |
|    total_timesteps  | 409013    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 195       |
|    n_updates        | 92253     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10044     |
|    fps              | 292       |
|    time_elapsed     | 1398      |
|    total_timesteps  | 409146    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 92286     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10048     |
|    fps              | 292       |
|    time_elapsed     | 1398      |
|    total_timesteps  | 409298    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 35.4      |
|    n_updates        | 92324     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10052     |
|    fps              | 292       |
|    time_elapsed     | 1398      |
|    total_timesteps  | 409442    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 92360     |
-----------------------------------
Eval num_timesteps=409500, episode_reward=-2692.38 +/- 1202.92
Episode length: 36.40 +/- 7.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 409500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.1      |
|    n_updates        | 92374     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.6     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10056    |
|    fps              | 292      |
|    time_elapsed     | 1400     |
|    total_timesteps  | 409566   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 137      |
|    n_updates        | 92391    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10060     |
|    fps              | 292       |
|    time_elapsed     | 1400      |
|    total_timesteps  | 409684    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 92420     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10064     |
|    fps              | 292       |
|    time_elapsed     | 1400      |
|    total_timesteps  | 409830    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 92457     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10068     |
|    fps              | 292       |
|    time_elapsed     | 1400      |
|    total_timesteps  | 409972    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 92492     |
-----------------------------------
Eval num_timesteps=410000, episode_reward=-2847.81 +/- 1036.79
Episode length: 35.02 +/- 6.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 410000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 92499     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10072    |
|    fps              | 292      |
|    time_elapsed     | 1401     |
|    total_timesteps  | 410138   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 55.9     |
|    n_updates        | 92534    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10076     |
|    fps              | 292       |
|    time_elapsed     | 1402      |
|    total_timesteps  | 410279    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.8      |
|    n_updates        | 92569     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10080     |
|    fps              | 292       |
|    time_elapsed     | 1402      |
|    total_timesteps  | 410411    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 92602     |
-----------------------------------
Eval num_timesteps=410500, episode_reward=-2979.10 +/- 1054.55
Episode length: 36.72 +/- 6.73
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.7      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 410500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 92624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10084     |
|    fps              | 292       |
|    time_elapsed     | 1403      |
|    total_timesteps  | 410572    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 92642     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10088     |
|    fps              | 292       |
|    time_elapsed     | 1403      |
|    total_timesteps  | 410725    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 226       |
|    n_updates        | 92681     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10092     |
|    fps              | 292       |
|    time_elapsed     | 1404      |
|    total_timesteps  | 410875    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 56.7      |
|    n_updates        | 92718     |
-----------------------------------
Eval num_timesteps=411000, episode_reward=-3081.17 +/- 921.18
Episode length: 35.52 +/- 6.36
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 411000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 92749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10096     |
|    fps              | 292       |
|    time_elapsed     | 1405      |
|    total_timesteps  | 411007    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 186       |
|    n_updates        | 92751     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10100     |
|    fps              | 292       |
|    time_elapsed     | 1405      |
|    total_timesteps  | 411131    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78        |
|    n_updates        | 92782     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10104     |
|    fps              | 292       |
|    time_elapsed     | 1405      |
|    total_timesteps  | 411267    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 222       |
|    n_updates        | 92816     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10108     |
|    fps              | 292       |
|    time_elapsed     | 1405      |
|    total_timesteps  | 411409    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 92852     |
-----------------------------------
Eval num_timesteps=411500, episode_reward=-3163.16 +/- 1183.07
Episode length: 32.82 +/- 6.98
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.8      |
|    mean_reward      | -3.16e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 411500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 92874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10112     |
|    fps              | 292       |
|    time_elapsed     | 1407      |
|    total_timesteps  | 411570    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.8      |
|    n_updates        | 92892     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10116     |
|    fps              | 292       |
|    time_elapsed     | 1407      |
|    total_timesteps  | 411723    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 92930     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10120     |
|    fps              | 292       |
|    time_elapsed     | 1407      |
|    total_timesteps  | 411870    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 92967     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10124     |
|    fps              | 292       |
|    time_elapsed     | 1407      |
|    total_timesteps  | 411998    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 92999     |
-----------------------------------
Eval num_timesteps=412000, episode_reward=-2697.79 +/- 1355.01
Episode length: 35.96 +/- 6.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36       |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 412000   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10128     |
|    fps              | 292       |
|    time_elapsed     | 1409      |
|    total_timesteps  | 412162    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.9      |
|    n_updates        | 93040     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10132     |
|    fps              | 292       |
|    time_elapsed     | 1409      |
|    total_timesteps  | 412296    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 93073     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10136     |
|    fps              | 292       |
|    time_elapsed     | 1409      |
|    total_timesteps  | 412438    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 198       |
|    n_updates        | 93109     |
-----------------------------------
Eval num_timesteps=412500, episode_reward=-2967.54 +/- 1094.57
Episode length: 32.90 +/- 7.39
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 32.9      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 412500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 93124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10140     |
|    fps              | 292       |
|    time_elapsed     | 1410      |
|    total_timesteps  | 412585    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 20.3      |
|    n_updates        | 93146     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10144     |
|    fps              | 292       |
|    time_elapsed     | 1410      |
|    total_timesteps  | 412731    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 93182     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10148     |
|    fps              | 292       |
|    time_elapsed     | 1411      |
|    total_timesteps  | 412871    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 93217     |
-----------------------------------
Eval num_timesteps=413000, episode_reward=-2415.57 +/- 1501.21
Episode length: 36.26 +/- 6.60
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.42e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 413000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 93249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10152     |
|    fps              | 292       |
|    time_elapsed     | 1412      |
|    total_timesteps  | 413038    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 203       |
|    n_updates        | 93259     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10156     |
|    fps              | 292       |
|    time_elapsed     | 1412      |
|    total_timesteps  | 413189    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 93297     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10160     |
|    fps              | 292       |
|    time_elapsed     | 1412      |
|    total_timesteps  | 413324    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.2      |
|    n_updates        | 93330     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10164     |
|    fps              | 292       |
|    time_elapsed     | 1412      |
|    total_timesteps  | 413459    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 222       |
|    n_updates        | 93364     |
-----------------------------------
Eval num_timesteps=413500, episode_reward=-2668.73 +/- 1406.31
Episode length: 35.96 +/- 6.10
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 413500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 269       |
|    n_updates        | 93374     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.3     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10168    |
|    fps              | 292      |
|    time_elapsed     | 1414     |
|    total_timesteps  | 413598   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 163      |
|    n_updates        | 93399    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10172     |
|    fps              | 292       |
|    time_elapsed     | 1414      |
|    total_timesteps  | 413735    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.5      |
|    n_updates        | 93433     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10176     |
|    fps              | 292       |
|    time_elapsed     | 1414      |
|    total_timesteps  | 413861    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.2      |
|    n_updates        | 93465     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10180     |
|    fps              | 292       |
|    time_elapsed     | 1414      |
|    total_timesteps  | 413975    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 93493     |
-----------------------------------
Eval num_timesteps=414000, episode_reward=-2572.25 +/- 1150.95
Episode length: 35.32 +/- 7.19
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.57e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 414000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 233       |
|    n_updates        | 93499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10184     |
|    fps              | 292       |
|    time_elapsed     | 1416      |
|    total_timesteps  | 414117    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89        |
|    n_updates        | 93529     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10188     |
|    fps              | 292       |
|    time_elapsed     | 1416      |
|    total_timesteps  | 414286    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 93571     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10192     |
|    fps              | 292       |
|    time_elapsed     | 1416      |
|    total_timesteps  | 414414    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 200       |
|    n_updates        | 93603     |
-----------------------------------
Eval num_timesteps=414500, episode_reward=-2786.18 +/- 1034.67
Episode length: 35.96 +/- 6.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 414500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 93624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10196     |
|    fps              | 292       |
|    time_elapsed     | 1417      |
|    total_timesteps  | 414526    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 93631     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10200     |
|    fps              | 292       |
|    time_elapsed     | 1417      |
|    total_timesteps  | 414654    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 50.7      |
|    n_updates        | 93663     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10204     |
|    fps              | 292       |
|    time_elapsed     | 1418      |
|    total_timesteps  | 414807    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 93701     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10208     |
|    fps              | 292       |
|    time_elapsed     | 1418      |
|    total_timesteps  | 414940    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.8      |
|    n_updates        | 93734     |
-----------------------------------
Eval num_timesteps=415000, episode_reward=-2892.67 +/- 1173.68
Episode length: 35.78 +/- 6.73
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 415000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.9      |
|    n_updates        | 93749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10212     |
|    fps              | 292       |
|    time_elapsed     | 1419      |
|    total_timesteps  | 415054    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 200       |
|    n_updates        | 93763     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10216     |
|    fps              | 292       |
|    time_elapsed     | 1419      |
|    total_timesteps  | 415193    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 93798     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10220     |
|    fps              | 292       |
|    time_elapsed     | 1419      |
|    total_timesteps  | 415357    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 273       |
|    n_updates        | 93839     |
-----------------------------------
Eval num_timesteps=415500, episode_reward=-2653.25 +/- 1175.91
Episode length: 35.86 +/- 5.36
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 415500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 93874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10224     |
|    fps              | 292       |
|    time_elapsed     | 1421      |
|    total_timesteps  | 415501    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 93875     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10228     |
|    fps              | 292       |
|    time_elapsed     | 1421      |
|    total_timesteps  | 415646    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 93911     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10232     |
|    fps              | 292       |
|    time_elapsed     | 1421      |
|    total_timesteps  | 415776    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.1      |
|    n_updates        | 93943     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10236     |
|    fps              | 292       |
|    time_elapsed     | 1421      |
|    total_timesteps  | 415941    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 93985     |
-----------------------------------
Eval num_timesteps=416000, episode_reward=-2497.71 +/- 1321.81
Episode length: 37.12 +/- 5.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.1     |
|    mean_reward      | -2.5e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 416000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 89.5     |
|    n_updates        | 93999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10240     |
|    fps              | 292       |
|    time_elapsed     | 1423      |
|    total_timesteps  | 416108    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.3      |
|    n_updates        | 94026     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10244     |
|    fps              | 292       |
|    time_elapsed     | 1423      |
|    total_timesteps  | 416250    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 94062     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10248     |
|    fps              | 292       |
|    time_elapsed     | 1423      |
|    total_timesteps  | 416384    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.5      |
|    n_updates        | 94095     |
-----------------------------------
Eval num_timesteps=416500, episode_reward=-2773.10 +/- 1306.02
Episode length: 34.86 +/- 7.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 416500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 94124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10252     |
|    fps              | 292       |
|    time_elapsed     | 1424      |
|    total_timesteps  | 416520    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 242       |
|    n_updates        | 94129     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10256     |
|    fps              | 292       |
|    time_elapsed     | 1425      |
|    total_timesteps  | 416671    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 63.6      |
|    n_updates        | 94167     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10260     |
|    fps              | 292       |
|    time_elapsed     | 1425      |
|    total_timesteps  | 416820    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 94204     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10264     |
|    fps              | 292       |
|    time_elapsed     | 1425      |
|    total_timesteps  | 416962    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 94240     |
-----------------------------------
Eval num_timesteps=417000, episode_reward=-2988.32 +/- 1143.34
Episode length: 34.96 +/- 6.54
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 417000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.1      |
|    n_updates        | 94249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10268     |
|    fps              | 292       |
|    time_elapsed     | 1426      |
|    total_timesteps  | 417109    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 55.4      |
|    n_updates        | 94277     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10272     |
|    fps              | 292       |
|    time_elapsed     | 1426      |
|    total_timesteps  | 417229    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 94307     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10276     |
|    fps              | 292       |
|    time_elapsed     | 1427      |
|    total_timesteps  | 417361    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.4      |
|    n_updates        | 94340     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.2     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10280    |
|    fps              | 292      |
|    time_elapsed     | 1427     |
|    total_timesteps  | 417499   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 145      |
|    n_updates        | 94374    |
----------------------------------
Eval num_timesteps=417500, episode_reward=-2874.11 +/- 1201.12
Episode length: 36.04 +/- 6.40
----------------------------------
| eval/              |           |
|    mean_ep_length  | 36        |
|    mean_reward     | -2.87e+03 |
| time/              |           |
|    total_timesteps | 417500    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10284     |
|    fps              | 292       |
|    time_elapsed     | 1428      |
|    total_timesteps  | 417644    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.4      |
|    n_updates        | 94410     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10288     |
|    fps              | 292       |
|    time_elapsed     | 1428      |
|    total_timesteps  | 417762    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 94440     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10292     |
|    fps              | 292       |
|    time_elapsed     | 1428      |
|    total_timesteps  | 417900    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 94474     |
-----------------------------------
Eval num_timesteps=418000, episode_reward=-2823.68 +/- 1117.66
Episode length: 35.40 +/- 6.10
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 418000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 94499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10296     |
|    fps              | 292       |
|    time_elapsed     | 1430      |
|    total_timesteps  | 418032    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 255       |
|    n_updates        | 94507     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10300     |
|    fps              | 292       |
|    time_elapsed     | 1430      |
|    total_timesteps  | 418162    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.9      |
|    n_updates        | 94540     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10304     |
|    fps              | 292       |
|    time_elapsed     | 1430      |
|    total_timesteps  | 418313    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 94578     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10308     |
|    fps              | 292       |
|    time_elapsed     | 1430      |
|    total_timesteps  | 418439    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 94609     |
-----------------------------------
Eval num_timesteps=418500, episode_reward=-2854.28 +/- 1408.18
Episode length: 34.64 +/- 6.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 418500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 45.4      |
|    n_updates        | 94624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10312     |
|    fps              | 292       |
|    time_elapsed     | 1432      |
|    total_timesteps  | 418569    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.7      |
|    n_updates        | 94642     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.2     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10316    |
|    fps              | 292      |
|    time_elapsed     | 1432     |
|    total_timesteps  | 418713   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 106      |
|    n_updates        | 94678    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10320     |
|    fps              | 292       |
|    time_elapsed     | 1432      |
|    total_timesteps  | 418852    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 94712     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10324     |
|    fps              | 292       |
|    time_elapsed     | 1432      |
|    total_timesteps  | 418995    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 228       |
|    n_updates        | 94748     |
-----------------------------------
Eval num_timesteps=419000, episode_reward=-2866.60 +/- 970.35
Episode length: 35.24 +/- 6.45
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 419000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 94749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10328     |
|    fps              | 292       |
|    time_elapsed     | 1433      |
|    total_timesteps  | 419129    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58        |
|    n_updates        | 94782     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10332     |
|    fps              | 292       |
|    time_elapsed     | 1434      |
|    total_timesteps  | 419284    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 94820     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10336     |
|    fps              | 292       |
|    time_elapsed     | 1434      |
|    total_timesteps  | 419420    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 256       |
|    n_updates        | 94854     |
-----------------------------------
Eval num_timesteps=419500, episode_reward=-2698.89 +/- 1275.96
Episode length: 36.16 +/- 7.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.2     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 419500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 115      |
|    n_updates        | 94874    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10340     |
|    fps              | 292       |
|    time_elapsed     | 1435      |
|    total_timesteps  | 419560    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 233       |
|    n_updates        | 94889     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10344     |
|    fps              | 292       |
|    time_elapsed     | 1435      |
|    total_timesteps  | 419708    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 94926     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10348     |
|    fps              | 292       |
|    time_elapsed     | 1436      |
|    total_timesteps  | 419851    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95        |
|    n_updates        | 94962     |
-----------------------------------
Eval num_timesteps=420000, episode_reward=-2682.54 +/- 1215.21
Episode length: 36.36 +/- 6.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 420000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 94999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10352     |
|    fps              | 292       |
|    time_elapsed     | 1437      |
|    total_timesteps  | 420012    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.3      |
|    n_updates        | 95002     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10356     |
|    fps              | 292       |
|    time_elapsed     | 1437      |
|    total_timesteps  | 420135    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 95033     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10360     |
|    fps              | 292       |
|    time_elapsed     | 1437      |
|    total_timesteps  | 420287    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 95071     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10364     |
|    fps              | 292       |
|    time_elapsed     | 1437      |
|    total_timesteps  | 420427    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 95106     |
-----------------------------------
Eval num_timesteps=420500, episode_reward=-2951.19 +/- 1180.54
Episode length: 35.68 +/- 6.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 420500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 174       |
|    n_updates        | 95124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10368     |
|    fps              | 292       |
|    time_elapsed     | 1439      |
|    total_timesteps  | 420574    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 95143     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10372     |
|    fps              | 292       |
|    time_elapsed     | 1439      |
|    total_timesteps  | 420720    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 95179     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10376     |
|    fps              | 292       |
|    time_elapsed     | 1439      |
|    total_timesteps  | 420837    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.1      |
|    n_updates        | 95209     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10380     |
|    fps              | 292       |
|    time_elapsed     | 1439      |
|    total_timesteps  | 420969    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 95242     |
-----------------------------------
Eval num_timesteps=421000, episode_reward=-2437.48 +/- 1430.03
Episode length: 36.88 +/- 5.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.9      |
|    mean_reward      | -2.44e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 421000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 95249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10384     |
|    fps              | 292       |
|    time_elapsed     | 1441      |
|    total_timesteps  | 421095    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 95273     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10388     |
|    fps              | 292       |
|    time_elapsed     | 1441      |
|    total_timesteps  | 421233    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 95308     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.04e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10392     |
|    fps              | 292       |
|    time_elapsed     | 1441      |
|    total_timesteps  | 421378    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 95344     |
-----------------------------------
Eval num_timesteps=421500, episode_reward=-2895.99 +/- 1213.17
Episode length: 32.84 +/- 7.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 32.8     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 421500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 217      |
|    n_updates        | 95374    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10396     |
|    fps              | 292       |
|    time_elapsed     | 1442      |
|    total_timesteps  | 421523    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 95380     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10400     |
|    fps              | 292       |
|    time_elapsed     | 1442      |
|    total_timesteps  | 421660    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 95414     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.9     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10404    |
|    fps              | 292      |
|    time_elapsed     | 1443     |
|    total_timesteps  | 421805   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 86.6     |
|    n_updates        | 95451    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10408     |
|    fps              | 292       |
|    time_elapsed     | 1443      |
|    total_timesteps  | 421949    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 95487     |
-----------------------------------
Eval num_timesteps=422000, episode_reward=-2874.75 +/- 1057.51
Episode length: 35.60 +/- 5.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 422000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 95499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10412     |
|    fps              | 292       |
|    time_elapsed     | 1444      |
|    total_timesteps  | 422109    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62        |
|    n_updates        | 95527     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.4     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10416    |
|    fps              | 292      |
|    time_elapsed     | 1444     |
|    total_timesteps  | 422249   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 191      |
|    n_updates        | 95562    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10420     |
|    fps              | 292       |
|    time_elapsed     | 1445      |
|    total_timesteps  | 422407    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.2      |
|    n_updates        | 95601     |
-----------------------------------
Eval num_timesteps=422500, episode_reward=-2551.94 +/- 1236.44
Episode length: 37.40 +/- 4.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.4      |
|    mean_reward      | -2.55e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 422500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 269       |
|    n_updates        | 95624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10424     |
|    fps              | 292       |
|    time_elapsed     | 1446      |
|    total_timesteps  | 422530    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 95632     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10428     |
|    fps              | 292       |
|    time_elapsed     | 1446      |
|    total_timesteps  | 422690    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.1      |
|    n_updates        | 95672     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -3.03e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10432     |
|    fps              | 292       |
|    time_elapsed     | 1446      |
|    total_timesteps  | 422823    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 248       |
|    n_updates        | 95705     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10436     |
|    fps              | 292       |
|    time_elapsed     | 1447      |
|    total_timesteps  | 422978    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 180       |
|    n_updates        | 95744     |
-----------------------------------
Eval num_timesteps=423000, episode_reward=-2805.25 +/- 1179.41
Episode length: 34.94 +/- 6.79
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 423000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 95749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10440     |
|    fps              | 292       |
|    time_elapsed     | 1448      |
|    total_timesteps  | 423102    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 95775     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10444     |
|    fps              | 292       |
|    time_elapsed     | 1448      |
|    total_timesteps  | 423230    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.7      |
|    n_updates        | 95807     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10448     |
|    fps              | 292       |
|    time_elapsed     | 1448      |
|    total_timesteps  | 423364    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 200       |
|    n_updates        | 95840     |
-----------------------------------
Eval num_timesteps=423500, episode_reward=-2832.39 +/- 1099.61
Episode length: 35.84 +/- 5.80
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 423500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 95874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10452     |
|    fps              | 292       |
|    time_elapsed     | 1450      |
|    total_timesteps  | 423520    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 95879     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10456     |
|    fps              | 292       |
|    time_elapsed     | 1450      |
|    total_timesteps  | 423661    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.3      |
|    n_updates        | 95915     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10460     |
|    fps              | 292       |
|    time_elapsed     | 1450      |
|    total_timesteps  | 423803    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 95950     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10464     |
|    fps              | 292       |
|    time_elapsed     | 1450      |
|    total_timesteps  | 423956    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 219       |
|    n_updates        | 95988     |
-----------------------------------
Eval num_timesteps=424000, episode_reward=-2927.96 +/- 1084.95
Episode length: 33.14 +/- 6.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.1      |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 424000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.2      |
|    n_updates        | 95999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10468     |
|    fps              | 292       |
|    time_elapsed     | 1451      |
|    total_timesteps  | 424075    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88.9      |
|    n_updates        | 96018     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10472     |
|    fps              | 292       |
|    time_elapsed     | 1452      |
|    total_timesteps  | 424226    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 96056     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10476     |
|    fps              | 292       |
|    time_elapsed     | 1452      |
|    total_timesteps  | 424362    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 96090     |
-----------------------------------
Eval num_timesteps=424500, episode_reward=-2776.95 +/- 1231.54
Episode length: 35.12 +/- 7.28
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 424500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 33.5      |
|    n_updates        | 96124     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.4     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10480    |
|    fps              | 292      |
|    time_elapsed     | 1453     |
|    total_timesteps  | 424509   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 178      |
|    n_updates        | 96127    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10484     |
|    fps              | 292       |
|    time_elapsed     | 1453      |
|    total_timesteps  | 424679    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 96169     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10488     |
|    fps              | 292       |
|    time_elapsed     | 1453      |
|    total_timesteps  | 424817    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83        |
|    n_updates        | 96204     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10492     |
|    fps              | 292       |
|    time_elapsed     | 1454      |
|    total_timesteps  | 424972    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 261       |
|    n_updates        | 96242     |
-----------------------------------
Eval num_timesteps=425000, episode_reward=-2603.22 +/- 1327.30
Episode length: 36.18 +/- 6.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.2     |
|    mean_reward      | -2.6e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 425000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 127      |
|    n_updates        | 96249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10496     |
|    fps              | 292       |
|    time_elapsed     | 1455      |
|    total_timesteps  | 425090    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 273       |
|    n_updates        | 96272     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10500    |
|    fps              | 292      |
|    time_elapsed     | 1455     |
|    total_timesteps  | 425231   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 86.4     |
|    n_updates        | 96307    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10504     |
|    fps              | 292       |
|    time_elapsed     | 1455      |
|    total_timesteps  | 425333    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 96333     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10508     |
|    fps              | 292       |
|    time_elapsed     | 1456      |
|    total_timesteps  | 425472    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 185       |
|    n_updates        | 96367     |
-----------------------------------
Eval num_timesteps=425500, episode_reward=-2546.22 +/- 1313.83
Episode length: 35.10 +/- 7.03
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.55e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 425500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 96374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.03e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10512     |
|    fps              | 292       |
|    time_elapsed     | 1457      |
|    total_timesteps  | 425602    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 96400     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.05e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10516     |
|    fps              | 292       |
|    time_elapsed     | 1457      |
|    total_timesteps  | 425730    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 210       |
|    n_updates        | 96432     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.08e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10520     |
|    fps              | 292       |
|    time_elapsed     | 1457      |
|    total_timesteps  | 425881    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 96470     |
-----------------------------------
Eval num_timesteps=426000, episode_reward=-2957.58 +/- 938.04
Episode length: 35.50 +/- 6.11
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 426000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 96499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -3.12e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10524     |
|    fps              | 291       |
|    time_elapsed     | 1459      |
|    total_timesteps  | 426006    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 277       |
|    n_updates        | 96501     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.08e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10528     |
|    fps              | 292       |
|    time_elapsed     | 1459      |
|    total_timesteps  | 426154    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 96538     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10532     |
|    fps              | 292       |
|    time_elapsed     | 1459      |
|    total_timesteps  | 426292    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 96572     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10536     |
|    fps              | 292       |
|    time_elapsed     | 1459      |
|    total_timesteps  | 426421    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 96605     |
-----------------------------------
Eval num_timesteps=426500, episode_reward=-2707.57 +/- 1298.25
Episode length: 36.40 +/- 5.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.71e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 426500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 96624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10540     |
|    fps              | 291       |
|    time_elapsed     | 1461      |
|    total_timesteps  | 426574    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.4      |
|    n_updates        | 96643     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.03e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10544     |
|    fps              | 292       |
|    time_elapsed     | 1461      |
|    total_timesteps  | 426733    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 96683     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.08e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10548     |
|    fps              | 292       |
|    time_elapsed     | 1461      |
|    total_timesteps  | 426865    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 96716     |
-----------------------------------
Eval num_timesteps=427000, episode_reward=-3318.80 +/- 1026.46
Episode length: 33.04 +/- 6.61
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33        |
|    mean_reward      | -3.32e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 427000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 192       |
|    n_updates        | 96749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -3.09e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10552     |
|    fps              | 291       |
|    time_elapsed     | 1462      |
|    total_timesteps  | 427017    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 96754     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.08e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10556     |
|    fps              | 292       |
|    time_elapsed     | 1462      |
|    total_timesteps  | 427167    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 96791     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -3.05e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10560     |
|    fps              | 292       |
|    time_elapsed     | 1463      |
|    total_timesteps  | 427324    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 30.6      |
|    n_updates        | 96830     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -3.07e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10564     |
|    fps              | 292       |
|    time_elapsed     | 1463      |
|    total_timesteps  | 427444    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 96860     |
-----------------------------------
Eval num_timesteps=427500, episode_reward=-2741.36 +/- 1278.74
Episode length: 36.00 +/- 6.08
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.74e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 427500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.3      |
|    n_updates        | 96874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10568     |
|    fps              | 291       |
|    time_elapsed     | 1464      |
|    total_timesteps  | 427596    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 96898     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10572     |
|    fps              | 292       |
|    time_elapsed     | 1464      |
|    total_timesteps  | 427741    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 96935     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -3.07e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10576     |
|    fps              | 292       |
|    time_elapsed     | 1464      |
|    total_timesteps  | 427883    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 96970     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.9     |
|    ep_rew_mean      | -3.1e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10580    |
|    fps              | 292      |
|    time_elapsed     | 1465     |
|    total_timesteps  | 427998   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 221      |
|    n_updates        | 96999    |
----------------------------------
Eval num_timesteps=428000, episode_reward=-2533.66 +/- 1346.80
Episode length: 36.76 +/- 6.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.8      |
|    mean_reward      | -2.53e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 428000    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.08e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10584     |
|    fps              | 291       |
|    time_elapsed     | 1466      |
|    total_timesteps  | 428134    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 97033     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.07e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10588     |
|    fps              | 291       |
|    time_elapsed     | 1466      |
|    total_timesteps  | 428256    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 251       |
|    n_updates        | 97063     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10592     |
|    fps              | 292       |
|    time_elapsed     | 1466      |
|    total_timesteps  | 428406    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 219       |
|    n_updates        | 97101     |
-----------------------------------
Eval num_timesteps=428500, episode_reward=-2758.28 +/- 1460.53
Episode length: 35.20 +/- 6.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 428500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 195       |
|    n_updates        | 97124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10596     |
|    fps              | 291       |
|    time_elapsed     | 1468      |
|    total_timesteps  | 428534    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.3      |
|    n_updates        | 97133     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.08e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10600     |
|    fps              | 291       |
|    time_elapsed     | 1468      |
|    total_timesteps  | 428653    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.5      |
|    n_updates        | 97163     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10604     |
|    fps              | 291       |
|    time_elapsed     | 1468      |
|    total_timesteps  | 428785    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 97196     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -3.05e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10608     |
|    fps              | 292       |
|    time_elapsed     | 1468      |
|    total_timesteps  | 428922    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 97230     |
-----------------------------------
Eval num_timesteps=429000, episode_reward=-2897.58 +/- 1040.93
Episode length: 34.22 +/- 7.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.2     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 429000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 33.1     |
|    n_updates        | 97249    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10612     |
|    fps              | 291       |
|    time_elapsed     | 1470      |
|    total_timesteps  | 429064    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 97265     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10616     |
|    fps              | 291       |
|    time_elapsed     | 1470      |
|    total_timesteps  | 429215    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 97303     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10620     |
|    fps              | 292       |
|    time_elapsed     | 1470      |
|    total_timesteps  | 429350    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 97337     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10624     |
|    fps              | 292       |
|    time_elapsed     | 1470      |
|    total_timesteps  | 429474    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 174       |
|    n_updates        | 97368     |
-----------------------------------
Eval num_timesteps=429500, episode_reward=-2651.63 +/- 1178.40
Episode length: 36.40 +/- 6.49
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 429500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 97374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10628     |
|    fps              | 291       |
|    time_elapsed     | 1471      |
|    total_timesteps  | 429617    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 97404     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10632     |
|    fps              | 291       |
|    time_elapsed     | 1472      |
|    total_timesteps  | 429751    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 97437     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10636     |
|    fps              | 291       |
|    time_elapsed     | 1472      |
|    total_timesteps  | 429912    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 97477     |
-----------------------------------
Eval num_timesteps=430000, episode_reward=-2351.89 +/- 1428.17
Episode length: 36.90 +/- 6.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.9      |
|    mean_reward      | -2.35e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 430000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 97499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10640     |
|    fps              | 291       |
|    time_elapsed     | 1473      |
|    total_timesteps  | 430045    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 97511     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10644     |
|    fps              | 291       |
|    time_elapsed     | 1473      |
|    total_timesteps  | 430205    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 97551     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10648     |
|    fps              | 291       |
|    time_elapsed     | 1474      |
|    total_timesteps  | 430340    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 299       |
|    n_updates        | 97584     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10652     |
|    fps              | 292       |
|    time_elapsed     | 1474      |
|    total_timesteps  | 430490    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 97622     |
-----------------------------------
Eval num_timesteps=430500, episode_reward=-3017.60 +/- 878.18
Episode length: 36.04 +/- 6.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -3.02e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 430500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.8      |
|    n_updates        | 97624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10656     |
|    fps              | 291       |
|    time_elapsed     | 1475      |
|    total_timesteps  | 430633    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 97658     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10660     |
|    fps              | 291       |
|    time_elapsed     | 1475      |
|    total_timesteps  | 430774    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 200       |
|    n_updates        | 97693     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10664     |
|    fps              | 291       |
|    time_elapsed     | 1476      |
|    total_timesteps  | 430930    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 97732     |
-----------------------------------
Eval num_timesteps=431000, episode_reward=-2636.68 +/- 1293.54
Episode length: 35.96 +/- 6.17
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 431000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70        |
|    n_updates        | 97749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10668     |
|    fps              | 291       |
|    time_elapsed     | 1477      |
|    total_timesteps  | 431067    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 97766     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10672     |
|    fps              | 291       |
|    time_elapsed     | 1477      |
|    total_timesteps  | 431203    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 97800     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10676     |
|    fps              | 291       |
|    time_elapsed     | 1477      |
|    total_timesteps  | 431349    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 97837     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10680     |
|    fps              | 291       |
|    time_elapsed     | 1477      |
|    total_timesteps  | 431489    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 76.6      |
|    n_updates        | 97872     |
-----------------------------------
Eval num_timesteps=431500, episode_reward=-2741.94 +/- 1247.63
Episode length: 36.20 +/- 6.35
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.74e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 431500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 97874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10684     |
|    fps              | 291       |
|    time_elapsed     | 1479      |
|    total_timesteps  | 431619    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 97904     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10688     |
|    fps              | 291       |
|    time_elapsed     | 1479      |
|    total_timesteps  | 431753    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.4      |
|    n_updates        | 97938     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10692     |
|    fps              | 291       |
|    time_elapsed     | 1479      |
|    total_timesteps  | 431910    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 97977     |
-----------------------------------
Eval num_timesteps=432000, episode_reward=-3035.45 +/- 1195.01
Episode length: 33.50 +/- 7.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.5      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 432000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 97999     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10696     |
|    fps              | 291       |
|    time_elapsed     | 1480      |
|    total_timesteps  | 432042    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 98010     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10700     |
|    fps              | 291       |
|    time_elapsed     | 1481      |
|    total_timesteps  | 432196    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.6      |
|    n_updates        | 98048     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10704     |
|    fps              | 291       |
|    time_elapsed     | 1481      |
|    total_timesteps  | 432330    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 98082     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10708     |
|    fps              | 291       |
|    time_elapsed     | 1481      |
|    total_timesteps  | 432445    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 98111     |
-----------------------------------
Eval num_timesteps=432500, episode_reward=-3013.77 +/- 1007.00
Episode length: 34.30 +/- 5.96
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 432500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 98124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10712     |
|    fps              | 291       |
|    time_elapsed     | 1482      |
|    total_timesteps  | 432584    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 244       |
|    n_updates        | 98145     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10716     |
|    fps              | 291       |
|    time_elapsed     | 1482      |
|    total_timesteps  | 432734    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 98183     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10720     |
|    fps              | 291       |
|    time_elapsed     | 1483      |
|    total_timesteps  | 432856    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 98213     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10724     |
|    fps              | 291       |
|    time_elapsed     | 1483      |
|    total_timesteps  | 432989    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.1      |
|    n_updates        | 98247     |
-----------------------------------
Eval num_timesteps=433000, episode_reward=-2692.68 +/- 962.93
Episode length: 35.78 +/- 6.26
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 433000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 98249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10728     |
|    fps              | 291       |
|    time_elapsed     | 1484      |
|    total_timesteps  | 433135    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 98283     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10732     |
|    fps              | 291       |
|    time_elapsed     | 1484      |
|    total_timesteps  | 433268    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 98316     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10736     |
|    fps              | 291       |
|    time_elapsed     | 1484      |
|    total_timesteps  | 433412    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 98352     |
-----------------------------------
Eval num_timesteps=433500, episode_reward=-2646.93 +/- 1373.58
Episode length: 36.52 +/- 5.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 433500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 98374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10740     |
|    fps              | 291       |
|    time_elapsed     | 1486      |
|    total_timesteps  | 433549    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 98387     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10744     |
|    fps              | 291       |
|    time_elapsed     | 1486      |
|    total_timesteps  | 433703    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 37.9      |
|    n_updates        | 98425     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10748     |
|    fps              | 291       |
|    time_elapsed     | 1486      |
|    total_timesteps  | 433850    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 232       |
|    n_updates        | 98462     |
-----------------------------------
Eval num_timesteps=434000, episode_reward=-2909.03 +/- 1021.71
Episode length: 35.68 +/- 6.12
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 434000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 98499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10752     |
|    fps              | 291       |
|    time_elapsed     | 1488      |
|    total_timesteps  | 434016    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 98503     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10756     |
|    fps              | 291       |
|    time_elapsed     | 1488      |
|    total_timesteps  | 434158    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 98539     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10760     |
|    fps              | 291       |
|    time_elapsed     | 1488      |
|    total_timesteps  | 434290    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 98572     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10764     |
|    fps              | 291       |
|    time_elapsed     | 1488      |
|    total_timesteps  | 434433    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 98608     |
-----------------------------------
Eval num_timesteps=434500, episode_reward=-2824.90 +/- 1131.26
Episode length: 34.80 +/- 7.29
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 434500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 228       |
|    n_updates        | 98624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10768     |
|    fps              | 291       |
|    time_elapsed     | 1489      |
|    total_timesteps  | 434567    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 315       |
|    n_updates        | 98641     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10772     |
|    fps              | 291       |
|    time_elapsed     | 1489      |
|    total_timesteps  | 434700    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 98674     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10776     |
|    fps              | 291       |
|    time_elapsed     | 1490      |
|    total_timesteps  | 434851    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 98712     |
-----------------------------------
Eval num_timesteps=435000, episode_reward=-2776.67 +/- 1306.29
Episode length: 35.60 +/- 6.43
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 435000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69        |
|    n_updates        | 98749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10780     |
|    fps              | 291       |
|    time_elapsed     | 1491      |
|    total_timesteps  | 435007    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 98751     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10784     |
|    fps              | 291       |
|    time_elapsed     | 1491      |
|    total_timesteps  | 435163    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.4      |
|    n_updates        | 98790     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10788     |
|    fps              | 291       |
|    time_elapsed     | 1491      |
|    total_timesteps  | 435300    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 224       |
|    n_updates        | 98824     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10792     |
|    fps              | 291       |
|    time_elapsed     | 1492      |
|    total_timesteps  | 435439    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 98859     |
-----------------------------------
Eval num_timesteps=435500, episode_reward=-3005.74 +/- 1135.68
Episode length: 34.10 +/- 6.79
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 435500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 98874     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10796     |
|    fps              | 291       |
|    time_elapsed     | 1493      |
|    total_timesteps  | 435588    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 98896     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10800     |
|    fps              | 291       |
|    time_elapsed     | 1493      |
|    total_timesteps  | 435711    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.1      |
|    n_updates        | 98927     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10804     |
|    fps              | 291       |
|    time_elapsed     | 1493      |
|    total_timesteps  | 435847    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 247       |
|    n_updates        | 98961     |
-----------------------------------
Eval num_timesteps=436000, episode_reward=-2704.45 +/- 1346.35
Episode length: 35.42 +/- 6.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.4     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 436000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 110      |
|    n_updates        | 98999    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.57e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10808     |
|    fps              | 291       |
|    time_elapsed     | 1495      |
|    total_timesteps  | 436002    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 99000     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.58e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10812     |
|    fps              | 291       |
|    time_elapsed     | 1495      |
|    total_timesteps  | 436153    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 73.8      |
|    n_updates        | 99038     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10816     |
|    fps              | 291       |
|    time_elapsed     | 1495      |
|    total_timesteps  | 436304    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.5      |
|    n_updates        | 99075     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.8     |
|    ep_rew_mean      | -2.6e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10820    |
|    fps              | 291      |
|    time_elapsed     | 1495     |
|    total_timesteps  | 436440   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 131      |
|    n_updates        | 99109    |
----------------------------------
Eval num_timesteps=436500, episode_reward=-2749.81 +/- 1402.83
Episode length: 35.60 +/- 5.81
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 436500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 99124     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10824     |
|    fps              | 291       |
|    time_elapsed     | 1496      |
|    total_timesteps  | 436582    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.7      |
|    n_updates        | 99145     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10828     |
|    fps              | 291       |
|    time_elapsed     | 1497      |
|    total_timesteps  | 436731    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.4      |
|    n_updates        | 99182     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.62e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10832     |
|    fps              | 291       |
|    time_elapsed     | 1497      |
|    total_timesteps  | 436878    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.2      |
|    n_updates        | 99219     |
-----------------------------------
Eval num_timesteps=437000, episode_reward=-2576.65 +/- 1341.57
Episode length: 34.72 +/- 7.42
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.58e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 437000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 246       |
|    n_updates        | 99249     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10836     |
|    fps              | 291       |
|    time_elapsed     | 1498      |
|    total_timesteps  | 437023    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 99255     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10840     |
|    fps              | 291       |
|    time_elapsed     | 1498      |
|    total_timesteps  | 437147    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 138       |
|    n_updates        | 99286     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10844     |
|    fps              | 291       |
|    time_elapsed     | 1498      |
|    total_timesteps  | 437288    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 99321     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.8     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10848    |
|    fps              | 291      |
|    time_elapsed     | 1499     |
|    total_timesteps  | 437427   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 136      |
|    n_updates        | 99356    |
----------------------------------
Eval num_timesteps=437500, episode_reward=-2743.26 +/- 1402.48
Episode length: 34.44 +/- 6.76
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.74e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 437500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.2      |
|    n_updates        | 99374     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10852     |
|    fps              | 291       |
|    time_elapsed     | 1500      |
|    total_timesteps  | 437551    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 99387     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10856     |
|    fps              | 291       |
|    time_elapsed     | 1500      |
|    total_timesteps  | 437699    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 199       |
|    n_updates        | 99424     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.5     |
|    ep_rew_mean      | -2.6e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10860    |
|    fps              | 291      |
|    time_elapsed     | 1500     |
|    total_timesteps  | 437837   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 149      |
|    n_updates        | 99459    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.49e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10864     |
|    fps              | 291       |
|    time_elapsed     | 1500      |
|    total_timesteps  | 437990    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 99497     |
-----------------------------------
Eval num_timesteps=438000, episode_reward=-3076.85 +/- 1003.10
Episode length: 34.20 +/- 6.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -3.08e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 438000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 270       |
|    n_updates        | 99499     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.47e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10868     |
|    fps              | 291       |
|    time_elapsed     | 1502      |
|    total_timesteps  | 438129    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 99532     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.49e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10872     |
|    fps              | 291       |
|    time_elapsed     | 1502      |
|    total_timesteps  | 438265    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 99566     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.53e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10876     |
|    fps              | 291       |
|    time_elapsed     | 1502      |
|    total_timesteps  | 438409    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 99602     |
-----------------------------------
Eval num_timesteps=438500, episode_reward=-3130.42 +/- 993.82
Episode length: 35.06 +/- 7.25
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -3.13e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 438500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.9      |
|    n_updates        | 99624     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.56e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10880     |
|    fps              | 291       |
|    time_elapsed     | 1503      |
|    total_timesteps  | 438569    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 217       |
|    n_updates        | 99642     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10884     |
|    fps              | 291       |
|    time_elapsed     | 1504      |
|    total_timesteps  | 438729    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62.8      |
|    n_updates        | 99682     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10888     |
|    fps              | 291       |
|    time_elapsed     | 1504      |
|    total_timesteps  | 438859    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.9      |
|    n_updates        | 99714     |
-----------------------------------
Eval num_timesteps=439000, episode_reward=-3163.34 +/- 816.05
Episode length: 33.54 +/- 7.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.5      |
|    mean_reward      | -3.16e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 439000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 99749     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10892     |
|    fps              | 291       |
|    time_elapsed     | 1505      |
|    total_timesteps  | 439004    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 99750     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10896     |
|    fps              | 291       |
|    time_elapsed     | 1505      |
|    total_timesteps  | 439155    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 67        |
|    n_updates        | 99788     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10900    |
|    fps              | 291      |
|    time_elapsed     | 1505     |
|    total_timesteps  | 439282   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 234      |
|    n_updates        | 99820    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10904     |
|    fps              | 291       |
|    time_elapsed     | 1506      |
|    total_timesteps  | 439426    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72        |
|    n_updates        | 99856     |
-----------------------------------
Eval num_timesteps=439500, episode_reward=-2796.73 +/- 1213.35
Episode length: 34.88 +/- 8.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.9     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 439500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 142      |
|    n_updates        | 99874    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10908     |
|    fps              | 291       |
|    time_elapsed     | 1507      |
|    total_timesteps  | 439581    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 99895     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10912     |
|    fps              | 291       |
|    time_elapsed     | 1507      |
|    total_timesteps  | 439712    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 99927     |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10916     |
|    fps              | 291       |
|    time_elapsed     | 1507      |
|    total_timesteps  | 439849    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 99962     |
-----------------------------------
Eval num_timesteps=440000, episode_reward=-3136.49 +/- 1089.76
Episode length: 35.84 +/- 7.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -3.14e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 440000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88        |
|    n_updates        | 99999     |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.6e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10920    |
|    fps              | 291      |
|    time_elapsed     | 1509     |
|    total_timesteps  | 440014   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 155      |
|    n_updates        | 100003   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10924     |
|    fps              | 291       |
|    time_elapsed     | 1509      |
|    total_timesteps  | 440166    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 100041    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.62e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10928     |
|    fps              | 291       |
|    time_elapsed     | 1509      |
|    total_timesteps  | 440342    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 192       |
|    n_updates        | 100085    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10932     |
|    fps              | 291       |
|    time_elapsed     | 1509      |
|    total_timesteps  | 440495    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 100123    |
-----------------------------------
Eval num_timesteps=440500, episode_reward=-3105.40 +/- 933.92
Episode length: 34.54 +/- 7.29
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 440500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 184       |
|    n_updates        | 100124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.62e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10936     |
|    fps              | 291       |
|    time_elapsed     | 1511      |
|    total_timesteps  | 440623    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.5      |
|    n_updates        | 100155    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.3      |
|    ep_rew_mean      | -2.55e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10940     |
|    fps              | 291       |
|    time_elapsed     | 1511      |
|    total_timesteps  | 440780    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 100194    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10944     |
|    fps              | 291       |
|    time_elapsed     | 1511      |
|    total_timesteps  | 440899    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 100224    |
-----------------------------------
Eval num_timesteps=441000, episode_reward=-3147.73 +/- 988.74
Episode length: 36.24 +/- 7.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -3.15e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 441000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 100249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10948     |
|    fps              | 291       |
|    time_elapsed     | 1512      |
|    total_timesteps  | 441047    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 203       |
|    n_updates        | 100261    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.5      |
|    ep_rew_mean      | -2.55e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10952     |
|    fps              | 291       |
|    time_elapsed     | 1513      |
|    total_timesteps  | 441205    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 100301    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -2.57e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10956     |
|    fps              | 291       |
|    time_elapsed     | 1513      |
|    total_timesteps  | 441370    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 100342    |
-----------------------------------
Eval num_timesteps=441500, episode_reward=-2669.59 +/- 1361.88
Episode length: 35.52 +/- 7.15
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.67e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 441500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 100374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -2.57e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10960     |
|    fps              | 291       |
|    time_elapsed     | 1514      |
|    total_timesteps  | 441505    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 242       |
|    n_updates        | 100376    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10964     |
|    fps              | 291       |
|    time_elapsed     | 1514      |
|    total_timesteps  | 441666    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 190       |
|    n_updates        | 100416    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.7      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10968     |
|    fps              | 291       |
|    time_elapsed     | 1514      |
|    total_timesteps  | 441801    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 100450    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10972     |
|    fps              | 291       |
|    time_elapsed     | 1515      |
|    total_timesteps  | 441959    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 100489    |
-----------------------------------
Eval num_timesteps=442000, episode_reward=-2940.31 +/- 1153.25
Episode length: 35.04 +/- 6.32
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 442000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 100499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10976     |
|    fps              | 291       |
|    time_elapsed     | 1516      |
|    total_timesteps  | 442111    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 100527    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -2.62e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10980     |
|    fps              | 291       |
|    time_elapsed     | 1516      |
|    total_timesteps  | 442250    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.9      |
|    n_updates        | 100562    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10984     |
|    fps              | 291       |
|    time_elapsed     | 1516      |
|    total_timesteps  | 442390    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92        |
|    n_updates        | 100597    |
-----------------------------------
Eval num_timesteps=442500, episode_reward=-3111.22 +/- 1193.36
Episode length: 34.90 +/- 6.19
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 442500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 100624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10988     |
|    fps              | 291       |
|    time_elapsed     | 1518      |
|    total_timesteps  | 442546    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 100636    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 37        |
|    ep_rew_mean      | -2.62e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10992     |
|    fps              | 291       |
|    time_elapsed     | 1518      |
|    total_timesteps  | 442703    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.3      |
|    n_updates        | 100675    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 10996     |
|    fps              | 291       |
|    time_elapsed     | 1518      |
|    total_timesteps  | 442832    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 100707    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.9      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11000     |
|    fps              | 291       |
|    time_elapsed     | 1518      |
|    total_timesteps  | 442967    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 100741    |
-----------------------------------
Eval num_timesteps=443000, episode_reward=-2931.67 +/- 959.34
Episode length: 34.04 +/- 6.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34        |
|    mean_reward      | -2.93e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 443000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 100749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.8      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11004     |
|    fps              | 291       |
|    time_elapsed     | 1519      |
|    total_timesteps  | 443105    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 100776    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11008     |
|    fps              | 291       |
|    time_elapsed     | 1520      |
|    total_timesteps  | 443241    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 86.8      |
|    n_updates        | 100810    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.5     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11012    |
|    fps              | 291      |
|    time_elapsed     | 1520     |
|    total_timesteps  | 443366   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 64.4     |
|    n_updates        | 100841   |
----------------------------------
Eval num_timesteps=443500, episode_reward=-2911.81 +/- 1292.05
Episode length: 35.38 +/- 7.15
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.91e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 443500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 100874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.6      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11016     |
|    fps              | 291       |
|    time_elapsed     | 1521      |
|    total_timesteps  | 443514    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.5      |
|    n_updates        | 100878    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.4      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11020     |
|    fps              | 291       |
|    time_elapsed     | 1521      |
|    total_timesteps  | 443649    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.2      |
|    n_updates        | 100912    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11024     |
|    fps              | 291       |
|    time_elapsed     | 1521      |
|    total_timesteps  | 443782    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54        |
|    n_updates        | 100945    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11028     |
|    fps              | 291       |
|    time_elapsed     | 1522      |
|    total_timesteps  | 443884    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 186       |
|    n_updates        | 100970    |
-----------------------------------
Eval num_timesteps=444000, episode_reward=-2860.25 +/- 1165.05
Episode length: 35.88 +/- 5.69
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 444000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.7      |
|    n_updates        | 100999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11032     |
|    fps              | 291       |
|    time_elapsed     | 1523      |
|    total_timesteps  | 444029    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 218       |
|    n_updates        | 101007    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11036     |
|    fps              | 291       |
|    time_elapsed     | 1523      |
|    total_timesteps  | 444168    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 101041    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11040     |
|    fps              | 291       |
|    time_elapsed     | 1523      |
|    total_timesteps  | 444307    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 101076    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11044     |
|    fps              | 291       |
|    time_elapsed     | 1523      |
|    total_timesteps  | 444450    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.3      |
|    n_updates        | 101112    |
-----------------------------------
Eval num_timesteps=444500, episode_reward=-2347.16 +/- 1450.41
Episode length: 36.44 +/- 6.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.35e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 444500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 205       |
|    n_updates        | 101124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11048     |
|    fps              | 291       |
|    time_elapsed     | 1525      |
|    total_timesteps  | 444577    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 232       |
|    n_updates        | 101144    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11052     |
|    fps              | 291       |
|    time_elapsed     | 1525      |
|    total_timesteps  | 444745    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 101186    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11056     |
|    fps              | 291       |
|    time_elapsed     | 1525      |
|    total_timesteps  | 444868    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.5      |
|    n_updates        | 101216    |
-----------------------------------
Eval num_timesteps=445000, episode_reward=-2697.25 +/- 1236.32
Episode length: 36.32 +/- 5.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.3     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 445000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 181      |
|    n_updates        | 101249   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11060     |
|    fps              | 291       |
|    time_elapsed     | 1527      |
|    total_timesteps  | 445005    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.2      |
|    n_updates        | 101251    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11064     |
|    fps              | 291       |
|    time_elapsed     | 1527      |
|    total_timesteps  | 445151    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.1      |
|    n_updates        | 101287    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11068     |
|    fps              | 291       |
|    time_elapsed     | 1527      |
|    total_timesteps  | 445318    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 101329    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11072     |
|    fps              | 291       |
|    time_elapsed     | 1527      |
|    total_timesteps  | 445482    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 35.6      |
|    n_updates        | 101370    |
-----------------------------------
Eval num_timesteps=445500, episode_reward=-2862.90 +/- 1141.18
Episode length: 34.42 +/- 6.57
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 445500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 101374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11076     |
|    fps              | 291       |
|    time_elapsed     | 1529      |
|    total_timesteps  | 445639    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 101409    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11080     |
|    fps              | 291       |
|    time_elapsed     | 1529      |
|    total_timesteps  | 445782    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 101445    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11084     |
|    fps              | 291       |
|    time_elapsed     | 1529      |
|    total_timesteps  | 445921    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 101480    |
-----------------------------------
Eval num_timesteps=446000, episode_reward=-2698.45 +/- 1073.59
Episode length: 34.54 +/- 7.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.5     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 446000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 291      |
|    n_updates        | 101499   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11088     |
|    fps              | 291       |
|    time_elapsed     | 1530      |
|    total_timesteps  | 446069    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93        |
|    n_updates        | 101517    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11092     |
|    fps              | 291       |
|    time_elapsed     | 1530      |
|    total_timesteps  | 446217    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 101554    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11096     |
|    fps              | 291       |
|    time_elapsed     | 1531      |
|    total_timesteps  | 446363    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 101590    |
-----------------------------------
Eval num_timesteps=446500, episode_reward=-3216.22 +/- 769.58
Episode length: 35.94 +/- 7.98
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -3.22e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 446500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 101624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11100     |
|    fps              | 291       |
|    time_elapsed     | 1532      |
|    total_timesteps  | 446522    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 59.4      |
|    n_updates        | 101630    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11104     |
|    fps              | 291       |
|    time_elapsed     | 1532      |
|    total_timesteps  | 446658    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.9      |
|    n_updates        | 101664    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.6     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11108    |
|    fps              | 291      |
|    time_elapsed     | 1532     |
|    total_timesteps  | 446804   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 141      |
|    n_updates        | 101700   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11112     |
|    fps              | 291       |
|    time_elapsed     | 1532      |
|    total_timesteps  | 446939    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 80.6      |
|    n_updates        | 101734    |
-----------------------------------
Eval num_timesteps=447000, episode_reward=-2778.24 +/- 1352.71
Episode length: 36.10 +/- 6.60
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 447000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 101749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11116     |
|    fps              | 291       |
|    time_elapsed     | 1534      |
|    total_timesteps  | 447067    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 101766    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11120     |
|    fps              | 291       |
|    time_elapsed     | 1534      |
|    total_timesteps  | 447199    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.4      |
|    n_updates        | 101799    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11124     |
|    fps              | 291       |
|    time_elapsed     | 1534      |
|    total_timesteps  | 447322    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 81.1      |
|    n_updates        | 101830    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11128     |
|    fps              | 291       |
|    time_elapsed     | 1534      |
|    total_timesteps  | 447439    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.1      |
|    n_updates        | 101859    |
-----------------------------------
Eval num_timesteps=447500, episode_reward=-2983.22 +/- 952.22
Episode length: 35.80 +/- 5.96
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 447500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.4      |
|    n_updates        | 101874    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11132    |
|    fps              | 291      |
|    time_elapsed     | 1536     |
|    total_timesteps  | 447558   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 79.6     |
|    n_updates        | 101889   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11136     |
|    fps              | 291       |
|    time_elapsed     | 1536      |
|    total_timesteps  | 447682    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 223       |
|    n_updates        | 101920    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.2     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11140    |
|    fps              | 291      |
|    time_elapsed     | 1536     |
|    total_timesteps  | 447823   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 150      |
|    n_updates        | 101955   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11144     |
|    fps              | 291       |
|    time_elapsed     | 1536      |
|    total_timesteps  | 447983    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 237       |
|    n_updates        | 101995    |
-----------------------------------
Eval num_timesteps=448000, episode_reward=-2848.45 +/- 1215.93
Episode length: 35.46 +/- 6.35
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 448000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 101999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11148     |
|    fps              | 291       |
|    time_elapsed     | 1538      |
|    total_timesteps  | 448136    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47.6      |
|    n_updates        | 102033    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11152     |
|    fps              | 291       |
|    time_elapsed     | 1538      |
|    total_timesteps  | 448293    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 230       |
|    n_updates        | 102073    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11156     |
|    fps              | 291       |
|    time_elapsed     | 1538      |
|    total_timesteps  | 448413    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 102103    |
-----------------------------------
Eval num_timesteps=448500, episode_reward=-2435.91 +/- 1546.79
Episode length: 35.86 +/- 6.79
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.44e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 448500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 23.3      |
|    n_updates        | 102124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11160     |
|    fps              | 291       |
|    time_elapsed     | 1539      |
|    total_timesteps  | 448561    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.2      |
|    n_updates        | 102140    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11164     |
|    fps              | 291       |
|    time_elapsed     | 1539      |
|    total_timesteps  | 448699    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 102174    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.62e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11168     |
|    fps              | 291       |
|    time_elapsed     | 1540      |
|    total_timesteps  | 448864    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 102215    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11172     |
|    fps              | 291       |
|    time_elapsed     | 1540      |
|    total_timesteps  | 448976    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 102243    |
-----------------------------------
Eval num_timesteps=449000, episode_reward=-2982.00 +/- 999.13
Episode length: 33.92 +/- 6.07
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 449000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.3      |
|    n_updates        | 102249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11176     |
|    fps              | 291       |
|    time_elapsed     | 1541      |
|    total_timesteps  | 449115    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 102278    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11180     |
|    fps              | 291       |
|    time_elapsed     | 1541      |
|    total_timesteps  | 449256    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 199       |
|    n_updates        | 102313    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11184     |
|    fps              | 291       |
|    time_elapsed     | 1541      |
|    total_timesteps  | 449386    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 102346    |
-----------------------------------
Eval num_timesteps=449500, episode_reward=-2946.58 +/- 1059.03
Episode length: 33.24 +/- 7.55
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.2      |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 449500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 102374    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.8     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11188    |
|    fps              | 291      |
|    time_elapsed     | 1543     |
|    total_timesteps  | 449544   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 82.6     |
|    n_updates        | 102385   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11192     |
|    fps              | 291       |
|    time_elapsed     | 1543      |
|    total_timesteps  | 449676    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 102418    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11196     |
|    fps              | 291       |
|    time_elapsed     | 1543      |
|    total_timesteps  | 449833    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.1      |
|    n_updates        | 102458    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11200     |
|    fps              | 291       |
|    time_elapsed     | 1543      |
|    total_timesteps  | 449960    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 102489    |
-----------------------------------
Eval num_timesteps=450000, episode_reward=-2817.36 +/- 1057.23
Episode length: 35.40 +/- 6.09
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 450000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 237       |
|    n_updates        | 102499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11204     |
|    fps              | 291       |
|    time_elapsed     | 1545      |
|    total_timesteps  | 450094    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 39.4      |
|    n_updates        | 102523    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11208     |
|    fps              | 291       |
|    time_elapsed     | 1545      |
|    total_timesteps  | 450239    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.7      |
|    n_updates        | 102559    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11212     |
|    fps              | 291       |
|    time_elapsed     | 1545      |
|    total_timesteps  | 450350    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 102587    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11216     |
|    fps              | 291       |
|    time_elapsed     | 1545      |
|    total_timesteps  | 450484    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 74.2      |
|    n_updates        | 102620    |
-----------------------------------
Eval num_timesteps=450500, episode_reward=-2568.34 +/- 1244.11
Episode length: 35.70 +/- 6.44
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.57e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 450500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 102624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11220     |
|    fps              | 291       |
|    time_elapsed     | 1546      |
|    total_timesteps  | 450628    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 102656    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11224     |
|    fps              | 291       |
|    time_elapsed     | 1547      |
|    total_timesteps  | 450755    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 194       |
|    n_updates        | 102688    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11228     |
|    fps              | 291       |
|    time_elapsed     | 1547      |
|    total_timesteps  | 450897    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 102724    |
-----------------------------------
Eval num_timesteps=451000, episode_reward=-2842.76 +/- 1000.55
Episode length: 35.34 +/- 6.91
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 451000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.1      |
|    n_updates        | 102749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11232     |
|    fps              | 291       |
|    time_elapsed     | 1548      |
|    total_timesteps  | 451034    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 215       |
|    n_updates        | 102758    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11236     |
|    fps              | 291       |
|    time_elapsed     | 1548      |
|    total_timesteps  | 451180    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 102794    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11240     |
|    fps              | 291       |
|    time_elapsed     | 1548      |
|    total_timesteps  | 451320    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 102829    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11244     |
|    fps              | 291       |
|    time_elapsed     | 1549      |
|    total_timesteps  | 451483    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 102870    |
-----------------------------------
Eval num_timesteps=451500, episode_reward=-2682.52 +/- 1034.20
Episode length: 36.36 +/- 6.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 451500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 162       |
|    n_updates        | 102874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11248     |
|    fps              | 291       |
|    time_elapsed     | 1550      |
|    total_timesteps  | 451622    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 102905    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11252     |
|    fps              | 291       |
|    time_elapsed     | 1550      |
|    total_timesteps  | 451773    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 180       |
|    n_updates        | 102943    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.2     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11256    |
|    fps              | 291      |
|    time_elapsed     | 1550     |
|    total_timesteps  | 451929   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 208      |
|    n_updates        | 102982   |
----------------------------------
Eval num_timesteps=452000, episode_reward=-2999.68 +/- 966.69
Episode length: 34.68 +/- 6.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.7     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 452000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 175      |
|    n_updates        | 102999   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11260     |
|    fps              | 291       |
|    time_elapsed     | 1552      |
|    total_timesteps  | 452076    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 103018    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11264     |
|    fps              | 291       |
|    time_elapsed     | 1552      |
|    total_timesteps  | 452229    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 103057    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11268     |
|    fps              | 291       |
|    time_elapsed     | 1552      |
|    total_timesteps  | 452363    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 103090    |
-----------------------------------
Eval num_timesteps=452500, episode_reward=-2818.19 +/- 1020.06
Episode length: 35.86 +/- 6.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 452500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 103124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11272     |
|    fps              | 291       |
|    time_elapsed     | 1553      |
|    total_timesteps  | 452515    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 212       |
|    n_updates        | 103128    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11276     |
|    fps              | 291       |
|    time_elapsed     | 1554      |
|    total_timesteps  | 452665    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.9      |
|    n_updates        | 103166    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11280     |
|    fps              | 291       |
|    time_elapsed     | 1554      |
|    total_timesteps  | 452799    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.4      |
|    n_updates        | 103199    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11284     |
|    fps              | 291       |
|    time_elapsed     | 1554      |
|    total_timesteps  | 452945    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.3      |
|    n_updates        | 103236    |
-----------------------------------
Eval num_timesteps=453000, episode_reward=-2771.42 +/- 1282.75
Episode length: 36.38 +/- 5.04
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 453000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 103249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11288     |
|    fps              | 291       |
|    time_elapsed     | 1555      |
|    total_timesteps  | 453066    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 103266    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11292     |
|    fps              | 291       |
|    time_elapsed     | 1555      |
|    total_timesteps  | 453212    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 103302    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11296     |
|    fps              | 291       |
|    time_elapsed     | 1556      |
|    total_timesteps  | 453366    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 103341    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11300     |
|    fps              | 291       |
|    time_elapsed     | 1556      |
|    total_timesteps  | 453493    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 103373    |
-----------------------------------
Eval num_timesteps=453500, episode_reward=-2766.85 +/- 1335.51
Episode length: 35.28 +/- 7.34
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 453500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 103374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11304     |
|    fps              | 291       |
|    time_elapsed     | 1557      |
|    total_timesteps  | 453629    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 103407    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11308     |
|    fps              | 291       |
|    time_elapsed     | 1557      |
|    total_timesteps  | 453779    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 103444    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11312     |
|    fps              | 291       |
|    time_elapsed     | 1557      |
|    total_timesteps  | 453921    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 103480    |
-----------------------------------
Eval num_timesteps=454000, episode_reward=-2684.73 +/- 1254.03
Episode length: 35.04 +/- 7.76
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 454000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 103499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11316     |
|    fps              | 291       |
|    time_elapsed     | 1559      |
|    total_timesteps  | 454048    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 226       |
|    n_updates        | 103511    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11320     |
|    fps              | 291       |
|    time_elapsed     | 1559      |
|    total_timesteps  | 454181    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 173       |
|    n_updates        | 103545    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11324     |
|    fps              | 291       |
|    time_elapsed     | 1559      |
|    total_timesteps  | 454297    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 103574    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11328     |
|    fps              | 291       |
|    time_elapsed     | 1559      |
|    total_timesteps  | 454419    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 103604    |
-----------------------------------
Eval num_timesteps=454500, episode_reward=-2766.46 +/- 1045.41
Episode length: 36.48 +/- 6.33
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.5      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 454500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 235       |
|    n_updates        | 103624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11332     |
|    fps              | 291       |
|    time_elapsed     | 1561      |
|    total_timesteps  | 454572    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.2      |
|    n_updates        | 103642    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11336     |
|    fps              | 291       |
|    time_elapsed     | 1561      |
|    total_timesteps  | 454703    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 103675    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11340     |
|    fps              | 291       |
|    time_elapsed     | 1561      |
|    total_timesteps  | 454803    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 116       |
|    n_updates        | 103700    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.7     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11344    |
|    fps              | 291      |
|    time_elapsed     | 1561     |
|    total_timesteps  | 454949   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 202      |
|    n_updates        | 103737   |
----------------------------------
Eval num_timesteps=455000, episode_reward=-2749.50 +/- 1193.18
Episode length: 35.38 +/- 6.40
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.75e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 455000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 103749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11348     |
|    fps              | 291       |
|    time_elapsed     | 1563      |
|    total_timesteps  | 455121    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 103780    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11352     |
|    fps              | 291       |
|    time_elapsed     | 1563      |
|    total_timesteps  | 455264    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 103815    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11356     |
|    fps              | 291       |
|    time_elapsed     | 1563      |
|    total_timesteps  | 455377    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 103844    |
-----------------------------------
Eval num_timesteps=455500, episode_reward=-2519.92 +/- 1444.48
Episode length: 35.44 +/- 6.55
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.52e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 455500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 103874    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.5     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11360    |
|    fps              | 291      |
|    time_elapsed     | 1564     |
|    total_timesteps  | 455530   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 183      |
|    n_updates        | 103882   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11364     |
|    fps              | 291       |
|    time_elapsed     | 1564      |
|    total_timesteps  | 455682    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 103920    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11368     |
|    fps              | 291       |
|    time_elapsed     | 1565      |
|    total_timesteps  | 455839    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 103959    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11372     |
|    fps              | 291       |
|    time_elapsed     | 1565      |
|    total_timesteps  | 455984    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.2      |
|    n_updates        | 103995    |
-----------------------------------
Eval num_timesteps=456000, episode_reward=-3013.92 +/- 1087.32
Episode length: 35.16 +/- 6.41
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 456000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 236       |
|    n_updates        | 103999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11376     |
|    fps              | 291       |
|    time_elapsed     | 1566      |
|    total_timesteps  | 456105    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 104026    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11380     |
|    fps              | 291       |
|    time_elapsed     | 1566      |
|    total_timesteps  | 456245    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.3      |
|    n_updates        | 104061    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11384     |
|    fps              | 291       |
|    time_elapsed     | 1566      |
|    total_timesteps  | 456375    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 104093    |
-----------------------------------
Eval num_timesteps=456500, episode_reward=-2598.02 +/- 1230.32
Episode length: 34.96 +/- 6.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35       |
|    mean_reward      | -2.6e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 456500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 84.7     |
|    n_updates        | 104124   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11388     |
|    fps              | 291       |
|    time_elapsed     | 1568      |
|    total_timesteps  | 456516    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.3      |
|    n_updates        | 104128    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11392     |
|    fps              | 291       |
|    time_elapsed     | 1568      |
|    total_timesteps  | 456648    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 104161    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11396     |
|    fps              | 291       |
|    time_elapsed     | 1568      |
|    total_timesteps  | 456792    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 104197    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11400     |
|    fps              | 291       |
|    time_elapsed     | 1568      |
|    total_timesteps  | 456927    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 104231    |
-----------------------------------
Eval num_timesteps=457000, episode_reward=-2675.64 +/- 1249.83
Episode length: 37.26 +/- 5.13
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.3      |
|    mean_reward      | -2.68e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 457000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 94.8      |
|    n_updates        | 104249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11404     |
|    fps              | 291       |
|    time_elapsed     | 1570      |
|    total_timesteps  | 457082    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 104270    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.3     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11408    |
|    fps              | 291      |
|    time_elapsed     | 1570     |
|    total_timesteps  | 457208   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 155      |
|    n_updates        | 104301   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11412     |
|    fps              | 291       |
|    time_elapsed     | 1570      |
|    total_timesteps  | 457346    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 192       |
|    n_updates        | 104336    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11416     |
|    fps              | 291       |
|    time_elapsed     | 1570      |
|    total_timesteps  | 457499    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.8      |
|    n_updates        | 104374    |
-----------------------------------
Eval num_timesteps=457500, episode_reward=-2968.48 +/- 1074.51
Episode length: 34.26 +/- 6.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 34.3      |
|    mean_reward     | -2.97e+03 |
| time/              |           |
|    total_timesteps | 457500    |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11420     |
|    fps              | 291       |
|    time_elapsed     | 1572      |
|    total_timesteps  | 457659    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 104414    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11424     |
|    fps              | 291       |
|    time_elapsed     | 1572      |
|    total_timesteps  | 457823    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 104455    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11428     |
|    fps              | 291       |
|    time_elapsed     | 1572      |
|    total_timesteps  | 457973    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 104493    |
-----------------------------------
Eval num_timesteps=458000, episode_reward=-3044.72 +/- 994.06
Episode length: 33.92 +/- 6.56
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -3.04e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 458000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 104499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11432     |
|    fps              | 291       |
|    time_elapsed     | 1573      |
|    total_timesteps  | 458124    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 136       |
|    n_updates        | 104530    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11436     |
|    fps              | 291       |
|    time_elapsed     | 1573      |
|    total_timesteps  | 458256    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 180       |
|    n_updates        | 104563    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11440     |
|    fps              | 291       |
|    time_elapsed     | 1574      |
|    total_timesteps  | 458407    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 104601    |
-----------------------------------
Eval num_timesteps=458500, episode_reward=-2942.77 +/- 1203.03
Episode length: 35.58 +/- 6.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.94e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 458500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 104624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11444     |
|    fps              | 291       |
|    time_elapsed     | 1575      |
|    total_timesteps  | 458548    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88        |
|    n_updates        | 104636    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11448     |
|    fps              | 291       |
|    time_elapsed     | 1575      |
|    total_timesteps  | 458691    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 104672    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11452     |
|    fps              | 291       |
|    time_elapsed     | 1575      |
|    total_timesteps  | 458848    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.5      |
|    n_updates        | 104711    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11456     |
|    fps              | 291       |
|    time_elapsed     | 1576      |
|    total_timesteps  | 458979    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 85.3      |
|    n_updates        | 104744    |
-----------------------------------
Eval num_timesteps=459000, episode_reward=-2759.66 +/- 1230.48
Episode length: 34.98 +/- 7.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.76e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 459000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 104749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11460     |
|    fps              | 291       |
|    time_elapsed     | 1577      |
|    total_timesteps  | 459093    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 125       |
|    n_updates        | 104773    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11464     |
|    fps              | 291       |
|    time_elapsed     | 1577      |
|    total_timesteps  | 459217    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 104804    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11468     |
|    fps              | 291       |
|    time_elapsed     | 1577      |
|    total_timesteps  | 459322    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 104830    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11472     |
|    fps              | 291       |
|    time_elapsed     | 1577      |
|    total_timesteps  | 459449    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 104862    |
-----------------------------------
Eval num_timesteps=459500, episode_reward=-2473.09 +/- 1452.51
Episode length: 35.66 +/- 6.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.47e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 459500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 104874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11476     |
|    fps              | 291       |
|    time_elapsed     | 1579      |
|    total_timesteps  | 459598    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 104899    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11480     |
|    fps              | 291       |
|    time_elapsed     | 1579      |
|    total_timesteps  | 459746    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 104936    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11484     |
|    fps              | 291       |
|    time_elapsed     | 1579      |
|    total_timesteps  | 459883    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 104970    |
-----------------------------------
Eval num_timesteps=460000, episode_reward=-2474.19 +/- 1438.00
Episode length: 36.08 +/- 6.10
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.47e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 460000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 104999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11488     |
|    fps              | 290       |
|    time_elapsed     | 1580      |
|    total_timesteps  | 460019    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 208       |
|    n_updates        | 105004    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11492     |
|    fps              | 291       |
|    time_elapsed     | 1581      |
|    total_timesteps  | 460182    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 105045    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11496     |
|    fps              | 291       |
|    time_elapsed     | 1581      |
|    total_timesteps  | 460294    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 108       |
|    n_updates        | 105073    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11500     |
|    fps              | 291       |
|    time_elapsed     | 1581      |
|    total_timesteps  | 460449    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 105112    |
-----------------------------------
Eval num_timesteps=460500, episode_reward=-3102.78 +/- 935.37
Episode length: 32.94 +/- 7.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 32.9     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 460500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 79.2     |
|    n_updates        | 105124   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11504     |
|    fps              | 291       |
|    time_elapsed     | 1582      |
|    total_timesteps  | 460599    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 105149    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11508     |
|    fps              | 291       |
|    time_elapsed     | 1582      |
|    total_timesteps  | 460760    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 105189    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11512     |
|    fps              | 291       |
|    time_elapsed     | 1582      |
|    total_timesteps  | 460908    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 105226    |
-----------------------------------
Eval num_timesteps=461000, episode_reward=-2921.73 +/- 1155.12
Episode length: 34.60 +/- 7.27
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 461000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.5      |
|    n_updates        | 105249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11516     |
|    fps              | 290       |
|    time_elapsed     | 1584      |
|    total_timesteps  | 461040    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 105259    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11520     |
|    fps              | 291       |
|    time_elapsed     | 1584      |
|    total_timesteps  | 461176    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 214       |
|    n_updates        | 105293    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11524     |
|    fps              | 291       |
|    time_elapsed     | 1584      |
|    total_timesteps  | 461310    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 217       |
|    n_updates        | 105327    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11528     |
|    fps              | 291       |
|    time_elapsed     | 1584      |
|    total_timesteps  | 461438    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 201       |
|    n_updates        | 105359    |
-----------------------------------
Eval num_timesteps=461500, episode_reward=-2727.17 +/- 1092.12
Episode length: 34.88 +/- 6.88
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 461500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 238       |
|    n_updates        | 105374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11532     |
|    fps              | 291       |
|    time_elapsed     | 1586      |
|    total_timesteps  | 461569    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70.9      |
|    n_updates        | 105392    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11536     |
|    fps              | 291       |
|    time_elapsed     | 1586      |
|    total_timesteps  | 461700    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 266       |
|    n_updates        | 105424    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11540     |
|    fps              | 291       |
|    time_elapsed     | 1586      |
|    total_timesteps  | 461845    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 163       |
|    n_updates        | 105461    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11544     |
|    fps              | 291       |
|    time_elapsed     | 1586      |
|    total_timesteps  | 461997    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 295       |
|    n_updates        | 105499    |
-----------------------------------
Eval num_timesteps=462000, episode_reward=-2464.98 +/- 1370.22
Episode length: 37.32 +/- 5.72
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.3      |
|    mean_reward      | -2.46e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 462000    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.4     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11548    |
|    fps              | 291      |
|    time_elapsed     | 1588     |
|    total_timesteps  | 462133   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 181      |
|    n_updates        | 105533   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11552     |
|    fps              | 291       |
|    time_elapsed     | 1588      |
|    total_timesteps  | 462296    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.5      |
|    n_updates        | 105573    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11556     |
|    fps              | 291       |
|    time_elapsed     | 1588      |
|    total_timesteps  | 462431    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 222       |
|    n_updates        | 105607    |
-----------------------------------
Eval num_timesteps=462500, episode_reward=-3169.12 +/- 794.72
Episode length: 34.26 +/- 6.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -3.17e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 462500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 105624    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.8     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11560    |
|    fps              | 290      |
|    time_elapsed     | 1589     |
|    total_timesteps  | 462570   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 95.6     |
|    n_updates        | 105642   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11564     |
|    fps              | 291       |
|    time_elapsed     | 1589      |
|    total_timesteps  | 462725    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 105681    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11568     |
|    fps              | 291       |
|    time_elapsed     | 1590      |
|    total_timesteps  | 462869    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.1      |
|    n_updates        | 105717    |
-----------------------------------
Eval num_timesteps=463000, episode_reward=-2738.84 +/- 1183.88
Episode length: 36.20 +/- 6.00
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.2      |
|    mean_reward      | -2.74e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 463000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 217       |
|    n_updates        | 105749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11572     |
|    fps              | 290       |
|    time_elapsed     | 1591      |
|    total_timesteps  | 463026    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 105756    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11576     |
|    fps              | 290       |
|    time_elapsed     | 1591      |
|    total_timesteps  | 463167    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 189       |
|    n_updates        | 105791    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11580     |
|    fps              | 291       |
|    time_elapsed     | 1591      |
|    total_timesteps  | 463310    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 105827    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11584     |
|    fps              | 291       |
|    time_elapsed     | 1591      |
|    total_timesteps  | 463454    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.5      |
|    n_updates        | 105863    |
-----------------------------------
Eval num_timesteps=463500, episode_reward=-2974.91 +/- 1154.71
Episode length: 34.24 +/- 7.70
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 463500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 105874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11588     |
|    fps              | 290       |
|    time_elapsed     | 1593      |
|    total_timesteps  | 463606    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 105901    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11592     |
|    fps              | 291       |
|    time_elapsed     | 1593      |
|    total_timesteps  | 463740    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.7      |
|    n_updates        | 105934    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11596     |
|    fps              | 291       |
|    time_elapsed     | 1593      |
|    total_timesteps  | 463883    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 135       |
|    n_updates        | 105970    |
-----------------------------------
Eval num_timesteps=464000, episode_reward=-2511.72 +/- 1214.44
Episode length: 35.78 +/- 7.21
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.51e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 464000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 105999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11600     |
|    fps              | 290       |
|    time_elapsed     | 1595      |
|    total_timesteps  | 464029    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 106007    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11604     |
|    fps              | 290       |
|    time_elapsed     | 1595      |
|    total_timesteps  | 464183    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 213       |
|    n_updates        | 106045    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11608     |
|    fps              | 291       |
|    time_elapsed     | 1595      |
|    total_timesteps  | 464321    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66        |
|    n_updates        | 106080    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11612     |
|    fps              | 291       |
|    time_elapsed     | 1595      |
|    total_timesteps  | 464468    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 106116    |
-----------------------------------
Eval num_timesteps=464500, episode_reward=-3106.11 +/- 877.68
Episode length: 33.80 +/- 6.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -3.11e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 464500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 106124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11616     |
|    fps              | 290       |
|    time_elapsed     | 1596      |
|    total_timesteps  | 464614    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.1      |
|    n_updates        | 106153    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11620     |
|    fps              | 291       |
|    time_elapsed     | 1597      |
|    total_timesteps  | 464753    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.4      |
|    n_updates        | 106188    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11624     |
|    fps              | 291       |
|    time_elapsed     | 1597      |
|    total_timesteps  | 464881    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 106220    |
-----------------------------------
Eval num_timesteps=465000, episode_reward=-2692.66 +/- 1134.43
Episode length: 34.64 +/- 7.35
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.69e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 465000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 106249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11628     |
|    fps              | 290       |
|    time_elapsed     | 1598      |
|    total_timesteps  | 465049    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 235       |
|    n_updates        | 106262    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11632     |
|    fps              | 290       |
|    time_elapsed     | 1598      |
|    total_timesteps  | 465181    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 97.2      |
|    n_updates        | 106295    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11636     |
|    fps              | 291       |
|    time_elapsed     | 1598      |
|    total_timesteps  | 465319    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 212       |
|    n_updates        | 106329    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11640     |
|    fps              | 291       |
|    time_elapsed     | 1599      |
|    total_timesteps  | 465467    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 106366    |
-----------------------------------
Eval num_timesteps=465500, episode_reward=-2897.40 +/- 1003.66
Episode length: 35.60 +/- 6.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.6     |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 465500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 85       |
|    n_updates        | 106374   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11644     |
|    fps              | 290       |
|    time_elapsed     | 1600      |
|    total_timesteps  | 465616    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 106403    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11648     |
|    fps              | 290       |
|    time_elapsed     | 1600      |
|    total_timesteps  | 465754    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 106438    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11652     |
|    fps              | 291       |
|    time_elapsed     | 1600      |
|    total_timesteps  | 465904    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.2      |
|    n_updates        | 106475    |
-----------------------------------
Eval num_timesteps=466000, episode_reward=-2975.00 +/- 1035.73
Episode length: 34.90 +/- 6.84
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 466000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 106499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11656     |
|    fps              | 290       |
|    time_elapsed     | 1602      |
|    total_timesteps  | 466030    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 106507    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11660     |
|    fps              | 290       |
|    time_elapsed     | 1602      |
|    total_timesteps  | 466164    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 106540    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11664    |
|    fps              | 291      |
|    time_elapsed     | 1602     |
|    total_timesteps  | 466329   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 203      |
|    n_updates        | 106582   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11668     |
|    fps              | 291       |
|    time_elapsed     | 1602      |
|    total_timesteps  | 466473    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.4      |
|    n_updates        | 106618    |
-----------------------------------
Eval num_timesteps=466500, episode_reward=-2807.11 +/- 977.06
Episode length: 35.64 +/- 6.66
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 466500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 178       |
|    n_updates        | 106624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11672     |
|    fps              | 290       |
|    time_elapsed     | 1604      |
|    total_timesteps  | 466625    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 106656    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11676     |
|    fps              | 290       |
|    time_elapsed     | 1604      |
|    total_timesteps  | 466765    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 78.2      |
|    n_updates        | 106691    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11680     |
|    fps              | 291       |
|    time_elapsed     | 1604      |
|    total_timesteps  | 466871    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 106717    |
-----------------------------------
Eval num_timesteps=467000, episode_reward=-2917.08 +/- 1263.17
Episode length: 33.80 +/- 7.73
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 467000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 159       |
|    n_updates        | 106749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11684     |
|    fps              | 290       |
|    time_elapsed     | 1605      |
|    total_timesteps  | 467005    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 106751    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11688     |
|    fps              | 290       |
|    time_elapsed     | 1605      |
|    total_timesteps  | 467150    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.9      |
|    n_updates        | 106787    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11692     |
|    fps              | 290       |
|    time_elapsed     | 1605      |
|    total_timesteps  | 467311    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 111       |
|    n_updates        | 106827    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11696     |
|    fps              | 291       |
|    time_elapsed     | 1606      |
|    total_timesteps  | 467466    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 180       |
|    n_updates        | 106866    |
-----------------------------------
Eval num_timesteps=467500, episode_reward=-2898.97 +/- 1315.08
Episode length: 34.04 +/- 7.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34       |
|    mean_reward      | -2.9e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 467500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 88.5     |
|    n_updates        | 106874   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11700     |
|    fps              | 290       |
|    time_elapsed     | 1607      |
|    total_timesteps  | 467611    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 106902    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11704     |
|    fps              | 290       |
|    time_elapsed     | 1607      |
|    total_timesteps  | 467768    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 106941    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11708     |
|    fps              | 291       |
|    time_elapsed     | 1607      |
|    total_timesteps  | 467901    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 217       |
|    n_updates        | 106975    |
-----------------------------------
Eval num_timesteps=468000, episode_reward=-2459.59 +/- 1480.77
Episode length: 35.20 +/- 6.97
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.46e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 468000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 106999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11712     |
|    fps              | 290       |
|    time_elapsed     | 1609      |
|    total_timesteps  | 468017    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.8      |
|    n_updates        | 107004    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11716     |
|    fps              | 290       |
|    time_elapsed     | 1609      |
|    total_timesteps  | 468153    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 107038    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11720     |
|    fps              | 290       |
|    time_elapsed     | 1609      |
|    total_timesteps  | 468321    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 107080    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11724     |
|    fps              | 291       |
|    time_elapsed     | 1609      |
|    total_timesteps  | 468468    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 127       |
|    n_updates        | 107116    |
-----------------------------------
Eval num_timesteps=468500, episode_reward=-2414.50 +/- 1354.57
Episode length: 37.88 +/- 6.29
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.9      |
|    mean_reward      | -2.41e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 468500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 107124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11728     |
|    fps              | 290       |
|    time_elapsed     | 1611      |
|    total_timesteps  | 468600    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 309       |
|    n_updates        | 107149    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.5     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11732    |
|    fps              | 290      |
|    time_elapsed     | 1611     |
|    total_timesteps  | 468733   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 45       |
|    n_updates        | 107183   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11736     |
|    fps              | 290       |
|    time_elapsed     | 1611      |
|    total_timesteps  | 468873    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.6      |
|    n_updates        | 107218    |
-----------------------------------
Eval num_timesteps=469000, episode_reward=-2717.51 +/- 1025.84
Episode length: 36.62 +/- 5.93
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.6      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 469000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 204       |
|    n_updates        | 107249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11740     |
|    fps              | 290       |
|    time_elapsed     | 1612      |
|    total_timesteps  | 469001    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 115       |
|    n_updates        | 107250    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.4     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11744    |
|    fps              | 290      |
|    time_elapsed     | 1612     |
|    total_timesteps  | 469155   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 188      |
|    n_updates        | 107288   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11748     |
|    fps              | 290       |
|    time_elapsed     | 1613      |
|    total_timesteps  | 469313    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 107328    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11752     |
|    fps              | 290       |
|    time_elapsed     | 1613      |
|    total_timesteps  | 469473    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 107368    |
-----------------------------------
Eval num_timesteps=469500, episode_reward=-2821.60 +/- 1105.06
Episode length: 34.42 +/- 6.85
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 469500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 146       |
|    n_updates        | 107374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11756     |
|    fps              | 290       |
|    time_elapsed     | 1614      |
|    total_timesteps  | 469595    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 107398    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.56e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11760     |
|    fps              | 290       |
|    time_elapsed     | 1614      |
|    total_timesteps  | 469765    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 107441    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11764     |
|    fps              | 290       |
|    time_elapsed     | 1615      |
|    total_timesteps  | 469894    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 107473    |
-----------------------------------
Eval num_timesteps=470000, episode_reward=-2837.97 +/- 1194.89
Episode length: 34.44 +/- 6.24
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 470000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 107499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11768     |
|    fps              | 290       |
|    time_elapsed     | 1616      |
|    total_timesteps  | 470048    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 107511    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11772     |
|    fps              | 290       |
|    time_elapsed     | 1616      |
|    total_timesteps  | 470195    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 169       |
|    n_updates        | 107548    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11776     |
|    fps              | 290       |
|    time_elapsed     | 1616      |
|    total_timesteps  | 470319    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 107579    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11780     |
|    fps              | 290       |
|    time_elapsed     | 1616      |
|    total_timesteps  | 470467    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 107616    |
-----------------------------------
Eval num_timesteps=470500, episode_reward=-3048.55 +/- 1146.03
Episode length: 34.74 +/- 6.96
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -3.05e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 470500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.8      |
|    n_updates        | 107624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11784     |
|    fps              | 290       |
|    time_elapsed     | 1618      |
|    total_timesteps  | 470581    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 107645    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11788     |
|    fps              | 290       |
|    time_elapsed     | 1618      |
|    total_timesteps  | 470714    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 107678    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11792     |
|    fps              | 290       |
|    time_elapsed     | 1618      |
|    total_timesteps  | 470842    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 107710    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11796     |
|    fps              | 290       |
|    time_elapsed     | 1618      |
|    total_timesteps  | 470974    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 20.2      |
|    n_updates        | 107743    |
-----------------------------------
Eval num_timesteps=471000, episode_reward=-2975.81 +/- 1172.68
Episode length: 34.16 +/- 7.18
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 471000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 40        |
|    n_updates        | 107749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11800     |
|    fps              | 290       |
|    time_elapsed     | 1619      |
|    total_timesteps  | 471129    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 186       |
|    n_updates        | 107782    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11804     |
|    fps              | 290       |
|    time_elapsed     | 1620      |
|    total_timesteps  | 471259    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 107814    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11808     |
|    fps              | 290       |
|    time_elapsed     | 1620      |
|    total_timesteps  | 471394    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 107848    |
-----------------------------------
Eval num_timesteps=471500, episode_reward=-2511.28 +/- 1473.45
Episode length: 37.48 +/- 5.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.5      |
|    mean_reward      | -2.51e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 471500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 107874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11812     |
|    fps              | 290       |
|    time_elapsed     | 1621      |
|    total_timesteps  | 471536    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 232       |
|    n_updates        | 107883    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.59e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11816     |
|    fps              | 290       |
|    time_elapsed     | 1621      |
|    total_timesteps  | 471687    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 134       |
|    n_updates        | 107921    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11820     |
|    fps              | 290       |
|    time_elapsed     | 1622      |
|    total_timesteps  | 471811    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 107952    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11824     |
|    fps              | 290       |
|    time_elapsed     | 1622      |
|    total_timesteps  | 471929    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 105       |
|    n_updates        | 107982    |
-----------------------------------
Eval num_timesteps=472000, episode_reward=-2771.47 +/- 1169.41
Episode length: 35.24 +/- 6.04
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 472000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.9      |
|    n_updates        | 107999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11828     |
|    fps              | 290       |
|    time_elapsed     | 1623      |
|    total_timesteps  | 472068    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.1      |
|    n_updates        | 108016    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11832     |
|    fps              | 290       |
|    time_elapsed     | 1623      |
|    total_timesteps  | 472224    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.8      |
|    n_updates        | 108055    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11836     |
|    fps              | 290       |
|    time_elapsed     | 1623      |
|    total_timesteps  | 472337    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 108084    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11840     |
|    fps              | 290       |
|    time_elapsed     | 1624      |
|    total_timesteps  | 472487    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 149       |
|    n_updates        | 108121    |
-----------------------------------
Eval num_timesteps=472500, episode_reward=-2610.33 +/- 1424.87
Episode length: 34.94 +/- 6.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.9      |
|    mean_reward      | -2.61e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 472500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64.4      |
|    n_updates        | 108124    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.8     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11844    |
|    fps              | 290      |
|    time_elapsed     | 1625     |
|    total_timesteps  | 472639   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 131      |
|    n_updates        | 108159   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11848     |
|    fps              | 290       |
|    time_elapsed     | 1625      |
|    total_timesteps  | 472765    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 108191    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.6     |
|    ep_rew_mean      | -2.7e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11852    |
|    fps              | 290      |
|    time_elapsed     | 1625     |
|    total_timesteps  | 472931   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 113      |
|    n_updates        | 108232   |
----------------------------------
Eval num_timesteps=473000, episode_reward=-2774.40 +/- 1177.06
Episode length: 35.06 +/- 6.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.77e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 473000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.3      |
|    n_updates        | 108249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11856     |
|    fps              | 290       |
|    time_elapsed     | 1627      |
|    total_timesteps  | 473056    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 61.4      |
|    n_updates        | 108263    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11860     |
|    fps              | 290       |
|    time_elapsed     | 1627      |
|    total_timesteps  | 473202    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 210       |
|    n_updates        | 108300    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11864     |
|    fps              | 290       |
|    time_elapsed     | 1627      |
|    total_timesteps  | 473340    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 60.4      |
|    n_updates        | 108334    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11868     |
|    fps              | 290       |
|    time_elapsed     | 1627      |
|    total_timesteps  | 473470    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 301       |
|    n_updates        | 108367    |
-----------------------------------
Eval num_timesteps=473500, episode_reward=-2782.27 +/- 1172.47
Episode length: 35.92 +/- 6.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 473500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 108374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11872     |
|    fps              | 290       |
|    time_elapsed     | 1629      |
|    total_timesteps  | 473609    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 108402    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11876     |
|    fps              | 290       |
|    time_elapsed     | 1629      |
|    total_timesteps  | 473760    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 108439    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11880     |
|    fps              | 290       |
|    time_elapsed     | 1629      |
|    total_timesteps  | 473895    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 108473    |
-----------------------------------
Eval num_timesteps=474000, episode_reward=-2435.18 +/- 1315.37
Episode length: 36.08 +/- 6.90
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.44e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 474000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 181       |
|    n_updates        | 108499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11884     |
|    fps              | 290       |
|    time_elapsed     | 1630      |
|    total_timesteps  | 474040    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 108509    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11888     |
|    fps              | 290       |
|    time_elapsed     | 1630      |
|    total_timesteps  | 474190    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 108547    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11892     |
|    fps              | 290       |
|    time_elapsed     | 1631      |
|    total_timesteps  | 474330    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 108582    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11896     |
|    fps              | 290       |
|    time_elapsed     | 1631      |
|    total_timesteps  | 474489    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 197       |
|    n_updates        | 108622    |
-----------------------------------
Eval num_timesteps=474500, episode_reward=-2637.06 +/- 1179.23
Episode length: 36.28 +/- 6.71
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.64e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 474500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 108624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11900     |
|    fps              | 290       |
|    time_elapsed     | 1632      |
|    total_timesteps  | 474642    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 191       |
|    n_updates        | 108660    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11904     |
|    fps              | 290       |
|    time_elapsed     | 1632      |
|    total_timesteps  | 474765    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 108691    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11908     |
|    fps              | 290       |
|    time_elapsed     | 1633      |
|    total_timesteps  | 474919    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 108729    |
-----------------------------------
Eval num_timesteps=475000, episode_reward=-2776.33 +/- 1215.21
Episode length: 34.96 +/- 6.60
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35        |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 475000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 100       |
|    n_updates        | 108749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11912     |
|    fps              | 290       |
|    time_elapsed     | 1634      |
|    total_timesteps  | 475051    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90.5      |
|    n_updates        | 108762    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.2     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11916    |
|    fps              | 290      |
|    time_elapsed     | 1634     |
|    total_timesteps  | 475207   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 156      |
|    n_updates        | 108801   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11920     |
|    fps              | 290       |
|    time_elapsed     | 1634      |
|    total_timesteps  | 475346    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 108836    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11924     |
|    fps              | 290       |
|    time_elapsed     | 1634      |
|    total_timesteps  | 475493    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 108873    |
-----------------------------------
Eval num_timesteps=475500, episode_reward=-2791.86 +/- 1014.67
Episode length: 36.00 +/- 7.01
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.79e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 475500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 108874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11928     |
|    fps              | 290       |
|    time_elapsed     | 1636      |
|    total_timesteps  | 475643    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 108910    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11932     |
|    fps              | 290       |
|    time_elapsed     | 1636      |
|    total_timesteps  | 475791    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 108947    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11936     |
|    fps              | 290       |
|    time_elapsed     | 1636      |
|    total_timesteps  | 475912    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 64        |
|    n_updates        | 108977    |
-----------------------------------
Eval num_timesteps=476000, episode_reward=-2712.06 +/- 1047.52
Episode length: 35.26 +/- 6.54
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.3      |
|    mean_reward      | -2.71e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 476000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 152       |
|    n_updates        | 108999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11940     |
|    fps              | 290       |
|    time_elapsed     | 1638      |
|    total_timesteps  | 476034    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 109008    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11944     |
|    fps              | 290       |
|    time_elapsed     | 1638      |
|    total_timesteps  | 476179    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 109044    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.6     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11948    |
|    fps              | 290      |
|    time_elapsed     | 1638     |
|    total_timesteps  | 476328   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 156      |
|    n_updates        | 109081   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11952     |
|    fps              | 290       |
|    time_elapsed     | 1638      |
|    total_timesteps  | 476473    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 195       |
|    n_updates        | 109118    |
-----------------------------------
Eval num_timesteps=476500, episode_reward=-2733.35 +/- 1046.07
Episode length: 35.40 +/- 6.65
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.4      |
|    mean_reward      | -2.73e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 476500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 199       |
|    n_updates        | 109124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11956     |
|    fps              | 290       |
|    time_elapsed     | 1640      |
|    total_timesteps  | 476607    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 109151    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11960     |
|    fps              | 290       |
|    time_elapsed     | 1640      |
|    total_timesteps  | 476753    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 109188    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11964     |
|    fps              | 290       |
|    time_elapsed     | 1640      |
|    total_timesteps  | 476905    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.6      |
|    n_updates        | 109226    |
-----------------------------------
Eval num_timesteps=477000, episode_reward=-3167.61 +/- 852.11
Episode length: 33.90 +/- 6.62
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.9      |
|    mean_reward      | -3.17e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 477000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 175       |
|    n_updates        | 109249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11968     |
|    fps              | 290       |
|    time_elapsed     | 1641      |
|    total_timesteps  | 477021    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 109255    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11972     |
|    fps              | 290       |
|    time_elapsed     | 1641      |
|    total_timesteps  | 477176    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 194       |
|    n_updates        | 109293    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11976     |
|    fps              | 290       |
|    time_elapsed     | 1642      |
|    total_timesteps  | 477317    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 109329    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11980     |
|    fps              | 290       |
|    time_elapsed     | 1642      |
|    total_timesteps  | 477463    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 109365    |
-----------------------------------
Eval num_timesteps=477500, episode_reward=-2860.92 +/- 1100.67
Episode length: 36.36 +/- 6.33
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 477500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 144       |
|    n_updates        | 109374    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11984    |
|    fps              | 290      |
|    time_elapsed     | 1643     |
|    total_timesteps  | 477609   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 112      |
|    n_updates        | 109402   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11988    |
|    fps              | 290      |
|    time_elapsed     | 1643     |
|    total_timesteps  | 477716   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 119      |
|    n_updates        | 109428   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11992     |
|    fps              | 290       |
|    time_elapsed     | 1643      |
|    total_timesteps  | 477866    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 109466    |
-----------------------------------
Eval num_timesteps=478000, episode_reward=-2804.51 +/- 1214.80
Episode length: 35.30 +/- 6.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.3     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 478000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 186      |
|    n_updates        | 109499   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 11996     |
|    fps              | 290       |
|    time_elapsed     | 1645      |
|    total_timesteps  | 478024    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.3      |
|    n_updates        | 109505    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12000     |
|    fps              | 290       |
|    time_elapsed     | 1645      |
|    total_timesteps  | 478169    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 109542    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12004     |
|    fps              | 290       |
|    time_elapsed     | 1645      |
|    total_timesteps  | 478288    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 77.5      |
|    n_updates        | 109571    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12008    |
|    fps              | 290      |
|    time_elapsed     | 1645     |
|    total_timesteps  | 478445   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 145      |
|    n_updates        | 109611   |
----------------------------------
Eval num_timesteps=478500, episode_reward=-2960.81 +/- 826.87
Episode length: 36.40 +/- 5.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.4      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 478500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.8      |
|    n_updates        | 109624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12012     |
|    fps              | 290       |
|    time_elapsed     | 1647      |
|    total_timesteps  | 478587    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 109646    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12016     |
|    fps              | 290       |
|    time_elapsed     | 1647      |
|    total_timesteps  | 478709    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 109677    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12020     |
|    fps              | 290       |
|    time_elapsed     | 1647      |
|    total_timesteps  | 478850    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 109712    |
-----------------------------------
Eval num_timesteps=479000, episode_reward=-3003.13 +/- 1007.21
Episode length: 33.58 +/- 6.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.6     |
|    mean_reward      | -3e+03   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 479000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 209      |
|    n_updates        | 109749   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12024     |
|    fps              | 290       |
|    time_elapsed     | 1648      |
|    total_timesteps  | 479013    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.7      |
|    n_updates        | 109753    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12028     |
|    fps              | 290       |
|    time_elapsed     | 1649      |
|    total_timesteps  | 479166    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 109791    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12032     |
|    fps              | 290       |
|    time_elapsed     | 1649      |
|    total_timesteps  | 479303    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99        |
|    n_updates        | 109825    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12036     |
|    fps              | 290       |
|    time_elapsed     | 1649      |
|    total_timesteps  | 479439    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 151       |
|    n_updates        | 109859    |
-----------------------------------
Eval num_timesteps=479500, episode_reward=-2984.36 +/- 898.26
Episode length: 35.06 +/- 6.40
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.98e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 479500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 109874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12040     |
|    fps              | 290       |
|    time_elapsed     | 1650      |
|    total_timesteps  | 479603    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 170       |
|    n_updates        | 109900    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12044     |
|    fps              | 290       |
|    time_elapsed     | 1650      |
|    total_timesteps  | 479732    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 109932    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12048     |
|    fps              | 290       |
|    time_elapsed     | 1651      |
|    total_timesteps  | 479904    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 109975    |
-----------------------------------
Eval num_timesteps=480000, episode_reward=-2588.24 +/- 1369.28
Episode length: 36.64 +/- 6.60
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.6      |
|    mean_reward      | -2.59e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 480000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 69.6      |
|    n_updates        | 109999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12052     |
|    fps              | 290       |
|    time_elapsed     | 1652      |
|    total_timesteps  | 480047    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 58.9      |
|    n_updates        | 110011    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.74e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12056     |
|    fps              | 290       |
|    time_elapsed     | 1652      |
|    total_timesteps  | 480196    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 110048    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12060     |
|    fps              | 290       |
|    time_elapsed     | 1652      |
|    total_timesteps  | 480321    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 106       |
|    n_updates        | 110080    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12064     |
|    fps              | 290       |
|    time_elapsed     | 1653      |
|    total_timesteps  | 480482    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 110120    |
-----------------------------------
Eval num_timesteps=480500, episode_reward=-2805.59 +/- 1291.37
Episode length: 34.78 +/- 7.63
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 480500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 48.7      |
|    n_updates        | 110124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12068     |
|    fps              | 290       |
|    time_elapsed     | 1654      |
|    total_timesteps  | 480615    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 194       |
|    n_updates        | 110153    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12072     |
|    fps              | 290       |
|    time_elapsed     | 1654      |
|    total_timesteps  | 480754    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 209       |
|    n_updates        | 110188    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12076     |
|    fps              | 290       |
|    time_elapsed     | 1654      |
|    total_timesteps  | 480910    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 110227    |
-----------------------------------
Eval num_timesteps=481000, episode_reward=-3010.00 +/- 1164.38
Episode length: 34.36 +/- 7.21
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -3.01e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 481000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 198       |
|    n_updates        | 110249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.72e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12080     |
|    fps              | 290       |
|    time_elapsed     | 1656      |
|    total_timesteps  | 481049    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 110262    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12084     |
|    fps              | 290       |
|    time_elapsed     | 1656      |
|    total_timesteps  | 481170    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 110292    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12088     |
|    fps              | 290       |
|    time_elapsed     | 1656      |
|    total_timesteps  | 481300    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 230       |
|    n_updates        | 110324    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12092     |
|    fps              | 290       |
|    time_elapsed     | 1656      |
|    total_timesteps  | 481432    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47.2      |
|    n_updates        | 110357    |
-----------------------------------
Eval num_timesteps=481500, episode_reward=-2987.85 +/- 987.30
Episode length: 34.72 +/- 5.56
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 481500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.6      |
|    n_updates        | 110374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12096     |
|    fps              | 290       |
|    time_elapsed     | 1657      |
|    total_timesteps  | 481568    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 110391    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12100     |
|    fps              | 290       |
|    time_elapsed     | 1657      |
|    total_timesteps  | 481703    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 158       |
|    n_updates        | 110425    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12104     |
|    fps              | 290       |
|    time_elapsed     | 1658      |
|    total_timesteps  | 481838    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 110459    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12108     |
|    fps              | 290       |
|    time_elapsed     | 1658      |
|    total_timesteps  | 481966    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 110491    |
-----------------------------------
Eval num_timesteps=482000, episode_reward=-2842.92 +/- 1328.09
Episode length: 34.28 +/- 6.68
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 482000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 40.6      |
|    n_updates        | 110499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12112     |
|    fps              | 290       |
|    time_elapsed     | 1659      |
|    total_timesteps  | 482084    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 89.2      |
|    n_updates        | 110520    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12116     |
|    fps              | 290       |
|    time_elapsed     | 1659      |
|    total_timesteps  | 482243    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 194       |
|    n_updates        | 110560    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12120     |
|    fps              | 290       |
|    time_elapsed     | 1659      |
|    total_timesteps  | 482386    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 110596    |
-----------------------------------
Eval num_timesteps=482500, episode_reward=-2707.22 +/- 1136.92
Episode length: 36.34 +/- 6.31
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.3      |
|    mean_reward      | -2.71e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 482500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 132       |
|    n_updates        | 110624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12124     |
|    fps              | 290       |
|    time_elapsed     | 1661      |
|    total_timesteps  | 482530    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 51.7      |
|    n_updates        | 110632    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.82e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12128     |
|    fps              | 290       |
|    time_elapsed     | 1661      |
|    total_timesteps  | 482680    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 238       |
|    n_updates        | 110669    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12132     |
|    fps              | 290       |
|    time_elapsed     | 1661      |
|    total_timesteps  | 482824    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 110705    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12136     |
|    fps              | 290       |
|    time_elapsed     | 1661      |
|    total_timesteps  | 482972    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 83.7      |
|    n_updates        | 110742    |
-----------------------------------
Eval num_timesteps=483000, episode_reward=-2578.77 +/- 1401.72
Episode length: 35.52 +/- 6.95
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.5      |
|    mean_reward      | -2.58e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 483000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 110749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12140     |
|    fps              | 290       |
|    time_elapsed     | 1663      |
|    total_timesteps  | 483103    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 128       |
|    n_updates        | 110775    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12144     |
|    fps              | 290       |
|    time_elapsed     | 1663      |
|    total_timesteps  | 483232    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 110807    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -3.01e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12148     |
|    fps              | 290       |
|    time_elapsed     | 1663      |
|    total_timesteps  | 483360    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 172       |
|    n_updates        | 110839    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.03e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12152     |
|    fps              | 290       |
|    time_elapsed     | 1663      |
|    total_timesteps  | 483473    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 110868    |
-----------------------------------
Eval num_timesteps=483500, episode_reward=-2967.47 +/- 1210.77
Episode length: 34.20 +/- 6.60
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.2      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 483500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 110874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.03e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12156     |
|    fps              | 290       |
|    time_elapsed     | 1664      |
|    total_timesteps  | 483626    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 110906    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.04e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12160     |
|    fps              | 290       |
|    time_elapsed     | 1665      |
|    total_timesteps  | 483738    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 110934    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -3.08e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12164     |
|    fps              | 290       |
|    time_elapsed     | 1665      |
|    total_timesteps  | 483876    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 110968    |
-----------------------------------
Eval num_timesteps=484000, episode_reward=-2989.29 +/- 1130.52
Episode length: 34.10 +/- 7.15
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -2.99e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 484000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 110999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.05e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12168     |
|    fps              | 290       |
|    time_elapsed     | 1666      |
|    total_timesteps  | 484012    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 195       |
|    n_updates        | 111002    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -3.03e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12172     |
|    fps              | 290       |
|    time_elapsed     | 1666      |
|    total_timesteps  | 484172    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.3      |
|    n_updates        | 111042    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -3.06e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12176     |
|    fps              | 290       |
|    time_elapsed     | 1666      |
|    total_timesteps  | 484309    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 133       |
|    n_updates        | 111077    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.1      |
|    ep_rew_mean      | -3.02e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12180     |
|    fps              | 290       |
|    time_elapsed     | 1667      |
|    total_timesteps  | 484460    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 54.1      |
|    n_updates        | 111114    |
-----------------------------------
Eval num_timesteps=484500, episode_reward=-2647.97 +/- 1264.12
Episode length: 36.92 +/- 5.97
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.9      |
|    mean_reward      | -2.65e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 484500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 129       |
|    n_updates        | 111124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -3.03e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12184     |
|    fps              | 290       |
|    time_elapsed     | 1668      |
|    total_timesteps  | 484598    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 113       |
|    n_updates        | 111149    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12188     |
|    fps              | 290       |
|    time_elapsed     | 1668      |
|    total_timesteps  | 484757    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.1      |
|    n_updates        | 111189    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12192     |
|    fps              | 290       |
|    time_elapsed     | 1668      |
|    total_timesteps  | 484900    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 126       |
|    n_updates        | 111224    |
-----------------------------------
Eval num_timesteps=485000, episode_reward=-2513.53 +/- 1375.40
Episode length: 34.36 +/- 8.16
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.51e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 485000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 229       |
|    n_updates        | 111249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.7      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12196     |
|    fps              | 290       |
|    time_elapsed     | 1670      |
|    total_timesteps  | 485040    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 176       |
|    n_updates        | 111259    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12200     |
|    fps              | 290       |
|    time_elapsed     | 1670      |
|    total_timesteps  | 485194    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 137       |
|    n_updates        | 111298    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12204     |
|    fps              | 290       |
|    time_elapsed     | 1670      |
|    total_timesteps  | 485344    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 171       |
|    n_updates        | 111335    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12208     |
|    fps              | 290       |
|    time_elapsed     | 1670      |
|    total_timesteps  | 485486    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 143       |
|    n_updates        | 111371    |
-----------------------------------
Eval num_timesteps=485500, episode_reward=-2861.55 +/- 1153.79
Episode length: 35.56 +/- 7.53
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 485500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 65.6      |
|    n_updates        | 111374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12212     |
|    fps              | 290       |
|    time_elapsed     | 1672      |
|    total_timesteps  | 485619    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 253       |
|    n_updates        | 111404    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12216     |
|    fps              | 290       |
|    time_elapsed     | 1672      |
|    total_timesteps  | 485754    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 240       |
|    n_updates        | 111438    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12220     |
|    fps              | 290       |
|    time_elapsed     | 1672      |
|    total_timesteps  | 485905    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 248       |
|    n_updates        | 111476    |
-----------------------------------
Eval num_timesteps=486000, episode_reward=-2589.15 +/- 1264.14
Episode length: 35.90 +/- 6.84
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.9      |
|    mean_reward      | -2.59e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 486000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 188       |
|    n_updates        | 111499    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.3     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12224    |
|    fps              | 290      |
|    time_elapsed     | 1673     |
|    total_timesteps  | 486057   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 306      |
|    n_updates        | 111514   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12228     |
|    fps              | 290       |
|    time_elapsed     | 1673      |
|    total_timesteps  | 486210    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 88.5      |
|    n_updates        | 111552    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12232     |
|    fps              | 290       |
|    time_elapsed     | 1674      |
|    total_timesteps  | 486359    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 111589    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.2     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12236    |
|    fps              | 290      |
|    time_elapsed     | 1674     |
|    total_timesteps  | 486497   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 176      |
|    n_updates        | 111624   |
----------------------------------
Eval num_timesteps=486500, episode_reward=-2952.96 +/- 1110.81
Episode length: 35.98 +/- 7.46
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36        |
|    mean_reward      | -2.95e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 486500    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.2      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12240     |
|    fps              | 290       |
|    time_elapsed     | 1675      |
|    total_timesteps  | 486626    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 165       |
|    n_updates        | 111656    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12244     |
|    fps              | 290       |
|    time_elapsed     | 1675      |
|    total_timesteps  | 486759    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 111689    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.4     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12248    |
|    fps              | 290      |
|    time_elapsed     | 1676     |
|    total_timesteps  | 486900   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 85.1     |
|    n_updates        | 111724   |
----------------------------------
Eval num_timesteps=487000, episode_reward=-2845.79 +/- 1136.94
Episode length: 34.52 +/- 7.03
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.5      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 487000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.3      |
|    n_updates        | 111749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12252     |
|    fps              | 290       |
|    time_elapsed     | 1677      |
|    total_timesteps  | 487038    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68        |
|    n_updates        | 111759    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12256     |
|    fps              | 290       |
|    time_elapsed     | 1677      |
|    total_timesteps  | 487159    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.2      |
|    n_updates        | 111789    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12260     |
|    fps              | 290       |
|    time_elapsed     | 1677      |
|    total_timesteps  | 487272    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 210       |
|    n_updates        | 111817    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12264     |
|    fps              | 290       |
|    time_elapsed     | 1677      |
|    total_timesteps  | 487418    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 72.2      |
|    n_updates        | 111854    |
-----------------------------------
Eval num_timesteps=487500, episode_reward=-2699.30 +/- 1385.86
Episode length: 35.14 +/- 6.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.1     |
|    mean_reward      | -2.7e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 487500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 125      |
|    n_updates        | 111874   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.76e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12268     |
|    fps              | 290       |
|    time_elapsed     | 1679      |
|    total_timesteps  | 487573    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47.9      |
|    n_updates        | 111893    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.75e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12272     |
|    fps              | 290       |
|    time_elapsed     | 1679      |
|    total_timesteps  | 487730    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 96.3      |
|    n_updates        | 111932    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12276     |
|    fps              | 290       |
|    time_elapsed     | 1679      |
|    total_timesteps  | 487860    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 120       |
|    n_updates        | 111964    |
-----------------------------------
Eval num_timesteps=488000, episode_reward=-2965.82 +/- 1124.32
Episode length: 34.28 +/- 5.99
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.3      |
|    mean_reward      | -2.97e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 488000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 66.8      |
|    n_updates        | 111999    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.71e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12280     |
|    fps              | 290       |
|    time_elapsed     | 1680      |
|    total_timesteps  | 488017    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 62.5      |
|    n_updates        | 112004    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12284     |
|    fps              | 290       |
|    time_elapsed     | 1680      |
|    total_timesteps  | 488132    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 145       |
|    n_updates        | 112032    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12288     |
|    fps              | 290       |
|    time_elapsed     | 1681      |
|    total_timesteps  | 488299    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 187       |
|    n_updates        | 112074    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12292     |
|    fps              | 290       |
|    time_elapsed     | 1681      |
|    total_timesteps  | 488448    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.2      |
|    n_updates        | 112111    |
-----------------------------------
Eval num_timesteps=488500, episode_reward=-2923.02 +/- 1007.86
Episode length: 36.96 +/- 6.13
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37        |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 488500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 240       |
|    n_updates        | 112124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12296     |
|    fps              | 290       |
|    time_elapsed     | 1682      |
|    total_timesteps  | 488598    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 112149    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12300     |
|    fps              | 290       |
|    time_elapsed     | 1683      |
|    total_timesteps  | 488758    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 229       |
|    n_updates        | 112189    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12304     |
|    fps              | 290       |
|    time_elapsed     | 1683      |
|    total_timesteps  | 488892    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 179       |
|    n_updates        | 112222    |
-----------------------------------
Eval num_timesteps=489000, episode_reward=-2802.73 +/- 1122.09
Episode length: 35.54 +/- 6.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.5     |
|    mean_reward      | -2.8e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 489000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 231      |
|    n_updates        | 112249   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12308     |
|    fps              | 290       |
|    time_elapsed     | 1684      |
|    total_timesteps  | 489038    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.3      |
|    n_updates        | 112259    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.64e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12312     |
|    fps              | 290       |
|    time_elapsed     | 1684      |
|    total_timesteps  | 489186    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 68.4      |
|    n_updates        | 112296    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.73e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12316     |
|    fps              | 290       |
|    time_elapsed     | 1684      |
|    total_timesteps  | 489340    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 153       |
|    n_updates        | 112334    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12320     |
|    fps              | 290       |
|    time_elapsed     | 1685      |
|    total_timesteps  | 489467    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 296       |
|    n_updates        | 112366    |
-----------------------------------
Eval num_timesteps=489500, episode_reward=-3050.15 +/- 878.30
Episode length: 34.12 +/- 6.92
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.1      |
|    mean_reward      | -3.05e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 489500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 215       |
|    n_updates        | 112374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.67e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12324     |
|    fps              | 290       |
|    time_elapsed     | 1686      |
|    total_timesteps  | 489630    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 52.5      |
|    n_updates        | 112407    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12328     |
|    fps              | 290       |
|    time_elapsed     | 1686      |
|    total_timesteps  | 489785    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 154       |
|    n_updates        | 112446    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12332     |
|    fps              | 290       |
|    time_elapsed     | 1686      |
|    total_timesteps  | 489932    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 84.5      |
|    n_updates        | 112482    |
-----------------------------------
Eval num_timesteps=490000, episode_reward=-2624.99 +/- 1304.55
Episode length: 34.84 +/- 7.52
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.62e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 490000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 40.4      |
|    n_updates        | 112499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.9      |
|    ep_rew_mean      | -2.66e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12336     |
|    fps              | 290       |
|    time_elapsed     | 1688      |
|    total_timesteps  | 490082    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 140       |
|    n_updates        | 112520    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.65e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12340     |
|    fps              | 290       |
|    time_elapsed     | 1688      |
|    total_timesteps  | 490223    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 112555    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12344     |
|    fps              | 290       |
|    time_elapsed     | 1688      |
|    total_timesteps  | 490380    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 95.2      |
|    n_updates        | 112594    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.57e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12348     |
|    fps              | 290       |
|    time_elapsed     | 1688      |
|    total_timesteps  | 490498    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 47.9      |
|    n_updates        | 112624    |
-----------------------------------
Eval num_timesteps=490500, episode_reward=-2916.57 +/- 1033.13
Episode length: 35.78 +/- 6.06
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.92e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 490500    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.8      |
|    ep_rew_mean      | -2.57e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12352     |
|    fps              | 290       |
|    time_elapsed     | 1689      |
|    total_timesteps  | 490619    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 112654    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.2      |
|    ep_rew_mean      | -2.53e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12356     |
|    fps              | 290       |
|    time_elapsed     | 1690      |
|    total_timesteps  | 490778    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 167       |
|    n_updates        | 112694    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.4     |
|    ep_rew_mean      | -2.5e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12360    |
|    fps              | 290      |
|    time_elapsed     | 1690     |
|    total_timesteps  | 490915   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 184      |
|    n_updates        | 112728   |
----------------------------------
Eval num_timesteps=491000, episode_reward=-2827.65 +/- 1287.47
Episode length: 35.78 +/- 6.17
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 491000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 112749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36.1      |
|    ep_rew_mean      | -2.55e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12364     |
|    fps              | 290       |
|    time_elapsed     | 1691      |
|    total_timesteps  | 491029    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 118       |
|    n_updates        | 112757    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.57e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12368     |
|    fps              | 290       |
|    time_elapsed     | 1691      |
|    total_timesteps  | 491145    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 199       |
|    n_updates        | 112786    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.61e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12372     |
|    fps              | 290       |
|    time_elapsed     | 1691      |
|    total_timesteps  | 491276    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 109       |
|    n_updates        | 112818    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.63e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12376     |
|    fps              | 290       |
|    time_elapsed     | 1692      |
|    total_timesteps  | 491427    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 193       |
|    n_updates        | 112856    |
-----------------------------------
Eval num_timesteps=491500, episode_reward=-2835.91 +/- 1189.52
Episode length: 34.58 +/- 6.94
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.6      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 491500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 91.9      |
|    n_updates        | 112874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.68e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12380     |
|    fps              | 290       |
|    time_elapsed     | 1693      |
|    total_timesteps  | 491568    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 255       |
|    n_updates        | 112891    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.69e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12384     |
|    fps              | 290       |
|    time_elapsed     | 1693      |
|    total_timesteps  | 491727    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 44.5      |
|    n_updates        | 112931    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.78e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12388     |
|    fps              | 290       |
|    time_elapsed     | 1693      |
|    total_timesteps  | 491870    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 112967    |
-----------------------------------
Eval num_timesteps=492000, episode_reward=-3099.46 +/- 799.08
Episode length: 33.70 +/- 6.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.7     |
|    mean_reward      | -3.1e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 492000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 193      |
|    n_updates        | 112999   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12392     |
|    fps              | 290       |
|    time_elapsed     | 1695      |
|    total_timesteps  | 492006    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 113001    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.77e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12396     |
|    fps              | 290       |
|    time_elapsed     | 1695      |
|    total_timesteps  | 492145    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 113036    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12400     |
|    fps              | 290       |
|    time_elapsed     | 1695      |
|    total_timesteps  | 492288    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 213       |
|    n_updates        | 113071    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.2     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12404    |
|    fps              | 290      |
|    time_elapsed     | 1695     |
|    total_timesteps  | 492413   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 139      |
|    n_updates        | 113103   |
----------------------------------
Eval num_timesteps=492500, episode_reward=-2831.86 +/- 1144.67
Episode length: 34.74 +/- 7.78
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.83e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 492500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 299       |
|    n_updates        | 113124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12408     |
|    fps              | 290       |
|    time_elapsed     | 1696      |
|    total_timesteps  | 492536    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 164       |
|    n_updates        | 113133    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35        |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12412     |
|    fps              | 290       |
|    time_elapsed     | 1696      |
|    total_timesteps  | 492686    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 75.3      |
|    n_updates        | 113171    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12416     |
|    fps              | 290       |
|    time_elapsed     | 1697      |
|    total_timesteps  | 492793    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 46.7      |
|    n_updates        | 113198    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12420     |
|    fps              | 290       |
|    time_elapsed     | 1697      |
|    total_timesteps  | 492930    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 177       |
|    n_updates        | 113232    |
-----------------------------------
Eval num_timesteps=493000, episode_reward=-2542.64 +/- 1403.00
Episode length: 35.68 +/- 7.37
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.7      |
|    mean_reward      | -2.54e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 493000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 113249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12424     |
|    fps              | 290       |
|    time_elapsed     | 1698      |
|    total_timesteps  | 493072    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 114       |
|    n_updates        | 113267    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34        |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12428     |
|    fps              | 290       |
|    time_elapsed     | 1698      |
|    total_timesteps  | 493184    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 150       |
|    n_updates        | 113295    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.96e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12432     |
|    fps              | 290       |
|    time_elapsed     | 1698      |
|    total_timesteps  | 493325    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 142       |
|    n_updates        | 113331    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12436     |
|    fps              | 290       |
|    time_elapsed     | 1699      |
|    total_timesteps  | 493461    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 161       |
|    n_updates        | 113365    |
-----------------------------------
Eval num_timesteps=493500, episode_reward=-2956.52 +/- 1096.06
Episode length: 34.38 +/- 6.60
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.96e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 493500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 312       |
|    n_updates        | 113374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12440     |
|    fps              | 290       |
|    time_elapsed     | 1700      |
|    total_timesteps  | 493601    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 209       |
|    n_updates        | 113400    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.6      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12444     |
|    fps              | 290       |
|    time_elapsed     | 1700      |
|    total_timesteps  | 493743    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 102       |
|    n_updates        | 113435    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12448     |
|    fps              | 290       |
|    time_elapsed     | 1700      |
|    total_timesteps  | 493887    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 215       |
|    n_updates        | 113471    |
-----------------------------------
Eval num_timesteps=494000, episode_reward=-2863.04 +/- 1156.34
Episode length: 35.60 +/- 6.40
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.86e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 494000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 139       |
|    n_updates        | 113499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.9      |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12452     |
|    fps              | 290       |
|    time_elapsed     | 1702      |
|    total_timesteps  | 494011    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 98.3      |
|    n_updates        | 113502    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12456     |
|    fps              | 290       |
|    time_elapsed     | 1702      |
|    total_timesteps  | 494157    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 251       |
|    n_updates        | 113539    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 33.8      |
|    ep_rew_mean      | -2.99e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12460     |
|    fps              | 290       |
|    time_elapsed     | 1702      |
|    total_timesteps  | 494296    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 103       |
|    n_updates        | 113573    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12464     |
|    fps              | 290       |
|    time_elapsed     | 1702      |
|    total_timesteps  | 494449    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 213       |
|    n_updates        | 113612    |
-----------------------------------
Eval num_timesteps=494500, episode_reward=-2581.24 +/- 1387.98
Episode length: 37.56 +/- 7.47
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 37.6      |
|    mean_reward      | -2.58e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 494500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 160       |
|    n_updates        | 113624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12468     |
|    fps              | 290       |
|    time_elapsed     | 1704      |
|    total_timesteps  | 494599    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 124       |
|    n_updates        | 113649    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.8      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12472     |
|    fps              | 290       |
|    time_elapsed     | 1704      |
|    total_timesteps  | 494756    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 113688    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.88e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12476     |
|    fps              | 290       |
|    time_elapsed     | 1704      |
|    total_timesteps  | 494879    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 168       |
|    n_updates        | 113719    |
-----------------------------------
Eval num_timesteps=495000, episode_reward=-2842.51 +/- 1102.94
Episode length: 33.84 +/- 7.55
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 33.8      |
|    mean_reward      | -2.84e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 495000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 82.1      |
|    n_updates        | 113749    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.89e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12480     |
|    fps              | 290       |
|    time_elapsed     | 1705      |
|    total_timesteps  | 495019    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 141       |
|    n_updates        | 113754    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12484     |
|    fps              | 290       |
|    time_elapsed     | 1705      |
|    total_timesteps  | 495172    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 70        |
|    n_updates        | 113792    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.4      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12488     |
|    fps              | 290       |
|    time_elapsed     | 1705      |
|    total_timesteps  | 495313    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 113828    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.2      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12492     |
|    fps              | 290       |
|    time_elapsed     | 1706      |
|    total_timesteps  | 495426    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 130       |
|    n_updates        | 113856    |
-----------------------------------
Eval num_timesteps=495500, episode_reward=-2866.75 +/- 1032.50
Episode length: 34.80 +/- 7.08
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.8      |
|    mean_reward      | -2.87e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 495500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 183       |
|    n_updates        | 113874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.3      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12496     |
|    fps              | 290       |
|    time_elapsed     | 1707      |
|    total_timesteps  | 495575    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 113893    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.3     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12500    |
|    fps              | 290      |
|    time_elapsed     | 1707     |
|    total_timesteps  | 495714   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 140      |
|    n_updates        | 113928   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12504     |
|    fps              | 290       |
|    time_elapsed     | 1707      |
|    total_timesteps  | 495861    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 313       |
|    n_updates        | 113965    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.5      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12508     |
|    fps              | 290       |
|    time_elapsed     | 1707      |
|    total_timesteps  | 495989    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 148       |
|    n_updates        | 113997    |
-----------------------------------
Eval num_timesteps=496000, episode_reward=-3299.88 +/- 849.01
Episode length: 34.90 +/- 5.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.9     |
|    mean_reward      | -3.3e+03 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 496000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 260      |
|    n_updates        | 113999   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.6      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12512     |
|    fps              | 290       |
|    time_elapsed     | 1709      |
|    total_timesteps  | 496150    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 90        |
|    n_updates        | 114037    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 34.9      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12516     |
|    fps              | 290       |
|    time_elapsed     | 1709      |
|    total_timesteps  | 496286    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 93.4      |
|    n_updates        | 114071    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12520     |
|    fps              | 290       |
|    time_elapsed     | 1709      |
|    total_timesteps  | 496438    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 110       |
|    n_updates        | 114109    |
-----------------------------------
Eval num_timesteps=496500, episode_reward=-2823.00 +/- 1238.79
Episode length: 35.20 +/- 6.80
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.82e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 496500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 114124    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.1      |
|    ep_rew_mean      | -2.84e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12524     |
|    fps              | 290       |
|    time_elapsed     | 1710      |
|    total_timesteps  | 496578    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 87.6      |
|    n_updates        | 114144    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12528     |
|    fps              | 290       |
|    time_elapsed     | 1711      |
|    total_timesteps  | 496713    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 155       |
|    n_updates        | 114178    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.79e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12532     |
|    fps              | 290       |
|    time_elapsed     | 1711      |
|    total_timesteps  | 496870    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 232       |
|    n_updates        | 114217    |
-----------------------------------
Eval num_timesteps=497000, episode_reward=-2533.80 +/- 1446.84
Episode length: 35.24 +/- 7.20
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.2      |
|    mean_reward      | -2.53e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 497000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 215       |
|    n_updates        | 114249    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.81e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12536     |
|    fps              | 290       |
|    time_elapsed     | 1712      |
|    total_timesteps  | 497001    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 92.1      |
|    n_updates        | 114250    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.83e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12540     |
|    fps              | 290       |
|    time_elapsed     | 1712      |
|    total_timesteps  | 497145    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 112       |
|    n_updates        | 114286    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.6     |
|    ep_rew_mean      | -2.8e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12544    |
|    fps              | 290      |
|    time_elapsed     | 1712     |
|    total_timesteps  | 497303   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 145      |
|    n_updates        | 114325   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.5      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12548     |
|    fps              | 290       |
|    time_elapsed     | 1713      |
|    total_timesteps  | 497439    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 131       |
|    n_updates        | 114359    |
-----------------------------------
Eval num_timesteps=497500, episode_reward=-2813.81 +/- 1185.42
Episode length: 34.74 +/- 7.60
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.7      |
|    mean_reward      | -2.81e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 497500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 202       |
|    n_updates        | 114374    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12552     |
|    fps              | 290       |
|    time_elapsed     | 1714      |
|    total_timesteps  | 497580    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 114394    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.85e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12556     |
|    fps              | 290       |
|    time_elapsed     | 1714      |
|    total_timesteps  | 497729    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 101       |
|    n_updates        | 114432    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.86e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12560     |
|    fps              | 290       |
|    time_elapsed     | 1714      |
|    total_timesteps  | 497859    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 157       |
|    n_updates        | 114464    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12564     |
|    fps              | 290       |
|    time_elapsed     | 1714      |
|    total_timesteps  | 497993    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.6      |
|    n_updates        | 114498    |
-----------------------------------
Eval num_timesteps=498000, episode_reward=-2847.61 +/- 1097.02
Episode length: 35.76 +/- 5.58
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.8      |
|    mean_reward      | -2.85e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 498000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 79.7      |
|    n_updates        | 114499    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.87e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12568     |
|    fps              | 290       |
|    time_elapsed     | 1716      |
|    total_timesteps  | 498156    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 207       |
|    n_updates        | 114538    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.3      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12572     |
|    fps              | 290       |
|    time_elapsed     | 1716      |
|    total_timesteps  | 498286    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 166       |
|    n_updates        | 114571    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.92e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12576     |
|    fps              | 290       |
|    time_elapsed     | 1716      |
|    total_timesteps  | 498441    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 122       |
|    n_updates        | 114610    |
-----------------------------------
Eval num_timesteps=498500, episode_reward=-2656.46 +/- 1431.69
Episode length: 36.14 +/- 7.22
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 36.1      |
|    mean_reward      | -2.66e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 498500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 107       |
|    n_updates        | 114624    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.94e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12580     |
|    fps              | 290       |
|    time_elapsed     | 1718      |
|    total_timesteps  | 498586    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 123       |
|    n_updates        | 114646    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12584     |
|    fps              | 290       |
|    time_elapsed     | 1718      |
|    total_timesteps  | 498736    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 147       |
|    n_updates        | 114683    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.4      |
|    ep_rew_mean      | -2.93e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12588     |
|    fps              | 290       |
|    time_elapsed     | 1718      |
|    total_timesteps  | 498855    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 39.6      |
|    n_updates        | 114713    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.6      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12592     |
|    fps              | 290       |
|    time_elapsed     | 1718      |
|    total_timesteps  | 498984    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 119       |
|    n_updates        | 114745    |
-----------------------------------
Eval num_timesteps=499000, episode_reward=-2885.63 +/- 1107.78
Episode length: 34.40 +/- 7.48
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 34.4      |
|    mean_reward      | -2.89e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 499000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 104       |
|    n_updates        | 114749    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.6     |
|    ep_rew_mean      | -2.9e+03 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12596    |
|    fps              | 290      |
|    time_elapsed     | 1719     |
|    total_timesteps  | 499133   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 58.7     |
|    n_updates        | 114783   |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.91e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12600     |
|    fps              | 290       |
|    time_elapsed     | 1720      |
|    total_timesteps  | 499284    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 57.6      |
|    n_updates        | 114820    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.97e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12604     |
|    fps              | 290       |
|    time_elapsed     | 1720      |
|    total_timesteps  | 499433    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 99.7      |
|    n_updates        | 114858    |
-----------------------------------
Eval num_timesteps=499500, episode_reward=-2775.59 +/- 1391.40
Episode length: 35.08 +/- 6.55
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.1      |
|    mean_reward      | -2.78e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 499500    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 253       |
|    n_updates        | 114874    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 36        |
|    ep_rew_mean      | -2.95e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12608     |
|    fps              | 290       |
|    time_elapsed     | 1721      |
|    total_timesteps  | 499585    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 156       |
|    n_updates        | 114896    |
-----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 35.7      |
|    ep_rew_mean      | -2.98e+03 |
|    exploration_rate | 0.001     |
| time/               |           |
|    episodes         | 12612     |
|    fps              | 290       |
|    time_elapsed     | 1721      |
|    total_timesteps  | 499719    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 117       |
|    n_updates        | 114929    |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -3e+03   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12616    |
|    fps              | 290      |
|    time_elapsed     | 1721     |
|    total_timesteps  | 499854   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 227      |
|    n_updates        | 114963   |
----------------------------------
Eval num_timesteps=500000, episode_reward=-2722.07 +/- 1183.81
Episode length: 35.64 +/- 6.19
-----------------------------------
| eval/               |           |
|    mean_ep_length   | 35.6      |
|    mean_reward      | -2.72e+03 |
| rollout/            |           |
|    exploration_rate | 0.001     |
| time/               |           |
|    total_timesteps  | 500000    |
| train/              |           |
|    learning_rate    | 0.0001    |
|    loss             | 182       |
|    n_updates        | 114999    |
-----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/dqn-10/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'batch_size': 64, 'learning_rate': 0.0001, 'buffer_size': 15000, 'gamma': 0.957, 'exploration_fraction': 0.4, 'exploration_final_eps': 0.001, 'learning_starts': 40000.0, 'decay_start_steps': 40000.0, 'decay_end_steps': 280000.0}
Training steps: 500000
Frame skip: 4
