/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-45.03 +/- 172.06
Episode length: 93.58 +/- 32.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | -45      |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-55.29 +/- 173.00
Episode length: 99.38 +/- 53.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | -55.3    |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 30.2     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 1        |
|    time_elapsed    | 13       |
|    total_timesteps | 1024     |
---------------------------------
Eval num_timesteps=1500, episode_reward=1402.75 +/- 671.54
Episode length: 45.66 +/- 7.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 45.7       |
|    mean_reward          | 1.4e+03    |
| time/                   |            |
|    total_timesteps      | 1500       |
| train/                  |            |
|    approx_kl            | 0.03501728 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.04      |
|    explained_variance   | -6e-05     |
|    learning_rate        | 0.001      |
|    loss                 | 542        |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00686   |
|    value_loss           | 1.24e+03   |
----------------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=1517.68 +/- 684.52
Episode length: 46.16 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 3.87     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 2        |
|    time_elapsed    | 20       |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=737.56 +/- 483.45
Episode length: 48.28 +/- 8.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.3       |
|    mean_reward          | 738        |
| time/                   |            |
|    total_timesteps      | 2500       |
| train/                  |            |
|    approx_kl            | 0.11616846 |
|    clip_fraction        | 0.7        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.87      |
|    explained_variance   | 0.0227     |
|    learning_rate        | 0.001      |
|    loss                 | 258        |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.0754     |
|    value_loss           | 1.06e+03   |
----------------------------------------
Eval num_timesteps=3000, episode_reward=660.01 +/- 283.91
Episode length: 48.40 +/- 10.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | 660      |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 121      |
|    iterations      | 3        |
|    time_elapsed    | 25       |
|    total_timesteps | 3072     |
---------------------------------
Eval num_timesteps=3500, episode_reward=2129.94 +/- 408.25
Episode length: 42.08 +/- 2.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.1       |
|    mean_reward          | 2.13e+03   |
| time/                   |            |
|    total_timesteps      | 3500       |
| train/                  |            |
|    approx_kl            | 0.08242126 |
|    clip_fraction        | 0.663      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.69      |
|    explained_variance   | 0.2        |
|    learning_rate        | 0.001      |
|    loss                 | 754        |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0622     |
|    value_loss           | 1.67e+03   |
----------------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=2170.46 +/- 375.65
Episode length: 42.10 +/- 2.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.9     |
|    ep_rew_mean     | 243      |
| time/              |          |
|    fps             | 139      |
|    iterations      | 4        |
|    time_elapsed    | 29       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=2107.55 +/- 426.70
Episode length: 42.28 +/- 1.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.11e+03  |
| time/                   |           |
|    total_timesteps      | 4500      |
| train/                  |           |
|    approx_kl            | 3.1073313 |
|    clip_fraction        | 0.89      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.964    |
|    explained_variance   | 0.398     |
|    learning_rate        | 0.001     |
|    loss                 | 1.59e+03  |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.229     |
|    value_loss           | 3.57e+03  |
---------------------------------------
Eval num_timesteps=5000, episode_reward=2079.29 +/- 461.95
Episode length: 41.70 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.5     |
|    ep_rew_mean     | 282      |
| time/              |          |
|    fps             | 154      |
|    iterations      | 5        |
|    time_elapsed    | 33       |
|    total_timesteps | 5120     |
---------------------------------
Eval num_timesteps=5500, episode_reward=2126.83 +/- 414.58
Episode length: 41.94 +/- 2.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 5500      |
| train/                  |           |
|    approx_kl            | 1.2680082 |
|    clip_fraction        | 0.708     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.409    |
|    explained_variance   | 0.433     |
|    learning_rate        | 0.001     |
|    loss                 | 1.89e+03  |
|    n_updates            | 50        |
|    policy_gradient_loss | 0.0975    |
|    value_loss           | 6.52e+03  |
---------------------------------------
Eval num_timesteps=6000, episode_reward=2047.31 +/- 499.80
Episode length: 41.76 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.3     |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 6        |
|    time_elapsed    | 37       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=2056.35 +/- 478.43
Episode length: 42.16 +/- 2.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 6500      |
| train/                  |           |
|    approx_kl            | 0.7018647 |
|    clip_fraction        | 0.029     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000772 |
|    explained_variance   | -0.245    |
|    learning_rate        | 0.001     |
|    loss                 | 3.3e+04   |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.00557  |
|    value_loss           | 5.75e+04  |
---------------------------------------
Eval num_timesteps=7000, episode_reward=2071.58 +/- 473.92
Episode length: 41.82 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57       |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 174      |
|    iterations      | 7        |
|    time_elapsed    | 41       |
|    total_timesteps | 7168     |
---------------------------------
Eval num_timesteps=7500, episode_reward=2115.71 +/- 409.04
Episode length: 42.56 +/- 1.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.6      |
|    mean_reward          | 2.12e+03  |
| time/                   |           |
|    total_timesteps      | 7500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.19e-15 |
|    explained_variance   | 0.0151    |
|    learning_rate        | 0.001     |
|    loss                 | 4.75e+04  |
|    n_updates            | 70        |
|    policy_gradient_loss | -2.71e-09 |
|    value_loss           | 5.99e+04  |
---------------------------------------
Eval num_timesteps=8000, episode_reward=2102.15 +/- 441.11
Episode length: 42.16 +/- 2.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | 1.62e+03 |
| time/              |          |
|    fps             | 181      |
|    iterations      | 8        |
|    time_elapsed    | 45       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=2154.67 +/- 377.54
Episode length: 42.02 +/- 1.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 8500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.04e-17 |
|    explained_variance   | 0.295     |
|    learning_rate        | 0.001     |
|    loss                 | 1.55e+04  |
|    n_updates            | 80        |
|    policy_gradient_loss | -3.84e-09 |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=9000, episode_reward=2051.60 +/- 488.15
Episode length: 41.70 +/- 2.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.8     |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 188      |
|    iterations      | 9        |
|    time_elapsed    | 48       |
|    total_timesteps | 9216     |
---------------------------------
Eval num_timesteps=9500, episode_reward=2067.70 +/- 454.02
Episode length: 42.26 +/- 1.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.07e+03  |
| time/                   |           |
|    total_timesteps      | 9500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.98e-17 |
|    explained_variance   | 0.393     |
|    learning_rate        | 0.001     |
|    loss                 | 1.65e+04  |
|    n_updates            | 90        |
|    policy_gradient_loss | -5.12e-10 |
|    value_loss           | 4.34e+04  |
---------------------------------------
Eval num_timesteps=10000, episode_reward=2154.66 +/- 377.60
Episode length: 42.22 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 193      |
|    iterations      | 10       |
|    time_elapsed    | 52       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=2149.77 +/- 394.21
Episode length: 41.98 +/- 2.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 10500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.62e-16 |
|    explained_variance   | 0.653     |
|    learning_rate        | 0.001     |
|    loss                 | 2.94e+03  |
|    n_updates            | 100       |
|    policy_gradient_loss | -2.29e-09 |
|    value_loss           | 1.69e+04  |
---------------------------------------
Eval num_timesteps=11000, episode_reward=2008.96 +/- 508.22
Episode length: 41.88 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 197      |
|    iterations      | 11       |
|    time_elapsed    | 56       |
|    total_timesteps | 11264    |
---------------------------------
Eval num_timesteps=11500, episode_reward=2000.34 +/- 529.29
Episode length: 41.74 +/- 2.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 11500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.8e-16  |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.001     |
|    loss                 | 7.31e+03  |
|    n_updates            | 110       |
|    policy_gradient_loss | -5.01e-10 |
|    value_loss           | 2.82e+04  |
---------------------------------------
Eval num_timesteps=12000, episode_reward=2108.95 +/- 420.22
Episode length: 42.28 +/- 1.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 201      |
|    iterations      | 12       |
|    time_elapsed    | 60       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=2057.55 +/- 472.62
Episode length: 42.02 +/- 2.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 12500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.93e-14 |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.001     |
|    loss                 | 6.82e+03  |
|    n_updates            | 120       |
|    policy_gradient_loss | -2.01e-10 |
|    value_loss           | 2.51e+04  |
---------------------------------------
Eval num_timesteps=13000, episode_reward=1952.26 +/- 557.68
Episode length: 41.48 +/- 3.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 205      |
|    iterations      | 13       |
|    time_elapsed    | 64       |
|    total_timesteps | 13312    |
---------------------------------
Eval num_timesteps=13500, episode_reward=2107.03 +/- 431.53
Episode length: 42.00 +/- 2.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.11e+03  |
| time/                   |           |
|    total_timesteps      | 13500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.01e-13 |
|    explained_variance   | 0.574     |
|    learning_rate        | 0.001     |
|    loss                 | 2.04e+04  |
|    n_updates            | 130       |
|    policy_gradient_loss | -8.79e-10 |
|    value_loss           | 4.06e+04  |
---------------------------------------
Eval num_timesteps=14000, episode_reward=2100.20 +/- 447.68
Episode length: 42.04 +/- 2.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 208      |
|    iterations      | 14       |
|    time_elapsed    | 68       |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=2112.51 +/- 418.36
Episode length: 42.34 +/- 2.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.11e+03  |
| time/                   |           |
|    total_timesteps      | 14500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.54e-14 |
|    explained_variance   | 0.659     |
|    learning_rate        | 0.001     |
|    loss                 | 9.27e+03  |
|    n_updates            | 140       |
|    policy_gradient_loss | 1.08e-10  |
|    value_loss           | 2.24e+04  |
---------------------------------------
Eval num_timesteps=15000, episode_reward=2052.62 +/- 487.74
Episode length: 42.04 +/- 2.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 15       |
|    time_elapsed    | 72       |
|    total_timesteps | 15360    |
---------------------------------
Eval num_timesteps=15500, episode_reward=2051.09 +/- 493.22
Episode length: 41.68 +/- 2.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.05e+03  |
| time/                   |           |
|    total_timesteps      | 15500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.77e-13 |
|    explained_variance   | 0.519     |
|    learning_rate        | 0.001     |
|    loss                 | 9.54e+03  |
|    n_updates            | 150       |
|    policy_gradient_loss | -2.49e-09 |
|    value_loss           | 2.93e+04  |
---------------------------------------
Eval num_timesteps=16000, episode_reward=2026.08 +/- 509.81
Episode length: 42.06 +/- 2.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 16       |
|    time_elapsed    | 76       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=2056.93 +/- 478.42
Episode length: 42.16 +/- 2.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 16500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.78e-11 |
|    explained_variance   | 0.604     |
|    learning_rate        | 0.001     |
|    loss                 | 1.66e+04  |
|    n_updates            | 160       |
|    policy_gradient_loss | -2.12e-09 |
|    value_loss           | 2.9e+04   |
---------------------------------------
Eval num_timesteps=17000, episode_reward=2103.80 +/- 437.22
Episode length: 42.02 +/- 1.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 215      |
|    iterations      | 17       |
|    time_elapsed    | 80       |
|    total_timesteps | 17408    |
---------------------------------
Eval num_timesteps=17500, episode_reward=2106.24 +/- 435.06
Episode length: 42.06 +/- 1.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.11e+03  |
| time/                   |           |
|    total_timesteps      | 17500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.5e-10  |
|    explained_variance   | 0.496     |
|    learning_rate        | 0.001     |
|    loss                 | 1.89e+04  |
|    n_updates            | 170       |
|    policy_gradient_loss | -1.05e-09 |
|    value_loss           | 5.64e+04  |
---------------------------------------
Eval num_timesteps=18000, episode_reward=1957.12 +/- 549.02
Episode length: 41.76 +/- 2.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 18       |
|    time_elapsed    | 84       |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=2161.51 +/- 356.77
Episode length: 42.40 +/- 1.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.16e+03  |
| time/                   |           |
|    total_timesteps      | 18500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.31e-11 |
|    explained_variance   | 0.455     |
|    learning_rate        | 0.001     |
|    loss                 | 1.71e+04  |
|    n_updates            | 180       |
|    policy_gradient_loss | -7.57e-11 |
|    value_loss           | 4.09e+04  |
---------------------------------------
Eval num_timesteps=19000, episode_reward=2034.67 +/- 493.37
Episode length: 41.96 +/- 2.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 19       |
|    time_elapsed    | 88       |
|    total_timesteps | 19456    |
---------------------------------
Eval num_timesteps=19500, episode_reward=1969.11 +/- 553.53
Episode length: 41.72 +/- 3.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 1.97e+03  |
| time/                   |           |
|    total_timesteps      | 19500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.13e-10 |
|    explained_variance   | 0.48      |
|    learning_rate        | 0.001     |
|    loss                 | 1.73e+04  |
|    n_updates            | 190       |
|    policy_gradient_loss | 2.13e-09  |
|    value_loss           | 3.93e+04  |
---------------------------------------
Eval num_timesteps=20000, episode_reward=2075.48 +/- 470.81
Episode length: 41.82 +/- 2.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 20       |
|    time_elapsed    | 92       |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=2074.78 +/- 469.83
Episode length: 42.14 +/- 2.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.07e+03  |
| time/                   |           |
|    total_timesteps      | 20500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.29e-11 |
|    explained_variance   | 0.47      |
|    learning_rate        | 0.001     |
|    loss                 | 1.33e+04  |
|    n_updates            | 200       |
|    policy_gradient_loss | -6.81e-10 |
|    value_loss           | 3.93e+04  |
---------------------------------------
Eval num_timesteps=21000, episode_reward=2175.75 +/- 355.64
Episode length: 42.00 +/- 1.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
New best mean reward!
Eval num_timesteps=21500, episode_reward=1998.03 +/- 534.89
Episode length: 41.44 +/- 2.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 21       |
|    time_elapsed    | 98       |
|    total_timesteps | 21504    |
---------------------------------
Eval num_timesteps=22000, episode_reward=2180.44 +/- 340.72
Episode length: 42.22 +/- 1.90
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 42.2     |
|    mean_reward          | 2.18e+03 |
| time/                   |          |
|    total_timesteps      | 22000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.5e-11 |
|    explained_variance   | 0.46     |
|    learning_rate        | 0.001    |
|    loss                 | 2.27e+04 |
|    n_updates            | 210      |
|    policy_gradient_loss | 7.22e-10 |
|    value_loss           | 5e+04    |
--------------------------------------
New best mean reward!
Eval num_timesteps=22500, episode_reward=2230.43 +/- 244.51
Episode length: 42.42 +/- 1.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 22       |
|    time_elapsed    | 101      |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=2206.59 +/- 290.96
Episode length: 42.48 +/- 1.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.21e+03  |
| time/                   |           |
|    total_timesteps      | 23000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.25e-12 |
|    explained_variance   | 0.322     |
|    learning_rate        | 0.001     |
|    loss                 | 4.21e+03  |
|    n_updates            | 220       |
|    policy_gradient_loss | -3.72e-09 |
|    value_loss           | 1.79e+04  |
---------------------------------------
Eval num_timesteps=23500, episode_reward=2113.98 +/- 411.45
Episode length: 42.26 +/- 1.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 222      |
|    iterations      | 23       |
|    time_elapsed    | 105      |
|    total_timesteps | 23552    |
---------------------------------
Eval num_timesteps=24000, episode_reward=2152.97 +/- 382.94
Episode length: 42.20 +/- 1.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 24000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.99e-13 |
|    explained_variance   | 0.639     |
|    learning_rate        | 0.001     |
|    loss                 | 4.17e+03  |
|    n_updates            | 230       |
|    policy_gradient_loss | -1.75e-09 |
|    value_loss           | 1.55e+04  |
---------------------------------------
Eval num_timesteps=24500, episode_reward=2229.41 +/- 249.86
Episode length: 42.38 +/- 1.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 223      |
|    iterations      | 24       |
|    time_elapsed    | 109      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=2002.63 +/- 527.38
Episode length: 41.82 +/- 3.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.44e-12 |
|    explained_variance   | 0.601     |
|    learning_rate        | 0.001     |
|    loss                 | 2.12e+04  |
|    n_updates            | 240       |
|    policy_gradient_loss | -5.47e-10 |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=25500, episode_reward=2180.54 +/- 338.44
Episode length: 42.34 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.6     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 25       |
|    time_elapsed    | 113      |
|    total_timesteps | 25600    |
---------------------------------
Eval num_timesteps=26000, episode_reward=2079.49 +/- 463.13
Episode length: 42.10 +/- 2.44
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 42.1     |
|    mean_reward          | 2.08e+03 |
| time/                   |          |
|    total_timesteps      | 26000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.2e-11 |
|    explained_variance   | 0.607    |
|    learning_rate        | 0.001    |
|    loss                 | 7.02e+03 |
|    n_updates            | 250      |
|    policy_gradient_loss | 5.41e-10 |
|    value_loss           | 2.86e+04 |
--------------------------------------
Eval num_timesteps=26500, episode_reward=2127.44 +/- 415.23
Episode length: 42.08 +/- 2.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 26       |
|    time_elapsed    | 117      |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=2081.33 +/- 457.14
Episode length: 41.98 +/- 2.41
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 42       |
|    mean_reward          | 2.08e+03 |
| time/                   |          |
|    total_timesteps      | 27000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.7e-10 |
|    explained_variance   | 0.625    |
|    learning_rate        | 0.001    |
|    loss                 | 1.4e+04  |
|    n_updates            | 260      |
|    policy_gradient_loss | 5.73e-10 |
|    value_loss           | 2.79e+04 |
--------------------------------------
Eval num_timesteps=27500, episode_reward=2079.40 +/- 463.32
Episode length: 41.68 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 226      |
|    iterations      | 27       |
|    time_elapsed    | 121      |
|    total_timesteps | 27648    |
---------------------------------
Eval num_timesteps=28000, episode_reward=2077.79 +/- 464.79
Episode length: 42.04 +/- 2.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 28000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.47e-11 |
|    explained_variance   | 0.401     |
|    learning_rate        | 0.001     |
|    loss                 | 1.37e+04  |
|    n_updates            | 270       |
|    policy_gradient_loss | -2.88e-10 |
|    value_loss           | 3.74e+04  |
---------------------------------------
Eval num_timesteps=28500, episode_reward=1998.12 +/- 537.20
Episode length: 41.78 +/- 2.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 28       |
|    time_elapsed    | 125      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=2031.13 +/- 500.79
Episode length: 41.92 +/- 2.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.03e+03  |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.28e-12 |
|    explained_variance   | 0.619     |
|    learning_rate        | 0.001     |
|    loss                 | 1e+04     |
|    n_updates            | 280       |
|    policy_gradient_loss | 1.25e-10  |
|    value_loss           | 2.19e+04  |
---------------------------------------
Eval num_timesteps=29500, episode_reward=2078.66 +/- 464.80
Episode length: 41.88 +/- 2.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 228      |
|    iterations      | 29       |
|    time_elapsed    | 129      |
|    total_timesteps | 29696    |
---------------------------------
Eval num_timesteps=30000, episode_reward=2127.53 +/- 415.56
Episode length: 42.04 +/- 1.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.8e-10  |
|    explained_variance   | 0.63      |
|    learning_rate        | 0.001     |
|    loss                 | 1.1e+04   |
|    n_updates            | 290       |
|    policy_gradient_loss | -1.37e-09 |
|    value_loss           | 3.23e+04  |
---------------------------------------
Eval num_timesteps=30500, episode_reward=2109.33 +/- 424.97
Episode length: 42.46 +/- 1.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 229      |
|    iterations      | 30       |
|    time_elapsed    | 133      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=2137.01 +/- 387.37
Episode length: 42.32 +/- 1.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.14e+03  |
| time/                   |           |
|    total_timesteps      | 31000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.31e-11 |
|    explained_variance   | 0.499     |
|    learning_rate        | 0.001     |
|    loss                 | 1.18e+04  |
|    n_updates            | 300       |
|    policy_gradient_loss | -1.03e-09 |
|    value_loss           | 3.68e+04  |
---------------------------------------
Eval num_timesteps=31500, episode_reward=2102.27 +/- 444.07
Episode length: 42.18 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 230      |
|    iterations      | 31       |
|    time_elapsed    | 137      |
|    total_timesteps | 31744    |
---------------------------------
Eval num_timesteps=32000, episode_reward=2011.48 +/- 507.56
Episode length: 41.88 +/- 1.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.01e+03  |
| time/                   |           |
|    total_timesteps      | 32000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.32e-10 |
|    explained_variance   | 0.488     |
|    learning_rate        | 0.001     |
|    loss                 | 1.2e+04   |
|    n_updates            | 310       |
|    policy_gradient_loss | -1.02e-09 |
|    value_loss           | 2.86e+04  |
---------------------------------------
Eval num_timesteps=32500, episode_reward=2034.64 +/- 493.47
Episode length: 42.26 +/- 2.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 231      |
|    iterations      | 32       |
|    time_elapsed    | 141      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=2062.24 +/- 466.47
Episode length: 42.16 +/- 1.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.04e-10 |
|    explained_variance   | 0.534     |
|    learning_rate        | 0.001     |
|    loss                 | 2.1e+04   |
|    n_updates            | 320       |
|    policy_gradient_loss | 1.43e-09  |
|    value_loss           | 3.69e+04  |
---------------------------------------
Eval num_timesteps=33500, episode_reward=2101.88 +/- 445.22
Episode length: 41.86 +/- 2.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 232      |
|    iterations      | 33       |
|    time_elapsed    | 145      |
|    total_timesteps | 33792    |
---------------------------------
Eval num_timesteps=34000, episode_reward=2151.01 +/- 388.35
Episode length: 42.20 +/- 2.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 34000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.19e-10 |
|    explained_variance   | 0.555     |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+04  |
|    n_updates            | 330       |
|    policy_gradient_loss | -3.78e-10 |
|    value_loss           | 3.61e+04  |
---------------------------------------
Eval num_timesteps=34500, episode_reward=2024.16 +/- 513.93
Episode length: 41.50 +/- 2.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 233      |
|    iterations      | 34       |
|    time_elapsed    | 149      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=2013.43 +/- 500.00
Episode length: 42.18 +/- 2.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.01e+03  |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.06e-08 |
|    explained_variance   | 0.473     |
|    learning_rate        | 0.001     |
|    loss                 | 7.52e+03  |
|    n_updates            | 340       |
|    policy_gradient_loss | -3.14e-07 |
|    value_loss           | 2.81e+04  |
---------------------------------------
Eval num_timesteps=35500, episode_reward=2112.92 +/- 417.78
Episode length: 42.52 +/- 1.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 234      |
|    iterations      | 35       |
|    time_elapsed    | 153      |
|    total_timesteps | 35840    |
---------------------------------
Eval num_timesteps=36000, episode_reward=2077.26 +/- 467.74
Episode length: 41.88 +/- 2.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 36000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.26e-11 |
|    explained_variance   | 0.451     |
|    learning_rate        | 0.001     |
|    loss                 | 1.2e+04   |
|    n_updates            | 350       |
|    policy_gradient_loss | -1.19e-10 |
|    value_loss           | 3.19e+04  |
---------------------------------------
Eval num_timesteps=36500, episode_reward=2058.06 +/- 475.26
Episode length: 42.16 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 234      |
|    iterations      | 36       |
|    time_elapsed    | 157      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=2082.39 +/- 448.82
Episode length: 42.18 +/- 1.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 37000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.44e-10 |
|    explained_variance   | 0.52      |
|    learning_rate        | 0.001     |
|    loss                 | 1.78e+04  |
|    n_updates            | 360       |
|    policy_gradient_loss | 6.4e-10   |
|    value_loss           | 3.85e+04  |
---------------------------------------
Eval num_timesteps=37500, episode_reward=2184.03 +/- 327.11
Episode length: 42.58 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 235      |
|    iterations      | 37       |
|    time_elapsed    | 160      |
|    total_timesteps | 37888    |
---------------------------------
Eval num_timesteps=38000, episode_reward=2071.40 +/- 479.19
Episode length: 41.90 +/- 2.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.07e+03  |
| time/                   |           |
|    total_timesteps      | 38000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.02e-12 |
|    explained_variance   | 0.551     |
|    learning_rate        | 0.001     |
|    loss                 | 1.47e+04  |
|    n_updates            | 370       |
|    policy_gradient_loss | -7.8e-10  |
|    value_loss           | 3.26e+04  |
---------------------------------------
Eval num_timesteps=38500, episode_reward=1981.76 +/- 532.62
Episode length: 41.64 +/- 2.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 236      |
|    iterations      | 38       |
|    time_elapsed    | 164      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=2075.15 +/- 472.50
Episode length: 41.94 +/- 2.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 39000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.05e-09 |
|    explained_variance   | 0.519     |
|    learning_rate        | 0.001     |
|    loss                 | 1.11e+04  |
|    n_updates            | 380       |
|    policy_gradient_loss | 5.01e-10  |
|    value_loss           | 3.42e+04  |
---------------------------------------
Eval num_timesteps=39500, episode_reward=1983.82 +/- 529.16
Episode length: 41.62 +/- 2.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 236      |
|    iterations      | 39       |
|    time_elapsed    | 168      |
|    total_timesteps | 39936    |
---------------------------------
Eval num_timesteps=40000, episode_reward=2076.39 +/- 467.93
Episode length: 41.78 +/- 2.39
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 41.8     |
|    mean_reward          | 2.08e+03 |
| time/                   |          |
|    total_timesteps      | 40000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.2e-11 |
|    explained_variance   | 0.489    |
|    learning_rate        | 0.001    |
|    loss                 | 1.99e+03 |
|    n_updates            | 390      |
|    policy_gradient_loss | 1.61e-09 |
|    value_loss           | 1.33e+04 |
--------------------------------------
Eval num_timesteps=40500, episode_reward=2133.18 +/- 399.43
Episode length: 42.30 +/- 1.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 237      |
|    iterations      | 40       |
|    time_elapsed    | 172      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=2082.78 +/- 454.18
Episode length: 41.80 +/- 1.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.41e-11 |
|    explained_variance   | 0.54      |
|    learning_rate        | 0.001     |
|    loss                 | 2.37e+04  |
|    n_updates            | 400       |
|    policy_gradient_loss | 6.52e-10  |
|    value_loss           | 4.53e+04  |
---------------------------------------
Eval num_timesteps=41500, episode_reward=2058.21 +/- 473.54
Episode length: 42.20 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 237      |
|    iterations      | 41       |
|    time_elapsed    | 176      |
|    total_timesteps | 41984    |
---------------------------------
Eval num_timesteps=42000, episode_reward=2103.89 +/- 439.19
Episode length: 42.28 +/- 2.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 42000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.04e-12 |
|    explained_variance   | 0.616     |
|    learning_rate        | 0.001     |
|    loss                 | 1.49e+04  |
|    n_updates            | 410       |
|    policy_gradient_loss | 1.51e-10  |
|    value_loss           | 2.64e+04  |
---------------------------------------
Eval num_timesteps=42500, episode_reward=2081.88 +/- 454.96
Episode length: 42.02 +/- 1.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=2098.51 +/- 450.93
Episode length: 42.00 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 236      |
|    iterations      | 42       |
|    time_elapsed    | 182      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=2156.85 +/- 372.79
Episode length: 42.38 +/- 1.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.16e+03  |
| time/                   |           |
|    total_timesteps      | 43500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.64e-10 |
|    explained_variance   | 0.632     |
|    learning_rate        | 0.001     |
|    loss                 | 7.7e+03   |
|    n_updates            | 420       |
|    policy_gradient_loss | -1.55e-09 |
|    value_loss           | 1.97e+04  |
---------------------------------------
Eval num_timesteps=44000, episode_reward=2082.52 +/- 453.80
Episode length: 42.00 +/- 2.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 236      |
|    iterations      | 43       |
|    time_elapsed    | 186      |
|    total_timesteps | 44032    |
---------------------------------
Eval num_timesteps=44500, episode_reward=2057.25 +/- 478.74
Episode length: 42.02 +/- 2.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 44500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.53e-12 |
|    explained_variance   | 0.608     |
|    learning_rate        | 0.001     |
|    loss                 | 1.14e+04  |
|    n_updates            | 430       |
|    policy_gradient_loss | 1.63e-10  |
|    value_loss           | 2.53e+04  |
---------------------------------------
Eval num_timesteps=45000, episode_reward=2006.23 +/- 516.94
Episode length: 42.00 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 237      |
|    iterations      | 44       |
|    time_elapsed    | 190      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=2126.73 +/- 416.88
Episode length: 42.06 +/- 2.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 45500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.12e-10 |
|    explained_variance   | 0.615     |
|    learning_rate        | 0.001     |
|    loss                 | 6.54e+03  |
|    n_updates            | 440       |
|    policy_gradient_loss | -5.2e-10  |
|    value_loss           | 2.49e+04  |
---------------------------------------
Eval num_timesteps=46000, episode_reward=2026.14 +/- 509.57
Episode length: 41.86 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 237      |
|    iterations      | 45       |
|    time_elapsed    | 193      |
|    total_timesteps | 46080    |
---------------------------------
Eval num_timesteps=46500, episode_reward=2012.38 +/- 535.25
Episode length: 41.54 +/- 3.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.5      |
|    mean_reward          | 2.01e+03  |
| time/                   |           |
|    total_timesteps      | 46500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.72e-10 |
|    explained_variance   | 0.548     |
|    learning_rate        | 0.001     |
|    loss                 | 6.97e+03  |
|    n_updates            | 450       |
|    policy_gradient_loss | 7.57e-11  |
|    value_loss           | 3.07e+04  |
---------------------------------------
Eval num_timesteps=47000, episode_reward=2158.62 +/- 368.14
Episode length: 42.34 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 237      |
|    iterations      | 46       |
|    time_elapsed    | 197      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=2075.12 +/- 469.96
Episode length: 41.56 +/- 2.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.6      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 47500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.61e-09 |
|    explained_variance   | 0.381     |
|    learning_rate        | 0.001     |
|    loss                 | 5.77e+03  |
|    n_updates            | 460       |
|    policy_gradient_loss | -7.57e-10 |
|    value_loss           | 2.45e+04  |
---------------------------------------
Eval num_timesteps=48000, episode_reward=2100.53 +/- 448.54
Episode length: 41.86 +/- 2.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 238      |
|    iterations      | 47       |
|    time_elapsed    | 201      |
|    total_timesteps | 48128    |
---------------------------------
Eval num_timesteps=48500, episode_reward=2083.02 +/- 453.63
Episode length: 42.08 +/- 1.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 48500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.37e-09 |
|    explained_variance   | 0.554     |
|    learning_rate        | 0.001     |
|    loss                 | 1.03e+04  |
|    n_updates            | 470       |
|    policy_gradient_loss | -5.82e-11 |
|    value_loss           | 3.67e+04  |
---------------------------------------
Eval num_timesteps=49000, episode_reward=2106.28 +/- 431.95
Episode length: 42.28 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 238      |
|    iterations      | 48       |
|    time_elapsed    | 205      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=1904.12 +/- 576.64
Episode length: 41.58 +/- 2.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.6      |
|    mean_reward          | 1.9e+03   |
| time/                   |           |
|    total_timesteps      | 49500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.82e-10 |
|    explained_variance   | 0.569     |
|    learning_rate        | 0.001     |
|    loss                 | 8.44e+03  |
|    n_updates            | 480       |
|    policy_gradient_loss | 4.13e-10  |
|    value_loss           | 1.99e+04  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=2179.49 +/- 343.87
Episode length: 42.38 +/- 1.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 239      |
|    iterations      | 49       |
|    time_elapsed    | 209      |
|    total_timesteps | 50176    |
---------------------------------
Eval num_timesteps=50500, episode_reward=2086.30 +/- 444.70
Episode length: 42.20 +/- 1.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.09e+03  |
| time/                   |           |
|    total_timesteps      | 50500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.51e-10 |
|    explained_variance   | 0.641     |
|    learning_rate        | 0.001     |
|    loss                 | 1.07e+04  |
|    n_updates            | 490       |
|    policy_gradient_loss | -5.21e-10 |
|    value_loss           | 2.84e+04  |
---------------------------------------
Eval num_timesteps=51000, episode_reward=1978.63 +/- 538.60
Episode length: 41.66 +/- 2.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 239      |
|    iterations      | 50       |
|    time_elapsed    | 213      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=2182.22 +/- 331.52
Episode length: 42.64 +/- 1.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.6      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 51500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.77e-08 |
|    explained_variance   | 0.34      |
|    learning_rate        | 0.001     |
|    loss                 | 2.7e+04   |
|    n_updates            | 500       |
|    policy_gradient_loss | -2.85e-10 |
|    value_loss           | 5.45e+04  |
---------------------------------------
Eval num_timesteps=52000, episode_reward=2104.68 +/- 434.97
Episode length: 42.24 +/- 1.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 239      |
|    iterations      | 51       |
|    time_elapsed    | 217      |
|    total_timesteps | 52224    |
---------------------------------
Eval num_timesteps=52500, episode_reward=2084.35 +/- 451.09
Episode length: 42.30 +/- 2.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 52500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.87e-09 |
|    explained_variance   | 0.51      |
|    learning_rate        | 0.001     |
|    loss                 | 2.17e+04  |
|    n_updates            | 510       |
|    policy_gradient_loss | -3.55e-10 |
|    value_loss           | 4.74e+04  |
---------------------------------------
Eval num_timesteps=53000, episode_reward=1993.15 +/- 541.83
Episode length: 41.38 +/- 3.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 240      |
|    iterations      | 52       |
|    time_elapsed    | 221      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=1988.64 +/- 521.30
Episode length: 41.88 +/- 2.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 1.99e+03  |
| time/                   |           |
|    total_timesteps      | 53500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.26e-09 |
|    explained_variance   | 0.526     |
|    learning_rate        | 0.001     |
|    loss                 | 1.44e+04  |
|    n_updates            | 520       |
|    policy_gradient_loss | 5.82e-10  |
|    value_loss           | 3.42e+04  |
---------------------------------------
Eval num_timesteps=54000, episode_reward=1947.36 +/- 559.04
Episode length: 41.52 +/- 2.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 240      |
|    iterations      | 53       |
|    time_elapsed    | 225      |
|    total_timesteps | 54272    |
---------------------------------
Eval num_timesteps=54500, episode_reward=2106.02 +/- 432.63
Episode length: 42.10 +/- 2.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.11e+03  |
| time/                   |           |
|    total_timesteps      | 54500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.21e-10 |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.001     |
|    loss                 | 8.86e+03  |
|    n_updates            | 530       |
|    policy_gradient_loss | -8.27e-10 |
|    value_loss           | 3.5e+04   |
---------------------------------------
Eval num_timesteps=55000, episode_reward=2007.45 +/- 514.58
Episode length: 41.92 +/- 1.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 240      |
|    iterations      | 54       |
|    time_elapsed    | 229      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=2006.91 +/- 514.50
Episode length: 41.84 +/- 2.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2.01e+03  |
| time/                   |           |
|    total_timesteps      | 55500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.02e-10 |
|    explained_variance   | 0.484     |
|    learning_rate        | 0.001     |
|    loss                 | 1.85e+04  |
|    n_updates            | 540       |
|    policy_gradient_loss | 1.46e-11  |
|    value_loss           | 3.16e+04  |
---------------------------------------
Eval num_timesteps=56000, episode_reward=2128.74 +/- 410.58
Episode length: 42.10 +/- 1.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 240      |
|    iterations      | 55       |
|    time_elapsed    | 233      |
|    total_timesteps | 56320    |
---------------------------------
Eval num_timesteps=56500, episode_reward=2077.03 +/- 466.37
Episode length: 41.86 +/- 2.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 56500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.05e-09 |
|    explained_variance   | 0.551     |
|    learning_rate        | 0.001     |
|    loss                 | 4.51e+03  |
|    n_updates            | 550       |
|    policy_gradient_loss | 2.67e-09  |
|    value_loss           | 1.5e+04   |
---------------------------------------
Eval num_timesteps=57000, episode_reward=2133.75 +/- 397.32
Episode length: 42.34 +/- 1.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 56       |
|    time_elapsed    | 237      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=2072.53 +/- 479.94
Episode length: 41.74 +/- 2.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.07e+03  |
| time/                   |           |
|    total_timesteps      | 57500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.98e-10 |
|    explained_variance   | 0.659     |
|    learning_rate        | 0.001     |
|    loss                 | 5.69e+03  |
|    n_updates            | 560       |
|    policy_gradient_loss | 6.26e-10  |
|    value_loss           | 1.78e+04  |
---------------------------------------
Eval num_timesteps=58000, episode_reward=2100.57 +/- 448.75
Episode length: 41.98 +/- 2.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 57       |
|    time_elapsed    | 241      |
|    total_timesteps | 58368    |
---------------------------------
Eval num_timesteps=58500, episode_reward=2037.29 +/- 489.88
Episode length: 41.90 +/- 2.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.04e+03  |
| time/                   |           |
|    total_timesteps      | 58500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.95e-10 |
|    explained_variance   | 0.621     |
|    learning_rate        | 0.001     |
|    loss                 | 1.31e+04  |
|    n_updates            | 570       |
|    policy_gradient_loss | 4.07e-10  |
|    value_loss           | 4.35e+04  |
---------------------------------------
Eval num_timesteps=59000, episode_reward=2100.72 +/- 446.58
Episode length: 42.16 +/- 2.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 58       |
|    time_elapsed    | 246      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=2135.03 +/- 395.14
Episode length: 42.26 +/- 1.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.14e+03  |
| time/                   |           |
|    total_timesteps      | 59500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.97e-09 |
|    explained_variance   | 0.583     |
|    learning_rate        | 0.001     |
|    loss                 | 9.86e+03  |
|    n_updates            | 580       |
|    policy_gradient_loss | 1.28e-09  |
|    value_loss           | 2.72e+04  |
---------------------------------------
Eval num_timesteps=60000, episode_reward=1925.58 +/- 570.75
Episode length: 41.68 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 59       |
|    time_elapsed    | 250      |
|    total_timesteps | 60416    |
---------------------------------
Eval num_timesteps=60500, episode_reward=2134.57 +/- 389.79
Episode length: 42.58 +/- 1.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.6      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 60500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.52e-10 |
|    explained_variance   | 0.56      |
|    learning_rate        | 0.001     |
|    loss                 | 1.09e+04  |
|    n_updates            | 590       |
|    policy_gradient_loss | 2.5e-10   |
|    value_loss           | 3.78e+04  |
---------------------------------------
Eval num_timesteps=61000, episode_reward=2043.70 +/- 503.53
Episode length: 41.38 +/- 3.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 60       |
|    time_elapsed    | 254      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=2078.46 +/- 464.67
Episode length: 41.86 +/- 2.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.29e-10 |
|    explained_variance   | 0.51      |
|    learning_rate        | 0.001     |
|    loss                 | 2.74e+04  |
|    n_updates            | 600       |
|    policy_gradient_loss | 4.31e-10  |
|    value_loss           | 3.48e+04  |
---------------------------------------
Eval num_timesteps=62000, episode_reward=2127.69 +/- 416.08
Episode length: 42.42 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 61       |
|    time_elapsed    | 258      |
|    total_timesteps | 62464    |
---------------------------------
Eval num_timesteps=62500, episode_reward=1964.48 +/- 565.03
Episode length: 41.18 +/- 3.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.2      |
|    mean_reward          | 1.96e+03  |
| time/                   |           |
|    total_timesteps      | 62500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.06e-10 |
|    explained_variance   | 0.592     |
|    learning_rate        | 0.001     |
|    loss                 | 4.22e+03  |
|    n_updates            | 610       |
|    policy_gradient_loss | 5.27e-10  |
|    value_loss           | 2.11e+04  |
---------------------------------------
Eval num_timesteps=63000, episode_reward=2106.36 +/- 432.70
Episode length: 41.84 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 242      |
|    iterations      | 62       |
|    time_elapsed    | 262      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=2126.47 +/- 416.62
Episode length: 42.00 +/- 2.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 63500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-10 |
|    explained_variance   | 0.623     |
|    learning_rate        | 0.001     |
|    loss                 | 6.85e+03  |
|    n_updates            | 620       |
|    policy_gradient_loss | -5.41e-10 |
|    value_loss           | 2.26e+04  |
---------------------------------------
Eval num_timesteps=64000, episode_reward=1973.69 +/- 547.46
Episode length: 41.84 +/- 2.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=2106.83 +/- 430.76
Episode length: 42.22 +/- 1.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 63       |
|    time_elapsed    | 267      |
|    total_timesteps | 64512    |
---------------------------------
Eval num_timesteps=65000, episode_reward=2180.29 +/- 338.98
Episode length: 42.46 +/- 1.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 65000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.02e-09 |
|    explained_variance   | 0.614     |
|    learning_rate        | 0.001     |
|    loss                 | 4.87e+03  |
|    n_updates            | 630       |
|    policy_gradient_loss | -3.73e-10 |
|    value_loss           | 2.6e+04   |
---------------------------------------
Eval num_timesteps=65500, episode_reward=1981.38 +/- 533.14
Episode length: 41.86 +/- 2.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 241      |
|    iterations      | 64       |
|    time_elapsed    | 271      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=2101.59 +/- 436.86
Episode length: 42.14 +/- 1.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 66000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.49e-09 |
|    explained_variance   | 0.48      |
|    learning_rate        | 0.001     |
|    loss                 | 1.15e+04  |
|    n_updates            | 640       |
|    policy_gradient_loss | 1.13e-09  |
|    value_loss           | 4.22e+04  |
---------------------------------------
Eval num_timesteps=66500, episode_reward=1982.66 +/- 531.46
Episode length: 41.80 +/- 2.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.7     |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 65       |
|    time_elapsed    | 275      |
|    total_timesteps | 66560    |
---------------------------------
Eval num_timesteps=67000, episode_reward=2034.91 +/- 491.24
Episode length: 41.76 +/- 1.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2.03e+03  |
| time/                   |           |
|    total_timesteps      | 67000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.6e-10  |
|    explained_variance   | 0.535     |
|    learning_rate        | 0.001     |
|    loss                 | 1.87e+04  |
|    n_updates            | 650       |
|    policy_gradient_loss | -1.62e-09 |
|    value_loss           | 4.03e+04  |
---------------------------------------
Eval num_timesteps=67500, episode_reward=2047.21 +/- 500.63
Episode length: 41.46 +/- 2.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 242      |
|    iterations      | 66       |
|    time_elapsed    | 279      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=2077.37 +/- 468.02
Episode length: 42.24 +/- 2.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 68000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.97e-11 |
|    explained_variance   | 0.332     |
|    learning_rate        | 0.001     |
|    loss                 | 3.1e+03   |
|    n_updates            | 660       |
|    policy_gradient_loss | 6.29e-10  |
|    value_loss           | 1.64e+04  |
---------------------------------------
Eval num_timesteps=68500, episode_reward=2130.74 +/- 406.99
Episode length: 42.06 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.7     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 242      |
|    iterations      | 67       |
|    time_elapsed    | 283      |
|    total_timesteps | 68608    |
---------------------------------
Eval num_timesteps=69000, episode_reward=2186.53 +/- 317.64
Episode length: 42.86 +/- 1.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.9      |
|    mean_reward          | 2.19e+03  |
| time/                   |           |
|    total_timesteps      | 69000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.18e-11 |
|    explained_variance   | 0.637     |
|    learning_rate        | 0.001     |
|    loss                 | 9.18e+03  |
|    n_updates            | 670       |
|    policy_gradient_loss | -4.45e-10 |
|    value_loss           | 2.42e+04  |
---------------------------------------
Eval num_timesteps=69500, episode_reward=2096.59 +/- 450.94
Episode length: 41.78 +/- 2.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 242      |
|    iterations      | 68       |
|    time_elapsed    | 287      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=2203.81 +/- 306.68
Episode length: 42.46 +/- 1.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.2e+03   |
| time/                   |           |
|    total_timesteps      | 70000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.14e-10 |
|    explained_variance   | 0.546     |
|    learning_rate        | 0.001     |
|    loss                 | 4.58e+03  |
|    n_updates            | 680       |
|    policy_gradient_loss | -4.19e-10 |
|    value_loss           | 2.82e+04  |
---------------------------------------
Eval num_timesteps=70500, episode_reward=2123.27 +/- 425.05
Episode length: 41.84 +/- 2.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 242      |
|    iterations      | 69       |
|    time_elapsed    | 290      |
|    total_timesteps | 70656    |
---------------------------------
Eval num_timesteps=71000, episode_reward=2046.69 +/- 499.34
Episode length: 41.64 +/- 2.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.6      |
|    mean_reward          | 2.05e+03  |
| time/                   |           |
|    total_timesteps      | 71000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.5e-10  |
|    explained_variance   | 0.598     |
|    learning_rate        | 0.001     |
|    loss                 | 3.8e+03   |
|    n_updates            | 690       |
|    policy_gradient_loss | -7.74e-10 |
|    value_loss           | 1.54e+04  |
---------------------------------------
Eval num_timesteps=71500, episode_reward=1999.86 +/- 531.35
Episode length: 41.70 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 243      |
|    iterations      | 70       |
|    time_elapsed    | 294      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=2152.87 +/- 386.84
Episode length: 42.08 +/- 2.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 72000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.11e-10 |
|    explained_variance   | 0.682     |
|    learning_rate        | 0.001     |
|    loss                 | 5.88e+03  |
|    n_updates            | 700       |
|    policy_gradient_loss | 1.46e-10  |
|    value_loss           | 1.92e+04  |
---------------------------------------
Eval num_timesteps=72500, episode_reward=2182.96 +/- 331.22
Episode length: 42.54 +/- 1.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 243      |
|    iterations      | 71       |
|    time_elapsed    | 298      |
|    total_timesteps | 72704    |
---------------------------------
Eval num_timesteps=73000, episode_reward=2160.28 +/- 361.30
Episode length: 42.52 +/- 1.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.16e+03  |
| time/                   |           |
|    total_timesteps      | 73000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.54e-11 |
|    explained_variance   | 0.692     |
|    learning_rate        | 0.001     |
|    loss                 | 4.17e+03  |
|    n_updates            | 710       |
|    policy_gradient_loss | 1.23e-09  |
|    value_loss           | 2.02e+04  |
---------------------------------------
Eval num_timesteps=73500, episode_reward=1985.17 +/- 527.77
Episode length: 41.86 +/- 2.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 243      |
|    iterations      | 72       |
|    time_elapsed    | 302      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=2076.97 +/- 467.82
Episode length: 41.90 +/- 2.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 74000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08e-09 |
|    explained_variance   | 0.504     |
|    learning_rate        | 0.001     |
|    loss                 | 1.01e+04  |
|    n_updates            | 720       |
|    policy_gradient_loss | -1.05e-09 |
|    value_loss           | 3.23e+04  |
---------------------------------------
Eval num_timesteps=74500, episode_reward=2133.84 +/- 398.37
Episode length: 42.32 +/- 1.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 243      |
|    iterations      | 73       |
|    time_elapsed    | 306      |
|    total_timesteps | 74752    |
---------------------------------
Eval num_timesteps=75000, episode_reward=2131.46 +/- 403.66
Episode length: 42.34 +/- 1.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 75000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.44e-10 |
|    explained_variance   | 0.589     |
|    learning_rate        | 0.001     |
|    loss                 | 1.04e+04  |
|    n_updates            | 730       |
|    policy_gradient_loss | -1.22e-09 |
|    value_loss           | 3.31e+04  |
---------------------------------------
Eval num_timesteps=75500, episode_reward=2009.76 +/- 510.46
Episode length: 41.88 +/- 1.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 243      |
|    iterations      | 74       |
|    time_elapsed    | 310      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=2060.57 +/- 469.43
Episode length: 42.04 +/- 2.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 76000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.16e-10 |
|    explained_variance   | 0.604     |
|    learning_rate        | 0.001     |
|    loss                 | 1.63e+04  |
|    n_updates            | 740       |
|    policy_gradient_loss | 9.78e-10  |
|    value_loss           | 3.11e+04  |
---------------------------------------
Eval num_timesteps=76500, episode_reward=2092.10 +/- 467.74
Episode length: 41.98 +/- 2.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 244      |
|    iterations      | 75       |
|    time_elapsed    | 314      |
|    total_timesteps | 76800    |
---------------------------------
Eval num_timesteps=77000, episode_reward=2160.02 +/- 363.10
Episode length: 42.56 +/- 1.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.6      |
|    mean_reward          | 2.16e+03  |
| time/                   |           |
|    total_timesteps      | 77000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.43e-10 |
|    explained_variance   | 0.565     |
|    learning_rate        | 0.001     |
|    loss                 | 5.61e+03  |
|    n_updates            | 750       |
|    policy_gradient_loss | 3.67e-10  |
|    value_loss           | 2.97e+04  |
---------------------------------------
Eval num_timesteps=77500, episode_reward=1978.63 +/- 539.42
Episode length: 41.62 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 244      |
|    iterations      | 76       |
|    time_elapsed    | 318      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=2077.08 +/- 466.32
Episode length: 41.80 +/- 2.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 78000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.59e-08 |
|    explained_variance   | 0.53      |
|    learning_rate        | 0.001     |
|    loss                 | 1.41e+04  |
|    n_updates            | 760       |
|    policy_gradient_loss | -3.92e-09 |
|    value_loss           | 2.85e+04  |
---------------------------------------
Eval num_timesteps=78500, episode_reward=2126.00 +/- 421.06
Episode length: 41.94 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 244      |
|    iterations      | 77       |
|    time_elapsed    | 322      |
|    total_timesteps | 78848    |
---------------------------------
Eval num_timesteps=79000, episode_reward=2094.89 +/- 462.26
Episode length: 42.04 +/- 3.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.09e+03  |
| time/                   |           |
|    total_timesteps      | 79000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.22e-10 |
|    explained_variance   | 0.527     |
|    learning_rate        | 0.001     |
|    loss                 | 3.07e+03  |
|    n_updates            | 770       |
|    policy_gradient_loss | 3.49e-11  |
|    value_loss           | 2.5e+04   |
---------------------------------------
Eval num_timesteps=79500, episode_reward=2036.63 +/- 487.94
Episode length: 41.98 +/- 1.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 244      |
|    iterations      | 78       |
|    time_elapsed    | 326      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=2125.87 +/- 421.94
Episode length: 42.28 +/- 2.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.39e-10 |
|    explained_variance   | 0.51      |
|    learning_rate        | 0.001     |
|    loss                 | 6.2e+03   |
|    n_updates            | 780       |
|    policy_gradient_loss | -8.61e-10 |
|    value_loss           | 1.79e+04  |
---------------------------------------
Eval num_timesteps=80500, episode_reward=2160.58 +/- 362.20
Episode length: 42.36 +/- 1.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 244      |
|    iterations      | 79       |
|    time_elapsed    | 330      |
|    total_timesteps | 80896    |
---------------------------------
Eval num_timesteps=81000, episode_reward=1949.18 +/- 561.56
Episode length: 41.68 +/- 3.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 1.95e+03  |
| time/                   |           |
|    total_timesteps      | 81000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.37e-11 |
|    explained_variance   | 0.685     |
|    learning_rate        | 0.001     |
|    loss                 | 3.91e+03  |
|    n_updates            | 790       |
|    policy_gradient_loss | 1.3e-09   |
|    value_loss           | 1.09e+04  |
---------------------------------------
Eval num_timesteps=81500, episode_reward=2155.66 +/- 377.09
Episode length: 42.36 +/- 1.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 245      |
|    iterations      | 80       |
|    time_elapsed    | 334      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=2071.29 +/- 483.30
Episode length: 41.88 +/- 2.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.07e+03  |
| time/                   |           |
|    total_timesteps      | 82000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.12e-11 |
|    explained_variance   | 0.574     |
|    learning_rate        | 0.001     |
|    loss                 | 5.07e+03  |
|    n_updates            | 800       |
|    policy_gradient_loss | -1.41e-09 |
|    value_loss           | 2.62e+04  |
---------------------------------------
Eval num_timesteps=82500, episode_reward=2088.26 +/- 445.31
Episode length: 42.30 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 245      |
|    iterations      | 81       |
|    time_elapsed    | 338      |
|    total_timesteps | 82944    |
---------------------------------
Eval num_timesteps=83000, episode_reward=2139.68 +/- 383.42
Episode length: 42.34 +/- 1.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.14e+03  |
| time/                   |           |
|    total_timesteps      | 83000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.94e-11 |
|    explained_variance   | 0.536     |
|    learning_rate        | 0.001     |
|    loss                 | 6.24e+03  |
|    n_updates            | 810       |
|    policy_gradient_loss | 6.69e-10  |
|    value_loss           | 1.56e+04  |
---------------------------------------
Eval num_timesteps=83500, episode_reward=2119.87 +/- 437.83
Episode length: 41.76 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 245      |
|    iterations      | 82       |
|    time_elapsed    | 342      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=1981.44 +/- 533.98
Episode length: 41.66 +/- 2.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 1.98e+03  |
| time/                   |           |
|    total_timesteps      | 84000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.02e-10 |
|    explained_variance   | 0.556     |
|    learning_rate        | 0.001     |
|    loss                 | 1.68e+04  |
|    n_updates            | 820       |
|    policy_gradient_loss | -7.89e-10 |
|    value_loss           | 4.23e+04  |
---------------------------------------
Eval num_timesteps=84500, episode_reward=2128.51 +/- 412.69
Episode length: 42.08 +/- 2.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2e+03    |
| time/              |          |
|    fps             | 245      |
|    iterations      | 83       |
|    time_elapsed    | 346      |
|    total_timesteps | 84992    |
---------------------------------
Eval num_timesteps=85000, episode_reward=2081.51 +/- 456.30
Episode length: 42.06 +/- 2.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 85000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.67e-10 |
|    explained_variance   | 0.53      |
|    learning_rate        | 0.001     |
|    loss                 | 9.42e+03  |
|    n_updates            | 830       |
|    policy_gradient_loss | -1.51e-10 |
|    value_loss           | 4.43e+04  |
---------------------------------------
Eval num_timesteps=85500, episode_reward=2011.99 +/- 506.43
Episode length: 42.08 +/- 2.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=2145.99 +/- 401.72
Episode length: 41.96 +/- 2.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 244      |
|    iterations      | 84       |
|    time_elapsed    | 351      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=1965.17 +/- 564.12
Episode length: 41.26 +/- 3.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.3      |
|    mean_reward          | 1.97e+03  |
| time/                   |           |
|    total_timesteps      | 86500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.52e-11 |
|    explained_variance   | 0.476     |
|    learning_rate        | 0.001     |
|    loss                 | 2.29e+03  |
|    n_updates            | 840       |
|    policy_gradient_loss | 1.57e-10  |
|    value_loss           | 6.79e+03  |
---------------------------------------
Eval num_timesteps=87000, episode_reward=2062.02 +/- 463.25
Episode length: 42.10 +/- 2.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 244      |
|    iterations      | 85       |
|    time_elapsed    | 355      |
|    total_timesteps | 87040    |
---------------------------------
Eval num_timesteps=87500, episode_reward=2030.94 +/- 501.68
Episode length: 41.64 +/- 2.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.6      |
|    mean_reward          | 2.03e+03  |
| time/                   |           |
|    total_timesteps      | 87500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.91e-11 |
|    explained_variance   | 0.523     |
|    learning_rate        | 0.001     |
|    loss                 | 5.12e+03  |
|    n_updates            | 850       |
|    policy_gradient_loss | 4.25e-10  |
|    value_loss           | 3.18e+04  |
---------------------------------------
Eval num_timesteps=88000, episode_reward=2159.20 +/- 363.95
Episode length: 42.40 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 245      |
|    iterations      | 86       |
|    time_elapsed    | 359      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=2156.74 +/- 374.23
Episode length: 42.36 +/- 1.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.16e+03  |
| time/                   |           |
|    total_timesteps      | 88500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.01e-11 |
|    explained_variance   | 0.569     |
|    learning_rate        | 0.001     |
|    loss                 | 5.15e+03  |
|    n_updates            | 860       |
|    policy_gradient_loss | -3.26e-10 |
|    value_loss           | 2.46e+04  |
---------------------------------------
Eval num_timesteps=89000, episode_reward=2054.43 +/- 484.25
Episode length: 41.76 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 245      |
|    iterations      | 87       |
|    time_elapsed    | 363      |
|    total_timesteps | 89088    |
---------------------------------
Eval num_timesteps=89500, episode_reward=2052.49 +/- 488.08
Episode length: 41.96 +/- 2.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.05e+03  |
| time/                   |           |
|    total_timesteps      | 89500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.46e-11 |
|    explained_variance   | 0.617     |
|    learning_rate        | 0.001     |
|    loss                 | 8.76e+03  |
|    n_updates            | 870       |
|    policy_gradient_loss | 5.12e-10  |
|    value_loss           | 2.87e+04  |
---------------------------------------
Eval num_timesteps=90000, episode_reward=2107.49 +/- 430.53
Episode length: 42.14 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 245      |
|    iterations      | 88       |
|    time_elapsed    | 367      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=2150.90 +/- 387.31
Episode length: 41.88 +/- 1.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 90500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.95e-10 |
|    explained_variance   | 0.525     |
|    learning_rate        | 0.001     |
|    loss                 | 1.13e+04  |
|    n_updates            | 880       |
|    policy_gradient_loss | -1.05e-09 |
|    value_loss           | 4.66e+04  |
---------------------------------------
Eval num_timesteps=91000, episode_reward=2060.49 +/- 470.35
Episode length: 42.14 +/- 2.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 245      |
|    iterations      | 89       |
|    time_elapsed    | 371      |
|    total_timesteps | 91136    |
---------------------------------
Eval num_timesteps=91500, episode_reward=2054.33 +/- 483.81
Episode length: 41.96 +/- 2.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.05e+03  |
| time/                   |           |
|    total_timesteps      | 91500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.96e-11 |
|    explained_variance   | 0.638     |
|    learning_rate        | 0.001     |
|    loss                 | 6.61e+03  |
|    n_updates            | 890       |
|    policy_gradient_loss | -1.78e-09 |
|    value_loss           | 1.69e+04  |
---------------------------------------
Eval num_timesteps=92000, episode_reward=2135.53 +/- 393.07
Episode length: 42.68 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 245      |
|    iterations      | 90       |
|    time_elapsed    | 375      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=2095.22 +/- 459.42
Episode length: 41.94 +/- 3.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 92500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.95e-10 |
|    explained_variance   | 0.523     |
|    learning_rate        | 0.001     |
|    loss                 | 1.45e+04  |
|    n_updates            | 900       |
|    policy_gradient_loss | 5.53e-10  |
|    value_loss           | 3.71e+04  |
---------------------------------------
Eval num_timesteps=93000, episode_reward=2130.08 +/- 408.04
Episode length: 42.42 +/- 1.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 245      |
|    iterations      | 91       |
|    time_elapsed    | 378      |
|    total_timesteps | 93184    |
---------------------------------
Eval num_timesteps=93500, episode_reward=2091.40 +/- 435.53
Episode length: 42.34 +/- 1.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.09e+03  |
| time/                   |           |
|    total_timesteps      | 93500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.45e-11 |
|    explained_variance   | 0.656     |
|    learning_rate        | 0.001     |
|    loss                 | 775       |
|    n_updates            | 910       |
|    policy_gradient_loss | 6.2e-10   |
|    value_loss           | 4.03e+03  |
---------------------------------------
Eval num_timesteps=94000, episode_reward=2128.41 +/- 411.33
Episode length: 42.04 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 246      |
|    iterations      | 92       |
|    time_elapsed    | 382      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=2084.59 +/- 449.69
Episode length: 42.02 +/- 1.94
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 42       |
|    mean_reward          | 2.08e+03 |
| time/                   |          |
|    total_timesteps      | 94500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.5e-11 |
|    explained_variance   | 0.747    |
|    learning_rate        | 0.001    |
|    loss                 | 1.65e+03 |
|    n_updates            | 920      |
|    policy_gradient_loss | 1.2e-09  |
|    value_loss           | 9.95e+03 |
--------------------------------------
Eval num_timesteps=95000, episode_reward=2074.03 +/- 474.44
Episode length: 41.70 +/- 2.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 246      |
|    iterations      | 93       |
|    time_elapsed    | 386      |
|    total_timesteps | 95232    |
---------------------------------
Eval num_timesteps=95500, episode_reward=2163.61 +/- 350.45
Episode length: 42.58 +/- 1.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.6      |
|    mean_reward          | 2.16e+03  |
| time/                   |           |
|    total_timesteps      | 95500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.56e-08 |
|    explained_variance   | 0.693     |
|    learning_rate        | 0.001     |
|    loss                 | 6.8e+03   |
|    n_updates            | 930       |
|    policy_gradient_loss | -1.13e-08 |
|    value_loss           | 1.7e+04   |
---------------------------------------
Eval num_timesteps=96000, episode_reward=2079.06 +/- 463.47
Episode length: 42.00 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 246      |
|    iterations      | 94       |
|    time_elapsed    | 390      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=2133.02 +/- 399.54
Episode length: 42.04 +/- 1.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 96500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.65e-10 |
|    explained_variance   | 0.597     |
|    learning_rate        | 0.001     |
|    loss                 | 5.24e+03  |
|    n_updates            | 940       |
|    policy_gradient_loss | 5.53e-10  |
|    value_loss           | 2.31e+04  |
---------------------------------------
Eval num_timesteps=97000, episode_reward=2058.56 +/- 474.99
Episode length: 42.16 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 246      |
|    iterations      | 95       |
|    time_elapsed    | 394      |
|    total_timesteps | 97280    |
---------------------------------
Eval num_timesteps=97500, episode_reward=1983.28 +/- 529.71
Episode length: 41.66 +/- 2.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 1.98e+03  |
| time/                   |           |
|    total_timesteps      | 97500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.88e-10 |
|    explained_variance   | 0.639     |
|    learning_rate        | 0.001     |
|    loss                 | 1.31e+04  |
|    n_updates            | 950       |
|    policy_gradient_loss | -1.05e-09 |
|    value_loss           | 2.26e+04  |
---------------------------------------
Eval num_timesteps=98000, episode_reward=2109.77 +/- 424.91
Episode length: 42.26 +/- 1.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 246      |
|    iterations      | 96       |
|    time_elapsed    | 398      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=2137.13 +/- 389.65
Episode length: 42.52 +/- 1.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.14e+03  |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.04e-11 |
|    explained_variance   | 0.67      |
|    learning_rate        | 0.001     |
|    loss                 | 8.66e+03  |
|    n_updates            | 960       |
|    policy_gradient_loss | 3.67e-10  |
|    value_loss           | 2.24e+04  |
---------------------------------------
Eval num_timesteps=99000, episode_reward=2136.60 +/- 391.72
Episode length: 42.32 +/- 1.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 246      |
|    iterations      | 97       |
|    time_elapsed    | 402      |
|    total_timesteps | 99328    |
---------------------------------
Eval num_timesteps=99500, episode_reward=2229.46 +/- 252.87
Episode length: 42.34 +/- 1.67
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 42.3     |
|    mean_reward          | 2.23e+03 |
| time/                   |          |
|    total_timesteps      | 99500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -2e-11   |
|    explained_variance   | 0.61     |
|    learning_rate        | 0.001    |
|    loss                 | 1.11e+04 |
|    n_updates            | 970      |
|    policy_gradient_loss | 4.83e-10 |
|    value_loss           | 2.51e+04 |
--------------------------------------
Eval num_timesteps=100000, episode_reward=2160.50 +/- 359.01
Episode length: 42.76 +/- 1.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 246      |
|    iterations      | 98       |
|    time_elapsed    | 406      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=1956.91 +/- 548.29
Episode length: 41.78 +/- 2.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 1.96e+03  |
| time/                   |           |
|    total_timesteps      | 100500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.74e-11 |
|    explained_variance   | 0.521     |
|    learning_rate        | 0.001     |
|    loss                 | 1.22e+04  |
|    n_updates            | 980       |
|    policy_gradient_loss | 9.17e-10  |
|    value_loss           | 3.27e+04  |
---------------------------------------
Eval num_timesteps=101000, episode_reward=2173.75 +/- 361.42
Episode length: 42.12 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 246      |
|    iterations      | 99       |
|    time_elapsed    | 410      |
|    total_timesteps | 101376   |
---------------------------------
Eval num_timesteps=101500, episode_reward=2080.27 +/- 459.34
Episode length: 42.48 +/- 1.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 101500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.53e-10 |
|    explained_variance   | 0.548     |
|    learning_rate        | 0.001     |
|    loss                 | 1.36e+04  |
|    n_updates            | 990       |
|    policy_gradient_loss | -1.25e-10 |
|    value_loss           | 3.35e+04  |
---------------------------------------
Eval num_timesteps=102000, episode_reward=2114.17 +/- 413.04
Episode length: 42.22 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 247      |
|    iterations      | 100      |
|    time_elapsed    | 414      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=2133.91 +/- 397.38
Episode length: 42.68 +/- 1.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.7      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.2e-11  |
|    explained_variance   | 0.628     |
|    learning_rate        | 0.001     |
|    loss                 | 1.99e+03  |
|    n_updates            | 1000      |
|    policy_gradient_loss | -1.01e-09 |
|    value_loss           | 1.12e+04  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=2053.56 +/- 486.27
Episode length: 41.94 +/- 2.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 101      |
|    time_elapsed    | 418      |
|    total_timesteps | 103424   |
---------------------------------
Eval num_timesteps=103500, episode_reward=1985.82 +/- 527.18
Episode length: 41.90 +/- 2.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 1.99e+03  |
| time/                   |           |
|    total_timesteps      | 103500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.13e-11 |
|    explained_variance   | 0.633     |
|    learning_rate        | 0.001     |
|    loss                 | 5.99e+03  |
|    n_updates            | 1010      |
|    policy_gradient_loss | -5.82e-10 |
|    value_loss           | 1.89e+04  |
---------------------------------------
Eval num_timesteps=104000, episode_reward=2106.47 +/- 431.45
Episode length: 42.20 +/- 2.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 102      |
|    time_elapsed    | 422      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=2086.48 +/- 446.30
Episode length: 41.96 +/- 2.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.09e+03  |
| time/                   |           |
|    total_timesteps      | 104500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.47e-12 |
|    explained_variance   | 0.718     |
|    learning_rate        | 0.001     |
|    loss                 | 3.35e+03  |
|    n_updates            | 1020      |
|    policy_gradient_loss | 3.78e-10  |
|    value_loss           | 1.08e+04  |
---------------------------------------
Eval num_timesteps=105000, episode_reward=1952.89 +/- 556.41
Episode length: 41.54 +/- 2.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 103      |
|    time_elapsed    | 426      |
|    total_timesteps | 105472   |
---------------------------------
Eval num_timesteps=105500, episode_reward=2055.37 +/- 479.58
Episode length: 41.66 +/- 2.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 105500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.63e-11 |
|    explained_variance   | 0.567     |
|    learning_rate        | 0.001     |
|    loss                 | 1.18e+04  |
|    n_updates            | 1030      |
|    policy_gradient_loss | 1.44e-09  |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=106000, episode_reward=2099.09 +/- 449.58
Episode length: 41.98 +/- 2.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 104      |
|    time_elapsed    | 430      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=2011.60 +/- 507.41
Episode length: 42.04 +/- 2.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.01e+03  |
| time/                   |           |
|    total_timesteps      | 106500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.73e-11 |
|    explained_variance   | 0.657     |
|    learning_rate        | 0.001     |
|    loss                 | 4.51e+03  |
|    n_updates            | 1040      |
|    policy_gradient_loss | 1.5e-09   |
|    value_loss           | 2.25e+04  |
---------------------------------------
Eval num_timesteps=107000, episode_reward=1967.45 +/- 561.86
Episode length: 41.50 +/- 3.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=2148.51 +/- 395.20
Episode length: 42.30 +/- 2.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 246      |
|    iterations      | 105      |
|    time_elapsed    | 435      |
|    total_timesteps | 107520   |
---------------------------------
Eval num_timesteps=108000, episode_reward=2055.79 +/- 478.99
Episode length: 41.94 +/- 2.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 108000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.84e-10 |
|    explained_variance   | 0.466     |
|    learning_rate        | 0.001     |
|    loss                 | 6.81e+03  |
|    n_updates            | 1050      |
|    policy_gradient_loss | -4.83e-10 |
|    value_loss           | 3.45e+04  |
---------------------------------------
Eval num_timesteps=108500, episode_reward=2080.14 +/- 459.65
Episode length: 42.08 +/- 2.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 106      |
|    time_elapsed    | 439      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=1927.46 +/- 569.29
Episode length: 41.30 +/- 3.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.3      |
|    mean_reward          | 1.93e+03  |
| time/                   |           |
|    total_timesteps      | 109000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.67e-11 |
|    explained_variance   | 0.647     |
|    learning_rate        | 0.001     |
|    loss                 | 5.61e+03  |
|    n_updates            | 1060      |
|    policy_gradient_loss | -6.46e-10 |
|    value_loss           | 2.15e+04  |
---------------------------------------
Eval num_timesteps=109500, episode_reward=2112.42 +/- 415.67
Episode length: 42.42 +/- 1.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 107      |
|    time_elapsed    | 443      |
|    total_timesteps | 109568   |
---------------------------------
Eval num_timesteps=110000, episode_reward=2102.07 +/- 440.97
Episode length: 41.82 +/- 2.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 110000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.27e-10 |
|    explained_variance   | 0.492     |
|    learning_rate        | 0.001     |
|    loss                 | 1.15e+04  |
|    n_updates            | 1070      |
|    policy_gradient_loss | -5.82e-10 |
|    value_loss           | 3.59e+04  |
---------------------------------------
Eval num_timesteps=110500, episode_reward=2154.99 +/- 375.50
Episode length: 42.14 +/- 1.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 108      |
|    time_elapsed    | 447      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=2004.64 +/- 520.77
Episode length: 41.96 +/- 1.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 111000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.74e-11 |
|    explained_variance   | 0.512     |
|    learning_rate        | 0.001     |
|    loss                 | 1.32e+04  |
|    n_updates            | 1080      |
|    policy_gradient_loss | -5.24e-10 |
|    value_loss           | 2.67e+04  |
---------------------------------------
Eval num_timesteps=111500, episode_reward=2155.28 +/- 374.89
Episode length: 42.42 +/- 1.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.5     |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 109      |
|    time_elapsed    | 451      |
|    total_timesteps | 111616   |
---------------------------------
Eval num_timesteps=112000, episode_reward=2075.58 +/- 472.14
Episode length: 41.88 +/- 2.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 112000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.06e-10 |
|    explained_variance   | 0.529     |
|    learning_rate        | 0.001     |
|    loss                 | 1.32e+04  |
|    n_updates            | 1090      |
|    policy_gradient_loss | -6.93e-10 |
|    value_loss           | 3.68e+04  |
---------------------------------------
Eval num_timesteps=112500, episode_reward=2182.40 +/- 332.29
Episode length: 42.28 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.5     |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 110      |
|    time_elapsed    | 455      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=2206.93 +/- 290.79
Episode length: 42.44 +/- 1.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.21e+03  |
| time/                   |           |
|    total_timesteps      | 113000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.94e-10 |
|    explained_variance   | 0.365     |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+04  |
|    n_updates            | 1100      |
|    policy_gradient_loss | -4.66e-10 |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=113500, episode_reward=2097.33 +/- 457.03
Episode length: 41.72 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.5     |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 111      |
|    time_elapsed    | 459      |
|    total_timesteps | 113664   |
---------------------------------
Eval num_timesteps=114000, episode_reward=2130.29 +/- 407.46
Episode length: 42.30 +/- 2.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 114000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.12e-09 |
|    explained_variance   | 0.534     |
|    learning_rate        | 0.001     |
|    loss                 | 6.67e+03  |
|    n_updates            | 1110      |
|    policy_gradient_loss | 2.15e-10  |
|    value_loss           | 3.76e+04  |
---------------------------------------
Eval num_timesteps=114500, episode_reward=2100.13 +/- 449.51
Episode length: 41.98 +/- 2.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.3     |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 112      |
|    time_elapsed    | 463      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=2093.04 +/- 465.91
Episode length: 41.62 +/- 2.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.6      |
|    mean_reward          | 2.09e+03  |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.47e-09 |
|    explained_variance   | 0.452     |
|    learning_rate        | 0.001     |
|    loss                 | 8.95e+03  |
|    n_updates            | 1120      |
|    policy_gradient_loss | -2.04e-10 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=115500, episode_reward=2002.47 +/- 525.39
Episode length: 41.58 +/- 2.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.5     |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 113      |
|    time_elapsed    | 466      |
|    total_timesteps | 115712   |
---------------------------------
Eval num_timesteps=116000, episode_reward=2153.96 +/- 378.84
Episode length: 42.48 +/- 1.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 116000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.97e-10 |
|    explained_variance   | 0.452     |
|    learning_rate        | 0.001     |
|    loss                 | 1.11e+04  |
|    n_updates            | 1130      |
|    policy_gradient_loss | 2.56e-10  |
|    value_loss           | 4e+04     |
---------------------------------------
Eval num_timesteps=116500, episode_reward=2110.57 +/- 422.12
Episode length: 42.38 +/- 1.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 114      |
|    time_elapsed    | 470      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=1952.23 +/- 557.57
Episode length: 41.60 +/- 3.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.6      |
|    mean_reward          | 1.95e+03  |
| time/                   |           |
|    total_timesteps      | 117000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.08e-09 |
|    explained_variance   | 0.332     |
|    learning_rate        | 0.001     |
|    loss                 | 1.32e+04  |
|    n_updates            | 1140      |
|    policy_gradient_loss | -1.69e-09 |
|    value_loss           | 1.86e+04  |
---------------------------------------
Eval num_timesteps=117500, episode_reward=2056.93 +/- 477.36
Episode length: 41.82 +/- 1.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 247      |
|    iterations      | 115      |
|    time_elapsed    | 474      |
|    total_timesteps | 117760   |
---------------------------------
Eval num_timesteps=118000, episode_reward=2078.51 +/- 465.83
Episode length: 42.08 +/- 2.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 118000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.23e-09 |
|    explained_variance   | 0.612     |
|    learning_rate        | 0.001     |
|    loss                 | 1.1e+04   |
|    n_updates            | 1150      |
|    policy_gradient_loss | 1.4e-10   |
|    value_loss           | 2.06e+04  |
---------------------------------------
Eval num_timesteps=118500, episode_reward=2037.49 +/- 487.97
Episode length: 41.90 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 248      |
|    iterations      | 116      |
|    time_elapsed    | 478      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=2057.07 +/- 477.64
Episode length: 42.08 +/- 1.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 119000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.18e-08 |
|    explained_variance   | 0.54      |
|    learning_rate        | 0.001     |
|    loss                 | 1.03e+04  |
|    n_updates            | 1160      |
|    policy_gradient_loss | -3.55e-10 |
|    value_loss           | 3.35e+04  |
---------------------------------------
Eval num_timesteps=119500, episode_reward=2077.55 +/- 460.93
Episode length: 42.08 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 117      |
|    time_elapsed    | 482      |
|    total_timesteps | 119808   |
---------------------------------
Eval num_timesteps=120000, episode_reward=2027.82 +/- 503.10
Episode length: 42.16 +/- 2.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.03e+03  |
| time/                   |           |
|    total_timesteps      | 120000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.76e-09 |
|    explained_variance   | 0.631     |
|    learning_rate        | 0.001     |
|    loss                 | 6.49e+03  |
|    n_updates            | 1170      |
|    policy_gradient_loss | -2.33e-10 |
|    value_loss           | 2.88e+04  |
---------------------------------------
Eval num_timesteps=120500, episode_reward=2125.80 +/- 423.56
Episode length: 42.36 +/- 2.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 118      |
|    time_elapsed    | 486      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=2030.80 +/- 501.53
Episode length: 41.92 +/- 2.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.03e+03  |
| time/                   |           |
|    total_timesteps      | 121000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.83e-10 |
|    explained_variance   | 0.549     |
|    learning_rate        | 0.001     |
|    loss                 | 7.4e+03   |
|    n_updates            | 1180      |
|    policy_gradient_loss | 2.91e-11  |
|    value_loss           | 1.7e+04   |
---------------------------------------
Eval num_timesteps=121500, episode_reward=2207.93 +/- 285.89
Episode length: 42.70 +/- 1.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 119      |
|    time_elapsed    | 490      |
|    total_timesteps | 121856   |
---------------------------------
Eval num_timesteps=122000, episode_reward=2086.87 +/- 445.04
Episode length: 42.12 +/- 1.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.09e+03  |
| time/                   |           |
|    total_timesteps      | 122000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.22e-09 |
|    explained_variance   | 0.57      |
|    learning_rate        | 0.001     |
|    loss                 | 5.19e+03  |
|    n_updates            | 1190      |
|    policy_gradient_loss | 2.01e-10  |
|    value_loss           | 2.94e+04  |
---------------------------------------
Eval num_timesteps=122500, episode_reward=2097.77 +/- 443.94
Episode length: 42.16 +/- 2.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 120      |
|    time_elapsed    | 494      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=2023.83 +/- 513.33
Episode length: 42.08 +/- 2.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.02e+03  |
| time/                   |           |
|    total_timesteps      | 123000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.27e-09 |
|    explained_variance   | 0.705     |
|    learning_rate        | 0.001     |
|    loss                 | 2.82e+03  |
|    n_updates            | 1200      |
|    policy_gradient_loss | -2.49e-09 |
|    value_loss           | 9.76e+03  |
---------------------------------------
Eval num_timesteps=123500, episode_reward=2204.78 +/- 300.40
Episode length: 42.40 +/- 1.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 121      |
|    time_elapsed    | 498      |
|    total_timesteps | 123904   |
---------------------------------
Eval num_timesteps=124000, episode_reward=2184.38 +/- 325.52
Episode length: 42.60 +/- 1.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.6      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 124000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.01e-10 |
|    explained_variance   | 0.6       |
|    learning_rate        | 0.001     |
|    loss                 | 6.75e+03  |
|    n_updates            | 1210      |
|    policy_gradient_loss | 8.96e-10  |
|    value_loss           | 2.82e+04  |
---------------------------------------
Eval num_timesteps=124500, episode_reward=2096.65 +/- 457.81
Episode length: 41.90 +/- 2.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 122      |
|    time_elapsed    | 502      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=2155.82 +/- 373.03
Episode length: 42.24 +/- 1.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.16e+03  |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.72e-09 |
|    explained_variance   | 0.535     |
|    learning_rate        | 0.001     |
|    loss                 | 1.78e+04  |
|    n_updates            | 1220      |
|    policy_gradient_loss | -4.22e-10 |
|    value_loss           | 3.25e+04  |
---------------------------------------
Eval num_timesteps=125500, episode_reward=2103.02 +/- 433.33
Episode length: 41.88 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 123      |
|    time_elapsed    | 506      |
|    total_timesteps | 125952   |
---------------------------------
Eval num_timesteps=126000, episode_reward=2103.65 +/- 440.27
Episode length: 42.32 +/- 2.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 126000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-10 |
|    explained_variance   | 0.554     |
|    learning_rate        | 0.001     |
|    loss                 | 5.32e+03  |
|    n_updates            | 1230      |
|    policy_gradient_loss | -2.07e-09 |
|    value_loss           | 2.23e+04  |
---------------------------------------
Eval num_timesteps=126500, episode_reward=2083.96 +/- 450.44
Episode length: 42.06 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 124      |
|    time_elapsed    | 510      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=2106.55 +/- 433.47
Episode length: 42.14 +/- 2.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.11e+03  |
| time/                   |           |
|    total_timesteps      | 127000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.76e-10 |
|    explained_variance   | 0.583     |
|    learning_rate        | 0.001     |
|    loss                 | 1.54e+04  |
|    n_updates            | 1240      |
|    policy_gradient_loss | -9.28e-10 |
|    value_loss           | 2.22e+04  |
---------------------------------------
Eval num_timesteps=127500, episode_reward=2010.01 +/- 511.50
Episode length: 41.72 +/- 2.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=2052.51 +/- 489.08
Episode length: 41.76 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 125      |
|    time_elapsed    | 515      |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=2086.40 +/- 444.99
Episode length: 42.40 +/- 2.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.09e+03  |
| time/                   |           |
|    total_timesteps      | 128500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.38e-10 |
|    explained_variance   | 0.657     |
|    learning_rate        | 0.001     |
|    loss                 | 4.21e+03  |
|    n_updates            | 1250      |
|    policy_gradient_loss | -1.68e-09 |
|    value_loss           | 1.15e+04  |
---------------------------------------
Eval num_timesteps=129000, episode_reward=2135.87 +/- 392.49
Episode length: 42.38 +/- 1.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 126      |
|    time_elapsed    | 519      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=1959.26 +/- 540.37
Episode length: 41.92 +/- 2.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 1.96e+03  |
| time/                   |           |
|    total_timesteps      | 129500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.12e-11 |
|    explained_variance   | 0.743     |
|    learning_rate        | 0.001     |
|    loss                 | 3.99e+03  |
|    n_updates            | 1260      |
|    policy_gradient_loss | -6e-10    |
|    value_loss           | 9.73e+03  |
---------------------------------------
Eval num_timesteps=130000, episode_reward=2078.92 +/- 462.69
Episode length: 42.08 +/- 2.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 127      |
|    time_elapsed    | 523      |
|    total_timesteps | 130048   |
---------------------------------
Eval num_timesteps=130500, episode_reward=2183.40 +/- 330.36
Episode length: 42.32 +/- 1.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 130500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.57e-11 |
|    explained_variance   | 0.704     |
|    learning_rate        | 0.001     |
|    loss                 | 4.94e+03  |
|    n_updates            | 1270      |
|    policy_gradient_loss | -2.74e-10 |
|    value_loss           | 1.25e+04  |
---------------------------------------
Eval num_timesteps=131000, episode_reward=2199.45 +/- 321.82
Episode length: 42.00 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 128      |
|    time_elapsed    | 527      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=2206.47 +/- 289.95
Episode length: 42.54 +/- 1.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.5         |
|    mean_reward          | 2.21e+03     |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 4.958594e-06 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000201    |
|    explained_variance   | 0.61         |
|    learning_rate        | 0.001        |
|    loss                 | 8.8e+03      |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.000217    |
|    value_loss           | 2.02e+04     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=1972.08 +/- 550.82
Episode length: 41.42 +/- 3.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 129      |
|    time_elapsed    | 531      |
|    total_timesteps | 132096   |
---------------------------------
Eval num_timesteps=132500, episode_reward=2104.66 +/- 437.45
Episode length: 42.08 +/- 2.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 132500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.69e-09 |
|    explained_variance   | 0.541     |
|    learning_rate        | 0.001     |
|    loss                 | 5.69e+03  |
|    n_updates            | 1290      |
|    policy_gradient_loss | -4.35e-09 |
|    value_loss           | 2.22e+04  |
---------------------------------------
Eval num_timesteps=133000, episode_reward=2159.29 +/- 361.98
Episode length: 42.20 +/- 1.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 130      |
|    time_elapsed    | 535      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=2064.71 +/- 460.53
Episode length: 42.16 +/- 1.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 133500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.33e-10 |
|    explained_variance   | 0.58      |
|    learning_rate        | 0.001     |
|    loss                 | 2.78e+03  |
|    n_updates            | 1300      |
|    policy_gradient_loss | 1.16e-11  |
|    value_loss           | 1.23e+04  |
---------------------------------------
Eval num_timesteps=134000, episode_reward=2024.11 +/- 515.71
Episode length: 41.64 +/- 3.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 131      |
|    time_elapsed    | 539      |
|    total_timesteps | 134144   |
---------------------------------
Eval num_timesteps=134500, episode_reward=2079.45 +/- 461.38
Episode length: 41.98 +/- 2.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 134500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08e-09 |
|    explained_variance   | 0.572     |
|    learning_rate        | 0.001     |
|    loss                 | 1.46e+04  |
|    n_updates            | 1310      |
|    policy_gradient_loss | -4.6e-10  |
|    value_loss           | 2.59e+04  |
---------------------------------------
Eval num_timesteps=135000, episode_reward=2051.35 +/- 485.40
Episode length: 41.96 +/- 2.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 132      |
|    time_elapsed    | 543      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=2044.07 +/- 510.21
Episode length: 41.70 +/- 3.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.04e+03  |
| time/                   |           |
|    total_timesteps      | 135500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.93e-10 |
|    explained_variance   | 0.698     |
|    learning_rate        | 0.001     |
|    loss                 | 1.97e+03  |
|    n_updates            | 1320      |
|    policy_gradient_loss | 1.52e-09  |
|    value_loss           | 9.07e+03  |
---------------------------------------
Eval num_timesteps=136000, episode_reward=2133.22 +/- 396.31
Episode length: 42.06 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 133      |
|    time_elapsed    | 547      |
|    total_timesteps | 136192   |
---------------------------------
Eval num_timesteps=136500, episode_reward=2084.52 +/- 450.55
Episode length: 41.94 +/- 1.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 136500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.4e-10  |
|    explained_variance   | 0.579     |
|    learning_rate        | 0.001     |
|    loss                 | 5.1e+03   |
|    n_updates            | 1330      |
|    policy_gradient_loss | -2.21e-10 |
|    value_loss           | 2.79e+04  |
---------------------------------------
Eval num_timesteps=137000, episode_reward=2056.26 +/- 480.17
Episode length: 41.78 +/- 2.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 134      |
|    time_elapsed    | 551      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=1996.35 +/- 538.08
Episode length: 41.32 +/- 2.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.3      |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 137500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.19e-09 |
|    explained_variance   | 0.564     |
|    learning_rate        | 0.001     |
|    loss                 | 1.4e+04   |
|    n_updates            | 1340      |
|    policy_gradient_loss | 4.6e-10   |
|    value_loss           | 2.4e+04   |
---------------------------------------
Eval num_timesteps=138000, episode_reward=2160.39 +/- 361.48
Episode length: 42.76 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.6     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 135      |
|    time_elapsed    | 555      |
|    total_timesteps | 138240   |
---------------------------------
Eval num_timesteps=138500, episode_reward=2104.41 +/- 430.25
Episode length: 42.14 +/- 1.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 138500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.43e-09 |
|    explained_variance   | 0.61      |
|    learning_rate        | 0.001     |
|    loss                 | 844       |
|    n_updates            | 1350      |
|    policy_gradient_loss | 7.54e-10  |
|    value_loss           | 1.16e+04  |
---------------------------------------
Eval num_timesteps=139000, episode_reward=2174.22 +/- 362.39
Episode length: 42.44 +/- 2.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 136      |
|    time_elapsed    | 559      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=2031.98 +/- 494.80
Episode length: 41.74 +/- 2.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.03e+03  |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.38e-10 |
|    explained_variance   | 0.608     |
|    learning_rate        | 0.001     |
|    loss                 | 7.45e+03  |
|    n_updates            | 1360      |
|    policy_gradient_loss | 8.5e-10   |
|    value_loss           | 2.73e+04  |
---------------------------------------
Eval num_timesteps=140000, episode_reward=2079.61 +/- 459.70
Episode length: 42.14 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 137      |
|    time_elapsed    | 563      |
|    total_timesteps | 140288   |
---------------------------------
Eval num_timesteps=140500, episode_reward=2014.41 +/- 502.58
Episode length: 42.16 +/- 1.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.01e+03  |
| time/                   |           |
|    total_timesteps      | 140500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.38e-10 |
|    explained_variance   | 0.636     |
|    learning_rate        | 0.001     |
|    loss                 | 3.95e+03  |
|    n_updates            | 1370      |
|    policy_gradient_loss | -1.07e-09 |
|    value_loss           | 1.31e+04  |
---------------------------------------
Eval num_timesteps=141000, episode_reward=1989.78 +/- 547.86
Episode length: 41.14 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.1     |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 138      |
|    time_elapsed    | 567      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=2027.68 +/- 508.29
Episode length: 41.74 +/- 2.50
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 41.7           |
|    mean_reward          | 2.03e+03       |
| time/                   |                |
|    total_timesteps      | 141500         |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.26e-08      |
|    explained_variance   | 0.545          |
|    learning_rate        | 0.001          |
|    loss                 | 7.08e+03       |
|    n_updates            | 1380           |
|    policy_gradient_loss | -5.54e-09      |
|    value_loss           | 1.95e+04       |
--------------------------------------------
Eval num_timesteps=142000, episode_reward=2210.78 +/- 271.75
Episode length: 42.58 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 139      |
|    time_elapsed    | 571      |
|    total_timesteps | 142336   |
---------------------------------
Eval num_timesteps=142500, episode_reward=2051.35 +/- 490.30
Episode length: 41.50 +/- 2.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.5      |
|    mean_reward          | 2.05e+03  |
| time/                   |           |
|    total_timesteps      | 142500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.51e-09 |
|    explained_variance   | 0.577     |
|    learning_rate        | 0.001     |
|    loss                 | 8.74e+03  |
|    n_updates            | 1390      |
|    policy_gradient_loss | 5.15e-10  |
|    value_loss           | 2.76e+04  |
---------------------------------------
Eval num_timesteps=143000, episode_reward=2157.57 +/- 367.33
Episode length: 42.64 +/- 1.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 140      |
|    time_elapsed    | 575      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=2077.38 +/- 467.12
Episode length: 41.92 +/- 2.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.9          |
|    mean_reward          | 2.08e+03      |
| time/                   |               |
|    total_timesteps      | 143500        |
| train/                  |               |
|    approx_kl            | 1.1641532e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.44e-05     |
|    explained_variance   | 0.513         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+04      |
|    n_updates            | 1400          |
|    policy_gradient_loss | -2.04e-05     |
|    value_loss           | 3.51e+04      |
-------------------------------------------
Eval num_timesteps=144000, episode_reward=2041.44 +/- 478.48
Episode length: 42.20 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 141      |
|    time_elapsed    | 579      |
|    total_timesteps | 144384   |
---------------------------------
Eval num_timesteps=144500, episode_reward=2157.00 +/- 369.04
Episode length: 42.42 +/- 1.36
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.4           |
|    mean_reward          | 2.16e+03       |
| time/                   |                |
|    total_timesteps      | 144500         |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.28e-08      |
|    explained_variance   | 0.434          |
|    learning_rate        | 0.001          |
|    loss                 | 1.28e+04       |
|    n_updates            | 1410           |
|    policy_gradient_loss | -1.59e-08      |
|    value_loss           | 2.41e+04       |
--------------------------------------------
Eval num_timesteps=145000, episode_reward=2114.58 +/- 450.17
Episode length: 42.20 +/- 3.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 142      |
|    time_elapsed    | 583      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=2095.62 +/- 462.77
Episode length: 41.94 +/- 3.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 145500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.19e-09 |
|    explained_variance   | 0.643     |
|    learning_rate        | 0.001     |
|    loss                 | 7.46e+03  |
|    n_updates            | 1420      |
|    policy_gradient_loss | -7.33e-10 |
|    value_loss           | 2.26e+04  |
---------------------------------------
Eval num_timesteps=146000, episode_reward=2053.60 +/- 483.09
Episode length: 41.88 +/- 1.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 143      |
|    time_elapsed    | 587      |
|    total_timesteps | 146432   |
---------------------------------
Eval num_timesteps=146500, episode_reward=2070.89 +/- 477.58
Episode length: 41.74 +/- 2.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.07e+03  |
| time/                   |           |
|    total_timesteps      | 146500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.75e-09 |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.001     |
|    loss                 | 4.64e+03  |
|    n_updates            | 1430      |
|    policy_gradient_loss | 3.11e-10  |
|    value_loss           | 1.5e+04   |
---------------------------------------
Eval num_timesteps=147000, episode_reward=2136.31 +/- 390.78
Episode length: 42.40 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 144      |
|    time_elapsed    | 591      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=2004.52 +/- 522.64
Episode length: 41.76 +/- 2.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 147500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.64e-08 |
|    explained_variance   | 0.434     |
|    learning_rate        | 0.001     |
|    loss                 | 8.44e+03  |
|    n_updates            | 1440      |
|    policy_gradient_loss | -1.22e-10 |
|    value_loss           | 2.68e+04  |
---------------------------------------
Eval num_timesteps=148000, episode_reward=2109.61 +/- 417.37
Episode length: 42.40 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 145      |
|    time_elapsed    | 595      |
|    total_timesteps | 148480   |
---------------------------------
Eval num_timesteps=148500, episode_reward=2077.90 +/- 465.83
Episode length: 41.76 +/- 2.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 148500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.3e-08  |
|    explained_variance   | 0.764     |
|    learning_rate        | 0.001     |
|    loss                 | 1.88e+03  |
|    n_updates            | 1450      |
|    policy_gradient_loss | -2.33e-09 |
|    value_loss           | 7.57e+03  |
---------------------------------------
Eval num_timesteps=149000, episode_reward=2073.27 +/- 465.94
Episode length: 42.02 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=2041.51 +/- 477.94
Episode length: 42.18 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 146      |
|    time_elapsed    | 600      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=2105.22 +/- 436.13
Episode length: 42.08 +/- 2.14
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 42.1     |
|    mean_reward          | 2.11e+03 |
| time/                   |          |
|    total_timesteps      | 150000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.7e-10 |
|    explained_variance   | 0.717    |
|    learning_rate        | 0.001    |
|    loss                 | 1.71e+03 |
|    n_updates            | 1460     |
|    policy_gradient_loss | 4.71e-10 |
|    value_loss           | 8.22e+03 |
--------------------------------------
Eval num_timesteps=150500, episode_reward=2092.40 +/- 430.39
Episode length: 42.64 +/- 1.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 147      |
|    time_elapsed    | 604      |
|    total_timesteps | 150528   |
---------------------------------
Eval num_timesteps=151000, episode_reward=2108.90 +/- 426.58
Episode length: 42.40 +/- 1.88
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.4           |
|    mean_reward          | 2.11e+03       |
| time/                   |                |
|    total_timesteps      | 151000         |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.69e-08      |
|    explained_variance   | 0.442          |
|    learning_rate        | 0.001          |
|    loss                 | 1.96e+04       |
|    n_updates            | 1470           |
|    policy_gradient_loss | -1.9e-09       |
|    value_loss           | 5.67e+04       |
--------------------------------------------
Eval num_timesteps=151500, episode_reward=2055.13 +/- 482.53
Episode length: 41.96 +/- 2.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 148      |
|    time_elapsed    | 608      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=2175.47 +/- 347.96
Episode length: 42.24 +/- 1.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 152000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.05e-09 |
|    explained_variance   | 0.462     |
|    learning_rate        | 0.001     |
|    loss                 | 8.51e+03  |
|    n_updates            | 1480      |
|    policy_gradient_loss | -7.59e-08 |
|    value_loss           | 2.54e+04  |
---------------------------------------
Eval num_timesteps=152500, episode_reward=2056.49 +/- 478.87
Episode length: 41.78 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2e+03    |
| time/              |          |
|    fps             | 249      |
|    iterations      | 149      |
|    time_elapsed    | 612      |
|    total_timesteps | 152576   |
---------------------------------
Eval num_timesteps=153000, episode_reward=2036.37 +/- 523.18
Episode length: 41.44 +/- 3.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.4      |
|    mean_reward          | 2.04e+03  |
| time/                   |           |
|    total_timesteps      | 153000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.55e-09 |
|    explained_variance   | 0.384     |
|    learning_rate        | 0.001     |
|    loss                 | 1.35e+04  |
|    n_updates            | 1490      |
|    policy_gradient_loss | -1.05e-09 |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=153500, episode_reward=1979.45 +/- 535.69
Episode length: 41.72 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.7     |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 150      |
|    time_elapsed    | 616      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=2154.34 +/- 378.23
Episode length: 42.34 +/- 1.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 154000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.86e-09 |
|    explained_variance   | 0.575     |
|    learning_rate        | 0.001     |
|    loss                 | 2.73e+03  |
|    n_updates            | 1500      |
|    policy_gradient_loss | -4.77e-10 |
|    value_loss           | 1.56e+04  |
---------------------------------------
Eval num_timesteps=154500, episode_reward=2011.58 +/- 508.58
Episode length: 42.06 +/- 1.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 151      |
|    time_elapsed    | 620      |
|    total_timesteps | 154624   |
---------------------------------
Eval num_timesteps=155000, episode_reward=2108.32 +/- 427.95
Episode length: 42.46 +/- 2.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.11e+03  |
| time/                   |           |
|    total_timesteps      | 155000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.49e-09 |
|    explained_variance   | 0.466     |
|    learning_rate        | 0.001     |
|    loss                 | 4.84e+03  |
|    n_updates            | 1510      |
|    policy_gradient_loss | -6.58e-10 |
|    value_loss           | 2.82e+04  |
---------------------------------------
Eval num_timesteps=155500, episode_reward=2074.94 +/- 473.24
Episode length: 41.80 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 152      |
|    time_elapsed    | 624      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=2134.07 +/- 395.98
Episode length: 42.28 +/- 1.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 156000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.64e-07 |
|    explained_variance   | 0.533     |
|    learning_rate        | 0.001     |
|    loss                 | 8.78e+03  |
|    n_updates            | 1520      |
|    policy_gradient_loss | -9.7e-08  |
|    value_loss           | 3.39e+04  |
---------------------------------------
Eval num_timesteps=156500, episode_reward=2059.82 +/- 470.33
Episode length: 41.54 +/- 1.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 153      |
|    time_elapsed    | 628      |
|    total_timesteps | 156672   |
---------------------------------
Eval num_timesteps=157000, episode_reward=2130.71 +/- 407.20
Episode length: 42.12 +/- 2.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 157000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.38e-08 |
|    explained_variance   | 0.443     |
|    learning_rate        | 0.001     |
|    loss                 | 1.39e+04  |
|    n_updates            | 1530      |
|    policy_gradient_loss | -1.35e-09 |
|    value_loss           | 3.61e+04  |
---------------------------------------
Eval num_timesteps=157500, episode_reward=2104.55 +/- 437.84
Episode length: 42.24 +/- 1.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 154      |
|    time_elapsed    | 632      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=2059.90 +/- 466.29
Episode length: 42.22 +/- 1.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 158000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.07e-08 |
|    explained_variance   | 0.612     |
|    learning_rate        | 0.001     |
|    loss                 | 7.55e+03  |
|    n_updates            | 1540      |
|    policy_gradient_loss | -1.4e-10  |
|    value_loss           | 2.55e+04  |
---------------------------------------
Eval num_timesteps=158500, episode_reward=2129.29 +/- 410.78
Episode length: 42.12 +/- 1.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 155      |
|    time_elapsed    | 636      |
|    total_timesteps | 158720   |
---------------------------------
Eval num_timesteps=159000, episode_reward=2147.09 +/- 401.45
Episode length: 42.06 +/- 2.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.1          |
|    mean_reward          | 2.15e+03      |
| time/                   |               |
|    total_timesteps      | 159000        |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.6e-08      |
|    explained_variance   | 0.518         |
|    learning_rate        | 0.001         |
|    loss                 | 2.81e+03      |
|    n_updates            | 1550          |
|    policy_gradient_loss | -3.86e-09     |
|    value_loss           | 1.89e+04      |
-------------------------------------------
Eval num_timesteps=159500, episode_reward=2151.62 +/- 388.16
Episode length: 41.92 +/- 2.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 156      |
|    time_elapsed    | 640      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=2150.91 +/- 388.90
Episode length: 42.42 +/- 1.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.04e-09 |
|    explained_variance   | 0.593     |
|    learning_rate        | 0.001     |
|    loss                 | 1.56e+03  |
|    n_updates            | 1560      |
|    policy_gradient_loss | -1.34e-09 |
|    value_loss           | 9.17e+03  |
---------------------------------------
Eval num_timesteps=160500, episode_reward=2208.34 +/- 286.13
Episode length: 42.66 +/- 1.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 157      |
|    time_elapsed    | 644      |
|    total_timesteps | 160768   |
---------------------------------
Eval num_timesteps=161000, episode_reward=2124.96 +/- 420.00
Episode length: 41.88 +/- 2.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.12e+03  |
| time/                   |           |
|    total_timesteps      | 161000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.09e-10 |
|    explained_variance   | 0.597     |
|    learning_rate        | 0.001     |
|    loss                 | 5.89e+03  |
|    n_updates            | 1570      |
|    policy_gradient_loss | -7.74e-10 |
|    value_loss           | 1.98e+04  |
---------------------------------------
Eval num_timesteps=161500, episode_reward=2073.64 +/- 474.24
Episode length: 41.88 +/- 2.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 158      |
|    time_elapsed    | 648      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=1981.83 +/- 532.25
Episode length: 41.86 +/- 2.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.9          |
|    mean_reward          | 1.98e+03      |
| time/                   |               |
|    total_timesteps      | 162000        |
| train/                  |               |
|    approx_kl            | 1.0232907e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.81e-05     |
|    explained_variance   | 0.599         |
|    learning_rate        | 0.001         |
|    loss                 | 1.7e+03       |
|    n_updates            | 1580          |
|    policy_gradient_loss | -2.23e-05     |
|    value_loss           | 1.26e+04      |
-------------------------------------------
Eval num_timesteps=162500, episode_reward=2035.57 +/- 490.22
Episode length: 41.88 +/- 2.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 159      |
|    time_elapsed    | 652      |
|    total_timesteps | 162816   |
---------------------------------
Eval num_timesteps=163000, episode_reward=2124.61 +/- 424.89
Episode length: 41.92 +/- 2.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 2.12e+03     |
| time/                   |              |
|    total_timesteps      | 163000       |
| train/                  |              |
|    approx_kl            | 5.820766e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.6e-08     |
|    explained_variance   | 0.5          |
|    learning_rate        | 0.001        |
|    loss                 | 6.34e+03     |
|    n_updates            | 1590         |
|    policy_gradient_loss | -1.15e-09    |
|    value_loss           | 2.52e+04     |
------------------------------------------
Eval num_timesteps=163500, episode_reward=2158.90 +/- 365.04
Episode length: 42.32 +/- 1.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 160      |
|    time_elapsed    | 656      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=1998.62 +/- 530.31
Episode length: 41.90 +/- 2.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 164000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.95e-09 |
|    explained_variance   | 0.607     |
|    learning_rate        | 0.001     |
|    loss                 | 8.62e+03  |
|    n_updates            | 1600      |
|    policy_gradient_loss | 1.72e-10  |
|    value_loss           | 1.64e+04  |
---------------------------------------
Eval num_timesteps=164500, episode_reward=2088.52 +/- 441.68
Episode length: 42.06 +/- 1.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 249      |
|    iterations      | 161      |
|    time_elapsed    | 660      |
|    total_timesteps | 164864   |
---------------------------------
Eval num_timesteps=165000, episode_reward=2134.44 +/- 396.51
Episode length: 42.20 +/- 1.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 165000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.92e-09 |
|    explained_variance   | 0.6       |
|    learning_rate        | 0.001     |
|    loss                 | 7.18e+03  |
|    n_updates            | 1610      |
|    policy_gradient_loss | -6.23e-10 |
|    value_loss           | 1.83e+04  |
---------------------------------------
Eval num_timesteps=165500, episode_reward=1983.23 +/- 530.45
Episode length: 41.90 +/- 2.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 249      |
|    iterations      | 162      |
|    time_elapsed    | 664      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=2035.89 +/- 489.52
Episode length: 42.00 +/- 1.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42            |
|    mean_reward          | 2.04e+03      |
| time/                   |               |
|    total_timesteps      | 166000        |
| train/                  |               |
|    approx_kl            | 4.1909516e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.24e-05     |
|    explained_variance   | 0.531         |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+04      |
|    n_updates            | 1620          |
|    policy_gradient_loss | -1.6e-05      |
|    value_loss           | 2.49e+04      |
-------------------------------------------
Eval num_timesteps=166500, episode_reward=2129.62 +/- 408.71
Episode length: 42.60 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 249      |
|    iterations      | 163      |
|    time_elapsed    | 668      |
|    total_timesteps | 166912   |
---------------------------------
Eval num_timesteps=167000, episode_reward=2151.81 +/- 373.98
Episode length: 42.22 +/- 1.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 167000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.03e-07 |
|    explained_variance   | 0.521     |
|    learning_rate        | 0.001     |
|    loss                 | 4.82e+03  |
|    n_updates            | 1630      |
|    policy_gradient_loss | -7.83e-08 |
|    value_loss           | 2.8e+04   |
---------------------------------------
Eval num_timesteps=167500, episode_reward=2078.37 +/- 464.86
Episode length: 41.96 +/- 2.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 164      |
|    time_elapsed    | 672      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=2177.39 +/- 349.43
Episode length: 42.26 +/- 2.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.67e-09 |
|    explained_variance   | 0.582     |
|    learning_rate        | 0.001     |
|    loss                 | 1.6e+03   |
|    n_updates            | 1640      |
|    policy_gradient_loss | 2.23e-09  |
|    value_loss           | 1.05e+04  |
---------------------------------------
Eval num_timesteps=168500, episode_reward=2102.54 +/- 435.70
Episode length: 42.12 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 165      |
|    time_elapsed    | 676      |
|    total_timesteps | 168960   |
---------------------------------
Eval num_timesteps=169000, episode_reward=2105.81 +/- 427.12
Episode length: 42.14 +/- 1.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 2.11e+03     |
| time/                   |              |
|    total_timesteps      | 169000       |
| train/                  |              |
|    approx_kl            | 1.057866e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.01e-05    |
|    explained_variance   | 0.608        |
|    learning_rate        | 0.001        |
|    loss                 | 5.07e+03     |
|    n_updates            | 1650         |
|    policy_gradient_loss | -2.43e-05    |
|    value_loss           | 2.14e+04     |
------------------------------------------
Eval num_timesteps=169500, episode_reward=2149.40 +/- 395.82
Episode length: 41.90 +/- 2.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 166      |
|    time_elapsed    | 680      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=2034.21 +/- 494.58
Episode length: 42.00 +/- 2.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 2.03e+03     |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 5.820766e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.22e-07    |
|    explained_variance   | 0.514        |
|    learning_rate        | 0.001        |
|    loss                 | 6.68e+03     |
|    n_updates            | 1660         |
|    policy_gradient_loss | -6.95e-08    |
|    value_loss           | 2.36e+04     |
------------------------------------------
Eval num_timesteps=170500, episode_reward=2079.57 +/- 461.23
Episode length: 42.00 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=2077.88 +/- 467.60
Episode length: 42.18 +/- 2.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 167      |
|    time_elapsed    | 685      |
|    total_timesteps | 171008   |
---------------------------------
Eval num_timesteps=171500, episode_reward=2003.16 +/- 523.14
Episode length: 42.08 +/- 2.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 171500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.07e-08 |
|    explained_variance   | 0.641     |
|    learning_rate        | 0.001     |
|    loss                 | 3.11e+03  |
|    n_updates            | 1670      |
|    policy_gradient_loss | -4.6e-10  |
|    value_loss           | 1.54e+04  |
---------------------------------------
Eval num_timesteps=172000, episode_reward=2056.37 +/- 477.93
Episode length: 41.80 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 168      |
|    time_elapsed    | 689      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=2175.62 +/- 355.38
Episode length: 42.32 +/- 2.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.3          |
|    mean_reward          | 2.18e+03      |
| time/                   |               |
|    total_timesteps      | 172500        |
| train/                  |               |
|    approx_kl            | 3.4313416e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.89e-05     |
|    explained_variance   | 0.545         |
|    learning_rate        | 0.001         |
|    loss                 | 1.13e+04      |
|    n_updates            | 1680          |
|    policy_gradient_loss | -8.45e-06     |
|    value_loss           | 4.22e+04      |
-------------------------------------------
Eval num_timesteps=173000, episode_reward=1975.35 +/- 542.88
Episode length: 41.62 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 169      |
|    time_elapsed    | 693      |
|    total_timesteps | 173056   |
---------------------------------
Eval num_timesteps=173500, episode_reward=2173.94 +/- 354.86
Episode length: 42.12 +/- 2.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.1          |
|    mean_reward          | 2.17e+03      |
| time/                   |               |
|    total_timesteps      | 173500        |
| train/                  |               |
|    approx_kl            | 5.7319296e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00016      |
|    explained_variance   | 0.557         |
|    learning_rate        | 0.001         |
|    loss                 | 3.88e+03      |
|    n_updates            | 1690          |
|    policy_gradient_loss | -0.000157     |
|    value_loss           | 1.51e+04      |
-------------------------------------------
Eval num_timesteps=174000, episode_reward=2142.06 +/- 376.39
Episode length: 42.74 +/- 1.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 170      |
|    time_elapsed    | 697      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=2057.65 +/- 477.30
Episode length: 41.74 +/- 2.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 174500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.2e-07  |
|    explained_variance   | 0.588     |
|    learning_rate        | 0.001     |
|    loss                 | 6.49e+03  |
|    n_updates            | 1700      |
|    policy_gradient_loss | -1.48e-07 |
|    value_loss           | 2.18e+04  |
---------------------------------------
Eval num_timesteps=175000, episode_reward=2081.88 +/- 455.29
Episode length: 42.02 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 171      |
|    time_elapsed    | 701      |
|    total_timesteps | 175104   |
---------------------------------
Eval num_timesteps=175500, episode_reward=1973.82 +/- 548.21
Episode length: 41.64 +/- 2.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 1.97e+03     |
| time/                   |              |
|    total_timesteps      | 175500       |
| train/                  |              |
|    approx_kl            | 5.820766e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.21e-07    |
|    explained_variance   | 0.59         |
|    learning_rate        | 0.001        |
|    loss                 | 9e+03        |
|    n_updates            | 1710         |
|    policy_gradient_loss | -4.35e-08    |
|    value_loss           | 2.35e+04     |
------------------------------------------
Eval num_timesteps=176000, episode_reward=2100.10 +/- 442.90
Episode length: 41.78 +/- 2.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 249      |
|    iterations      | 172      |
|    time_elapsed    | 705      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=1982.20 +/- 531.94
Episode length: 41.90 +/- 2.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.98e+03     |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 5.820766e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.15e-06    |
|    explained_variance   | 0.625        |
|    learning_rate        | 0.001        |
|    loss                 | 5.28e+03     |
|    n_updates            | 1720         |
|    policy_gradient_loss | -1.41e-06    |
|    value_loss           | 1.71e+04     |
------------------------------------------
Eval num_timesteps=177000, episode_reward=2055.85 +/- 483.11
Episode length: 41.78 +/- 2.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 173      |
|    time_elapsed    | 709      |
|    total_timesteps | 177152   |
---------------------------------
Eval num_timesteps=177500, episode_reward=2134.85 +/- 396.09
Episode length: 42.42 +/- 1.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.4         |
|    mean_reward          | 2.13e+03     |
| time/                   |              |
|    total_timesteps      | 177500       |
| train/                  |              |
|    approx_kl            | 7.683411e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.45e-06    |
|    explained_variance   | 0.505        |
|    learning_rate        | 0.001        |
|    loss                 | 3.81e+03     |
|    n_updates            | 1730         |
|    policy_gradient_loss | -1.4e-05     |
|    value_loss           | 2.06e+04     |
------------------------------------------
Eval num_timesteps=178000, episode_reward=2205.24 +/- 296.30
Episode length: 42.20 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 174      |
|    time_elapsed    | 713      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=2108.80 +/- 427.24
Episode length: 42.16 +/- 1.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 2.11e+03     |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 5.820766e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.8e-07     |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.001        |
|    loss                 | 4.94e+03     |
|    n_updates            | 1740         |
|    policy_gradient_loss | -1.79e-07    |
|    value_loss           | 2.66e+04     |
------------------------------------------
Eval num_timesteps=179000, episode_reward=2073.23 +/- 474.00
Episode length: 41.64 +/- 2.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 175      |
|    time_elapsed    | 716      |
|    total_timesteps | 179200   |
---------------------------------
Eval num_timesteps=179500, episode_reward=2034.46 +/- 495.56
Episode length: 41.84 +/- 2.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 2.03e+03     |
| time/                   |              |
|    total_timesteps      | 179500       |
| train/                  |              |
|    approx_kl            | 3.882451e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09e-05    |
|    explained_variance   | 0.471        |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+04     |
|    n_updates            | 1750         |
|    policy_gradient_loss | -6.38e-06    |
|    value_loss           | 3.68e+04     |
------------------------------------------
Eval num_timesteps=180000, episode_reward=2075.87 +/- 472.55
Episode length: 42.02 +/- 2.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 249      |
|    iterations      | 176      |
|    time_elapsed    | 720      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=2047.59 +/- 497.86
Episode length: 41.64 +/- 2.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.6      |
|    mean_reward          | 2.05e+03  |
| time/                   |           |
|    total_timesteps      | 180500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.24e-08 |
|    explained_variance   | 0.622     |
|    learning_rate        | 0.001     |
|    loss                 | 1.19e+04  |
|    n_updates            | 1760      |
|    policy_gradient_loss | -1.05e-08 |
|    value_loss           | 1.33e+04  |
---------------------------------------
Eval num_timesteps=181000, episode_reward=1988.97 +/- 520.23
Episode length: 42.32 +/- 2.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 177      |
|    time_elapsed    | 724      |
|    total_timesteps | 181248   |
---------------------------------
Eval num_timesteps=181500, episode_reward=2040.91 +/- 480.31
Episode length: 42.30 +/- 1.60
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.3           |
|    mean_reward          | 2.04e+03       |
| time/                   |                |
|    total_timesteps      | 181500         |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.96e-08      |
|    explained_variance   | 0.641          |
|    learning_rate        | 0.001          |
|    loss                 | 4.55e+03       |
|    n_updates            | 1770           |
|    policy_gradient_loss | -1.86e-08      |
|    value_loss           | 1.7e+04        |
--------------------------------------------
Eval num_timesteps=182000, episode_reward=2101.70 +/- 442.73
Episode length: 42.10 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 178      |
|    time_elapsed    | 728      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=2030.01 +/- 501.22
Episode length: 42.04 +/- 2.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42            |
|    mean_reward          | 2.03e+03      |
| time/                   |               |
|    total_timesteps      | 182500        |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.92e-07     |
|    explained_variance   | 0.534         |
|    learning_rate        | 0.001         |
|    loss                 | 9.01e+03      |
|    n_updates            | 1780          |
|    policy_gradient_loss | -3.65e-08     |
|    value_loss           | 3.43e+04      |
-------------------------------------------
Eval num_timesteps=183000, episode_reward=2075.46 +/- 468.44
Episode length: 42.00 +/- 2.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 179      |
|    time_elapsed    | 732      |
|    total_timesteps | 183296   |
---------------------------------
Eval num_timesteps=183500, episode_reward=2075.16 +/- 473.08
Episode length: 41.80 +/- 3.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 183500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.65e-08 |
|    explained_variance   | 0.605     |
|    learning_rate        | 0.001     |
|    loss                 | 5.67e+03  |
|    n_updates            | 1790      |
|    policy_gradient_loss | -1.09e-08 |
|    value_loss           | 2.55e+04  |
---------------------------------------
Eval num_timesteps=184000, episode_reward=2157.41 +/- 368.73
Episode length: 42.10 +/- 1.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 180      |
|    time_elapsed    | 736      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=2125.73 +/- 412.49
Episode length: 41.88 +/- 1.96
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 41.9           |
|    mean_reward          | 2.13e+03       |
| time/                   |                |
|    total_timesteps      | 184500         |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.5e-07       |
|    explained_variance   | 0.521          |
|    learning_rate        | 0.001          |
|    loss                 | 4.19e+03       |
|    n_updates            | 1800           |
|    policy_gradient_loss | -8.45e-08      |
|    value_loss           | 2.62e+04       |
--------------------------------------------
Eval num_timesteps=185000, episode_reward=1985.74 +/- 524.78
Episode length: 41.88 +/- 2.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 181      |
|    time_elapsed    | 740      |
|    total_timesteps | 185344   |
---------------------------------
Eval num_timesteps=185500, episode_reward=2074.13 +/- 476.94
Episode length: 41.80 +/- 2.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.8          |
|    mean_reward          | 2.07e+03      |
| time/                   |               |
|    total_timesteps      | 185500        |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.51e-08     |
|    explained_variance   | 0.521         |
|    learning_rate        | 0.001         |
|    loss                 | 3.89e+03      |
|    n_updates            | 1810          |
|    policy_gradient_loss | -2.15e-09     |
|    value_loss           | 1.77e+04      |
-------------------------------------------
Eval num_timesteps=186000, episode_reward=2182.56 +/- 334.11
Episode length: 42.16 +/- 1.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 182      |
|    time_elapsed    | 744      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=2110.08 +/- 421.01
Episode length: 42.20 +/- 1.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 2.11e+03     |
| time/                   |              |
|    total_timesteps      | 186500       |
| train/                  |              |
|    approx_kl            | 2.910383e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26e-06    |
|    explained_variance   | 0.724        |
|    learning_rate        | 0.001        |
|    loss                 | 1.48e+03     |
|    n_updates            | 1820         |
|    policy_gradient_loss | -2.69e-06    |
|    value_loss           | 7.91e+03     |
------------------------------------------
Eval num_timesteps=187000, episode_reward=2136.32 +/- 391.14
Episode length: 42.02 +/- 1.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 183      |
|    time_elapsed    | 748      |
|    total_timesteps | 187392   |
---------------------------------
Eval num_timesteps=187500, episode_reward=2152.61 +/- 379.15
Episode length: 42.26 +/- 2.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 187500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.64e-06 |
|    explained_variance   | 0.625     |
|    learning_rate        | 0.001     |
|    loss                 | 4.85e+03  |
|    n_updates            | 1830      |
|    policy_gradient_loss | -1.86e-07 |
|    value_loss           | 1.76e+04  |
---------------------------------------
Eval num_timesteps=188000, episode_reward=2078.43 +/- 463.38
Episode length: 41.94 +/- 2.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 184      |
|    time_elapsed    | 752      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=1990.19 +/- 516.19
Episode length: 41.94 +/- 2.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.99e+03     |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 2.910383e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.21e-06    |
|    explained_variance   | 0.568        |
|    learning_rate        | 0.001        |
|    loss                 | 3.72e+03     |
|    n_updates            | 1840         |
|    policy_gradient_loss | -3.32e-06    |
|    value_loss           | 1.91e+04     |
------------------------------------------
Eval num_timesteps=189000, episode_reward=2012.04 +/- 506.66
Episode length: 42.10 +/- 2.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 185      |
|    time_elapsed    | 756      |
|    total_timesteps | 189440   |
---------------------------------
Eval num_timesteps=189500, episode_reward=2184.09 +/- 324.87
Episode length: 42.46 +/- 1.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.5          |
|    mean_reward          | 2.18e+03      |
| time/                   |               |
|    total_timesteps      | 189500        |
| train/                  |               |
|    approx_kl            | 2.1536835e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.05e-05     |
|    explained_variance   | 0.49          |
|    learning_rate        | 0.001         |
|    loss                 | 6.45e+03      |
|    n_updates            | 1850          |
|    policy_gradient_loss | -5.07e-06     |
|    value_loss           | 2.37e+04      |
-------------------------------------------
Eval num_timesteps=190000, episode_reward=2080.53 +/- 454.93
Episode length: 42.04 +/- 2.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 186      |
|    time_elapsed    | 760      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=2057.46 +/- 477.98
Episode length: 42.34 +/- 2.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.3          |
|    mean_reward          | 2.06e+03      |
| time/                   |               |
|    total_timesteps      | 190500        |
| train/                  |               |
|    approx_kl            | 2.9609131e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000478     |
|    explained_variance   | 0.62          |
|    learning_rate        | 0.001         |
|    loss                 | 8.17e+03      |
|    n_updates            | 1860          |
|    policy_gradient_loss | -0.000419     |
|    value_loss           | 2.99e+04      |
-------------------------------------------
Eval num_timesteps=191000, episode_reward=2122.76 +/- 428.57
Episode length: 42.18 +/- 2.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 187      |
|    time_elapsed    | 764      |
|    total_timesteps | 191488   |
---------------------------------
Eval num_timesteps=191500, episode_reward=2029.48 +/- 503.70
Episode length: 41.78 +/- 2.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.8          |
|    mean_reward          | 2.03e+03      |
| time/                   |               |
|    total_timesteps      | 191500        |
| train/                  |               |
|    approx_kl            | 3.0267984e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.68e-05     |
|    explained_variance   | 0.42          |
|    learning_rate        | 0.001         |
|    loss                 | 7.32e+03      |
|    n_updates            | 1870          |
|    policy_gradient_loss | -9.31e-06     |
|    value_loss           | 2.92e+04      |
-------------------------------------------
Eval num_timesteps=192000, episode_reward=2208.54 +/- 282.76
Episode length: 42.66 +/- 1.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=2054.12 +/- 485.47
Episode length: 41.94 +/- 2.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 188      |
|    time_elapsed    | 769      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=2082.22 +/- 454.28
Episode length: 42.12 +/- 1.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.1          |
|    mean_reward          | 2.08e+03      |
| time/                   |               |
|    total_timesteps      | 193000        |
| train/                  |               |
|    approx_kl            | 1.0477379e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.73e-06     |
|    explained_variance   | 0.321         |
|    learning_rate        | 0.001         |
|    loss                 | 3.53e+03      |
|    n_updates            | 1880          |
|    policy_gradient_loss | -5.05e-06     |
|    value_loss           | 1.68e+04      |
-------------------------------------------
Eval num_timesteps=193500, episode_reward=2178.95 +/- 344.45
Episode length: 42.54 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 189      |
|    time_elapsed    | 773      |
|    total_timesteps | 193536   |
---------------------------------
Eval num_timesteps=194000, episode_reward=2036.91 +/- 488.50
Episode length: 42.12 +/- 1.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.1          |
|    mean_reward          | 2.04e+03      |
| time/                   |               |
|    total_timesteps      | 194000        |
| train/                  |               |
|    approx_kl            | 1.9790605e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.32e-05     |
|    explained_variance   | 0.477         |
|    learning_rate        | 0.001         |
|    loss                 | 8.01e+03      |
|    n_updates            | 1890          |
|    policy_gradient_loss | -5.24e-06     |
|    value_loss           | 3.18e+04      |
-------------------------------------------
Eval num_timesteps=194500, episode_reward=2075.46 +/- 468.81
Episode length: 41.88 +/- 2.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 190      |
|    time_elapsed    | 777      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=2105.30 +/- 438.66
Episode length: 42.46 +/- 2.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.5          |
|    mean_reward          | 2.11e+03      |
| time/                   |               |
|    total_timesteps      | 195000        |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.73e-07     |
|    explained_variance   | 0.372         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+04      |
|    n_updates            | 1900          |
|    policy_gradient_loss | -3.81e-08     |
|    value_loss           | 3.33e+04      |
-------------------------------------------
Eval num_timesteps=195500, episode_reward=2102.43 +/- 443.09
Episode length: 42.10 +/- 2.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 191      |
|    time_elapsed    | 781      |
|    total_timesteps | 195584   |
---------------------------------
Eval num_timesteps=196000, episode_reward=2104.13 +/- 439.11
Episode length: 42.02 +/- 1.97
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 2.1e+03        |
| time/                   |                |
|    total_timesteps      | 196000         |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.75e-07      |
|    explained_variance   | 0.532          |
|    learning_rate        | 0.001          |
|    loss                 | 5.27e+03       |
|    n_updates            | 1910           |
|    policy_gradient_loss | -2.12e-08      |
|    value_loss           | 2.5e+04        |
--------------------------------------------
Eval num_timesteps=196500, episode_reward=2062.07 +/- 467.33
Episode length: 42.18 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 192      |
|    time_elapsed    | 785      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=2105.77 +/- 431.55
Episode length: 42.04 +/- 2.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 2.11e+03     |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 5.889451e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.3e-05     |
|    explained_variance   | 0.595        |
|    learning_rate        | 0.001        |
|    loss                 | 9.3e+03      |
|    n_updates            | 1920         |
|    policy_gradient_loss | -3.14e-05    |
|    value_loss           | 2.48e+04     |
------------------------------------------
Eval num_timesteps=197500, episode_reward=1958.10 +/- 541.09
Episode length: 41.74 +/- 2.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 193      |
|    time_elapsed    | 789      |
|    total_timesteps | 197632   |
---------------------------------
Eval num_timesteps=198000, episode_reward=2080.86 +/- 453.85
Episode length: 41.96 +/- 2.56
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 2.08e+03       |
| time/                   |                |
|    total_timesteps      | 198000         |
| train/                  |                |
|    approx_kl            | -2.3283064e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.34e-06      |
|    explained_variance   | 0.614          |
|    learning_rate        | 0.001          |
|    loss                 | 7.1e+03        |
|    n_updates            | 1930           |
|    policy_gradient_loss | -3.49e-07      |
|    value_loss           | 2.54e+04       |
--------------------------------------------
Eval num_timesteps=198500, episode_reward=2207.94 +/- 286.44
Episode length: 42.40 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 194      |
|    time_elapsed    | 793      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=2204.90 +/- 298.21
Episode length: 42.70 +/- 1.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.7      |
|    mean_reward          | 2.2e+03   |
| time/                   |           |
|    total_timesteps      | 199000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.62e-06 |
|    explained_variance   | 0.456     |
|    learning_rate        | 0.001     |
|    loss                 | 1.26e+04  |
|    n_updates            | 1940      |
|    policy_gradient_loss | -8.7e-07  |
|    value_loss           | 2.58e+04  |
---------------------------------------
Eval num_timesteps=199500, episode_reward=2104.77 +/- 436.70
Episode length: 42.16 +/- 2.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 195      |
|    time_elapsed    | 797      |
|    total_timesteps | 199680   |
---------------------------------
Eval num_timesteps=200000, episode_reward=2059.48 +/- 471.35
Episode length: 42.16 +/- 1.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 200000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.73e-06 |
|    explained_variance   | 0.629     |
|    learning_rate        | 0.001     |
|    loss                 | 9.54e+03  |
|    n_updates            | 1950      |
|    policy_gradient_loss | -6.11e-07 |
|    value_loss           | 2.91e+04  |
---------------------------------------
Eval num_timesteps=200500, episode_reward=2153.50 +/- 381.96
Episode length: 42.38 +/- 2.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 196      |
|    time_elapsed    | 801      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=2030.23 +/- 502.19
Episode length: 41.94 +/- 2.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.9          |
|    mean_reward          | 2.03e+03      |
| time/                   |               |
|    total_timesteps      | 201000        |
| train/                  |               |
|    approx_kl            | 1.2048986e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.78e-05     |
|    explained_variance   | 0.521         |
|    learning_rate        | 0.001         |
|    loss                 | 8.69e+03      |
|    n_updates            | 1960          |
|    policy_gradient_loss | -9.04e-06     |
|    value_loss           | 2.87e+04      |
-------------------------------------------
Eval num_timesteps=201500, episode_reward=2135.36 +/- 389.25
Episode length: 42.54 +/- 1.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 197      |
|    time_elapsed    | 805      |
|    total_timesteps | 201728   |
---------------------------------
Eval num_timesteps=202000, episode_reward=2085.15 +/- 442.32
Episode length: 42.38 +/- 1.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.4          |
|    mean_reward          | 2.09e+03      |
| time/                   |               |
|    total_timesteps      | 202000        |
| train/                  |               |
|    approx_kl            | 1.1641532e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.75e-06     |
|    explained_variance   | 0.434         |
|    learning_rate        | 0.001         |
|    loss                 | 5.76e+03      |
|    n_updates            | 1970          |
|    policy_gradient_loss | -3.76e-06     |
|    value_loss           | 2.45e+04      |
-------------------------------------------
Eval num_timesteps=202500, episode_reward=1961.26 +/- 541.75
Episode length: 41.66 +/- 2.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 198      |
|    time_elapsed    | 809      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=2085.82 +/- 447.00
Episode length: 42.30 +/- 1.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.3          |
|    mean_reward          | 2.09e+03      |
| time/                   |               |
|    total_timesteps      | 203000        |
| train/                  |               |
|    approx_kl            | -2.910383e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.12e-06     |
|    explained_variance   | 0.432         |
|    learning_rate        | 0.001         |
|    loss                 | 9.14e+03      |
|    n_updates            | 1980          |
|    policy_gradient_loss | -1.53e-06     |
|    value_loss           | 3.51e+04      |
-------------------------------------------
Eval num_timesteps=203500, episode_reward=2105.52 +/- 435.18
Episode length: 42.18 +/- 1.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 199      |
|    time_elapsed    | 813      |
|    total_timesteps | 203776   |
---------------------------------
Eval num_timesteps=204000, episode_reward=2001.38 +/- 527.30
Episode length: 41.94 +/- 2.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.9          |
|    mean_reward          | 2e+03         |
| time/                   |               |
|    total_timesteps      | 204000        |
| train/                  |               |
|    approx_kl            | 1.0477379e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.5e-05      |
|    explained_variance   | 0.7           |
|    learning_rate        | 0.001         |
|    loss                 | 2.49e+03      |
|    n_updates            | 1990          |
|    policy_gradient_loss | -6.16e-06     |
|    value_loss           | 1.35e+04      |
-------------------------------------------
Eval num_timesteps=204500, episode_reward=2054.37 +/- 481.74
Episode length: 41.96 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 200      |
|    time_elapsed    | 817      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=1959.43 +/- 541.98
Episode length: 41.66 +/- 2.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.7         |
|    mean_reward          | 1.96e+03     |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 5.820766e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.75e-06    |
|    explained_variance   | 0.588        |
|    learning_rate        | 0.001        |
|    loss                 | 5e+03        |
|    n_updates            | 2000         |
|    policy_gradient_loss | -7.22e-07    |
|    value_loss           | 2.3e+04      |
------------------------------------------
Eval num_timesteps=205500, episode_reward=1981.80 +/- 532.50
Episode length: 41.76 +/- 2.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.6     |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 201      |
|    time_elapsed    | 821      |
|    total_timesteps | 205824   |
---------------------------------
Eval num_timesteps=206000, episode_reward=2182.26 +/- 333.81
Episode length: 42.30 +/- 1.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.3          |
|    mean_reward          | 2.18e+03      |
| time/                   |               |
|    total_timesteps      | 206000        |
| train/                  |               |
|    approx_kl            | 1.9131694e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000147     |
|    explained_variance   | 0.492         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+04      |
|    n_updates            | 2010          |
|    policy_gradient_loss | -9.91e-05     |
|    value_loss           | 3.86e+04      |
-------------------------------------------
Eval num_timesteps=206500, episode_reward=2080.47 +/- 460.04
Episode length: 42.02 +/- 1.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 202      |
|    time_elapsed    | 825      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=2108.25 +/- 427.99
Episode length: 42.20 +/- 1.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.11e+03  |
| time/                   |           |
|    total_timesteps      | 207000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.73e-06 |
|    explained_variance   | 0.632     |
|    learning_rate        | 0.001     |
|    loss                 | 1.85e+03  |
|    n_updates            | 2020      |
|    policy_gradient_loss | 1.07e-06  |
|    value_loss           | 8.87e+03  |
---------------------------------------
Eval num_timesteps=207500, episode_reward=2083.18 +/- 453.62
Episode length: 42.26 +/- 2.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 203      |
|    time_elapsed    | 829      |
|    total_timesteps | 207872   |
---------------------------------
Eval num_timesteps=208000, episode_reward=2030.29 +/- 501.72
Episode length: 41.68 +/- 2.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.7          |
|    mean_reward          | 2.03e+03      |
| time/                   |               |
|    total_timesteps      | 208000        |
| train/                  |               |
|    approx_kl            | 3.3379765e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000125     |
|    explained_variance   | 0.58          |
|    learning_rate        | 0.001         |
|    loss                 | 1.51e+03      |
|    n_updates            | 2030          |
|    policy_gradient_loss | -0.000119     |
|    value_loss           | 1.61e+04      |
-------------------------------------------
Eval num_timesteps=208500, episode_reward=2080.18 +/- 461.25
Episode length: 41.80 +/- 2.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 250      |
|    iterations      | 204      |
|    time_elapsed    | 833      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=1950.98 +/- 559.05
Episode length: 41.74 +/- 3.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.7          |
|    mean_reward          | 1.95e+03      |
| time/                   |               |
|    total_timesteps      | 209000        |
| train/                  |               |
|    approx_kl            | 7.1013346e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.97e-05     |
|    explained_variance   | 0.591         |
|    learning_rate        | 0.001         |
|    loss                 | 8.22e+03      |
|    n_updates            | 2040          |
|    policy_gradient_loss | -9.51e-06     |
|    value_loss           | 2.28e+04      |
-------------------------------------------
Eval num_timesteps=209500, episode_reward=2157.49 +/- 367.19
Episode length: 42.46 +/- 1.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 205      |
|    time_elapsed    | 837      |
|    total_timesteps | 209920   |
---------------------------------
Eval num_timesteps=210000, episode_reward=2182.30 +/- 334.51
Episode length: 42.16 +/- 1.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 2.18e+03      |
| time/                   |               |
|    total_timesteps      | 210000        |
| train/                  |               |
|    approx_kl            | 4.5401976e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.63e-06     |
|    explained_variance   | 0.621         |
|    learning_rate        | 0.001         |
|    loss                 | 5.14e+03      |
|    n_updates            | 2050          |
|    policy_gradient_loss | -1.6e-05      |
|    value_loss           | 1.54e+04      |
-------------------------------------------
Eval num_timesteps=210500, episode_reward=2082.85 +/- 451.44
Episode length: 42.10 +/- 2.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 206      |
|    time_elapsed    | 841      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=2081.82 +/- 455.77
Episode length: 42.22 +/- 2.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 2.08e+03      |
| time/                   |               |
|    total_timesteps      | 211000        |
| train/                  |               |
|    approx_kl            | 1.2863893e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.65e-05     |
|    explained_variance   | 0.681         |
|    learning_rate        | 0.001         |
|    loss                 | 2.64e+03      |
|    n_updates            | 2060          |
|    policy_gradient_loss | -1.99e-05     |
|    value_loss           | 1.31e+04      |
-------------------------------------------
Eval num_timesteps=211500, episode_reward=2010.95 +/- 509.35
Episode length: 41.78 +/- 2.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 207      |
|    time_elapsed    | 845      |
|    total_timesteps | 211968   |
---------------------------------
Eval num_timesteps=212000, episode_reward=2082.51 +/- 453.70
Episode length: 41.96 +/- 2.22
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 2.08e+03       |
| time/                   |                |
|    total_timesteps      | 212000         |
| train/                  |                |
|    approx_kl            | -1.0477379e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -8.69e-06      |
|    explained_variance   | 0.639          |
|    learning_rate        | 0.001          |
|    loss                 | 1.19e+03       |
|    n_updates            | 2070           |
|    policy_gradient_loss | -1.52e-06      |
|    value_loss           | 1.47e+04       |
--------------------------------------------
Eval num_timesteps=212500, episode_reward=2083.89 +/- 452.93
Episode length: 42.20 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 208      |
|    time_elapsed    | 849      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=2130.73 +/- 404.64
Episode length: 42.24 +/- 1.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 2.13e+03     |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 8.731149e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.7e-05     |
|    explained_variance   | 0.741        |
|    learning_rate        | 0.001        |
|    loss                 | 8.5e+03      |
|    n_updates            | 2080         |
|    policy_gradient_loss | -7.35e-06    |
|    value_loss           | 1.05e+04     |
------------------------------------------
Eval num_timesteps=213500, episode_reward=2082.85 +/- 448.13
Episode length: 42.02 +/- 1.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=2150.19 +/- 391.47
Episode length: 41.94 +/- 2.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 209      |
|    time_elapsed    | 855      |
|    total_timesteps | 214016   |
---------------------------------
Eval num_timesteps=214500, episode_reward=2004.55 +/- 519.22
Episode length: 41.70 +/- 2.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.7          |
|    mean_reward          | 2e+03         |
| time/                   |               |
|    total_timesteps      | 214500        |
| train/                  |               |
|    approx_kl            | 2.4447218e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.33e-05     |
|    explained_variance   | 0.555         |
|    learning_rate        | 0.001         |
|    loss                 | 3.76e+03      |
|    n_updates            | 2090          |
|    policy_gradient_loss | -1.37e-05     |
|    value_loss           | 1.95e+04      |
-------------------------------------------
Eval num_timesteps=215000, episode_reward=2211.21 +/- 272.80
Episode length: 42.52 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 210      |
|    time_elapsed    | 858      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=2121.24 +/- 433.10
Episode length: 42.04 +/- 2.94
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 2.12e+03       |
| time/                   |                |
|    total_timesteps      | 215500         |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.2e-06       |
|    explained_variance   | 0.629          |
|    learning_rate        | 0.001          |
|    loss                 | 1.92e+03       |
|    n_updates            | 2100           |
|    policy_gradient_loss | -3.73e-06      |
|    value_loss           | 1.08e+04       |
--------------------------------------------
Eval num_timesteps=216000, episode_reward=2024.74 +/- 513.92
Episode length: 41.78 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 211      |
|    time_elapsed    | 862      |
|    total_timesteps | 216064   |
---------------------------------
Eval num_timesteps=216500, episode_reward=2017.16 +/- 528.55
Episode length: 41.68 +/- 3.23
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 41.7           |
|    mean_reward          | 2.02e+03       |
| time/                   |                |
|    total_timesteps      | 216500         |
| train/                  |                |
|    approx_kl            | -6.9849193e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.03e-06      |
|    explained_variance   | 0.641          |
|    learning_rate        | 0.001          |
|    loss                 | 1.51e+03       |
|    n_updates            | 2110           |
|    policy_gradient_loss | -9.8e-07       |
|    value_loss           | 1.32e+04       |
--------------------------------------------
Eval num_timesteps=217000, episode_reward=2068.49 +/- 481.98
Episode length: 41.64 +/- 2.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 212      |
|    time_elapsed    | 866      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=2075.79 +/- 470.96
Episode length: 42.28 +/- 2.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.3         |
|    mean_reward          | 2.08e+03     |
| time/                   |              |
|    total_timesteps      | 217500       |
| train/                  |              |
|    approx_kl            | 0.0009439962 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000682    |
|    explained_variance   | 0.634        |
|    learning_rate        | 0.001        |
|    loss                 | 940          |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.000585    |
|    value_loss           | 1.02e+04     |
------------------------------------------
Eval num_timesteps=218000, episode_reward=2150.51 +/- 390.76
Episode length: 42.10 +/- 2.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 213      |
|    time_elapsed    | 870      |
|    total_timesteps | 218112   |
---------------------------------
Eval num_timesteps=218500, episode_reward=2065.83 +/- 489.85
Episode length: 41.90 +/- 3.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 2.07e+03    |
| time/                   |             |
|    total_timesteps      | 218500      |
| train/                  |             |
|    approx_kl            | 0.040058553 |
|    clip_fraction        | 0.0188      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0217     |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.001       |
|    loss                 | 3.36e+03    |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.00836    |
|    value_loss           | 1.41e+04    |
-----------------------------------------
Eval num_timesteps=219000, episode_reward=2131.89 +/- 404.38
Episode length: 42.26 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 214      |
|    time_elapsed    | 874      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=2106.18 +/- 434.31
Episode length: 42.14 +/- 1.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.1          |
|    mean_reward          | 2.11e+03      |
| time/                   |               |
|    total_timesteps      | 219500        |
| train/                  |               |
|    approx_kl            | 4.5401976e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.02e-05     |
|    explained_variance   | 0.694         |
|    learning_rate        | 0.001         |
|    loss                 | 3.29e+03      |
|    n_updates            | 2140          |
|    policy_gradient_loss | 2.01e-06      |
|    value_loss           | 1.31e+04      |
-------------------------------------------
Eval num_timesteps=220000, episode_reward=2173.33 +/- 362.71
Episode length: 42.04 +/- 2.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 215      |
|    time_elapsed    | 878      |
|    total_timesteps | 220160   |
---------------------------------
Eval num_timesteps=220500, episode_reward=2057.50 +/- 480.85
Episode length: 42.12 +/- 2.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 2.06e+03     |
| time/                   |              |
|    total_timesteps      | 220500       |
| train/                  |              |
|    approx_kl            | 5.820766e-11 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00011     |
|    explained_variance   | 0.738        |
|    learning_rate        | 0.001        |
|    loss                 | 2.71e+03     |
|    n_updates            | 2150         |
|    policy_gradient_loss | 3.26e-06     |
|    value_loss           | 8.38e+03     |
------------------------------------------
Eval num_timesteps=221000, episode_reward=2101.24 +/- 446.30
Episode length: 42.10 +/- 2.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 216      |
|    time_elapsed    | 882      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=2052.68 +/- 487.43
Episode length: 41.74 +/- 2.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.7         |
|    mean_reward          | 2.05e+03     |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 2.910383e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.53e-06    |
|    explained_variance   | 0.579        |
|    learning_rate        | 0.001        |
|    loss                 | 5.38e+03     |
|    n_updates            | 2160         |
|    policy_gradient_loss | -1.6e-06     |
|    value_loss           | 2.36e+04     |
------------------------------------------
Eval num_timesteps=222000, episode_reward=2101.69 +/- 446.55
Episode length: 41.80 +/- 2.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 217      |
|    time_elapsed    | 886      |
|    total_timesteps | 222208   |
---------------------------------
Eval num_timesteps=222500, episode_reward=2032.26 +/- 501.19
Episode length: 41.86 +/- 2.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.9          |
|    mean_reward          | 2.03e+03      |
| time/                   |               |
|    total_timesteps      | 222500        |
| train/                  |               |
|    approx_kl            | 8.8190427e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.71e-05     |
|    explained_variance   | 0.496         |
|    learning_rate        | 0.001         |
|    loss                 | 2.73e+03      |
|    n_updates            | 2170          |
|    policy_gradient_loss | -0.000311     |
|    value_loss           | 1.12e+04      |
-------------------------------------------
Eval num_timesteps=223000, episode_reward=2103.88 +/- 438.26
Episode length: 42.00 +/- 2.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 218      |
|    time_elapsed    | 890      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=2048.69 +/- 497.44
Episode length: 42.06 +/- 2.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.1          |
|    mean_reward          | 2.05e+03      |
| time/                   |               |
|    total_timesteps      | 223500        |
| train/                  |               |
|    approx_kl            | 6.4028427e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.77e-05     |
|    explained_variance   | 0.466         |
|    learning_rate        | 0.001         |
|    loss                 | 3.57e+03      |
|    n_updates            | 2180          |
|    policy_gradient_loss | -9.41e-06     |
|    value_loss           | 2.54e+04      |
-------------------------------------------
Eval num_timesteps=224000, episode_reward=2036.80 +/- 486.93
Episode length: 42.30 +/- 1.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 219      |
|    time_elapsed    | 894      |
|    total_timesteps | 224256   |
---------------------------------
Eval num_timesteps=224500, episode_reward=2154.43 +/- 377.42
Episode length: 42.14 +/- 1.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.1          |
|    mean_reward          | 2.15e+03      |
| time/                   |               |
|    total_timesteps      | 224500        |
| train/                  |               |
|    approx_kl            | 1.3969839e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.31e-06     |
|    explained_variance   | 0.552         |
|    learning_rate        | 0.001         |
|    loss                 | 3.8e+03       |
|    n_updates            | 2190          |
|    policy_gradient_loss | -1.48e-06     |
|    value_loss           | 2.44e+04      |
-------------------------------------------
Eval num_timesteps=225000, episode_reward=2028.66 +/- 503.60
Episode length: 41.64 +/- 2.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 220      |
|    time_elapsed    | 898      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=2135.40 +/- 393.45
Episode length: 42.38 +/- 1.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.4          |
|    mean_reward          | 2.14e+03      |
| time/                   |               |
|    total_timesteps      | 225500        |
| train/                  |               |
|    approx_kl            | 1.6608916e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000434     |
|    explained_variance   | 0.494         |
|    learning_rate        | 0.001         |
|    loss                 | 1.95e+04      |
|    n_updates            | 2200          |
|    policy_gradient_loss | -0.000425     |
|    value_loss           | 3.6e+04       |
-------------------------------------------
Eval num_timesteps=226000, episode_reward=2065.85 +/- 491.88
Episode length: 41.84 +/- 2.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.6     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 221      |
|    time_elapsed    | 902      |
|    total_timesteps | 226304   |
---------------------------------
Eval num_timesteps=226500, episode_reward=2079.21 +/- 461.49
Episode length: 41.98 +/- 1.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 2.08e+03     |
| time/                   |              |
|    total_timesteps      | 226500       |
| train/                  |              |
|    approx_kl            | 4.240038e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000323    |
|    explained_variance   | 0.38         |
|    learning_rate        | 0.001        |
|    loss                 | 6.79e+03     |
|    n_updates            | 2210         |
|    policy_gradient_loss | -8.72e-05    |
|    value_loss           | 1.85e+04     |
------------------------------------------
Eval num_timesteps=227000, episode_reward=2042.16 +/- 477.40
Episode length: 42.20 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 222      |
|    time_elapsed    | 906      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=1895.53 +/- 623.56
Episode length: 41.90 +/- 3.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.9       |
|    mean_reward          | 1.9e+03    |
| time/                   |            |
|    total_timesteps      | 227500     |
| train/                  |            |
|    approx_kl            | 0.06831779 |
|    clip_fraction        | 0.0163     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0244    |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.001      |
|    loss                 | 611        |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 1.09e+04   |
----------------------------------------
Eval num_timesteps=228000, episode_reward=1955.75 +/- 579.15
Episode length: 43.60 +/- 9.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.6     |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 223      |
|    time_elapsed    | 910      |
|    total_timesteps | 228352   |
---------------------------------
Eval num_timesteps=228500, episode_reward=2102.03 +/- 445.20
Episode length: 41.92 +/- 2.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 228500    |
| train/                  |           |
|    approx_kl            | 0.5589389 |
|    clip_fraction        | 0.0951    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00847  |
|    explained_variance   | 0.635     |
|    learning_rate        | 0.001     |
|    loss                 | 8.77e+03  |
|    n_updates            | 2230      |
|    policy_gradient_loss | 0.0293    |
|    value_loss           | 3.79e+04  |
---------------------------------------
Eval num_timesteps=229000, episode_reward=2045.96 +/- 498.53
Episode length: 41.34 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 224      |
|    time_elapsed    | 914      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=1989.61 +/- 547.81
Episode length: 41.54 +/- 2.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.5      |
|    mean_reward          | 1.99e+03  |
| time/                   |           |
|    total_timesteps      | 229500    |
| train/                  |           |
|    approx_kl            | 0.0673669 |
|    clip_fraction        | 0.0107    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00758  |
|    explained_variance   | 0.418     |
|    learning_rate        | 0.001     |
|    loss                 | 2.64e+03  |
|    n_updates            | 2240      |
|    policy_gradient_loss | -0.00234  |
|    value_loss           | 2.03e+04  |
---------------------------------------
Eval num_timesteps=230000, episode_reward=1933.38 +/- 588.01
Episode length: 41.80 +/- 3.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 225      |
|    time_elapsed    | 918      |
|    total_timesteps | 230400   |
---------------------------------
Eval num_timesteps=230500, episode_reward=2101.21 +/- 446.14
Episode length: 42.38 +/- 2.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 230500    |
| train/                  |           |
|    approx_kl            | 1.5727644 |
|    clip_fraction        | 0.608     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0304   |
|    explained_variance   | 0.434     |
|    learning_rate        | 0.001     |
|    loss                 | 6.7e+03   |
|    n_updates            | 2250      |
|    policy_gradient_loss | 0.199     |
|    value_loss           | 3.01e+04  |
---------------------------------------
Eval num_timesteps=231000, episode_reward=2034.06 +/- 494.80
Episode length: 42.00 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 226      |
|    time_elapsed    | 922      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=2094.89 +/- 461.00
Episode length: 42.88 +/- 2.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.9      |
|    mean_reward          | 2.09e+03  |
| time/                   |           |
|    total_timesteps      | 231500    |
| train/                  |           |
|    approx_kl            | 0.1932985 |
|    clip_fraction        | 0.0134    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00461  |
|    explained_variance   | 0.572     |
|    learning_rate        | 0.001     |
|    loss                 | 3.56e+03  |
|    n_updates            | 2260      |
|    policy_gradient_loss | -0.00408  |
|    value_loss           | 1.49e+04  |
---------------------------------------
Eval num_timesteps=232000, episode_reward=2095.58 +/- 461.11
Episode length: 42.00 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 227      |
|    time_elapsed    | 926      |
|    total_timesteps | 232448   |
---------------------------------
Eval num_timesteps=232500, episode_reward=1732.07 +/- 708.49
Episode length: 43.82 +/- 12.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 43.8       |
|    mean_reward          | 1.73e+03   |
| time/                   |            |
|    total_timesteps      | 232500     |
| train/                  |            |
|    approx_kl            | 0.64628845 |
|    clip_fraction        | 0.0764     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0342    |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.001      |
|    loss                 | 7.27e+03   |
|    n_updates            | 2270       |
|    policy_gradient_loss | -0.00664   |
|    value_loss           | 2.96e+04   |
----------------------------------------
Eval num_timesteps=233000, episode_reward=1663.65 +/- 701.47
Episode length: 42.62 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.8     |
|    ep_rew_mean     | 1.96e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 228      |
|    time_elapsed    | 930      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=923.22 +/- 622.55
Episode length: 50.36 +/- 23.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | 923       |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.8510854 |
|    clip_fraction        | 0.0922    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0248   |
|    explained_variance   | 0.705     |
|    learning_rate        | 0.001     |
|    loss                 | 6.64e+03  |
|    n_updates            | 2280      |
|    policy_gradient_loss | -0.00812  |
|    value_loss           | 3.67e+04  |
---------------------------------------
Eval num_timesteps=234000, episode_reward=939.45 +/- 618.86
Episode length: 51.76 +/- 26.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.6     |
|    ep_rew_mean     | 1.7e+03  |
| time/              |          |
|    fps             | 250      |
|    iterations      | 229      |
|    time_elapsed    | 934      |
|    total_timesteps | 234496   |
---------------------------------
Eval num_timesteps=234500, episode_reward=2023.78 +/- 513.96
Episode length: 41.50 +/- 2.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.5      |
|    mean_reward          | 2.02e+03  |
| time/                   |           |
|    total_timesteps      | 234500    |
| train/                  |           |
|    approx_kl            | 50.698627 |
|    clip_fraction        | 0.327     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00375  |
|    explained_variance   | 0.693     |
|    learning_rate        | 0.001     |
|    loss                 | 1.98e+04  |
|    n_updates            | 2290      |
|    policy_gradient_loss | 0.188     |
|    value_loss           | 5.95e+04  |
---------------------------------------
Eval num_timesteps=235000, episode_reward=2025.55 +/- 511.92
Episode length: 41.72 +/- 2.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=2105.55 +/- 434.02
Episode length: 42.02 +/- 1.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.6     |
|    ep_rew_mean     | 1.7e+03  |
| time/              |          |
|    fps             | 250      |
|    iterations      | 230      |
|    time_elapsed    | 940      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=2145.56 +/- 399.92
Episode length: 42.12 +/- 2.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.15e+03  |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.58e-23 |
|    explained_variance   | -0.919    |
|    learning_rate        | 0.001     |
|    loss                 | 6.73e+03  |
|    n_updates            | 2300      |
|    policy_gradient_loss | -2.08e-09 |
|    value_loss           | 3.18e+04  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=2143.35 +/- 413.71
Episode length: 42.24 +/- 2.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.8     |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 231      |
|    time_elapsed    | 943      |
|    total_timesteps | 236544   |
---------------------------------
Eval num_timesteps=237000, episode_reward=2212.20 +/- 270.42
Episode length: 42.48 +/- 1.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.21e+03  |
| time/                   |           |
|    total_timesteps      | 237000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.92e-38 |
|    explained_variance   | 0.434     |
|    learning_rate        | 0.001     |
|    loss                 | 347       |
|    n_updates            | 2310      |
|    policy_gradient_loss | 1.2e-09   |
|    value_loss           | 4.44e+03  |
---------------------------------------
Eval num_timesteps=237500, episode_reward=2019.49 +/- 526.72
Episode length: 41.40 +/- 3.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 232      |
|    time_elapsed    | 947      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=2083.41 +/- 451.52
Episode length: 41.94 +/- 1.74
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 41.9     |
|    mean_reward          | 2.08e+03 |
| time/                   |          |
|    total_timesteps      | 238000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.6e-35 |
|    explained_variance   | 0.499    |
|    learning_rate        | 0.001    |
|    loss                 | 8.06e+03 |
|    n_updates            | 2320     |
|    policy_gradient_loss | 1.5e-09  |
|    value_loss           | 1.86e+04 |
--------------------------------------
Eval num_timesteps=238500, episode_reward=2010.29 +/- 509.07
Episode length: 42.00 +/- 2.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 233      |
|    time_elapsed    | 951      |
|    total_timesteps | 238592   |
---------------------------------
Eval num_timesteps=239000, episode_reward=1984.12 +/- 528.77
Episode length: 41.84 +/- 2.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 1.98e+03  |
| time/                   |           |
|    total_timesteps      | 239000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.58e-36 |
|    explained_variance   | 0.619     |
|    learning_rate        | 0.001     |
|    loss                 | 3.3e+03   |
|    n_updates            | 2330      |
|    policy_gradient_loss | 2.62e-11  |
|    value_loss           | 1.41e+04  |
---------------------------------------
Eval num_timesteps=239500, episode_reward=2132.66 +/- 395.59
Episode length: 42.34 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 234      |
|    time_elapsed    | 955      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=2055.81 +/- 476.38
Episode length: 42.04 +/- 2.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.74e-33 |
|    explained_variance   | 0.558     |
|    learning_rate        | 0.001     |
|    loss                 | 5.99e+03  |
|    n_updates            | 2340      |
|    policy_gradient_loss | -8.73e-10 |
|    value_loss           | 1.83e+04  |
---------------------------------------
Eval num_timesteps=240500, episode_reward=1997.42 +/- 534.37
Episode length: 41.56 +/- 2.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 235      |
|    time_elapsed    | 959      |
|    total_timesteps | 240640   |
---------------------------------
Eval num_timesteps=241000, episode_reward=2130.11 +/- 408.10
Episode length: 42.08 +/- 1.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 241000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.35e-25 |
|    explained_variance   | 0.512     |
|    learning_rate        | 0.001     |
|    loss                 | 1.26e+04  |
|    n_updates            | 2350      |
|    policy_gradient_loss | -1.94e-09 |
|    value_loss           | 3.4e+04   |
---------------------------------------
Eval num_timesteps=241500, episode_reward=2080.21 +/- 453.93
Episode length: 42.18 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 236      |
|    time_elapsed    | 963      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=2063.30 +/- 464.03
Episode length: 42.34 +/- 2.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.03e-31 |
|    explained_variance   | 0.471     |
|    learning_rate        | 0.001     |
|    loss                 | 7.05e+03  |
|    n_updates            | 2360      |
|    policy_gradient_loss | -2.12e-09 |
|    value_loss           | 2.92e+04  |
---------------------------------------
Eval num_timesteps=242500, episode_reward=2000.61 +/- 532.57
Episode length: 41.74 +/- 2.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 237      |
|    time_elapsed    | 967      |
|    total_timesteps | 242688   |
---------------------------------
Eval num_timesteps=243000, episode_reward=2002.25 +/- 525.40
Episode length: 41.68 +/- 2.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 243000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.82e-24 |
|    explained_variance   | 0.543     |
|    learning_rate        | 0.001     |
|    loss                 | 4.04e+03  |
|    n_updates            | 2370      |
|    policy_gradient_loss | 1.11e-09  |
|    value_loss           | 1.48e+04  |
---------------------------------------
Eval num_timesteps=243500, episode_reward=2058.79 +/- 475.15
Episode length: 42.26 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 238      |
|    time_elapsed    | 971      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=2036.92 +/- 490.64
Episode length: 42.24 +/- 2.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.04e+03  |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.97e-36 |
|    explained_variance   | 0.582     |
|    learning_rate        | 0.001     |
|    loss                 | 4.66e+03  |
|    n_updates            | 2380      |
|    policy_gradient_loss | -4.71e-10 |
|    value_loss           | 1.95e+04  |
---------------------------------------
Eval num_timesteps=244500, episode_reward=2130.60 +/- 406.11
Episode length: 42.54 +/- 1.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 239      |
|    time_elapsed    | 975      |
|    total_timesteps | 244736   |
---------------------------------
Eval num_timesteps=245000, episode_reward=2158.22 +/- 367.30
Episode length: 42.86 +/- 1.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.9      |
|    mean_reward          | 2.16e+03  |
| time/                   |           |
|    total_timesteps      | 245000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.28e-21 |
|    explained_variance   | 0.519     |
|    learning_rate        | 0.001     |
|    loss                 | 8.8e+03   |
|    n_updates            | 2390      |
|    policy_gradient_loss | -2.59e-10 |
|    value_loss           | 2.19e+04  |
---------------------------------------
Eval num_timesteps=245500, episode_reward=2107.77 +/- 428.62
Episode length: 41.98 +/- 1.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 240      |
|    time_elapsed    | 979      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=2176.51 +/- 352.88
Episode length: 42.24 +/- 2.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.44e-31 |
|    explained_variance   | 0.56      |
|    learning_rate        | 0.001     |
|    loss                 | 6.52e+03  |
|    n_updates            | 2400      |
|    policy_gradient_loss | -7.04e-10 |
|    value_loss           | 2.93e+04  |
---------------------------------------
Eval num_timesteps=246500, episode_reward=2004.45 +/- 521.65
Episode length: 41.68 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 241      |
|    time_elapsed    | 983      |
|    total_timesteps | 246784   |
---------------------------------
Eval num_timesteps=247000, episode_reward=2046.77 +/- 501.70
Episode length: 41.70 +/- 3.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.05e+03  |
| time/                   |           |
|    total_timesteps      | 247000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.87e-28 |
|    explained_variance   | 0.454     |
|    learning_rate        | 0.001     |
|    loss                 | 2.94e+03  |
|    n_updates            | 2410      |
|    policy_gradient_loss | 1.75e-11  |
|    value_loss           | 2.15e+04  |
---------------------------------------
Eval num_timesteps=247500, episode_reward=2154.71 +/- 375.65
Episode length: 42.10 +/- 1.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 242      |
|    time_elapsed    | 987      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=2200.37 +/- 302.49
Episode length: 42.56 +/- 1.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.6      |
|    mean_reward          | 2.2e+03   |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.71e-21 |
|    explained_variance   | 0.448     |
|    learning_rate        | 0.001     |
|    loss                 | 9.19e+03  |
|    n_updates            | 2420      |
|    policy_gradient_loss | 1.44e-09  |
|    value_loss           | 3.3e+04   |
---------------------------------------
Eval num_timesteps=248500, episode_reward=1979.40 +/- 538.84
Episode length: 41.92 +/- 2.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 243      |
|    time_elapsed    | 991      |
|    total_timesteps | 248832   |
---------------------------------
Eval num_timesteps=249000, episode_reward=2127.21 +/- 415.82
Episode length: 42.20 +/- 2.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 249000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.17e-20 |
|    explained_variance   | 0.601     |
|    learning_rate        | 0.001     |
|    loss                 | 3.72e+03  |
|    n_updates            | 2430      |
|    policy_gradient_loss | 3.43e-10  |
|    value_loss           | 2.14e+04  |
---------------------------------------
Eval num_timesteps=249500, episode_reward=2104.44 +/- 438.37
Episode length: 42.16 +/- 1.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 244      |
|    time_elapsed    | 995      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=2051.75 +/- 489.38
Episode length: 41.88 +/- 2.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.05e+03  |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.67e-27 |
|    explained_variance   | 0.584     |
|    learning_rate        | 0.001     |
|    loss                 | 3.37e+03  |
|    n_updates            | 2440      |
|    policy_gradient_loss | 2.4e-09   |
|    value_loss           | 1.77e+04  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=2008.67 +/- 513.49
Episode length: 41.86 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 245      |
|    time_elapsed    | 999      |
|    total_timesteps | 250880   |
---------------------------------
Eval num_timesteps=251000, episode_reward=2129.84 +/- 409.73
Episode length: 42.12 +/- 2.15
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 42.1     |
|    mean_reward          | 2.13e+03 |
| time/                   |          |
|    total_timesteps      | 251000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.7e-34 |
|    explained_variance   | 0.587    |
|    learning_rate        | 0.001    |
|    loss                 | 5.23e+03 |
|    n_updates            | 2450     |
|    policy_gradient_loss | 3.36e-10 |
|    value_loss           | 1.01e+04 |
--------------------------------------
Eval num_timesteps=251500, episode_reward=2051.96 +/- 488.00
Episode length: 42.04 +/- 2.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 246      |
|    time_elapsed    | 1003     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=1982.67 +/- 526.95
Episode length: 41.96 +/- 2.26
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 42       |
|    mean_reward          | 1.98e+03 |
| time/                   |          |
|    total_timesteps      | 252000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -9.6e-38 |
|    explained_variance   | 0.693    |
|    learning_rate        | 0.001    |
|    loss                 | 3.63e+03 |
|    n_updates            | 2460     |
|    policy_gradient_loss | -2.1e-10 |
|    value_loss           | 1.09e+04 |
--------------------------------------
Eval num_timesteps=252500, episode_reward=2081.72 +/- 451.89
Episode length: 41.86 +/- 2.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 247      |
|    time_elapsed    | 1007     |
|    total_timesteps | 252928   |
---------------------------------
Eval num_timesteps=253000, episode_reward=2104.47 +/- 437.82
Episode length: 42.28 +/- 2.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 253000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.64e-27 |
|    explained_variance   | 0.634     |
|    learning_rate        | 0.001     |
|    loss                 | 6.4e+03   |
|    n_updates            | 2470      |
|    policy_gradient_loss | 1.29e-09  |
|    value_loss           | 2.33e+04  |
---------------------------------------
Eval num_timesteps=253500, episode_reward=2155.09 +/- 375.48
Episode length: 42.50 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 248      |
|    time_elapsed    | 1011     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=2103.71 +/- 439.32
Episode length: 41.76 +/- 2.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.32e-25 |
|    explained_variance   | 0.591     |
|    learning_rate        | 0.001     |
|    loss                 | 6.66e+03  |
|    n_updates            | 2480      |
|    policy_gradient_loss | -7.63e-10 |
|    value_loss           | 1.59e+04  |
---------------------------------------
Eval num_timesteps=254500, episode_reward=2112.07 +/- 419.14
Episode length: 42.10 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 249      |
|    time_elapsed    | 1014     |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=255000, episode_reward=2155.07 +/- 377.87
Episode length: 41.92 +/- 2.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.16e+03  |
| time/                   |           |
|    total_timesteps      | 255000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.65e-35 |
|    explained_variance   | 0.639     |
|    learning_rate        | 0.001     |
|    loss                 | 7.57e+03  |
|    n_updates            | 2490      |
|    policy_gradient_loss | -1.15e-09 |
|    value_loss           | 2.83e+04  |
---------------------------------------
Eval num_timesteps=255500, episode_reward=2041.61 +/- 507.92
Episode length: 41.64 +/- 2.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=2116.16 +/- 449.18
Episode length: 41.82 +/- 3.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 250      |
|    time_elapsed    | 1020     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=2027.77 +/- 501.69
Episode length: 41.78 +/- 2.60
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 41.8     |
|    mean_reward          | 2.03e+03 |
| time/                   |          |
|    total_timesteps      | 256500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -8.4e-35 |
|    explained_variance   | 0.601    |
|    learning_rate        | 0.001    |
|    loss                 | 4.16e+03 |
|    n_updates            | 2500     |
|    policy_gradient_loss | 3.43e-10 |
|    value_loss           | 2.07e+04 |
--------------------------------------
Eval num_timesteps=257000, episode_reward=2052.15 +/- 488.81
Episode length: 41.94 +/- 2.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 251      |
|    time_elapsed    | 1024     |
|    total_timesteps | 257024   |
---------------------------------
Eval num_timesteps=257500, episode_reward=2129.24 +/- 409.02
Episode length: 42.12 +/- 1.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 257500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.43e-28 |
|    explained_variance   | 0.488     |
|    learning_rate        | 0.001     |
|    loss                 | 6.97e+03  |
|    n_updates            | 2510      |
|    policy_gradient_loss | 6.98e-11  |
|    value_loss           | 2.7e+04   |
---------------------------------------
Eval num_timesteps=258000, episode_reward=2079.56 +/- 462.13
Episode length: 42.08 +/- 2.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 252      |
|    time_elapsed    | 1028     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=2203.30 +/- 306.60
Episode length: 42.40 +/- 1.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.2e+03   |
| time/                   |           |
|    total_timesteps      | 258500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.99e-27 |
|    explained_variance   | 0.525     |
|    learning_rate        | 0.001     |
|    loss                 | 3.11e+03  |
|    n_updates            | 2520      |
|    policy_gradient_loss | 8.32e-10  |
|    value_loss           | 2.85e+04  |
---------------------------------------
Eval num_timesteps=259000, episode_reward=2150.64 +/- 388.93
Episode length: 42.12 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 250      |
|    iterations      | 253      |
|    time_elapsed    | 1032     |
|    total_timesteps | 259072   |
---------------------------------
Eval num_timesteps=259500, episode_reward=2188.58 +/- 311.80
Episode length: 42.66 +/- 1.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.7      |
|    mean_reward          | 2.19e+03  |
| time/                   |           |
|    total_timesteps      | 259500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.23e-22 |
|    explained_variance   | 0.481     |
|    learning_rate        | 0.001     |
|    loss                 | 5.34e+03  |
|    n_updates            | 2530      |
|    policy_gradient_loss | 4.92e-10  |
|    value_loss           | 1.43e+04  |
---------------------------------------
Eval num_timesteps=260000, episode_reward=2104.99 +/- 435.39
Episode length: 41.92 +/- 2.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 254      |
|    time_elapsed    | 1036     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=2083.44 +/- 451.73
Episode length: 42.18 +/- 2.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.32e-23 |
|    explained_variance   | 0.499     |
|    learning_rate        | 0.001     |
|    loss                 | 5.25e+03  |
|    n_updates            | 2540      |
|    policy_gradient_loss | 1.12e-09  |
|    value_loss           | 2.49e+04  |
---------------------------------------
Eval num_timesteps=261000, episode_reward=2130.78 +/- 405.66
Episode length: 42.52 +/- 1.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 255      |
|    time_elapsed    | 1040     |
|    total_timesteps | 261120   |
---------------------------------
Eval num_timesteps=261500, episode_reward=2078.39 +/- 464.80
Episode length: 42.00 +/- 2.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 261500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.39e-32 |
|    explained_variance   | 0.599     |
|    learning_rate        | 0.001     |
|    loss                 | 3.5e+03   |
|    n_updates            | 2550      |
|    policy_gradient_loss | -1.92e-10 |
|    value_loss           | 1.56e+04  |
---------------------------------------
Eval num_timesteps=262000, episode_reward=2103.07 +/- 441.44
Episode length: 42.18 +/- 2.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 256      |
|    time_elapsed    | 1044     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=2059.16 +/- 473.24
Episode length: 42.40 +/- 2.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 262500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.15e-27 |
|    explained_variance   | 0.515     |
|    learning_rate        | 0.001     |
|    loss                 | 2.14e+03  |
|    n_updates            | 2560      |
|    policy_gradient_loss | 1.37e-09  |
|    value_loss           | 2.4e+04   |
---------------------------------------
Eval num_timesteps=263000, episode_reward=2054.04 +/- 482.27
Episode length: 41.80 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.6     |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 257      |
|    time_elapsed    | 1048     |
|    total_timesteps | 263168   |
---------------------------------
Eval num_timesteps=263500, episode_reward=2000.44 +/- 529.90
Episode length: 41.70 +/- 2.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 263500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.99e-35 |
|    explained_variance   | 0.587     |
|    learning_rate        | 0.001     |
|    loss                 | 3.68e+03  |
|    n_updates            | 2570      |
|    policy_gradient_loss | -7.68e-10 |
|    value_loss           | 1.74e+04  |
---------------------------------------
Eval num_timesteps=264000, episode_reward=2020.56 +/- 521.83
Episode length: 41.32 +/- 3.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 258      |
|    time_elapsed    | 1051     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=1954.81 +/- 551.19
Episode length: 41.80 +/- 2.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 1.95e+03  |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.02e-34 |
|    explained_variance   | 0.681     |
|    learning_rate        | 0.001     |
|    loss                 | 5.18e+03  |
|    n_updates            | 2580      |
|    policy_gradient_loss | -1.22e-10 |
|    value_loss           | 1.19e+04  |
---------------------------------------
Eval num_timesteps=265000, episode_reward=2081.24 +/- 457.50
Episode length: 41.98 +/- 2.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 259      |
|    time_elapsed    | 1055     |
|    total_timesteps | 265216   |
---------------------------------
Eval num_timesteps=265500, episode_reward=2057.35 +/- 478.87
Episode length: 41.90 +/- 1.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 265500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.07e-25 |
|    explained_variance   | 0.535     |
|    learning_rate        | 0.001     |
|    loss                 | 1.23e+04  |
|    n_updates            | 2590      |
|    policy_gradient_loss | 1e-09     |
|    value_loss           | 3.27e+04  |
---------------------------------------
Eval num_timesteps=266000, episode_reward=2018.59 +/- 523.18
Episode length: 41.70 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 260      |
|    time_elapsed    | 1059     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=2000.13 +/- 527.97
Episode length: 41.72 +/- 2.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.04e-27 |
|    explained_variance   | 0.5       |
|    learning_rate        | 0.001     |
|    loss                 | 3.25e+03  |
|    n_updates            | 2600      |
|    policy_gradient_loss | -3.2e-10  |
|    value_loss           | 1.26e+04  |
---------------------------------------
Eval num_timesteps=267000, episode_reward=2091.66 +/- 470.48
Episode length: 42.08 +/- 2.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 261      |
|    time_elapsed    | 1063     |
|    total_timesteps | 267264   |
---------------------------------
Eval num_timesteps=267500, episode_reward=2039.86 +/- 484.19
Episode length: 42.12 +/- 2.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.04e+03  |
| time/                   |           |
|    total_timesteps      | 267500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.81e-34 |
|    explained_variance   | 0.605     |
|    learning_rate        | 0.001     |
|    loss                 | 3.97e+03  |
|    n_updates            | 2610      |
|    policy_gradient_loss | -3.03e-10 |
|    value_loss           | 1.12e+04  |
---------------------------------------
Eval num_timesteps=268000, episode_reward=2125.87 +/- 419.48
Episode length: 41.92 +/- 2.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 262      |
|    time_elapsed    | 1067     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=1924.42 +/- 573.82
Episode length: 41.26 +/- 3.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.3      |
|    mean_reward          | 1.92e+03  |
| time/                   |           |
|    total_timesteps      | 268500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.21e-36 |
|    explained_variance   | 0.549     |
|    learning_rate        | 0.001     |
|    loss                 | 5.96e+03  |
|    n_updates            | 2620      |
|    policy_gradient_loss | 6.26e-10  |
|    value_loss           | 2.03e+04  |
---------------------------------------
Eval num_timesteps=269000, episode_reward=2116.69 +/- 404.75
Episode length: 42.52 +/- 1.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 263      |
|    time_elapsed    | 1071     |
|    total_timesteps | 269312   |
---------------------------------
Eval num_timesteps=269500, episode_reward=2074.06 +/- 468.52
Episode length: 41.72 +/- 2.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.07e+03  |
| time/                   |           |
|    total_timesteps      | 269500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.04e-28 |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.001     |
|    loss                 | 4.21e+03  |
|    n_updates            | 2630      |
|    policy_gradient_loss | 2.36e-10  |
|    value_loss           | 1.66e+04  |
---------------------------------------
Eval num_timesteps=270000, episode_reward=2000.23 +/- 528.78
Episode length: 41.34 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 264      |
|    time_elapsed    | 1075     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=2077.14 +/- 468.00
Episode length: 41.98 +/- 2.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 270500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.28e-28 |
|    explained_variance   | 0.442     |
|    learning_rate        | 0.001     |
|    loss                 | 2.55e+03  |
|    n_updates            | 2640      |
|    policy_gradient_loss | 1.34e-09  |
|    value_loss           | 1.73e+04  |
---------------------------------------
Eval num_timesteps=271000, episode_reward=2035.31 +/- 492.07
Episode length: 41.90 +/- 1.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 265      |
|    time_elapsed    | 1079     |
|    total_timesteps | 271360   |
---------------------------------
Eval num_timesteps=271500, episode_reward=2079.12 +/- 460.11
Episode length: 42.12 +/- 2.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 271500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 9.77e-05  |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000104 |
|    explained_variance   | 0.491     |
|    learning_rate        | 0.001     |
|    loss                 | 5.88e+03  |
|    n_updates            | 2650      |
|    policy_gradient_loss | -6.3e-05  |
|    value_loss           | 2.53e+04  |
---------------------------------------
Eval num_timesteps=272000, episode_reward=2174.89 +/- 361.50
Episode length: 42.34 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 266      |
|    time_elapsed    | 1083     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=2121.12 +/- 431.96
Episode length: 41.82 +/- 2.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.8      |
|    mean_reward          | 2.12e+03  |
| time/                   |           |
|    total_timesteps      | 272500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.19e-43 |
|    explained_variance   | 0.762     |
|    learning_rate        | 0.001     |
|    loss                 | 629       |
|    n_updates            | 2660      |
|    policy_gradient_loss | 1.05e-09  |
|    value_loss           | 2.58e+03  |
---------------------------------------
Eval num_timesteps=273000, episode_reward=1878.45 +/- 590.07
Episode length: 40.98 +/- 2.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 267      |
|    time_elapsed    | 1087     |
|    total_timesteps | 273408   |
---------------------------------
Eval num_timesteps=273500, episode_reward=2081.41 +/- 457.47
Episode length: 42.02 +/- 1.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 273500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.98e-28 |
|    explained_variance   | 0.647     |
|    learning_rate        | 0.001     |
|    loss                 | 4.65e+03  |
|    n_updates            | 2670      |
|    policy_gradient_loss | -8.73e-10 |
|    value_loss           | 1.55e+04  |
---------------------------------------
Eval num_timesteps=274000, episode_reward=2107.84 +/- 428.79
Episode length: 42.38 +/- 2.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 268      |
|    time_elapsed    | 1091     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=2106.40 +/- 475.77
Episode length: 41.62 +/- 3.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.6      |
|    mean_reward          | 2.11e+03  |
| time/                   |           |
|    total_timesteps      | 274500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.83e-30 |
|    explained_variance   | 0.581     |
|    learning_rate        | 0.001     |
|    loss                 | 1.66e+03  |
|    n_updates            | 2680      |
|    policy_gradient_loss | 1.14e-09  |
|    value_loss           | 1.76e+04  |
---------------------------------------
Eval num_timesteps=275000, episode_reward=2070.47 +/- 478.68
Episode length: 41.90 +/- 2.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 269      |
|    time_elapsed    | 1095     |
|    total_timesteps | 275456   |
---------------------------------
Eval num_timesteps=275500, episode_reward=2048.07 +/- 498.20
Episode length: 41.70 +/- 2.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.05e+03  |
| time/                   |           |
|    total_timesteps      | 275500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.78e-27 |
|    explained_variance   | 0.568     |
|    learning_rate        | 0.001     |
|    loss                 | 4.11e+03  |
|    n_updates            | 2690      |
|    policy_gradient_loss | -5.36e-10 |
|    value_loss           | 2.16e+04  |
---------------------------------------
Eval num_timesteps=276000, episode_reward=2033.07 +/- 495.71
Episode length: 41.74 +/- 2.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 270      |
|    time_elapsed    | 1099     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=2070.88 +/- 480.98
Episode length: 41.72 +/- 2.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.07e+03  |
| time/                   |           |
|    total_timesteps      | 276500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08e-15 |
|    explained_variance   | 0.544     |
|    learning_rate        | 0.001     |
|    loss                 | 3.81e+03  |
|    n_updates            | 2700      |
|    policy_gradient_loss | -1.95e-09 |
|    value_loss           | 1.97e+04  |
---------------------------------------
Eval num_timesteps=277000, episode_reward=1983.42 +/- 529.82
Episode length: 41.96 +/- 2.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=2129.85 +/- 407.48
Episode length: 42.46 +/- 1.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 251      |
|    iterations      | 271      |
|    time_elapsed    | 1104     |
|    total_timesteps | 277504   |
---------------------------------
Eval num_timesteps=278000, episode_reward=2121.62 +/- 434.62
Episode length: 41.74 +/- 2.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.12e+03  |
| time/                   |           |
|    total_timesteps      | 278000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.59e-26 |
|    explained_variance   | 0.733     |
|    learning_rate        | 0.001     |
|    loss                 | 1.18e+03  |
|    n_updates            | 2710      |
|    policy_gradient_loss | 7.68e-10  |
|    value_loss           | 9.98e+03  |
---------------------------------------
Eval num_timesteps=278500, episode_reward=2081.01 +/- 458.71
Episode length: 41.66 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 251      |
|    iterations      | 272      |
|    time_elapsed    | 1108     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=2011.04 +/- 508.47
Episode length: 41.74 +/- 2.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2.01e+03  |
| time/                   |           |
|    total_timesteps      | 279000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.82e-33 |
|    explained_variance   | 0.61      |
|    learning_rate        | 0.001     |
|    loss                 | 3.91e+03  |
|    n_updates            | 2720      |
|    policy_gradient_loss | 1.58e-09  |
|    value_loss           | 1.63e+04  |
---------------------------------------
Eval num_timesteps=279500, episode_reward=2153.85 +/- 383.03
Episode length: 42.20 +/- 2.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 273      |
|    time_elapsed    | 1112     |
|    total_timesteps | 279552   |
---------------------------------
Eval num_timesteps=280000, episode_reward=1968.95 +/- 556.55
Episode length: 41.48 +/- 3.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.5      |
|    mean_reward          | 1.97e+03  |
| time/                   |           |
|    total_timesteps      | 280000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.47e-23 |
|    explained_variance   | 0.566     |
|    learning_rate        | 0.001     |
|    loss                 | 7.93e+03  |
|    n_updates            | 2730      |
|    policy_gradient_loss | 7.33e-10  |
|    value_loss           | 2.35e+04  |
---------------------------------------
Eval num_timesteps=280500, episode_reward=2149.78 +/- 393.13
Episode length: 42.26 +/- 2.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 274      |
|    time_elapsed    | 1116     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=2197.68 +/- 321.47
Episode length: 42.44 +/- 2.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.2e+03   |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.33e-27 |
|    explained_variance   | 0.57      |
|    learning_rate        | 0.001     |
|    loss                 | 2.7e+03   |
|    n_updates            | 2740      |
|    policy_gradient_loss | -1.74e-09 |
|    value_loss           | 1.56e+04  |
---------------------------------------
Eval num_timesteps=281500, episode_reward=2061.33 +/- 469.98
Episode length: 42.08 +/- 2.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 275      |
|    time_elapsed    | 1120     |
|    total_timesteps | 281600   |
---------------------------------
Eval num_timesteps=282000, episode_reward=2031.69 +/- 498.56
Episode length: 42.14 +/- 2.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.03e+03  |
| time/                   |           |
|    total_timesteps      | 282000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.82e-28 |
|    explained_variance   | 0.542     |
|    learning_rate        | 0.001     |
|    loss                 | 3.81e+03  |
|    n_updates            | 2750      |
|    policy_gradient_loss | 5.65e-10  |
|    value_loss           | 1.88e+04  |
---------------------------------------
Eval num_timesteps=282500, episode_reward=2134.59 +/- 393.37
Episode length: 42.24 +/- 1.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 276      |
|    time_elapsed    | 1124     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=2177.62 +/- 346.98
Episode length: 42.30 +/- 2.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 283000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.35e-32 |
|    explained_variance   | 0.492     |
|    learning_rate        | 0.001     |
|    loss                 | 7.39e+03  |
|    n_updates            | 2760      |
|    policy_gradient_loss | -5.03e-10 |
|    value_loss           | 2.23e+04  |
---------------------------------------
Eval num_timesteps=283500, episode_reward=2134.81 +/- 396.04
Episode length: 42.24 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 277      |
|    time_elapsed    | 1128     |
|    total_timesteps | 283648   |
---------------------------------
Eval num_timesteps=284000, episode_reward=2181.94 +/- 334.36
Episode length: 42.58 +/- 1.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.6      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 284000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.37e-35 |
|    explained_variance   | 0.571     |
|    learning_rate        | 0.001     |
|    loss                 | 2.64e+03  |
|    n_updates            | 2770      |
|    policy_gradient_loss | -5.41e-10 |
|    value_loss           | 1.74e+04  |
---------------------------------------
Eval num_timesteps=284500, episode_reward=2178.38 +/- 345.74
Episode length: 42.46 +/- 1.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 278      |
|    time_elapsed    | 1132     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=2160.33 +/- 361.99
Episode length: 42.06 +/- 1.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.16e+03  |
| time/                   |           |
|    total_timesteps      | 285000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.83e-25 |
|    explained_variance   | 0.559     |
|    learning_rate        | 0.001     |
|    loss                 | 9.46e+03  |
|    n_updates            | 2780      |
|    policy_gradient_loss | 1.51e-09  |
|    value_loss           | 2.53e+04  |
---------------------------------------
Eval num_timesteps=285500, episode_reward=2204.10 +/- 298.91
Episode length: 42.66 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 279      |
|    time_elapsed    | 1136     |
|    total_timesteps | 285696   |
---------------------------------
Eval num_timesteps=286000, episode_reward=2033.98 +/- 493.94
Episode length: 42.28 +/- 2.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.03e+03  |
| time/                   |           |
|    total_timesteps      | 286000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.88e-20 |
|    explained_variance   | 0.568     |
|    learning_rate        | 0.001     |
|    loss                 | 2.05e+03  |
|    n_updates            | 2790      |
|    policy_gradient_loss | 2.76e-09  |
|    value_loss           | 8.46e+03  |
---------------------------------------
Eval num_timesteps=286500, episode_reward=2158.69 +/- 364.71
Episode length: 42.14 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 280      |
|    time_elapsed    | 1140     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=2078.67 +/- 464.41
Episode length: 42.08 +/- 2.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.1      |
|    mean_reward          | 2.08e+03  |
| time/                   |           |
|    total_timesteps      | 287000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.63e-37 |
|    explained_variance   | 0.634     |
|    learning_rate        | 0.001     |
|    loss                 | 4.68e+03  |
|    n_updates            | 2800      |
|    policy_gradient_loss | 1.55e-09  |
|    value_loss           | 1.94e+04  |
---------------------------------------
Eval num_timesteps=287500, episode_reward=2133.56 +/- 400.11
Episode length: 41.98 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 281      |
|    time_elapsed    | 1144     |
|    total_timesteps | 287744   |
---------------------------------
Eval num_timesteps=288000, episode_reward=2180.72 +/- 340.09
Episode length: 42.30 +/- 1.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 288000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.91e-29 |
|    explained_variance   | 0.692     |
|    learning_rate        | 0.001     |
|    loss                 | 3.94e+03  |
|    n_updates            | 2810      |
|    policy_gradient_loss | 1.5e-09   |
|    value_loss           | 1.68e+04  |
---------------------------------------
Eval num_timesteps=288500, episode_reward=2105.10 +/- 435.57
Episode length: 42.16 +/- 2.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 282      |
|    time_elapsed    | 1148     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=2110.59 +/- 422.09
Episode length: 42.52 +/- 1.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.11e+03  |
| time/                   |           |
|    total_timesteps      | 289000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.18e-28 |
|    explained_variance   | 0.471     |
|    learning_rate        | 0.001     |
|    loss                 | 6.24e+03  |
|    n_updates            | 2820      |
|    policy_gradient_loss | 0         |
|    value_loss           | 2.96e+04  |
---------------------------------------
Eval num_timesteps=289500, episode_reward=2075.49 +/- 466.79
Episode length: 42.34 +/- 2.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 283      |
|    time_elapsed    | 1152     |
|    total_timesteps | 289792   |
---------------------------------
Eval num_timesteps=290000, episode_reward=2129.30 +/- 413.18
Episode length: 42.38 +/- 2.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.4      |
|    mean_reward          | 2.13e+03  |
| time/                   |           |
|    total_timesteps      | 290000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.73e-27 |
|    explained_variance   | 0.423     |
|    learning_rate        | 0.001     |
|    loss                 | 3.36e+03  |
|    n_updates            | 2830      |
|    policy_gradient_loss | -2.3e-09  |
|    value_loss           | 2.03e+04  |
---------------------------------------
Eval num_timesteps=290500, episode_reward=2051.05 +/- 489.67
Episode length: 42.04 +/- 2.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 284      |
|    time_elapsed    | 1156     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=2135.44 +/- 388.62
Episode length: 42.28 +/- 1.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | 2.14e+03  |
| time/                   |           |
|    total_timesteps      | 291000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.44e-27 |
|    explained_variance   | 0.52      |
|    learning_rate        | 0.001     |
|    loss                 | 6.19e+03  |
|    n_updates            | 2840      |
|    policy_gradient_loss | 1.44e-09  |
|    value_loss           | 2.76e+04  |
---------------------------------------
Eval num_timesteps=291500, episode_reward=2030.41 +/- 501.56
Episode length: 42.12 +/- 2.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 285      |
|    time_elapsed    | 1160     |
|    total_timesteps | 291840   |
---------------------------------
Eval num_timesteps=292000, episode_reward=2039.67 +/- 514.55
Episode length: 41.36 +/- 3.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.4      |
|    mean_reward          | 2.04e+03  |
| time/                   |           |
|    total_timesteps      | 292000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.22e-27 |
|    explained_variance   | 0.354     |
|    learning_rate        | 0.001     |
|    loss                 | 7.39e+03  |
|    n_updates            | 2850      |
|    policy_gradient_loss | -1.51e-09 |
|    value_loss           | 3.17e+04  |
---------------------------------------
Eval num_timesteps=292500, episode_reward=2128.53 +/- 414.41
Episode length: 41.88 +/- 2.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 286      |
|    time_elapsed    | 1164     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=2123.41 +/- 428.22
Episode length: 42.18 +/- 2.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.12e+03  |
| time/                   |           |
|    total_timesteps      | 293000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.44e-31 |
|    explained_variance   | 0.511     |
|    learning_rate        | 0.001     |
|    loss                 | 8.29e+03  |
|    n_updates            | 2860      |
|    policy_gradient_loss | 1.11e-10  |
|    value_loss           | 3.33e+04  |
---------------------------------------
Eval num_timesteps=293500, episode_reward=2037.06 +/- 488.31
Episode length: 42.20 +/- 2.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 1.96e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 287      |
|    time_elapsed    | 1168     |
|    total_timesteps | 293888   |
---------------------------------
Eval num_timesteps=294000, episode_reward=1889.05 +/- 601.87
Episode length: 40.80 +/- 3.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 40.8      |
|    mean_reward          | 1.89e+03  |
| time/                   |           |
|    total_timesteps      | 294000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.31e-29 |
|    explained_variance   | 0.43      |
|    learning_rate        | 0.001     |
|    loss                 | 3.59e+03  |
|    n_updates            | 2870      |
|    policy_gradient_loss | -9.63e-10 |
|    value_loss           | 3.23e+04  |
---------------------------------------
Eval num_timesteps=294500, episode_reward=2130.67 +/- 403.59
Episode length: 41.98 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 288      |
|    time_elapsed    | 1172     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=2057.99 +/- 470.42
Episode length: 42.54 +/- 2.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.5      |
|    mean_reward          | 2.06e+03  |
| time/                   |           |
|    total_timesteps      | 295000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.51e-34 |
|    explained_variance   | 0.448     |
|    learning_rate        | 0.001     |
|    loss                 | 4.14e+03  |
|    n_updates            | 2880      |
|    policy_gradient_loss | 2.68e-10  |
|    value_loss           | 2.39e+04  |
---------------------------------------
Eval num_timesteps=295500, episode_reward=2160.12 +/- 361.29
Episode length: 42.22 +/- 1.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 289      |
|    time_elapsed    | 1176     |
|    total_timesteps | 295936   |
---------------------------------
Eval num_timesteps=296000, episode_reward=1983.16 +/- 530.48
Episode length: 41.68 +/- 2.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 1.98e+03  |
| time/                   |           |
|    total_timesteps      | 296000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.66e-26 |
|    explained_variance   | 0.535     |
|    learning_rate        | 0.001     |
|    loss                 | 2.96e+03  |
|    n_updates            | 2890      |
|    policy_gradient_loss | 1.28e-09  |
|    value_loss           | 1.43e+04  |
---------------------------------------
Eval num_timesteps=296500, episode_reward=2038.82 +/- 483.06
Episode length: 41.76 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 290      |
|    time_elapsed    | 1180     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=2180.37 +/- 331.02
Episode length: 42.22 +/- 1.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 2.18e+03  |
| time/                   |           |
|    total_timesteps      | 297000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.55e-33 |
|    explained_variance   | 0.544     |
|    learning_rate        | 0.001     |
|    loss                 | 2.12e+03  |
|    n_updates            | 2900      |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 2.13e+04  |
---------------------------------------
Eval num_timesteps=297500, episode_reward=2059.08 +/- 472.93
Episode length: 42.10 +/- 1.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    fps             | 251      |
|    iterations      | 291      |
|    time_elapsed    | 1184     |
|    total_timesteps | 297984   |
---------------------------------
Eval num_timesteps=298000, episode_reward=2003.30 +/- 519.85
Episode length: 41.74 +/- 2.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.7      |
|    mean_reward          | 2e+03     |
| time/                   |           |
|    total_timesteps      | 298000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.09e-15 |
|    explained_variance   | 0.575     |
|    learning_rate        | 0.001     |
|    loss                 | 5.05e+03  |
|    n_updates            | 2910      |
|    policy_gradient_loss | 5.82e-11  |
|    value_loss           | 1.96e+04  |
---------------------------------------
Eval num_timesteps=298500, episode_reward=1954.73 +/- 551.40
Episode length: 41.76 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=2073.47 +/- 476.19
Episode length: 41.72 +/- 2.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 292      |
|    time_elapsed    | 1189     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=2101.84 +/- 443.95
Episode length: 42.04 +/- 1.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42        |
|    mean_reward          | 2.1e+03   |
| time/                   |           |
|    total_timesteps      | 299500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.62e-34 |
|    explained_variance   | 0.539     |
|    learning_rate        | 0.001     |
|    loss                 | 4.16e+03  |
|    n_updates            | 2920      |
|    policy_gradient_loss | 9.92e-10  |
|    value_loss           | 1.88e+04  |
---------------------------------------
Eval num_timesteps=300000, episode_reward=2153.08 +/- 375.78
Episode length: 42.48 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 293      |
|    time_elapsed    | 1193     |
|    total_timesteps | 300032   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-7/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 1024, 'batch_size': 64, 'learning_rate': 0.001, 'gamma': 0.9635, 'gae_lambda': 0.879}
Training steps: 300000
Frame skip: 4
