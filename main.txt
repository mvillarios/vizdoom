2024-12-03 22:49:57.467661: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-12-03 22:50:02.550175: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-03 22:50:16.988518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/mnt/c/Proyecto/.venv/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-119.18 +/- 105.98
Episode length: 138.66 +/- 110.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-129.38 +/- 76.53
Episode length: 118.52 +/- 87.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-139.06 +/- 94.65
Episode length: 155.12 +/- 108.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-128.29 +/- 56.39
Episode length: 129.44 +/- 88.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 129      |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | -52.7    |
| time/              |          |
|    fps             | 26       |
|    iterations      | 1        |
|    time_elapsed    | 77       |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=-281.79 +/- 23.12
Episode length: 49.56 +/- 15.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.6         |
|    mean_reward          | -282         |
| time/                   |              |
|    total_timesteps      | 2500         |
| train/                  |              |
|    approx_kl            | 0.0044983574 |
|    clip_fraction        | 0.0024       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | 0.000187     |
|    learning_rate        | 0.001        |
|    loss                 | 528          |
|    n_updates            | 1            |
|    policy_gradient_loss | 0.00123      |
|    value_loss           | 989          |
------------------------------------------
Eval num_timesteps=3000, episode_reward=-281.31 +/- 30.29
Episode length: 48.98 +/- 15.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-275.79 +/- 27.31
Episode length: 47.00 +/- 13.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-283.23 +/- 28.14
Episode length: 46.94 +/- 19.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 5.93     |
| time/              |          |
|    fps             | 36       |
|    iterations      | 2        |
|    time_elapsed    | 113      |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=-278.90 +/- 27.48
Episode length: 49.30 +/- 17.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | -279        |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.003523767 |
|    clip_fraction        | 0.00463     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.000518    |
|    learning_rate        | 0.001       |
|    loss                 | 28.5        |
|    n_updates            | 2           |
|    policy_gradient_loss | 0.00361     |
|    value_loss           | 576         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-271.22 +/- 33.99
Episode length: 46.00 +/- 14.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-274.59 +/- 26.44
Episode length: 45.88 +/- 14.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-282.03 +/- 24.41
Episode length: 48.42 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | 5.56     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 3        |
|    time_elapsed    | 145      |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=-271.71 +/- 33.21
Episode length: 51.34 +/- 15.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.3         |
|    mean_reward          | -272         |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.0041181543 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.00205      |
|    learning_rate        | 0.001        |
|    loss                 | 61.1         |
|    n_updates            | 3            |
|    policy_gradient_loss | -9.03e-05    |
|    value_loss           | 588          |
------------------------------------------
Eval num_timesteps=7000, episode_reward=-276.98 +/- 29.61
Episode length: 48.60 +/- 13.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-284.91 +/- 20.90
Episode length: 53.66 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-282.51 +/- 25.82
Episode length: 51.28 +/- 19.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | -43.1    |
| time/              |          |
|    fps             | 45       |
|    iterations      | 4        |
|    time_elapsed    | 178      |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=8500, episode_reward=-269.54 +/- 29.20
Episode length: 47.98 +/- 15.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48          |
|    mean_reward          | -270        |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.009608495 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.00399     |
|    learning_rate        | 0.001       |
|    loss                 | 363         |
|    n_updates            | 4           |
|    policy_gradient_loss | 0.0129      |
|    value_loss           | 662         |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=-275.55 +/- 30.19
Episode length: 49.56 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-283.71 +/- 25.23
Episode length: 51.60 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-275.30 +/- 25.31
Episode length: 46.80 +/- 14.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | -58.6    |
| time/              |          |
|    fps             | 48       |
|    iterations      | 5        |
|    time_elapsed    | 211      |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=10500, episode_reward=-278.43 +/- 28.63
Episode length: 49.14 +/- 16.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.1        |
|    mean_reward          | -278        |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.020208124 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.00606     |
|    learning_rate        | 0.001       |
|    loss                 | 637         |
|    n_updates            | 5           |
|    policy_gradient_loss | 0.025       |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=-270.74 +/- 37.36
Episode length: 48.12 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-276.98 +/- 23.90
Episode length: 48.42 +/- 11.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-277.22 +/- 30.67
Episode length: 49.22 +/- 16.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | -85.5    |
| time/              |          |
|    fps             | 50       |
|    iterations      | 6        |
|    time_elapsed    | 244      |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=12500, episode_reward=-284.67 +/- 20.99
Episode length: 47.92 +/- 12.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.9        |
|    mean_reward          | -285        |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.016585493 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.793      |
|    explained_variance   | 0.00406     |
|    learning_rate        | 0.001       |
|    loss                 | 473         |
|    n_updates            | 6           |
|    policy_gradient_loss | 0.00867     |
|    value_loss           | 835         |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=-274.10 +/- 26.60
Episode length: 51.52 +/- 20.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-281.07 +/- 28.25
Episode length: 51.36 +/- 17.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-272.67 +/- 33.29
Episode length: 50.14 +/- 16.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    fps             | 51       |
|    iterations      | 7        |
|    time_elapsed    | 279      |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=14500, episode_reward=-277.94 +/- 25.66
Episode length: 49.98 +/- 18.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -278        |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.026229996 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.556      |
|    explained_variance   | 0.00426     |
|    learning_rate        | 0.001       |
|    loss                 | 257         |
|    n_updates            | 7           |
|    policy_gradient_loss | 0.00242     |
|    value_loss           | 766         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-276.27 +/- 22.99
Episode length: 46.04 +/- 13.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-272.91 +/- 28.50
Episode length: 52.56 +/- 18.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-278.67 +/- 31.31
Episode length: 46.64 +/- 19.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.3     |
|    ep_rew_mean     | -205     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 8        |
|    time_elapsed    | 313      |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=16500, episode_reward=-272.90 +/- 35.58
Episode length: 49.68 +/- 18.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.7        |
|    mean_reward          | -273        |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.021407321 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.315      |
|    explained_variance   | 0.00988     |
|    learning_rate        | 0.001       |
|    loss                 | 434         |
|    n_updates            | 8           |
|    policy_gradient_loss | 0.00566     |
|    value_loss           | 965         |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=-278.18 +/- 25.55
Episode length: 48.86 +/- 15.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-279.38 +/- 29.87
Episode length: 49.64 +/- 15.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-280.11 +/- 26.78
Episode length: 51.92 +/- 15.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 56.8     |
|    ep_rew_mean     | -257     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 9        |
|    time_elapsed    | 349      |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=18500, episode_reward=-279.62 +/- 29.17
Episode length: 51.14 +/- 14.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -280        |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.014538533 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.199      |
|    explained_variance   | 0.0127      |
|    learning_rate        | 0.001       |
|    loss                 | 210         |
|    n_updates            | 9           |
|    policy_gradient_loss | 0.00726     |
|    value_loss           | 734         |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=-274.34 +/- 36.97
Episode length: 49.34 +/- 15.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-276.76 +/- 30.74
Episode length: 51.56 +/- 18.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-270.74 +/- 30.20
Episode length: 49.60 +/- 15.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -273     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 10       |
|    time_elapsed    | 382      |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=20500, episode_reward=-273.15 +/- 31.78
Episode length: 44.52 +/- 13.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 44.5       |
|    mean_reward          | -273       |
| time/                   |            |
|    total_timesteps      | 20500      |
| train/                  |            |
|    approx_kl            | 0.01832148 |
|    clip_fraction        | 0.0312     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.479     |
|    explained_variance   | 0.025      |
|    learning_rate        | 0.001      |
|    loss                 | 656        |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.0168     |
|    value_loss           | 1.23e+03   |
----------------------------------------
Eval num_timesteps=21000, episode_reward=-274.35 +/- 32.12
Episode length: 53.16 +/- 19.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-274.33 +/- 31.85
Episode length: 47.14 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-282.02 +/- 24.17
Episode length: 53.42 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-278.90 +/- 25.52
Episode length: 52.28 +/- 16.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 11       |
|    time_elapsed    | 422      |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=23000, episode_reward=-280.11 +/- 23.93
Episode length: 52.28 +/- 15.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.3        |
|    mean_reward          | -280        |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.041864708 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.00458     |
|    learning_rate        | 0.001       |
|    loss                 | 805         |
|    n_updates            | 11          |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 1.47e+03    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=-285.62 +/- 22.66
Episode length: 52.00 +/- 14.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-276.99 +/- 25.76
Episode length: 50.60 +/- 17.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-274.83 +/- 27.95
Episode length: 46.58 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 12       |
|    time_elapsed    | 456      |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=25000, episode_reward=-275.07 +/- 30.05
Episode length: 49.42 +/- 15.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.4        |
|    mean_reward          | -275        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.030439073 |
|    clip_fraction        | 0.5         |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.695      |
|    explained_variance   | -0.00561    |
|    learning_rate        | 0.001       |
|    loss                 | 542         |
|    n_updates            | 12          |
|    policy_gradient_loss | 0.0191      |
|    value_loss           | 1.07e+03    |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=-279.37 +/- 25.85
Episode length: 47.14 +/- 14.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-277.47 +/- 27.28
Episode length: 49.82 +/- 15.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-277.94 +/- 29.62
Episode length: 46.06 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -267     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 13       |
|    time_elapsed    | 488      |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=27000, episode_reward=-277.94 +/- 26.10
Episode length: 46.40 +/- 15.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.4        |
|    mean_reward          | -278        |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.012596765 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.653      |
|    explained_variance   | -0.00422    |
|    learning_rate        | 0.001       |
|    loss                 | 845         |
|    n_updates            | 13          |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 1.49e+03    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=-280.59 +/- 27.67
Episode length: 45.86 +/- 18.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-272.66 +/- 35.48
Episode length: 48.78 +/- 14.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-274.60 +/- 28.64
Episode length: 47.44 +/- 12.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54       |
|    ep_rew_mean     | -263     |
| time/              |          |
|    fps             | 55       |
|    iterations      | 14       |
|    time_elapsed    | 520      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=29000, episode_reward=-274.35 +/- 28.11
Episode length: 44.66 +/- 14.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.7        |
|    mean_reward          | -274        |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.020204892 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.00909     |
|    learning_rate        | 0.001       |
|    loss                 | 452         |
|    n_updates            | 14          |
|    policy_gradient_loss | 0.00944     |
|    value_loss           | 975         |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=-276.99 +/- 25.07
Episode length: 50.32 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-277.70 +/- 26.96
Episode length: 50.48 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-276.99 +/- 25.53
Episode length: 48.66 +/- 14.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.3     |
|    ep_rew_mean     | -259     |
| time/              |          |
|    fps             | 55       |
|    iterations      | 15       |
|    time_elapsed    | 552      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=31000, episode_reward=-273.38 +/- 28.46
Episode length: 47.72 +/- 16.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.7        |
|    mean_reward          | -273        |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.009552542 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.426      |
|    explained_variance   | 0.00194     |
|    learning_rate        | 0.001       |
|    loss                 | 376         |
|    n_updates            | 15          |
|    policy_gradient_loss | 0.00235     |
|    value_loss           | 626         |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=-276.51 +/- 28.39
Episode length: 49.52 +/- 14.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-278.67 +/- 27.49
Episode length: 52.98 +/- 19.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-282.26 +/- 24.13
Episode length: 51.66 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.7     |
|    ep_rew_mean     | -260     |
| time/              |          |
|    fps             | 55       |
|    iterations      | 16       |
|    time_elapsed    | 586      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=33000, episode_reward=-273.62 +/- 28.71
Episode length: 51.06 +/- 16.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -274        |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.011021578 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.00492     |
|    learning_rate        | 0.001       |
|    loss                 | 364         |
|    n_updates            | 16          |
|    policy_gradient_loss | 0.00376     |
|    value_loss           | 910         |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=-281.30 +/- 22.94
Episode length: 45.92 +/- 14.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-266.19 +/- 37.04
Episode length: 48.58 +/- 21.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-273.61 +/- 31.03
Episode length: 50.76 +/- 18.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.6     |
|    ep_rew_mean     | -260     |
| time/              |          |
|    fps             | 56       |
|    iterations      | 17       |
|    time_elapsed    | 619      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=35000, episode_reward=-276.27 +/- 26.05
Episode length: 50.78 +/- 15.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.8        |
|    mean_reward          | -276        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012871853 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.139      |
|    explained_variance   | 0.0088      |
|    learning_rate        | 0.001       |
|    loss                 | 287         |
|    n_updates            | 17          |
|    policy_gradient_loss | -0.000948   |
|    value_loss           | 755         |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=-278.19 +/- 29.32
Episode length: 50.70 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-278.92 +/- 25.18
Episode length: 56.18 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.2     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-277.47 +/- 25.32
Episode length: 53.40 +/- 19.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.8     |
|    ep_rew_mean     | -267     |
| time/              |          |
|    fps             | 56       |
|    iterations      | 18       |
|    time_elapsed    | 654      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=37000, episode_reward=-276.26 +/- 28.07
Episode length: 48.68 +/- 16.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -276        |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.013296479 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0362     |
|    explained_variance   | 0.0133      |
|    learning_rate        | 0.001       |
|    loss                 | 393         |
|    n_updates            | 18          |
|    policy_gradient_loss | 0.00356     |
|    value_loss           | 674         |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=-280.70 +/- 37.34
Episode length: 47.80 +/- 16.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-278.19 +/- 33.01
Episode length: 51.96 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-283.70 +/- 25.23
Episode length: 54.52 +/- 17.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 56       |
|    iterations      | 19       |
|    time_elapsed    | 687      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.16
Eval num_timesteps=39000, episode_reward=-282.03 +/- 26.00
Episode length: 51.36 +/- 19.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.4         |
|    mean_reward          | -282         |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0065083406 |
|    clip_fraction        | 0.000651     |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00101     |
|    explained_variance   | 0.0169       |
|    learning_rate        | 0.001        |
|    loss                 | 367          |
|    n_updates            | 19           |
|    policy_gradient_loss | 0.000379     |
|    value_loss           | 694          |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-277.46 +/- 29.02
Episode length: 45.30 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-277.47 +/- 25.54
Episode length: 52.68 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-284.19 +/- 25.46
Episode length: 50.30 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 56       |
|    iterations      | 20       |
|    time_elapsed    | 721      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=-279.38 +/- 30.16
Episode length: 45.60 +/- 14.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 45.6          |
|    mean_reward          | -279          |
| time/                   |               |
|    total_timesteps      | 41000         |
| train/                  |               |
|    approx_kl            | 1.8859282e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000195     |
|    explained_variance   | 0.0206        |
|    learning_rate        | 0.001         |
|    loss                 | 384           |
|    n_updates            | 29            |
|    policy_gradient_loss | 4.89e-07      |
|    value_loss           | 807           |
-------------------------------------------
Eval num_timesteps=41500, episode_reward=-274.83 +/- 30.88
Episode length: 49.68 +/- 12.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-279.39 +/- 32.46
Episode length: 52.32 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-282.26 +/- 26.83
Episode length: 48.14 +/- 18.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-276.26 +/- 26.38
Episode length: 49.48 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 56       |
|    iterations      | 21       |
|    time_elapsed    | 764      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=-269.07 +/- 34.07
Episode length: 48.52 +/- 16.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.5          |
|    mean_reward          | -269          |
| time/                   |               |
|    total_timesteps      | 43500         |
| train/                  |               |
|    approx_kl            | 1.6210834e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00686      |
|    explained_variance   | 0.0175        |
|    learning_rate        | 0.001         |
|    loss                 | 267           |
|    n_updates            | 39            |
|    policy_gradient_loss | 3.19e-05      |
|    value_loss           | 562           |
-------------------------------------------
Eval num_timesteps=44000, episode_reward=-275.78 +/- 28.25
Episode length: 48.58 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-273.15 +/- 29.80
Episode length: 50.56 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-282.02 +/- 29.91
Episode length: 47.44 +/- 17.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 55       |
|    iterations      | 22       |
|    time_elapsed    | 813      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=-284.18 +/- 21.80
Episode length: 47.84 +/- 13.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.8      |
|    mean_reward          | -284      |
| time/                   |           |
|    total_timesteps      | 45500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.17e-06 |
|    explained_variance   | 0.00788   |
|    learning_rate        | 0.001     |
|    loss                 | 349       |
|    n_updates            | 49        |
|    policy_gradient_loss | -1.64e-06 |
|    value_loss           | 929       |
---------------------------------------
Eval num_timesteps=46000, episode_reward=-284.43 +/- 23.77
Episode length: 52.98 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-281.07 +/- 28.87
Episode length: 48.66 +/- 17.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-281.30 +/- 23.93
Episode length: 49.46 +/- 16.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 23       |
|    time_elapsed    | 871      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=-271.23 +/- 30.11
Episode length: 53.14 +/- 18.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.1          |
|    mean_reward          | -271          |
| time/                   |               |
|    total_timesteps      | 47500         |
| train/                  |               |
|    approx_kl            | 1.9033905e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00157      |
|    explained_variance   | 0.0349        |
|    learning_rate        | 0.001         |
|    loss                 | 478           |
|    n_updates            | 59            |
|    policy_gradient_loss | 6.08e-06      |
|    value_loss           | 1e+03         |
-------------------------------------------
Eval num_timesteps=48000, episode_reward=-279.38 +/- 27.04
Episode length: 49.36 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-278.18 +/- 26.98
Episode length: 48.30 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-280.83 +/- 26.59
Episode length: 46.34 +/- 13.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 24       |
|    time_elapsed    | 907      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=-276.60 +/- 36.93
Episode length: 49.70 +/- 17.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 49500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.15e-06 |
|    explained_variance   | 0.00775   |
|    learning_rate        | 0.001     |
|    loss                 | 551       |
|    n_updates            | 69        |
|    policy_gradient_loss | -2.24e-06 |
|    value_loss           | 1.13e+03  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=-276.74 +/- 32.82
Episode length: 49.44 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-279.13 +/- 27.58
Episode length: 51.24 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-281.07 +/- 28.95
Episode length: 51.18 +/- 16.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 25       |
|    time_elapsed    | 945      |
|    total_timesteps | 51200    |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=51500, episode_reward=-283.70 +/- 26.46
Episode length: 54.14 +/- 17.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.1        |
|    mean_reward          | -284        |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.010085321 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0417     |
|    explained_variance   | 0.029       |
|    learning_rate        | 0.001       |
|    loss                 | 936         |
|    n_updates            | 74          |
|    policy_gradient_loss | -3.56e-05   |
|    value_loss           | 2.01e+03    |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=-280.83 +/- 27.67
Episode length: 48.12 +/- 19.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-276.01 +/- 28.68
Episode length: 47.36 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-273.38 +/- 30.52
Episode length: 47.26 +/- 14.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.4     |
|    ep_rew_mean     | -264     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 26       |
|    time_elapsed    | 980      |
|    total_timesteps | 53248    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=53500, episode_reward=-277.70 +/- 29.53
Episode length: 50.42 +/- 17.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.4        |
|    mean_reward          | -278        |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.018390246 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.00323     |
|    learning_rate        | 0.001       |
|    loss                 | 463         |
|    n_updates            | 75          |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 818         |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=-273.38 +/- 34.08
Episode length: 49.62 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-276.26 +/- 26.16
Episode length: 50.26 +/- 16.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-275.29 +/- 30.85
Episode length: 48.94 +/- 17.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.4     |
|    ep_rew_mean     | -257     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 27       |
|    time_elapsed    | 1018     |
|    total_timesteps | 55296    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=55500, episode_reward=-277.47 +/- 31.13
Episode length: 49.34 +/- 14.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | -277        |
| time/                   |             |
|    total_timesteps      | 55500       |
| train/                  |             |
|    approx_kl            | 0.032925587 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.27       |
|    explained_variance   | 0.00415     |
|    learning_rate        | 0.001       |
|    loss                 | 1.01e+03    |
|    n_updates            | 76          |
|    policy_gradient_loss | 0.0272      |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=-276.26 +/- 31.47
Episode length: 55.60 +/- 17.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-274.81 +/- 30.13
Episode length: 47.26 +/- 20.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-271.70 +/- 30.50
Episode length: 47.76 +/- 17.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.5     |
|    ep_rew_mean     | -256     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 28       |
|    time_elapsed    | 1053     |
|    total_timesteps | 57344    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.38
Eval num_timesteps=57500, episode_reward=-277.95 +/- 28.63
Episode length: 52.64 +/- 17.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.6       |
|    mean_reward          | -278       |
| time/                   |            |
|    total_timesteps      | 57500      |
| train/                  |            |
|    approx_kl            | 0.12682407 |
|    clip_fraction        | 0.0208     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0468    |
|    explained_variance   | 0.00718    |
|    learning_rate        | 0.001      |
|    loss                 | 1.14e+03   |
|    n_updates            | 77         |
|    policy_gradient_loss | 0.0275     |
|    value_loss           | 1.85e+03   |
----------------------------------------
Eval num_timesteps=58000, episode_reward=-280.82 +/- 27.15
Episode length: 48.58 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-270.99 +/- 36.54
Episode length: 44.88 +/- 13.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.9     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-280.82 +/- 26.39
Episode length: 53.42 +/- 22.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.7     |
|    ep_rew_mean     | -265     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 29       |
|    time_elapsed    | 1088     |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=-280.83 +/- 26.17
Episode length: 52.46 +/- 21.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.5      |
|    mean_reward          | -281      |
| time/                   |           |
|    total_timesteps      | 59500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.94e-07 |
|    explained_variance   | 0.0114    |
|    learning_rate        | 0.001     |
|    loss                 | 162       |
|    n_updates            | 87        |
|    policy_gradient_loss | -1.32e-07 |
|    value_loss           | 605       |
---------------------------------------
Eval num_timesteps=60000, episode_reward=-281.31 +/- 20.13
Episode length: 52.20 +/- 18.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-273.39 +/- 31.27
Episode length: 50.38 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-277.70 +/- 27.60
Episode length: 44.74 +/- 12.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.3     |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 30       |
|    time_elapsed    | 1126     |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=-277.46 +/- 26.00
Episode length: 50.12 +/- 16.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.79e-40 |
|    explained_variance   | -0.0421   |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+06  |
|    n_updates            | 97        |
|    policy_gradient_loss | -1.57e-08 |
|    value_loss           | 2.76e+06  |
---------------------------------------
Eval num_timesteps=62000, episode_reward=-270.75 +/- 38.20
Episode length: 49.66 +/- 18.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-278.83 +/- 33.00
Episode length: 52.48 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-278.66 +/- 26.96
Episode length: 52.22 +/- 19.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 31       |
|    time_elapsed    | 1164     |
|    total_timesteps | 63488    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=63500, episode_reward=-283.71 +/- 23.09
Episode length: 55.70 +/- 18.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 55.7         |
|    mean_reward          | -284         |
| time/                   |              |
|    total_timesteps      | 63500        |
| train/                  |              |
|    approx_kl            | 0.0034212058 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0429      |
|    explained_variance   | -0.0425      |
|    learning_rate        | 0.001        |
|    loss                 | 8.51e+04     |
|    n_updates            | 98           |
|    policy_gradient_loss | 0.000748     |
|    value_loss           | 1.67e+05     |
------------------------------------------
Eval num_timesteps=64000, episode_reward=-278.42 +/- 28.83
Episode length: 46.72 +/- 13.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-272.19 +/- 28.15
Episode length: 47.06 +/- 17.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-281.55 +/- 25.18
Episode length: 47.66 +/- 13.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-270.74 +/- 26.11
Episode length: 48.42 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 63       |
|    ep_rew_mean     | -259     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 32       |
|    time_elapsed    | 1205     |
|    total_timesteps | 65536    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.46
Eval num_timesteps=66000, episode_reward=-285.51 +/- 46.07
Episode length: 49.12 +/- 14.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.1       |
|    mean_reward          | -286       |
| time/                   |            |
|    total_timesteps      | 66000      |
| train/                  |            |
|    approx_kl            | 0.22899523 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.0202     |
|    learning_rate        | 0.001      |
|    loss                 | 146        |
|    n_updates            | 99         |
|    policy_gradient_loss | 0.131      |
|    value_loss           | 460        |
----------------------------------------
Eval num_timesteps=66500, episode_reward=-271.35 +/- 60.18
Episode length: 48.62 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-279.08 +/- 56.37
Episode length: 45.70 +/- 19.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-274.57 +/- 55.81
Episode length: 49.16 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.4     |
|    ep_rew_mean     | -239     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 33       |
|    time_elapsed    | 1239     |
|    total_timesteps | 67584    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=68000, episode_reward=-287.12 +/- 43.66
Episode length: 50.76 +/- 15.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.8      |
|    mean_reward          | -287      |
| time/                   |           |
|    total_timesteps      | 68000     |
| train/                  |           |
|    approx_kl            | 0.0209744 |
|    clip_fraction        | 0.117     |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11     |
|    explained_variance   | 0.000594  |
|    learning_rate        | 0.001     |
|    loss                 | 75.4      |
|    n_updates            | 100       |
|    policy_gradient_loss | 0.00105   |
|    value_loss           | 115       |
---------------------------------------
Eval num_timesteps=68500, episode_reward=-282.97 +/- 50.74
Episode length: 48.36 +/- 15.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-288.72 +/- 34.88
Episode length: 47.24 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -289     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-288.30 +/- 36.17
Episode length: 50.24 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.4     |
|    ep_rew_mean     | -215     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 34       |
|    time_elapsed    | 1285     |
|    total_timesteps | 69632    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=70000, episode_reward=-283.11 +/- 46.84
Episode length: 49.30 +/- 15.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | -283        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.010903145 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.0036      |
|    learning_rate        | 0.001       |
|    loss                 | 195         |
|    n_updates            | 101         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 1.21e+03    |
-----------------------------------------
Eval num_timesteps=70500, episode_reward=-278.63 +/- 54.01
Episode length: 53.52 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-270.77 +/- 70.87
Episode length: 53.22 +/- 19.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-270.85 +/- 72.48
Episode length: 50.64 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 35       |
|    time_elapsed    | 1329     |
|    total_timesteps | 71680    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.23
Eval num_timesteps=72000, episode_reward=-269.20 +/- 45.41
Episode length: 45.54 +/- 15.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 45.5       |
|    mean_reward          | -269       |
| time/                   |            |
|    total_timesteps      | 72000      |
| train/                  |            |
|    approx_kl            | 0.07925107 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.869     |
|    explained_variance   | 0.00219    |
|    learning_rate        | 0.001      |
|    loss                 | 191        |
|    n_updates            | 102        |
|    policy_gradient_loss | 0.0628     |
|    value_loss           | 761        |
----------------------------------------
Eval num_timesteps=72500, episode_reward=-293.62 +/- 36.00
Episode length: 49.46 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-283.20 +/- 47.94
Episode length: 50.42 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-268.22 +/- 73.25
Episode length: 46.50 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 36       |
|    time_elapsed    | 1368     |
|    total_timesteps | 73728    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.22
Eval num_timesteps=74000, episode_reward=-279.50 +/- 50.69
Episode length: 50.94 +/- 17.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.9      |
|    mean_reward          | -280      |
| time/                   |           |
|    total_timesteps      | 74000     |
| train/                  |           |
|    approx_kl            | 0.1100076 |
|    clip_fraction        | 0.102     |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.384    |
|    explained_variance   | 0.00351   |
|    learning_rate        | 0.001     |
|    loss                 | 949       |
|    n_updates            | 103       |
|    policy_gradient_loss | 0.0794    |
|    value_loss           | 1.63e+03  |
---------------------------------------
Eval num_timesteps=74500, episode_reward=-279.98 +/- 48.71
Episode length: 48.46 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-268.61 +/- 63.75
Episode length: 47.92 +/- 13.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-267.42 +/- 71.05
Episode length: 50.22 +/- 15.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 37       |
|    time_elapsed    | 1405     |
|    total_timesteps | 75776    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=76000, episode_reward=-274.74 +/- 57.70
Episode length: 46.60 +/- 16.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.6       |
|    mean_reward          | -275       |
| time/                   |            |
|    total_timesteps      | 76000      |
| train/                  |            |
|    approx_kl            | 0.02516764 |
|    clip_fraction        | 0.00781    |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0697    |
|    explained_variance   | 0.00307    |
|    learning_rate        | 0.001      |
|    loss                 | 753        |
|    n_updates            | 104        |
|    policy_gradient_loss | 0.00328    |
|    value_loss           | 1.38e+03   |
----------------------------------------
Eval num_timesteps=76500, episode_reward=-292.75 +/- 41.60
Episode length: 47.08 +/- 14.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-278.59 +/- 50.99
Episode length: 51.64 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-274.19 +/- 74.42
Episode length: 51.64 +/- 15.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 64       |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 38       |
|    time_elapsed    | 1442     |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-267.91 +/- 61.26
Episode length: 48.88 +/- 16.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.9          |
|    mean_reward          | -268          |
| time/                   |               |
|    total_timesteps      | 78000         |
| train/                  |               |
|    approx_kl            | 5.0175004e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -7.19e-06     |
|    explained_variance   | -0.00355      |
|    learning_rate        | 0.001         |
|    loss                 | 414           |
|    n_updates            | 114           |
|    policy_gradient_loss | 7.72e-06      |
|    value_loss           | 944           |
-------------------------------------------
Eval num_timesteps=78500, episode_reward=-268.42 +/- 68.62
Episode length: 48.98 +/- 18.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-260.98 +/- 86.23
Episode length: 50.50 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-274.31 +/- 63.71
Episode length: 50.78 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 58.4     |
|    ep_rew_mean     | -252     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 39       |
|    time_elapsed    | 1488     |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-278.94 +/- 56.39
Episode length: 44.28 +/- 12.52
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 44.3     |
|    mean_reward          | -279     |
| time/                   |          |
|    total_timesteps      | 80000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -1.25    |
|    learning_rate        | 0.001    |
|    loss                 | 1.2e+06  |
|    n_updates            | 124      |
|    policy_gradient_loss | 1.32e-08 |
|    value_loss           | 2.41e+06 |
--------------------------------------
Eval num_timesteps=80500, episode_reward=-290.61 +/- 27.85
Episode length: 50.18 +/- 15.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-278.55 +/- 64.37
Episode length: 51.34 +/- 17.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-268.59 +/- 49.31
Episode length: 51.92 +/- 19.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -285     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 40       |
|    time_elapsed    | 1528     |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=-294.63 +/- 27.04
Episode length: 49.88 +/- 15.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.9      |
|    mean_reward          | -295      |
| time/                   |           |
|    total_timesteps      | 82000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.09e-14 |
|    explained_variance   | -1.44     |
|    learning_rate        | 0.001     |
|    loss                 | 4.4e+04   |
|    n_updates            | 134       |
|    policy_gradient_loss | 4.07e-10  |
|    value_loss           | 7.93e+04  |
---------------------------------------
Eval num_timesteps=82500, episode_reward=-279.26 +/- 52.45
Episode length: 48.84 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-275.37 +/- 67.84
Episode length: 55.86 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.9     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-283.33 +/- 48.27
Episode length: 54.16 +/- 21.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.9     |
|    ep_rew_mean     | -285     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 41       |
|    time_elapsed    | 1568     |
|    total_timesteps | 83968    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=84000, episode_reward=-283.22 +/- 41.95
Episode length: 50.42 +/- 16.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.4         |
|    mean_reward          | -283         |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0132519705 |
|    clip_fraction        | 0.125        |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | -0.0397      |
|    learning_rate        | 0.001        |
|    loss                 | 204          |
|    n_updates            | 135          |
|    policy_gradient_loss | 0.00419      |
|    value_loss           | 620          |
------------------------------------------
Eval num_timesteps=84500, episode_reward=-283.42 +/- 61.04
Episode length: 49.62 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-276.27 +/- 59.85
Episode length: 49.58 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-291.98 +/- 36.00
Episode length: 50.44 +/- 16.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-254.80 +/- 65.15
Episode length: 49.14 +/- 21.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.9     |
|    ep_rew_mean     | -259     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 42       |
|    time_elapsed    | 1608     |
|    total_timesteps | 86016    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=86500, episode_reward=-126.07 +/- 92.55
Episode length: 57.48 +/- 20.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 57.5        |
|    mean_reward          | -126        |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.033378564 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | -0.00215    |
|    learning_rate        | 0.001       |
|    loss                 | 920         |
|    n_updates            | 136         |
|    policy_gradient_loss | 0.0266      |
|    value_loss           | 1.52e+03    |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=-122.17 +/- 73.52
Episode length: 59.40 +/- 22.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.4     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-134.59 +/- 87.08
Episode length: 59.28 +/- 20.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.3     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-123.26 +/- 74.20
Episode length: 60.18 +/- 24.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.2     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.4     |
|    ep_rew_mean     | -225     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 43       |
|    time_elapsed    | 1648     |
|    total_timesteps | 88064    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=88500, episode_reward=-273.41 +/- 65.34
Episode length: 45.90 +/- 16.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.9        |
|    mean_reward          | -273        |
| time/                   |             |
|    total_timesteps      | 88500       |
| train/                  |             |
|    approx_kl            | 0.009587377 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.00479     |
|    learning_rate        | 0.001       |
|    loss                 | 446         |
|    n_updates            | 137         |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 999         |
-----------------------------------------
Eval num_timesteps=89000, episode_reward=-269.21 +/- 64.06
Episode length: 49.02 +/- 16.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-287.03 +/- 47.94
Episode length: 51.56 +/- 20.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-287.86 +/- 52.23
Episode length: 50.58 +/- 18.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79       |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 44       |
|    time_elapsed    | 1684     |
|    total_timesteps | 90112    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.31
Eval num_timesteps=90500, episode_reward=-261.77 +/- 86.48
Episode length: 50.32 +/- 19.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.3       |
|    mean_reward          | -262       |
| time/                   |            |
|    total_timesteps      | 90500      |
| train/                  |            |
|    approx_kl            | 0.15545356 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.745     |
|    explained_variance   | 0.0141     |
|    learning_rate        | 0.001      |
|    loss                 | 1.04e+03   |
|    n_updates            | 138        |
|    policy_gradient_loss | 0.0665     |
|    value_loss           | 1.93e+03   |
----------------------------------------
Eval num_timesteps=91000, episode_reward=-275.29 +/- 53.42
Episode length: 46.42 +/- 13.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-269.58 +/- 56.24
Episode length: 44.96 +/- 14.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-279.23 +/- 49.16
Episode length: 48.42 +/- 15.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | -106     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 45       |
|    time_elapsed    | 1719     |
|    total_timesteps | 92160    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.49
Eval num_timesteps=92500, episode_reward=-256.05 +/- 85.61
Episode length: 47.84 +/- 18.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.8       |
|    mean_reward          | -256       |
| time/                   |            |
|    total_timesteps      | 92500      |
| train/                  |            |
|    approx_kl            | 0.24343295 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.282     |
|    explained_variance   | 0.0216     |
|    learning_rate        | 0.001      |
|    loss                 | 1.14e+03   |
|    n_updates            | 139        |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 2.01e+03   |
----------------------------------------
Eval num_timesteps=93000, episode_reward=-285.58 +/- 44.58
Episode length: 47.68 +/- 12.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-277.32 +/- 61.31
Episode length: 56.64 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.6     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-277.49 +/- 53.68
Episode length: 51.44 +/- 16.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 56.7     |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 46       |
|    time_elapsed    | 1756     |
|    total_timesteps | 94208    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 6.28
Eval num_timesteps=94500, episode_reward=-280.83 +/- 25.95
Episode length: 48.62 +/- 18.62
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 48.6     |
|    mean_reward          | -281     |
| time/                   |          |
|    total_timesteps      | 94500    |
| train/                  |          |
|    approx_kl            | 1.572345 |
|    clip_fraction        | 0.25     |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -0.12    |
|    explained_variance   | -0.0189  |
|    learning_rate        | 0.001    |
|    loss                 | 675      |
|    n_updates            | 140      |
|    policy_gradient_loss | 0.0577   |
|    value_loss           | 1.22e+03 |
--------------------------------------
Eval num_timesteps=95000, episode_reward=-277.46 +/- 27.61
Episode length: 51.22 +/- 21.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-276.51 +/- 30.63
Episode length: 49.10 +/- 13.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-282.74 +/- 27.29
Episode length: 53.36 +/- 18.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.6     |
|    ep_rew_mean     | -215     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 47       |
|    time_elapsed    | 1790     |
|    total_timesteps | 96256    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.22
Eval num_timesteps=96500, episode_reward=-276.28 +/- 25.37
Episode length: 55.28 +/- 17.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.3        |
|    mean_reward          | -276        |
| time/                   |             |
|    total_timesteps      | 96500       |
| train/                  |             |
|    approx_kl            | 0.027199382 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00152    |
|    explained_variance   | 0.0155      |
|    learning_rate        | 0.001       |
|    loss                 | 334         |
|    n_updates            | 141         |
|    policy_gradient_loss | -0.00175    |
|    value_loss           | 698         |
-----------------------------------------
Eval num_timesteps=97000, episode_reward=-272.18 +/- 29.65
Episode length: 48.52 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-276.27 +/- 28.17
Episode length: 53.46 +/- 20.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-280.83 +/- 30.62
Episode length: 48.46 +/- 19.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -272     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 48       |
|    time_elapsed    | 1825     |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=-280.84 +/- 32.01
Episode length: 55.74 +/- 17.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 55.7      |
|    mean_reward          | -281      |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.35e-07 |
|    explained_variance   | 0.0107    |
|    learning_rate        | 0.001     |
|    loss                 | 539       |
|    n_updates            | 151       |
|    policy_gradient_loss | -2.48e-07 |
|    value_loss           | 908       |
---------------------------------------
Eval num_timesteps=99000, episode_reward=-279.02 +/- 35.77
Episode length: 48.72 +/- 20.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-279.16 +/- 30.17
Episode length: 48.78 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-276.50 +/- 30.72
Episode length: 50.52 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 49       |
|    time_elapsed    | 1860     |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=-275.55 +/- 25.88
Episode length: 53.20 +/- 20.30
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 53.2           |
|    mean_reward          | -276           |
| time/                   |                |
|    total_timesteps      | 100500         |
| train/                  |                |
|    approx_kl            | -2.9685907e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -8.49e-06      |
|    explained_variance   | -0.00888       |
|    learning_rate        | 0.001          |
|    loss                 | 2.65e+03       |
|    n_updates            | 161            |
|    policy_gradient_loss | -3.09e-08      |
|    value_loss           | 5.21e+03       |
--------------------------------------------
Eval num_timesteps=101000, episode_reward=-272.90 +/- 29.58
Episode length: 49.48 +/- 16.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-276.74 +/- 29.99
Episode length: 52.50 +/- 18.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-280.12 +/- 28.95
Episode length: 56.10 +/- 17.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.1     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -282     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 50       |
|    time_elapsed    | 1899     |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=-282.99 +/- 25.50
Episode length: 49.46 +/- 15.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.5      |
|    mean_reward          | -283      |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.85e-08 |
|    explained_variance   | 0.00902   |
|    learning_rate        | 0.001     |
|    loss                 | 534       |
|    n_updates            | 171       |
|    policy_gradient_loss | -9.1e-08  |
|    value_loss           | 1.08e+03  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=-286.11 +/- 19.90
Episode length: 52.96 +/- 17.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-276.74 +/- 26.30
Episode length: 53.28 +/- 19.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-279.13 +/- 20.78
Episode length: 50.10 +/- 19.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.7     |
|    ep_rew_mean     | -282     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 51       |
|    time_elapsed    | 1935     |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-271.94 +/- 34.12
Episode length: 49.34 +/- 15.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.3          |
|    mean_reward          | -272          |
| time/                   |               |
|    total_timesteps      | 104500        |
| train/                  |               |
|    approx_kl            | -1.580338e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -2.43e-05     |
|    explained_variance   | 0.026         |
|    learning_rate        | 0.001         |
|    loss                 | 1.62e+03      |
|    n_updates            | 181           |
|    policy_gradient_loss | 2.46e-08      |
|    value_loss           | 3.23e+03      |
-------------------------------------------
Eval num_timesteps=105000, episode_reward=-276.50 +/- 29.69
Episode length: 47.92 +/- 16.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-280.35 +/- 22.02
Episode length: 52.72 +/- 15.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-282.99 +/- 26.82
Episode length: 49.40 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.7     |
|    ep_rew_mean     | -284     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 52       |
|    time_elapsed    | 1971     |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=-278.91 +/- 26.19
Episode length: 48.46 +/- 14.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 106500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.21e-08 |
|    explained_variance   | 0.00817   |
|    learning_rate        | 0.001     |
|    loss                 | 790       |
|    n_updates            | 191       |
|    policy_gradient_loss | -2.27e-07 |
|    value_loss           | 1.16e+03  |
---------------------------------------
Eval num_timesteps=107000, episode_reward=-276.03 +/- 31.63
Episode length: 55.18 +/- 18.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-278.43 +/- 31.14
Episode length: 49.54 +/- 16.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-280.83 +/- 23.50
Episode length: 48.06 +/- 15.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-280.83 +/- 27.66
Episode length: 53.20 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -284     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 53       |
|    time_elapsed    | 2015     |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-277.70 +/- 24.98
Episode length: 49.84 +/- 17.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.8          |
|    mean_reward          | -278          |
| time/                   |               |
|    total_timesteps      | 109000        |
| train/                  |               |
|    approx_kl            | 5.4424163e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -3.93e-05     |
|    explained_variance   | 0.00807       |
|    learning_rate        | 0.001         |
|    loss                 | 1.33e+03      |
|    n_updates            | 201           |
|    policy_gradient_loss | -5.36e-08     |
|    value_loss           | 2.99e+03      |
-------------------------------------------
Eval num_timesteps=109500, episode_reward=-279.39 +/- 26.72
Episode length: 51.24 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-281.07 +/- 22.98
Episode length: 53.58 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-280.59 +/- 25.85
Episode length: 51.04 +/- 20.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -282     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 54       |
|    time_elapsed    | 2054     |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-275.78 +/- 27.42
Episode length: 49.34 +/- 19.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.3      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 111000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.88e-07 |
|    explained_variance   | 0.00586   |
|    learning_rate        | 0.001     |
|    loss                 | 488       |
|    n_updates            | 211       |
|    policy_gradient_loss | -2.88e-07 |
|    value_loss           | 1.1e+03   |
---------------------------------------
Eval num_timesteps=111500, episode_reward=-280.11 +/- 23.94
Episode length: 50.34 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-274.58 +/- 33.20
Episode length: 47.90 +/- 19.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-276.97 +/- 31.95
Episode length: 48.42 +/- 17.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 55       |
|    time_elapsed    | 2091     |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-276.74 +/- 29.30
Episode length: 53.16 +/- 18.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.2          |
|    mean_reward          | -277          |
| time/                   |               |
|    total_timesteps      | 113000        |
| train/                  |               |
|    approx_kl            | 1.9732397e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00011      |
|    explained_variance   | 0.00663       |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+03      |
|    n_updates            | 221           |
|    policy_gradient_loss | 2.26e-08      |
|    value_loss           | 2.48e+03      |
-------------------------------------------
Eval num_timesteps=113500, episode_reward=-276.02 +/- 24.56
Episode length: 50.44 +/- 15.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-271.23 +/- 28.34
Episode length: 49.74 +/- 19.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-275.29 +/- 26.74
Episode length: 47.42 +/- 15.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 56       |
|    time_elapsed    | 2130     |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-276.75 +/- 29.00
Episode length: 51.80 +/- 16.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.8      |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.56e-07 |
|    explained_variance   | 0.00902   |
|    learning_rate        | 0.001     |
|    loss                 | 987       |
|    n_updates            | 231       |
|    policy_gradient_loss | -1.38e-06 |
|    value_loss           | 1.19e+03  |
---------------------------------------
Eval num_timesteps=115500, episode_reward=-277.21 +/- 27.51
Episode length: 50.46 +/- 18.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-279.62 +/- 24.80
Episode length: 49.16 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-277.94 +/- 29.91
Episode length: 49.58 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 57       |
|    time_elapsed    | 2170     |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-282.51 +/- 25.59
Episode length: 47.78 +/- 13.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.8         |
|    mean_reward          | -283         |
| time/                   |              |
|    total_timesteps      | 117000       |
| train/                  |              |
|    approx_kl            | 4.156027e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.000501    |
|    explained_variance   | 0.0374       |
|    learning_rate        | 0.001        |
|    loss                 | 1.34e+03     |
|    n_updates            | 241          |
|    policy_gradient_loss | 3.09e-08     |
|    value_loss           | 2.77e+03     |
------------------------------------------
Eval num_timesteps=117500, episode_reward=-280.59 +/- 26.84
Episode length: 49.18 +/- 14.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-276.99 +/- 30.74
Episode length: 54.16 +/- 16.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-282.03 +/- 26.76
Episode length: 50.26 +/- 15.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.2     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 58       |
|    time_elapsed    | 2224     |
|    total_timesteps | 118784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.34
Eval num_timesteps=119000, episode_reward=-272.19 +/- 31.90
Episode length: 53.96 +/- 17.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54          |
|    mean_reward          | -272        |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.019838985 |
|    clip_fraction        | 0.000919    |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.000299   |
|    explained_variance   | 0.00876     |
|    learning_rate        | 0.001       |
|    loss                 | 521         |
|    n_updates            | 242         |
|    policy_gradient_loss | 0.00035     |
|    value_loss           | 1.2e+03     |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=-280.82 +/- 27.35
Episode length: 48.78 +/- 14.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-272.66 +/- 32.42
Episode length: 48.24 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-281.30 +/- 29.53
Episode length: 49.80 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 59       |
|    time_elapsed    | 2263     |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-276.98 +/- 27.50
Episode length: 48.02 +/- 15.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48           |
|    mean_reward          | -277         |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 2.188608e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.000425    |
|    explained_variance   | 0.0187       |
|    learning_rate        | 0.001        |
|    loss                 | 606          |
|    n_updates            | 252          |
|    policy_gradient_loss | 6.39e-07     |
|    value_loss           | 1.24e+03     |
------------------------------------------
Eval num_timesteps=121500, episode_reward=-277.94 +/- 27.61
Episode length: 52.64 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-282.98 +/- 22.50
Episode length: 51.16 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-273.14 +/- 35.46
Episode length: 46.94 +/- 13.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 60       |
|    time_elapsed    | 2306     |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-279.13 +/- 27.88
Episode length: 46.68 +/- 14.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.7      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 123000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.65e-07 |
|    explained_variance   | 0.00757   |
|    learning_rate        | 0.001     |
|    loss                 | 438       |
|    n_updates            | 262       |
|    policy_gradient_loss | -8.39e-07 |
|    value_loss           | 953       |
---------------------------------------
Eval num_timesteps=123500, episode_reward=-285.63 +/- 23.15
Episode length: 55.80 +/- 15.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-284.90 +/- 23.63
Episode length: 51.50 +/- 19.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-275.06 +/- 27.76
Episode length: 51.86 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 61       |
|    time_elapsed    | 2344     |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-283.95 +/- 25.28
Episode length: 51.14 +/- 16.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.1          |
|    mean_reward          | -284          |
| time/                   |               |
|    total_timesteps      | 125000        |
| train/                  |               |
|    approx_kl            | 2.1915184e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000164     |
|    explained_variance   | 0.0257        |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+03      |
|    n_updates            | 272           |
|    policy_gradient_loss | 3.12e-07      |
|    value_loss           | 2.03e+03      |
-------------------------------------------
Eval num_timesteps=125500, episode_reward=-277.23 +/- 23.29
Episode length: 49.22 +/- 12.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-280.81 +/- 25.38
Episode length: 49.54 +/- 18.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-272.67 +/- 36.03
Episode length: 50.64 +/- 18.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 62       |
|    time_elapsed    | 2381     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-276.27 +/- 29.18
Episode length: 47.52 +/- 12.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.5      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 127000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.13e-07 |
|    explained_variance   | 0.00582   |
|    learning_rate        | 0.001     |
|    loss                 | 494       |
|    n_updates            | 282       |
|    policy_gradient_loss | -3.59e-07 |
|    value_loss           | 983       |
---------------------------------------
Eval num_timesteps=127500, episode_reward=-272.90 +/- 31.56
Episode length: 44.36 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-282.02 +/- 24.87
Episode length: 50.00 +/- 22.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-279.40 +/- 26.06
Episode length: 50.16 +/- 15.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-275.78 +/- 28.35
Episode length: 50.48 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 63       |
|    time_elapsed    | 2422     |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=-274.11 +/- 30.81
Episode length: 50.90 +/- 17.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.9          |
|    mean_reward          | -274          |
| time/                   |               |
|    total_timesteps      | 129500        |
| train/                  |               |
|    approx_kl            | 1.7030368e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00151      |
|    explained_variance   | 0.0156        |
|    learning_rate        | 0.001         |
|    loss                 | 945           |
|    n_updates            | 292           |
|    policy_gradient_loss | 8.36e-07      |
|    value_loss           | 1.78e+03      |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=-275.06 +/- 32.28
Episode length: 47.68 +/- 19.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-278.90 +/- 29.90
Episode length: 53.88 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-281.30 +/- 27.92
Episode length: 49.98 +/- 18.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 64       |
|    time_elapsed    | 2466     |
|    total_timesteps | 131072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.50
Eval num_timesteps=131500, episode_reward=-280.09 +/- 27.00
Episode length: 47.86 +/- 19.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.9       |
|    mean_reward          | -280       |
| time/                   |            |
|    total_timesteps      | 131500     |
| train/                  |            |
|    approx_kl            | 0.16565686 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.0069     |
|    learning_rate        | 0.001      |
|    loss                 | 401        |
|    n_updates            | 293        |
|    policy_gradient_loss | 0.237      |
|    value_loss           | 1.05e+03   |
----------------------------------------
Eval num_timesteps=132000, episode_reward=-276.27 +/- 29.77
Episode length: 51.50 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-269.53 +/- 34.12
Episode length: 49.46 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-283.23 +/- 26.55
Episode length: 51.80 +/- 16.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 65       |
|    time_elapsed    | 2503     |
|    total_timesteps | 133120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 1.01
Eval num_timesteps=133500, episode_reward=-284.91 +/- 20.21
Episode length: 52.30 +/- 17.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.3      |
|    mean_reward          | -285      |
| time/                   |           |
|    total_timesteps      | 133500    |
| train/                  |           |
|    approx_kl            | 0.5037432 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.405    |
|    explained_variance   | -0.0029   |
|    learning_rate        | 0.001     |
|    loss                 | 411       |
|    n_updates            | 294       |
|    policy_gradient_loss | 0.112     |
|    value_loss           | 778       |
---------------------------------------
Eval num_timesteps=134000, episode_reward=-282.27 +/- 26.19
Episode length: 52.20 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-283.47 +/- 20.79
Episode length: 50.00 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-282.50 +/- 24.90
Episode length: 54.38 +/- 19.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 66       |
|    time_elapsed    | 2538     |
|    total_timesteps | 135168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=135500, episode_reward=-285.39 +/- 22.98
Episode length: 52.26 +/- 16.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.3       |
|    mean_reward          | -285       |
| time/                   |            |
|    total_timesteps      | 135500     |
| train/                  |            |
|    approx_kl            | 0.07192066 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0577    |
|    explained_variance   | 0.00871    |
|    learning_rate        | 0.001      |
|    loss                 | 222        |
|    n_updates            | 295        |
|    policy_gradient_loss | 0.0031     |
|    value_loss           | 437        |
----------------------------------------
Eval num_timesteps=136000, episode_reward=-276.99 +/- 23.64
Episode length: 49.38 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-277.89 +/- 29.43
Episode length: 49.72 +/- 17.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-276.27 +/- 27.96
Episode length: 49.70 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.8     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 67       |
|    time_elapsed    | 2572     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=-276.27 +/- 31.65
Episode length: 47.10 +/- 15.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.1      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 137500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.3e-06  |
|    explained_variance   | 0.00551   |
|    learning_rate        | 0.001     |
|    loss                 | 268       |
|    n_updates            | 305       |
|    policy_gradient_loss | -9.98e-07 |
|    value_loss           | 538       |
---------------------------------------
Eval num_timesteps=138000, episode_reward=-281.31 +/- 28.32
Episode length: 56.94 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.9     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-281.30 +/- 26.97
Episode length: 52.62 +/- 18.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-281.07 +/- 28.26
Episode length: 52.24 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 68       |
|    time_elapsed    | 2617     |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-274.57 +/- 26.87
Episode length: 49.08 +/- 16.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.1      |
|    mean_reward          | -275      |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.68e-20 |
|    explained_variance   | 0.0265    |
|    learning_rate        | 0.001     |
|    loss                 | 3.6e+03   |
|    n_updates            | 315       |
|    policy_gradient_loss | -8.76e-10 |
|    value_loss           | 6.72e+03  |
---------------------------------------
Eval num_timesteps=140000, episode_reward=-273.63 +/- 30.08
Episode length: 51.42 +/- 19.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-283.22 +/- 23.34
Episode length: 49.34 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-280.12 +/- 27.52
Episode length: 51.52 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 69       |
|    time_elapsed    | 2653     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-281.78 +/- 27.97
Episode length: 48.00 +/- 15.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48            |
|    mean_reward          | -282          |
| time/                   |               |
|    total_timesteps      | 141500        |
| train/                  |               |
|    approx_kl            | 1.9208528e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000254     |
|    explained_variance   | 0.0209        |
|    learning_rate        | 0.001         |
|    loss                 | 383           |
|    n_updates            | 325           |
|    policy_gradient_loss | 1.07e-06      |
|    value_loss           | 589           |
-------------------------------------------
Eval num_timesteps=142000, episode_reward=-280.34 +/- 29.91
Episode length: 46.68 +/- 15.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-276.51 +/- 26.73
Episode length: 49.58 +/- 15.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-281.79 +/- 23.48
Episode length: 51.86 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 70       |
|    time_elapsed    | 2719     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-279.87 +/- 29.56
Episode length: 53.78 +/- 16.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.8      |
|    mean_reward          | -280      |
| time/                   |           |
|    total_timesteps      | 143500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.38e-07 |
|    explained_variance   | 0.00486   |
|    learning_rate        | 0.001     |
|    loss                 | 932       |
|    n_updates            | 335       |
|    policy_gradient_loss | -8.89e-07 |
|    value_loss           | 1.57e+03  |
---------------------------------------
Eval num_timesteps=144000, episode_reward=-285.63 +/- 24.84
Episode length: 51.74 +/- 17.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-279.39 +/- 27.77
Episode length: 54.26 +/- 19.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-286.83 +/- 21.56
Episode length: 49.40 +/- 16.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 71       |
|    time_elapsed    | 2767     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=-275.54 +/- 29.73
Episode length: 52.20 +/- 17.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 145500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.67e-06 |
|    explained_variance   | 0.0301    |
|    learning_rate        | 0.001     |
|    loss                 | 253       |
|    n_updates            | 345       |
|    policy_gradient_loss | 6.28e-08  |
|    value_loss           | 598       |
---------------------------------------
Eval num_timesteps=146000, episode_reward=-275.05 +/- 29.07
Episode length: 48.38 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-276.50 +/- 26.83
Episode length: 48.46 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-271.46 +/- 31.57
Episode length: 46.62 +/- 12.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.9     |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 72       |
|    time_elapsed    | 2803     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-286.11 +/- 23.96
Episode length: 53.66 +/- 17.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.7      |
|    mean_reward          | -286      |
| time/                   |           |
|    total_timesteps      | 147500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.98e-08 |
|    explained_variance   | 0.00478   |
|    learning_rate        | 0.001     |
|    loss                 | 528       |
|    n_updates            | 355       |
|    policy_gradient_loss | -7.65e-08 |
|    value_loss           | 1.29e+03  |
---------------------------------------
Eval num_timesteps=148000, episode_reward=-280.58 +/- 26.75
Episode length: 51.24 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-279.87 +/- 26.26
Episode length: 52.00 +/- 17.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-283.93 +/- 22.38
Episode length: 47.46 +/- 20.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-279.14 +/- 25.41
Episode length: 49.18 +/- 17.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -282     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 73       |
|    time_elapsed    | 2861     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-281.79 +/- 24.20
Episode length: 50.42 +/- 14.77
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 50.4           |
|    mean_reward          | -282           |
| time/                   |                |
|    total_timesteps      | 150000         |
| train/                  |                |
|    approx_kl            | -1.6996637e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -3.24e-05      |
|    explained_variance   | 0.0153         |
|    learning_rate        | 0.001          |
|    loss                 | 192            |
|    n_updates            | 365            |
|    policy_gradient_loss | 1.21e-07       |
|    value_loss           | 631            |
--------------------------------------------
Eval num_timesteps=150500, episode_reward=-280.59 +/- 25.86
Episode length: 50.36 +/- 15.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-274.11 +/- 29.27
Episode length: 55.50 +/- 20.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.5     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-280.83 +/- 24.93
Episode length: 51.08 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -282     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 74       |
|    time_elapsed    | 2902     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-276.02 +/- 33.22
Episode length: 47.72 +/- 14.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.7      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 152000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.21e-07 |
|    explained_variance   | 0.00383   |
|    learning_rate        | 0.001     |
|    loss                 | 672       |
|    n_updates            | 375       |
|    policy_gradient_loss | -1.89e-07 |
|    value_loss           | 1.22e+03  |
---------------------------------------
Eval num_timesteps=152500, episode_reward=-279.14 +/- 25.28
Episode length: 45.42 +/- 15.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-276.75 +/- 25.63
Episode length: 48.44 +/- 16.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-279.38 +/- 26.62
Episode length: 47.98 +/- 15.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 75       |
|    time_elapsed    | 2938     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-277.23 +/- 26.85
Episode length: 54.44 +/- 21.70
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 54.4           |
|    mean_reward          | -277           |
| time/                   |                |
|    total_timesteps      | 154000         |
| train/                  |                |
|    approx_kl            | -1.6880222e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -2.43e-05      |
|    explained_variance   | 0.0162         |
|    learning_rate        | 0.001          |
|    loss                 | 187            |
|    n_updates            | 385            |
|    policy_gradient_loss | 1.12e-07       |
|    value_loss           | 600            |
--------------------------------------------
Eval num_timesteps=154500, episode_reward=-276.97 +/- 26.75
Episode length: 46.46 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-285.15 +/- 20.55
Episode length: 52.14 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-279.63 +/- 29.86
Episode length: 51.14 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 76       |
|    time_elapsed    | 2976     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-275.55 +/- 33.03
Episode length: 52.18 +/- 15.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 156000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.22e-08 |
|    explained_variance   | 0.00493   |
|    learning_rate        | 0.001     |
|    loss                 | 508       |
|    n_updates            | 395       |
|    policy_gradient_loss | -1.72e-07 |
|    value_loss           | 1.34e+03  |
---------------------------------------
Eval num_timesteps=156500, episode_reward=-281.08 +/- 23.84
Episode length: 50.84 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-284.44 +/- 27.15
Episode length: 55.08 +/- 21.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-270.27 +/- 30.93
Episode length: 50.94 +/- 20.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 51       |
|    iterations      | 77       |
|    time_elapsed    | 3041     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-277.70 +/- 27.08
Episode length: 48.40 +/- 19.37
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 48.4           |
|    mean_reward          | -278           |
| time/                   |                |
|    total_timesteps      | 158000         |
| train/                  |                |
|    approx_kl            | -1.2776582e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -4.25e-05      |
|    explained_variance   | 0.0121         |
|    learning_rate        | 0.001          |
|    loss                 | 297            |
|    n_updates            | 405            |
|    policy_gradient_loss | 1.57e-07       |
|    value_loss           | 634            |
--------------------------------------------
Eval num_timesteps=158500, episode_reward=-277.95 +/- 28.43
Episode length: 51.56 +/- 16.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-277.70 +/- 29.32
Episode length: 48.30 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-277.93 +/- 26.31
Episode length: 47.26 +/- 15.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -273     |
| time/              |          |
|    fps             | 51       |
|    iterations      | 78       |
|    time_elapsed    | 3116     |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-277.22 +/- 29.22
Episode length: 50.10 +/- 16.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.55e-07 |
|    explained_variance   | 0.00481   |
|    learning_rate        | 0.001     |
|    loss                 | 607       |
|    n_updates            | 415       |
|    policy_gradient_loss | -2.7e-07  |
|    value_loss           | 1.29e+03  |
---------------------------------------
Eval num_timesteps=160500, episode_reward=-276.27 +/- 30.06
Episode length: 50.50 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-269.30 +/- 31.70
Episode length: 49.06 +/- 15.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-278.67 +/- 25.76
Episode length: 52.54 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 51       |
|    iterations      | 79       |
|    time_elapsed    | 3157     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-284.42 +/- 23.65
Episode length: 52.24 +/- 16.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -284      |
| time/                   |           |
|    total_timesteps      | 162000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.52e-20 |
|    explained_variance   | 0.0175    |
|    learning_rate        | 0.001     |
|    loss                 | 332       |
|    n_updates            | 425       |
|    policy_gradient_loss | -3.35e-10 |
|    value_loss           | 624       |
---------------------------------------
Eval num_timesteps=162500, episode_reward=-281.29 +/- 28.72
Episode length: 48.18 +/- 15.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-277.23 +/- 29.22
Episode length: 51.10 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-274.10 +/- 31.36
Episode length: 49.40 +/- 18.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 80       |
|    time_elapsed    | 3223     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-283.70 +/- 24.90
Episode length: 51.22 +/- 16.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.2      |
|    mean_reward          | -284      |
| time/                   |           |
|    total_timesteps      | 164000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.18e-16 |
|    explained_variance   | 0.0241    |
|    learning_rate        | 0.001     |
|    loss                 | 2.44e+03  |
|    n_updates            | 435       |
|    policy_gradient_loss | 3.62e-09  |
|    value_loss           | 4.59e+03  |
---------------------------------------
Eval num_timesteps=164500, episode_reward=-282.26 +/- 28.80
Episode length: 50.94 +/- 16.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-276.27 +/- 26.81
Episode length: 52.98 +/- 17.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-278.19 +/- 32.31
Episode length: 53.04 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 81       |
|    time_elapsed    | 3259     |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-275.69 +/- 34.75
Episode length: 47.80 +/- 17.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.8      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 166000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.87e-18 |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.001     |
|    loss                 | 307       |
|    n_updates            | 445       |
|    policy_gradient_loss | -6.26e-10 |
|    value_loss           | 708       |
---------------------------------------
Eval num_timesteps=166500, episode_reward=-276.03 +/- 25.81
Episode length: 48.28 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-282.73 +/- 24.86
Episode length: 51.48 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-276.02 +/- 25.82
Episode length: 50.50 +/- 20.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.9     |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 82       |
|    time_elapsed    | 3293     |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-281.31 +/- 22.94
Episode length: 52.12 +/- 14.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.1      |
|    mean_reward          | -281      |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.77e-14 |
|    explained_variance   | 0.0449    |
|    learning_rate        | 0.001     |
|    loss                 | 1.68e+03  |
|    n_updates            | 455       |
|    policy_gradient_loss | -1.23e-09 |
|    value_loss           | 3.51e+03  |
---------------------------------------
Eval num_timesteps=168500, episode_reward=-274.09 +/- 27.65
Episode length: 46.82 +/- 15.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-287.30 +/- 22.14
Episode length: 49.88 +/- 20.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-278.91 +/- 27.17
Episode length: 50.40 +/- 20.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 51       |
|    iterations      | 83       |
|    time_elapsed    | 3329     |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-287.08 +/- 22.50
Episode length: 51.24 +/- 15.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.2      |
|    mean_reward          | -287      |
| time/                   |           |
|    total_timesteps      | 170000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-16 |
|    explained_variance   | 0.0178    |
|    learning_rate        | 0.001     |
|    loss                 | 261       |
|    n_updates            | 465       |
|    policy_gradient_loss | -3.17e-10 |
|    value_loss           | 772       |
---------------------------------------
Eval num_timesteps=170500, episode_reward=-274.58 +/- 27.09
Episode length: 51.92 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-277.23 +/- 30.28
Episode length: 52.24 +/- 19.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-282.98 +/- 25.51
Episode length: 51.88 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-274.10 +/- 34.26
Episode length: 50.44 +/- 18.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.1     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 84       |
|    time_elapsed    | 3380     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=-274.83 +/- 30.41
Episode length: 47.36 +/- 14.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.4      |
|    mean_reward          | -275      |
| time/                   |           |
|    total_timesteps      | 172500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.74e-15 |
|    explained_variance   | 0.0521    |
|    learning_rate        | 0.001     |
|    loss                 | 1.41e+03  |
|    n_updates            | 475       |
|    policy_gradient_loss | -1.12e-09 |
|    value_loss           | 3.04e+03  |
---------------------------------------
Eval num_timesteps=173000, episode_reward=-278.43 +/- 31.95
Episode length: 53.36 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-271.46 +/- 31.48
Episode length: 50.08 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-273.63 +/- 31.30
Episode length: 49.84 +/- 18.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.3     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 85       |
|    time_elapsed    | 3450     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-282.75 +/- 24.15
Episode length: 49.96 +/- 17.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -283      |
| time/                   |           |
|    total_timesteps      | 174500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.83e-18 |
|    explained_variance   | 0.0168    |
|    learning_rate        | 0.001     |
|    loss                 | 364       |
|    n_updates            | 485       |
|    policy_gradient_loss | -2.1e-10  |
|    value_loss           | 619       |
---------------------------------------
Eval num_timesteps=175000, episode_reward=-279.87 +/- 31.08
Episode length: 51.12 +/- 16.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-279.88 +/- 25.13
Episode length: 53.70 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-274.33 +/- 27.27
Episode length: 46.00 +/- 16.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.6     |
|    ep_rew_mean     | -272     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 86       |
|    time_elapsed    | 3493     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-277.19 +/- 35.16
Episode length: 50.42 +/- 18.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 176500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.32e-15 |
|    explained_variance   | 0.0187    |
|    learning_rate        | 0.001     |
|    loss                 | 1.5e+03   |
|    n_updates            | 495       |
|    policy_gradient_loss | -2.83e-09 |
|    value_loss           | 2.85e+03  |
---------------------------------------
Eval num_timesteps=177000, episode_reward=-279.38 +/- 28.09
Episode length: 52.28 +/- 20.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-277.47 +/- 28.73
Episode length: 52.12 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-279.13 +/- 22.39
Episode length: 49.98 +/- 18.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -273     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 87       |
|    time_elapsed    | 3538     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-275.80 +/- 26.34
Episode length: 56.98 +/- 17.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 57        |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 178500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.29e-17 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.001     |
|    loss                 | 324       |
|    n_updates            | 505       |
|    policy_gradient_loss | 1.75e-09  |
|    value_loss           | 580       |
---------------------------------------
Eval num_timesteps=179000, episode_reward=-279.38 +/- 26.82
Episode length: 52.22 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-281.06 +/- 31.17
Episode length: 48.74 +/- 18.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-282.74 +/- 19.84
Episode length: 50.98 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 88       |
|    time_elapsed    | 3613     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-277.94 +/- 29.72
Episode length: 48.80 +/- 17.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.8      |
|    mean_reward          | -278      |
| time/                   |           |
|    total_timesteps      | 180500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39e-13 |
|    explained_variance   | 0.0443    |
|    learning_rate        | 0.001     |
|    loss                 | 1.23e+03  |
|    n_updates            | 515       |
|    policy_gradient_loss | -7.31e-09 |
|    value_loss           | 2.51e+03  |
---------------------------------------
Eval num_timesteps=181000, episode_reward=-273.15 +/- 30.75
Episode length: 51.18 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-286.34 +/- 22.40
Episode length: 50.76 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-278.18 +/- 28.12
Episode length: 50.96 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.9     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 89       |
|    time_elapsed    | 3650     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-279.15 +/- 26.73
Episode length: 51.68 +/- 14.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 182500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.22e-16 |
|    explained_variance   | 0.0187    |
|    learning_rate        | 0.001     |
|    loss                 | 346       |
|    n_updates            | 525       |
|    policy_gradient_loss | 3.35e-10  |
|    value_loss           | 795       |
---------------------------------------
Eval num_timesteps=183000, episode_reward=-276.27 +/- 31.55
Episode length: 49.38 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-270.98 +/- 30.07
Episode length: 49.96 +/- 17.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-272.19 +/- 31.34
Episode length: 48.86 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.9     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 90       |
|    time_elapsed    | 3722     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=-276.98 +/- 24.39
Episode length: 45.86 +/- 11.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.9      |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 184500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.28e-14 |
|    explained_variance   | 0.0326    |
|    learning_rate        | 0.001     |
|    loss                 | 1.18e+03  |
|    n_updates            | 535       |
|    policy_gradient_loss | 2.12e-09  |
|    value_loss           | 2.28e+03  |
---------------------------------------
Eval num_timesteps=185000, episode_reward=-280.59 +/- 24.83
Episode length: 53.22 +/- 20.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-277.71 +/- 27.71
Episode length: 55.66 +/- 19.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.7     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-265.46 +/- 32.93
Episode length: 48.42 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 91       |
|    time_elapsed    | 3759     |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-281.55 +/- 25.85
Episode length: 50.56 +/- 16.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.6      |
|    mean_reward          | -282      |
| time/                   |           |
|    total_timesteps      | 186500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.95e-16 |
|    explained_variance   | 0.0164    |
|    learning_rate        | 0.001     |
|    loss                 | 228       |
|    n_updates            | 545       |
|    policy_gradient_loss | -2.52e-10 |
|    value_loss           | 702       |
---------------------------------------
Eval num_timesteps=187000, episode_reward=-276.98 +/- 33.96
Episode length: 50.58 +/- 18.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-270.99 +/- 27.14
Episode length: 46.82 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-278.67 +/- 27.28
Episode length: 51.64 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.9     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 92       |
|    time_elapsed    | 3794     |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=-283.23 +/- 25.57
Episode length: 52.16 +/- 15.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -283      |
| time/                   |           |
|    total_timesteps      | 188500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.14e-13 |
|    explained_variance   | 0.0267    |
|    learning_rate        | 0.001     |
|    loss                 | 962       |
|    n_updates            | 555       |
|    policy_gradient_loss | 3.7e-09   |
|    value_loss           | 2.08e+03  |
---------------------------------------
Eval num_timesteps=189000, episode_reward=-276.45 +/- 29.40
Episode length: 49.02 +/- 16.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-282.99 +/- 31.65
Episode length: 51.60 +/- 18.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-286.59 +/- 20.42
Episode length: 49.16 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 93       |
|    time_elapsed    | 3829     |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=-277.46 +/- 30.67
Episode length: 47.22 +/- 15.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.2      |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 190500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.39e-16 |
|    explained_variance   | 0.0077    |
|    learning_rate        | 0.001     |
|    loss                 | 314       |
|    n_updates            | 565       |
|    policy_gradient_loss | -5.47e-10 |
|    value_loss           | 695       |
---------------------------------------
Eval num_timesteps=191000, episode_reward=-279.39 +/- 31.57
Episode length: 51.70 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-276.51 +/- 30.07
Episode length: 53.74 +/- 19.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-276.98 +/- 31.50
Episode length: 48.46 +/- 17.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-280.59 +/- 26.63
Episode length: 49.28 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 94       |
|    time_elapsed    | 3871     |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-276.98 +/- 31.68
Episode length: 48.36 +/- 16.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.4      |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 193000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.1e-13  |
|    explained_variance   | 0.0258    |
|    learning_rate        | 0.001     |
|    loss                 | 915       |
|    n_updates            | 575       |
|    policy_gradient_loss | -5.75e-09 |
|    value_loss           | 1.85e+03  |
---------------------------------------
Eval num_timesteps=193500, episode_reward=-284.19 +/- 23.95
Episode length: 50.88 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-278.68 +/- 29.61
Episode length: 51.06 +/- 16.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-271.45 +/- 31.84
Episode length: 50.62 +/- 18.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 95       |
|    time_elapsed    | 3906     |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-282.02 +/- 22.57
Episode length: 49.46 +/- 15.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.5      |
|    mean_reward          | -282      |
| time/                   |           |
|    total_timesteps      | 195000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.99e-15 |
|    explained_variance   | 0.00981   |
|    learning_rate        | 0.001     |
|    loss                 | 308       |
|    n_updates            | 585       |
|    policy_gradient_loss | -2.02e-10 |
|    value_loss           | 640       |
---------------------------------------
Eval num_timesteps=195500, episode_reward=-278.19 +/- 29.32
Episode length: 52.64 +/- 18.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-277.95 +/- 23.67
Episode length: 49.78 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-282.51 +/- 23.96
Episode length: 54.10 +/- 19.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 96       |
|    time_elapsed    | 3942     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-282.03 +/- 29.72
Episode length: 46.68 +/- 15.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.7      |
|    mean_reward          | -282      |
| time/                   |           |
|    total_timesteps      | 197000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.75e-12 |
|    explained_variance   | 0.00934   |
|    learning_rate        | 0.001     |
|    loss                 | 958       |
|    n_updates            | 595       |
|    policy_gradient_loss | 8.95e-10  |
|    value_loss           | 1.72e+03  |
---------------------------------------
Eval num_timesteps=197500, episode_reward=-275.08 +/- 27.56
Episode length: 49.08 +/- 15.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-286.11 +/- 23.48
Episode length: 52.86 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-280.83 +/- 30.63
Episode length: 46.68 +/- 16.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 97       |
|    time_elapsed    | 3979     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-279.14 +/- 29.10
Episode length: 48.30 +/- 17.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.3      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 199000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.19e-15 |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.001     |
|    loss                 | 307       |
|    n_updates            | 605       |
|    policy_gradient_loss | -4.31e-10 |
|    value_loss           | 627       |
---------------------------------------
Eval num_timesteps=199500, episode_reward=-272.18 +/- 35.40
Episode length: 49.12 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-283.47 +/- 21.59
Episode length: 51.70 +/- 14.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-276.74 +/- 33.69
Episode length: 49.14 +/- 20.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 98       |
|    time_elapsed    | 4014     |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-283.47 +/- 27.48
Episode length: 50.36 +/- 17.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -283      |
| time/                   |           |
|    total_timesteps      | 201000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.32e-12 |
|    explained_variance   | 0.0339    |
|    learning_rate        | 0.001     |
|    loss                 | 758       |
|    n_updates            | 615       |
|    policy_gradient_loss | 4.42e-09  |
|    value_loss           | 1.52e+03  |
---------------------------------------
Eval num_timesteps=201500, episode_reward=-284.19 +/- 23.57
Episode length: 51.52 +/- 15.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-276.51 +/- 31.00
Episode length: 51.48 +/- 15.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-283.70 +/- 19.57
Episode length: 50.58 +/- 15.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 99       |
|    time_elapsed    | 4049     |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=-279.35 +/- 34.15
Episode length: 50.24 +/- 20.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.2      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 203000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.55e-14 |
|    explained_variance   | 0.0113    |
|    learning_rate        | 0.001     |
|    loss                 | 333       |
|    n_updates            | 625       |
|    policy_gradient_loss | 8.59e-10  |
|    value_loss           | 729       |
---------------------------------------
Eval num_timesteps=203500, episode_reward=-282.03 +/- 26.22
Episode length: 49.76 +/- 16.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-281.07 +/- 28.75
Episode length: 48.30 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-279.62 +/- 27.77
Episode length: 46.46 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 100      |
|    time_elapsed    | 4084     |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=-276.03 +/- 23.22
Episode length: 50.64 +/- 15.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.6      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 205000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.5e-12  |
|    explained_variance   | 0.0392    |
|    learning_rate        | 0.001     |
|    loss                 | 638       |
|    n_updates            | 635       |
|    policy_gradient_loss | -4.14e-09 |
|    value_loss           | 1.39e+03  |
---------------------------------------
Eval num_timesteps=205500, episode_reward=-281.78 +/- 27.86
Episode length: 54.86 +/- 18.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.9     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-275.55 +/- 28.21
Episode length: 45.12 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-273.38 +/- 29.95
Episode length: 53.22 +/- 18.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 101      |
|    time_elapsed    | 4119     |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=-279.39 +/- 24.24
Episode length: 51.02 +/- 14.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 207000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.39e-14 |
|    explained_variance   | 0.0158    |
|    learning_rate        | 0.001     |
|    loss                 | 601       |
|    n_updates            | 645       |
|    policy_gradient_loss | -7.68e-10 |
|    value_loss           | 747       |
---------------------------------------
Eval num_timesteps=207500, episode_reward=-277.95 +/- 27.50
Episode length: 47.66 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-282.75 +/- 24.52
Episode length: 46.76 +/- 14.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-277.35 +/- 26.36
Episode length: 48.38 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 102      |
|    time_elapsed    | 4154     |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=-276.52 +/- 30.35
Episode length: 51.98 +/- 18.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52        |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.08e-11 |
|    explained_variance   | 0.0324    |
|    learning_rate        | 0.001     |
|    loss                 | 603       |
|    n_updates            | 655       |
|    policy_gradient_loss | 2.15e-09  |
|    value_loss           | 1.26e+03  |
---------------------------------------
Eval num_timesteps=209500, episode_reward=-282.98 +/- 23.86
Episode length: 48.84 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-269.06 +/- 27.12
Episode length: 46.60 +/- 15.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-280.58 +/- 28.20
Episode length: 48.90 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -282     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 103      |
|    time_elapsed    | 4189     |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-273.38 +/- 31.92
Episode length: 51.32 +/- 21.83
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 51.3     |
|    mean_reward          | -273     |
| time/                   |          |
|    total_timesteps      | 211000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -7.3e-14 |
|    explained_variance   | 0.0151   |
|    learning_rate        | 0.001    |
|    loss                 | 392      |
|    n_updates            | 665      |
|    policy_gradient_loss | 1.35e-09 |
|    value_loss           | 706      |
--------------------------------------
Eval num_timesteps=211500, episode_reward=-272.18 +/- 29.16
Episode length: 48.64 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-276.99 +/- 23.89
Episode length: 47.52 +/- 14.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-274.59 +/- 33.38
Episode length: 49.88 +/- 15.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 104      |
|    time_elapsed    | 4224     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=-281.77 +/- 22.11
Episode length: 46.82 +/- 17.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.8      |
|    mean_reward          | -282      |
| time/                   |           |
|    total_timesteps      | 213000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.65e-11 |
|    explained_variance   | 0.0298    |
|    learning_rate        | 0.001     |
|    loss                 | 678       |
|    n_updates            | 675       |
|    policy_gradient_loss | 2.3e-09   |
|    value_loss           | 1.23e+03  |
---------------------------------------
Eval num_timesteps=213500, episode_reward=-277.71 +/- 29.34
Episode length: 47.74 +/- 15.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-271.23 +/- 29.25
Episode length: 46.18 +/- 12.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-284.91 +/- 19.47
Episode length: 51.42 +/- 14.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-276.50 +/- 25.04
Episode length: 49.00 +/- 13.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 105      |
|    time_elapsed    | 4265     |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=-279.15 +/- 24.95
Episode length: 51.04 +/- 18.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 215500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.69e-13 |
|    explained_variance   | 0.00912   |
|    learning_rate        | 0.001     |
|    loss                 | 653       |
|    n_updates            | 685       |
|    policy_gradient_loss | 1.13e-09  |
|    value_loss           | 780       |
---------------------------------------
Eval num_timesteps=216000, episode_reward=-273.87 +/- 32.85
Episode length: 48.32 +/- 13.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-283.47 +/- 23.40
Episode length: 53.04 +/- 19.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-271.95 +/- 33.35
Episode length: 47.76 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 106      |
|    time_elapsed    | 4299     |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=-278.17 +/- 27.39
Episode length: 48.14 +/- 15.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.1      |
|    mean_reward          | -278      |
| time/                   |           |
|    total_timesteps      | 217500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.25e-11 |
|    explained_variance   | 0.0278    |
|    learning_rate        | 0.001     |
|    loss                 | 641       |
|    n_updates            | 695       |
|    policy_gradient_loss | 2.15e-09  |
|    value_loss           | 1.16e+03  |
---------------------------------------
Eval num_timesteps=218000, episode_reward=-280.59 +/- 25.06
Episode length: 49.90 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-275.94 +/- 37.53
Episode length: 49.08 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=-283.23 +/- 26.23
Episode length: 51.04 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 107      |
|    time_elapsed    | 4334     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-279.86 +/- 26.04
Episode length: 48.76 +/- 15.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.8      |
|    mean_reward          | -280      |
| time/                   |           |
|    total_timesteps      | 219500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.65e-13 |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.001     |
|    loss                 | 558       |
|    n_updates            | 705       |
|    policy_gradient_loss | -1.04e-09 |
|    value_loss           | 836       |
---------------------------------------
Eval num_timesteps=220000, episode_reward=-274.35 +/- 28.61
Episode length: 47.86 +/- 13.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-275.78 +/- 35.16
Episode length: 48.68 +/- 18.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-280.34 +/- 31.68
Episode length: 46.00 +/- 13.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 108      |
|    time_elapsed    | 4368     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=-282.50 +/- 23.47
Episode length: 50.34 +/- 19.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.3      |
|    mean_reward          | -283      |
| time/                   |           |
|    total_timesteps      | 221500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.45e-10 |
|    explained_variance   | 0.0277    |
|    learning_rate        | 0.001     |
|    loss                 | 546       |
|    n_updates            | 715       |
|    policy_gradient_loss | -2.18e-09 |
|    value_loss           | 1.06e+03  |
---------------------------------------
Eval num_timesteps=222000, episode_reward=-272.67 +/- 26.24
Episode length: 52.22 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-278.67 +/- 26.95
Episode length: 50.90 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=-275.06 +/- 31.85
Episode length: 48.60 +/- 14.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 109      |
|    time_elapsed    | 4416     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=-271.22 +/- 32.51
Episode length: 50.20 +/- 15.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.2      |
|    mean_reward          | -271      |
| time/                   |           |
|    total_timesteps      | 223500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.79e-13 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.001     |
|    loss                 | 186       |
|    n_updates            | 725       |
|    policy_gradient_loss | -7.32e-10 |
|    value_loss           | 655       |
---------------------------------------
Eval num_timesteps=224000, episode_reward=-278.19 +/- 27.39
Episode length: 45.34 +/- 15.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-276.74 +/- 29.10
Episode length: 52.24 +/- 20.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-279.86 +/- 28.35
Episode length: 50.04 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.1     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 110      |
|    time_elapsed    | 4468     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=-285.14 +/- 24.75
Episode length: 53.10 +/- 20.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.1      |
|    mean_reward          | -285      |
| time/                   |           |
|    total_timesteps      | 225500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.77e-10 |
|    explained_variance   | 0.0258    |
|    learning_rate        | 0.001     |
|    loss                 | 511       |
|    n_updates            | 735       |
|    policy_gradient_loss | -1.54e-10 |
|    value_loss           | 1.03e+03  |
---------------------------------------
Eval num_timesteps=226000, episode_reward=-270.74 +/- 33.12
Episode length: 46.74 +/- 16.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-277.95 +/- 29.43
Episode length: 51.36 +/- 15.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=-273.62 +/- 33.09
Episode length: 48.94 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 111      |
|    time_elapsed    | 4522     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=-285.14 +/- 24.74
Episode length: 51.40 +/- 17.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -285      |
| time/                   |           |
|    total_timesteps      | 227500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.32e-12 |
|    explained_variance   | 0.00968   |
|    learning_rate        | 0.001     |
|    loss                 | 337       |
|    n_updates            | 745       |
|    policy_gradient_loss | -1.3e-09  |
|    value_loss           | 818       |
---------------------------------------
Eval num_timesteps=228000, episode_reward=-280.83 +/- 23.61
Episode length: 50.22 +/- 15.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-279.39 +/- 30.83
Episode length: 50.02 +/- 15.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-275.79 +/- 25.46
Episode length: 53.02 +/- 18.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 112      |
|    time_elapsed    | 4578     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=-278.90 +/- 32.66
Episode length: 48.26 +/- 17.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.3      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 229500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-09 |
|    explained_variance   | 0.0201    |
|    learning_rate        | 0.001     |
|    loss                 | 483       |
|    n_updates            | 755       |
|    policy_gradient_loss | -2.63e-09 |
|    value_loss           | 887       |
---------------------------------------
Eval num_timesteps=230000, episode_reward=-275.55 +/- 26.00
Episode length: 49.00 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-273.61 +/- 27.27
Episode length: 50.54 +/- 18.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-280.83 +/- 27.03
Episode length: 45.98 +/- 13.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 113      |
|    time_elapsed    | 4612     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=-279.14 +/- 25.41
Episode length: 48.50 +/- 13.93
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 48.5     |
|    mean_reward          | -279     |
| time/                   |          |
|    total_timesteps      | 231500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -4.1e-12 |
|    explained_variance   | 0.00975  |
|    learning_rate        | 0.001    |
|    loss                 | 250      |
|    n_updates            | 765      |
|    policy_gradient_loss | 9.94e-10 |
|    value_loss           | 762      |
--------------------------------------
Eval num_timesteps=232000, episode_reward=-279.39 +/- 27.16
Episode length: 51.78 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-274.33 +/- 31.29
Episode length: 46.06 +/- 15.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-284.91 +/- 27.35
Episode length: 52.24 +/- 15.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 114      |
|    time_elapsed    | 4647     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=-278.42 +/- 25.44
Episode length: 51.02 +/- 15.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -278      |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.97e-10 |
|    explained_variance   | 0.0235    |
|    learning_rate        | 0.001     |
|    loss                 | 518       |
|    n_updates            | 775       |
|    policy_gradient_loss | 6.98e-10  |
|    value_loss           | 1.04e+03  |
---------------------------------------
Eval num_timesteps=234000, episode_reward=-277.71 +/- 32.85
Episode length: 48.14 +/- 16.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-278.43 +/- 24.97
Episode length: 52.64 +/- 17.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-281.55 +/- 25.74
Episode length: 54.16 +/- 15.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-283.46 +/- 26.94
Episode length: 49.28 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 115      |
|    time_elapsed    | 4731     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=-276.26 +/- 28.08
Episode length: 52.68 +/- 17.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.7      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.41e-12 |
|    explained_variance   | 0.00664   |
|    learning_rate        | 0.001     |
|    loss                 | 344       |
|    n_updates            | 785       |
|    policy_gradient_loss | 2.03e-09  |
|    value_loss           | 684       |
---------------------------------------
Eval num_timesteps=236500, episode_reward=-279.61 +/- 24.20
Episode length: 50.50 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-276.51 +/- 32.73
Episode length: 52.20 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=-278.42 +/- 29.12
Episode length: 51.06 +/- 15.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.7     |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 116      |
|    time_elapsed    | 4793     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=-281.77 +/- 25.26
Episode length: 48.96 +/- 16.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49        |
|    mean_reward          | -282      |
| time/                   |           |
|    total_timesteps      | 238000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-09 |
|    explained_variance   | 0.0124    |
|    learning_rate        | 0.001     |
|    loss                 | 490       |
|    n_updates            | 795       |
|    policy_gradient_loss | 4.61e-09  |
|    value_loss           | 962       |
---------------------------------------
Eval num_timesteps=238500, episode_reward=-280.58 +/- 27.27
Episode length: 48.70 +/- 21.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-277.46 +/- 27.82
Episode length: 49.62 +/- 14.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-273.63 +/- 26.62
Episode length: 49.68 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.7     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 117      |
|    time_elapsed    | 4831     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-279.39 +/- 23.26
Episode length: 50.46 +/- 17.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.5      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.15e-12 |
|    explained_variance   | 0.0073    |
|    learning_rate        | 0.001     |
|    loss                 | 385       |
|    n_updates            | 805       |
|    policy_gradient_loss | -5.76e-10 |
|    value_loss           | 668       |
---------------------------------------
Eval num_timesteps=240500, episode_reward=-279.39 +/- 34.02
Episode length: 49.22 +/- 14.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-276.51 +/- 25.50
Episode length: 45.74 +/- 14.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=-278.91 +/- 24.73
Episode length: 47.36 +/- 16.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.7     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 118      |
|    time_elapsed    | 4870     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-277.71 +/- 23.06
Episode length: 50.44 +/- 17.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -278      |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.15e-09 |
|    explained_variance   | 0.0212    |
|    learning_rate        | 0.001     |
|    loss                 | 433       |
|    n_updates            | 815       |
|    policy_gradient_loss | 2.85e-10  |
|    value_loss           | 855       |
---------------------------------------
Eval num_timesteps=242500, episode_reward=-283.71 +/- 25.46
Episode length: 49.84 +/- 13.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-280.09 +/- 25.23
Episode length: 50.64 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=-271.47 +/- 28.39
Episode length: 48.08 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.7     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 119      |
|    time_elapsed    | 4918     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=-282.50 +/- 27.44
Episode length: 46.62 +/- 13.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.6      |
|    mean_reward          | -282      |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.86e-12 |
|    explained_variance   | 0.0136    |
|    learning_rate        | 0.001     |
|    loss                 | 389       |
|    n_updates            | 825       |
|    policy_gradient_loss | -1.22e-09 |
|    value_loss           | 736       |
---------------------------------------
Eval num_timesteps=244500, episode_reward=-278.42 +/- 26.55
Episode length: 50.28 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-283.23 +/- 22.04
Episode length: 54.48 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=-275.07 +/- 33.51
Episode length: 50.08 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 120      |
|    time_elapsed    | 4955     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-281.30 +/- 23.18
Episode length: 48.56 +/- 18.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.6      |
|    mean_reward          | -281      |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.69e-09 |
|    explained_variance   | 0.0214    |
|    learning_rate        | 0.001     |
|    loss                 | 409       |
|    n_updates            | 835       |
|    policy_gradient_loss | -3.13e-09 |
|    value_loss           | 776       |
---------------------------------------
Eval num_timesteps=246500, episode_reward=-280.11 +/- 27.41
Episode length: 51.46 +/- 16.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-276.03 +/- 32.78
Episode length: 49.00 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=-273.15 +/- 30.67
Episode length: 50.58 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 121      |
|    time_elapsed    | 5011     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=-279.14 +/- 26.95
Episode length: 51.12 +/- 18.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.1      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-11 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.001     |
|    loss                 | 550       |
|    n_updates            | 845       |
|    policy_gradient_loss | 2.04e-11  |
|    value_loss           | 791       |
---------------------------------------
Eval num_timesteps=248500, episode_reward=-279.64 +/- 31.82
Episode length: 52.80 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-279.38 +/- 27.99
Episode length: 48.98 +/- 13.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=-279.87 +/- 30.80
Episode length: 46.82 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 122      |
|    time_elapsed    | 5045     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-289.47 +/- 21.23
Episode length: 52.54 +/- 19.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.5      |
|    mean_reward          | -289      |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.71e-09 |
|    explained_variance   | 0.0182    |
|    learning_rate        | 0.001     |
|    loss                 | 350       |
|    n_updates            | 855       |
|    policy_gradient_loss | -3e-10    |
|    value_loss           | 755       |
---------------------------------------
Eval num_timesteps=250500, episode_reward=-273.87 +/- 28.44
Episode length: 51.22 +/- 15.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-276.50 +/- 31.76
Episode length: 51.98 +/- 20.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=-268.34 +/- 33.72
Episode length: 44.14 +/- 15.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.1     |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 123      |
|    time_elapsed    | 5077     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=-278.66 +/- 28.11
Episode length: 50.48 +/- 18.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.5      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 252000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.82e-11 |
|    explained_variance   | 0.014     |
|    learning_rate        | 0.001     |
|    loss                 | 297       |
|    n_updates            | 865       |
|    policy_gradient_loss | 4.48e-10  |
|    value_loss           | 779       |
---------------------------------------
Eval num_timesteps=252500, episode_reward=-268.59 +/- 34.21
Episode length: 48.26 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-276.03 +/- 27.95
Episode length: 48.12 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=-272.90 +/- 33.68
Episode length: 48.26 +/- 16.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 124      |
|    time_elapsed    | 5109     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=-281.30 +/- 27.83
Episode length: 46.74 +/- 15.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.7      |
|    mean_reward          | -281      |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.97e-09 |
|    explained_variance   | 0.0164    |
|    learning_rate        | 0.001     |
|    loss                 | 376       |
|    n_updates            | 875       |
|    policy_gradient_loss | -1.79e-09 |
|    value_loss           | 804       |
---------------------------------------
Eval num_timesteps=254500, episode_reward=-281.31 +/- 33.46
Episode length: 50.60 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-283.95 +/- 24.83
Episode length: 50.98 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=-278.41 +/- 25.55
Episode length: 47.36 +/- 16.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-281.07 +/- 21.54
Episode length: 46.42 +/- 14.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 125      |
|    time_elapsed    | 5149     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=-279.38 +/- 23.75
Episode length: 47.62 +/- 15.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.6      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 256500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.51e-11 |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.001     |
|    loss                 | 230       |
|    n_updates            | 885       |
|    policy_gradient_loss | -7.31e-10 |
|    value_loss           | 806       |
---------------------------------------
Eval num_timesteps=257000, episode_reward=-278.19 +/- 30.29
Episode length: 51.98 +/- 14.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-274.59 +/- 28.73
Episode length: 47.72 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-276.03 +/- 32.35
Episode length: 47.88 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 126      |
|    time_elapsed    | 5182     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=-278.67 +/- 29.22
Episode length: 47.86 +/- 16.05
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 47.9     |
|    mean_reward          | -279     |
| time/                   |          |
|    total_timesteps      | 258500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.1e-08 |
|    explained_variance   | 0.0239   |
|    learning_rate        | 0.001    |
|    loss                 | 336      |
|    n_updates            | 895      |
|    policy_gradient_loss | 1.37e-09 |
|    value_loss           | 777      |
--------------------------------------
Eval num_timesteps=259000, episode_reward=-282.75 +/- 29.22
Episode length: 50.60 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=-277.94 +/- 26.87
Episode length: 46.82 +/- 16.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-283.48 +/- 22.51
Episode length: 51.08 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 127      |
|    time_elapsed    | 5214     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=-283.47 +/- 26.51
Episode length: 47.58 +/- 13.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.6      |
|    mean_reward          | -283      |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.36e-11 |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.001     |
|    loss                 | 184       |
|    n_updates            | 905       |
|    policy_gradient_loss | -5.24e-10 |
|    value_loss           | 720       |
---------------------------------------
Eval num_timesteps=261000, episode_reward=-280.34 +/- 24.39
Episode length: 49.94 +/- 18.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-285.86 +/- 20.70
Episode length: 51.24 +/- 18.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=-270.99 +/- 34.01
Episode length: 48.62 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.3     |
|    ep_rew_mean     | -282     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 128      |
|    time_elapsed    | 5247     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=-279.15 +/- 31.56
Episode length: 48.18 +/- 14.15
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 48.2     |
|    mean_reward          | -279     |
| time/                   |          |
|    total_timesteps      | 262500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.5e-08 |
|    explained_variance   | 0.0237   |
|    learning_rate        | 0.001    |
|    loss                 | 298      |
|    n_updates            | 915      |
|    policy_gradient_loss | 2.62e-11 |
|    value_loss           | 740      |
--------------------------------------
Eval num_timesteps=263000, episode_reward=-279.14 +/- 28.49
Episode length: 52.04 +/- 17.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-274.35 +/- 31.12
Episode length: 54.00 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-270.51 +/- 32.79
Episode length: 47.98 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.3     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 129      |
|    time_elapsed    | 5284     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=-276.02 +/- 30.71
Episode length: 50.22 +/- 18.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.2      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.03e-11 |
|    explained_variance   | 0.00941   |
|    learning_rate        | 0.001     |
|    loss                 | 311       |
|    n_updates            | 925       |
|    policy_gradient_loss | 2.41e-09  |
|    value_loss           | 790       |
---------------------------------------
Eval num_timesteps=265000, episode_reward=-276.74 +/- 26.08
Episode length: 48.96 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-280.09 +/- 26.57
Episode length: 53.10 +/- 20.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=-284.42 +/- 27.04
Episode length: 51.88 +/- 18.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 130      |
|    time_elapsed    | 5329     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=-272.20 +/- 33.21
Episode length: 52.32 +/- 15.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.3      |
|    mean_reward          | -272      |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.21e-08 |
|    explained_variance   | 0.0312    |
|    learning_rate        | 0.001     |
|    loss                 | 502       |
|    n_updates            | 935       |
|    policy_gradient_loss | 4.02e-10  |
|    value_loss           | 718       |
---------------------------------------
Eval num_timesteps=267000, episode_reward=-283.46 +/- 25.86
Episode length: 46.38 +/- 14.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-272.43 +/- 28.50
Episode length: 48.94 +/- 14.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=-275.55 +/- 28.52
Episode length: 48.24 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 131      |
|    time_elapsed    | 5371     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=-276.99 +/- 27.37
Episode length: 50.18 +/- 15.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.2      |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 268500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-10 |
|    explained_variance   | 0.00713   |
|    learning_rate        | 0.001     |
|    loss                 | 522       |
|    n_updates            | 945       |
|    policy_gradient_loss | -8.53e-10 |
|    value_loss           | 741       |
---------------------------------------
Eval num_timesteps=269000, episode_reward=-276.87 +/- 30.28
Episode length: 50.90 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-274.59 +/- 23.55
Episode length: 48.72 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-276.75 +/- 26.74
Episode length: 49.50 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -271     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 132      |
|    time_elapsed    | 5420     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=-275.31 +/- 24.61
Episode length: 51.84 +/- 15.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.8      |
|    mean_reward          | -275      |
| time/                   |           |
|    total_timesteps      | 270500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3e-08    |
|    explained_variance   | 0.0198    |
|    learning_rate        | 0.001     |
|    loss                 | 394       |
|    n_updates            | 955       |
|    policy_gradient_loss | -1.08e-09 |
|    value_loss           | 742       |
---------------------------------------
Eval num_timesteps=271000, episode_reward=-282.98 +/- 26.72
Episode length: 49.84 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-282.99 +/- 23.01
Episode length: 53.44 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=-270.41 +/- 35.34
Episode length: 47.24 +/- 14.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 133      |
|    time_elapsed    | 5458     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=-272.17 +/- 32.52
Episode length: 45.30 +/- 16.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.3      |
|    mean_reward          | -272      |
| time/                   |           |
|    total_timesteps      | 272500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.46e-10 |
|    explained_variance   | 0.011     |
|    learning_rate        | 0.001     |
|    loss                 | 677       |
|    n_updates            | 965       |
|    policy_gradient_loss | 9.2e-10   |
|    value_loss           | 854       |
---------------------------------------
Eval num_timesteps=273000, episode_reward=-277.23 +/- 26.65
Episode length: 50.08 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-276.03 +/- 24.67
Episode length: 49.84 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=-276.98 +/- 24.26
Episode length: 49.98 +/- 17.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 134      |
|    time_elapsed    | 5491     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=-281.31 +/- 23.30
Episode length: 53.72 +/- 15.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.7      |
|    mean_reward          | -281      |
| time/                   |           |
|    total_timesteps      | 274500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.66e-08 |
|    explained_variance   | 0.0264    |
|    learning_rate        | 0.001     |
|    loss                 | 389       |
|    n_updates            | 975       |
|    policy_gradient_loss | -1.84e-09 |
|    value_loss           | 734       |
---------------------------------------
Eval num_timesteps=275000, episode_reward=-275.31 +/- 29.31
Episode length: 46.32 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-280.58 +/- 24.84
Episode length: 48.72 +/- 13.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-275.78 +/- 35.73
Episode length: 46.82 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -282     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 135      |
|    time_elapsed    | 5523     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=-273.88 +/- 33.89
Episode length: 52.84 +/- 18.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.8      |
|    mean_reward          | -274      |
| time/                   |           |
|    total_timesteps      | 276500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.75e-10 |
|    explained_variance   | 0.00591   |
|    learning_rate        | 0.001     |
|    loss                 | 398       |
|    n_updates            | 985       |
|    policy_gradient_loss | -2.03e-09 |
|    value_loss           | 657       |
---------------------------------------
Eval num_timesteps=277000, episode_reward=-280.36 +/- 28.33
Episode length: 56.44 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.4     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-272.90 +/- 27.56
Episode length: 49.14 +/- 19.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-281.79 +/- 28.58
Episode length: 50.06 +/- 18.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=-280.83 +/- 25.71
Episode length: 49.94 +/- 15.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 136      |
|    time_elapsed    | 5582     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=-277.95 +/- 22.93
Episode length: 46.98 +/- 17.93
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 47       |
|    mean_reward          | -278     |
| time/                   |          |
|    total_timesteps      | 279000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.2e-08 |
|    explained_variance   | 0.0265   |
|    learning_rate        | 0.001    |
|    loss                 | 516      |
|    n_updates            | 995      |
|    policy_gradient_loss | 2.07e-10 |
|    value_loss           | 771      |
--------------------------------------
Eval num_timesteps=279500, episode_reward=-275.06 +/- 30.54
Episode length: 45.38 +/- 13.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-280.11 +/- 29.14
Episode length: 50.06 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=-278.43 +/- 28.22
Episode length: 52.74 +/- 18.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 137      |
|    time_elapsed    | 5638     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=-275.77 +/- 25.57
Episode length: 45.52 +/- 15.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.5      |
|    mean_reward          | -276      |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.49e-10 |
|    explained_variance   | 0.00821   |
|    learning_rate        | 0.001     |
|    loss                 | 356       |
|    n_updates            | 1005      |
|    policy_gradient_loss | -8.85e-10 |
|    value_loss           | 804       |
---------------------------------------
Eval num_timesteps=281500, episode_reward=-275.78 +/- 32.95
Episode length: 51.00 +/- 19.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=-276.27 +/- 25.48
Episode length: 50.96 +/- 13.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=-274.83 +/- 34.67
Episode length: 48.22 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -276     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 138      |
|    time_elapsed    | 5677     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=-274.11 +/- 26.70
Episode length: 51.48 +/- 21.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.5      |
|    mean_reward          | -274      |
| time/                   |           |
|    total_timesteps      | 283000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.74e-08 |
|    explained_variance   | 0.0236    |
|    learning_rate        | 0.001     |
|    loss                 | 238       |
|    n_updates            | 1015      |
|    policy_gradient_loss | 7.19e-10  |
|    value_loss           | 712       |
---------------------------------------
Eval num_timesteps=283500, episode_reward=-280.58 +/- 22.27
Episode length: 49.62 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=-280.10 +/- 27.20
Episode length: 46.82 +/- 15.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=-275.07 +/- 24.93
Episode length: 49.88 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 139      |
|    time_elapsed    | 5709     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=-272.91 +/- 31.19
Episode length: 44.88 +/- 15.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 44.9      |
|    mean_reward          | -273      |
| time/                   |           |
|    total_timesteps      | 285000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.54e-10 |
|    explained_variance   | 0.00799   |
|    learning_rate        | 0.001     |
|    loss                 | 420       |
|    n_updates            | 1025      |
|    policy_gradient_loss | 1.32e-09  |
|    value_loss           | 765       |
---------------------------------------
Eval num_timesteps=285500, episode_reward=-275.54 +/- 33.64
Episode length: 48.90 +/- 15.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=-275.55 +/- 26.43
Episode length: 50.20 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=-278.65 +/- 26.98
Episode length: 48.74 +/- 17.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.2     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 140      |
|    time_elapsed    | 5739     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=-273.14 +/- 32.57
Episode length: 47.86 +/- 17.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.9      |
|    mean_reward          | -273      |
| time/                   |           |
|    total_timesteps      | 287000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-07 |
|    explained_variance   | 0.0287    |
|    learning_rate        | 0.001     |
|    loss                 | 330       |
|    n_updates            | 1035      |
|    policy_gradient_loss | 1.05e-09  |
|    value_loss           | 658       |
---------------------------------------
Eval num_timesteps=287500, episode_reward=-276.85 +/- 28.73
Episode length: 47.26 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-268.58 +/- 27.78
Episode length: 49.50 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-274.58 +/- 26.87
Episode length: 50.14 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 141      |
|    time_elapsed    | 5772     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=-278.91 +/- 28.11
Episode length: 53.22 +/- 19.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.2      |
|    mean_reward          | -279      |
| time/                   |           |
|    total_timesteps      | 289000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.14e-10 |
|    explained_variance   | 0.0085    |
|    learning_rate        | 0.001     |
|    loss                 | 199       |
|    n_updates            | 1045      |
|    policy_gradient_loss | 2.23e-09  |
|    value_loss           | 796       |
---------------------------------------
Eval num_timesteps=289500, episode_reward=-276.99 +/- 28.81
Episode length: 46.40 +/- 14.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-281.06 +/- 28.97
Episode length: 47.50 +/- 12.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=-276.02 +/- 29.94
Episode length: 48.56 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -280     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 142      |
|    time_elapsed    | 5805     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=-272.91 +/- 27.86
Episode length: 49.76 +/- 16.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -273      |
| time/                   |           |
|    total_timesteps      | 291000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-07 |
|    explained_variance   | 0.0236    |
|    learning_rate        | 0.001     |
|    loss                 | 292       |
|    n_updates            | 1055      |
|    policy_gradient_loss | -6.98e-11 |
|    value_loss           | 659       |
---------------------------------------
Eval num_timesteps=291500, episode_reward=-276.98 +/- 26.08
Episode length: 49.74 +/- 19.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=-278.91 +/- 22.91
Episode length: 48.60 +/- 14.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=-281.55 +/- 24.71
Episode length: 52.10 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 143      |
|    time_elapsed    | 5838     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=-282.75 +/- 28.83
Episode length: 51.42 +/- 18.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -283      |
| time/                   |           |
|    total_timesteps      | 293000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.53e-10 |
|    explained_variance   | 0.00617   |
|    learning_rate        | 0.001     |
|    loss                 | 258       |
|    n_updates            | 1065      |
|    policy_gradient_loss | 8.73e-12  |
|    value_loss           | 763       |
---------------------------------------
Eval num_timesteps=293500, episode_reward=-281.54 +/- 25.75
Episode length: 51.82 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-279.15 +/- 30.07
Episode length: 52.98 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=-274.59 +/- 31.61
Episode length: 49.86 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 144      |
|    time_elapsed    | 5872     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=-280.83 +/- 24.80
Episode length: 52.70 +/- 15.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.7      |
|    mean_reward          | -281      |
| time/                   |           |
|    total_timesteps      | 295000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.31e-07 |
|    explained_variance   | 0.0249    |
|    learning_rate        | 0.001     |
|    loss                 | 474       |
|    n_updates            | 1075      |
|    policy_gradient_loss | -1.32e-09 |
|    value_loss           | 671       |
---------------------------------------
Eval num_timesteps=295500, episode_reward=-277.22 +/- 28.20
Episode length: 46.72 +/- 13.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=-283.22 +/- 23.44
Episode length: 47.94 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=-272.90 +/- 28.98
Episode length: 49.12 +/- 19.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 145      |
|    time_elapsed    | 5905     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=-275.07 +/- 24.10
Episode length: 53.08 +/- 14.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.1      |
|    mean_reward          | -275      |
| time/                   |           |
|    total_timesteps      | 297000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.16e-10 |
|    explained_variance   | 0.0127    |
|    learning_rate        | 0.001     |
|    loss                 | 353       |
|    n_updates            | 1085      |
|    policy_gradient_loss | -5.97e-10 |
|    value_loss           | 814       |
---------------------------------------
Eval num_timesteps=297500, episode_reward=-278.67 +/- 29.23
Episode length: 52.80 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-280.59 +/- 22.91
Episode length: 54.22 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=-282.26 +/- 28.91
Episode length: 47.78 +/- 14.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-279.39 +/- 26.50
Episode length: 49.76 +/- 14.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -278     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 146      |
|    time_elapsed    | 5945     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=-282.27 +/- 22.52
Episode length: 54.22 +/- 20.05
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 54.2     |
|    mean_reward          | -282     |
| time/                   |          |
|    total_timesteps      | 299500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.6e-07 |
|    explained_variance   | 0.0273   |
|    learning_rate        | 0.001    |
|    loss                 | 264      |
|    n_updates            | 1095     |
|    policy_gradient_loss | -1.7e-09 |
|    value_loss           | 630      |
--------------------------------------
Eval num_timesteps=300000, episode_reward=-279.63 +/- 27.86
Episode length: 49.40 +/- 17.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-275.79 +/- 29.55
Episode length: 50.72 +/- 20.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=-270.50 +/- 31.18
Episode length: 49.36 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 147      |
|    time_elapsed    | 5978     |
|    total_timesteps | 301056   |
---------------------------------
