/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-525.17 +/- 68.95
Episode length: 49.82 +/- 15.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-515.38 +/- 88.09
Episode length: 50.92 +/- 15.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=-531.69 +/- 62.02
Episode length: 52.46 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-516.58 +/- 64.33
Episode length: 48.40 +/- 17.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97       |
|    ep_rew_mean     | -377     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=830.28 +/- 710.65
Episode length: 34.48 +/- 7.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.5        |
|    mean_reward          | 830         |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.009548645 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -7.26e-05   |
|    learning_rate        | 0.0001      |
|    loss                 | 1.35e+03    |
|    n_updates            | 199         |
|    policy_gradient_loss | -0.00955    |
|    value_loss           | 4.61e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=793.51 +/- 655.49
Episode length: 35.02 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=891.34 +/- 671.90
Episode length: 36.06 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=790.51 +/- 678.47
Episode length: 34.92 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -377     |
| time/              |          |
|    fps             | 262      |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=726.16 +/- 591.72
Episode length: 35.32 +/- 6.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 726          |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 0.0042116228 |
|    clip_fraction        | 0.0425       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -2.05        |
|    explained_variance   | 8.14e-05     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.51e+03     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00274     |
|    value_loss           | 7.94e+03     |
------------------------------------------
Eval num_timesteps=5000, episode_reward=583.20 +/- 572.62
Episode length: 32.72 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.7     |
|    mean_reward     | 583      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=818.22 +/- 678.58
Episode length: 36.02 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=867.42 +/- 691.97
Episode length: 35.52 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | -360     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 3        |
|    time_elapsed    | 22       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=908.21 +/- 690.18
Episode length: 36.00 +/- 5.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36          |
|    mean_reward          | 908         |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.004760683 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.000657    |
|    learning_rate        | 0.0001      |
|    loss                 | 2.69e+03    |
|    n_updates            | 201         |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 6.18e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=7000, episode_reward=881.39 +/- 739.01
Episode length: 34.74 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=896.88 +/- 665.97
Episode length: 36.42 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=1008.45 +/- 749.76
Episode length: 36.44 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 59       |
|    ep_rew_mean     | -322     |
| time/              |          |
|    fps             | 281      |
|    iterations      | 4        |
|    time_elapsed    | 29       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=8500, episode_reward=868.42 +/- 714.04
Episode length: 35.30 +/- 6.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 868          |
| time/                   |              |
|    total_timesteps      | 8500         |
| train/                  |              |
|    approx_kl            | 0.0069324523 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.000809     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.66e+03     |
|    n_updates            | 202          |
|    policy_gradient_loss | -0.00725     |
|    value_loss           | 5.91e+03     |
------------------------------------------
Eval num_timesteps=9000, episode_reward=829.65 +/- 666.53
Episode length: 35.72 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=838.61 +/- 682.23
Episode length: 35.44 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=787.67 +/- 648.92
Episode length: 35.34 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -249     |
| time/              |          |
|    fps             | 286      |
|    iterations      | 5        |
|    time_elapsed    | 35       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=10500, episode_reward=938.30 +/- 750.54
Episode length: 35.30 +/- 6.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.3        |
|    mean_reward          | 938         |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.004951854 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.000492    |
|    learning_rate        | 0.0001      |
|    loss                 | 3.47e+03    |
|    n_updates            | 203         |
|    policy_gradient_loss | -0.00736    |
|    value_loss           | 8.24e+03    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=828.26 +/- 643.45
Episode length: 35.94 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=671.44 +/- 622.87
Episode length: 33.82 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 671      |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=893.53 +/- 685.27
Episode length: 35.94 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.4     |
|    ep_rew_mean     | -199     |
| time/              |          |
|    fps             | 290      |
|    iterations      | 6        |
|    time_elapsed    | 42       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=12500, episode_reward=887.75 +/- 674.93
Episode length: 36.20 +/- 5.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 888          |
| time/                   |              |
|    total_timesteps      | 12500        |
| train/                  |              |
|    approx_kl            | 0.0073518776 |
|    clip_fraction        | 0.099        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | -0.000313    |
|    learning_rate        | 0.0001       |
|    loss                 | 3.88e+03     |
|    n_updates            | 204          |
|    policy_gradient_loss | -0.00523     |
|    value_loss           | 6.34e+03     |
------------------------------------------
Eval num_timesteps=13000, episode_reward=693.94 +/- 563.06
Episode length: 34.88 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=739.00 +/- 679.33
Episode length: 33.86 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=716.67 +/- 634.77
Episode length: 34.34 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.7     |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 292      |
|    iterations      | 7        |
|    time_elapsed    | 48       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=14500, episode_reward=903.77 +/- 748.15
Episode length: 34.98 +/- 7.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 904         |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.008454221 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.00066     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.24e+03    |
|    n_updates            | 205         |
|    policy_gradient_loss | -0.000669   |
|    value_loss           | 8.01e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=588.45 +/- 557.03
Episode length: 32.82 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 588      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=1028.11 +/- 750.21
Episode length: 36.06 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
New best mean reward!
Eval num_timesteps=16000, episode_reward=872.08 +/- 701.82
Episode length: 35.54 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | -65.6    |
| time/              |          |
|    fps             | 294      |
|    iterations      | 8        |
|    time_elapsed    | 55       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=16500, episode_reward=1080.47 +/- 736.71
Episode length: 37.14 +/- 6.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.1        |
|    mean_reward          | 1.08e+03    |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.007047575 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.00238     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.14e+03    |
|    n_updates            | 206         |
|    policy_gradient_loss | 0.0027      |
|    value_loss           | 6.75e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=17000, episode_reward=815.93 +/- 657.15
Episode length: 35.54 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=699.50 +/- 617.14
Episode length: 34.74 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=766.22 +/- 651.45
Episode length: 35.04 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.6     |
|    ep_rew_mean     | 5.33     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 9        |
|    time_elapsed    | 62       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=18500, episode_reward=943.51 +/- 727.66
Episode length: 36.62 +/- 6.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.6        |
|    mean_reward          | 944         |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.006254511 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.000762    |
|    learning_rate        | 0.0001      |
|    loss                 | 4.08e+03    |
|    n_updates            | 207         |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 1.38e+04    |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=722.80 +/- 597.41
Episode length: 35.34 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=740.35 +/- 618.35
Episode length: 34.46 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=635.22 +/- 531.27
Episode length: 33.96 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 73.3     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 10       |
|    time_elapsed    | 68       |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=715.01 +/- 631.29
Episode length: 33.94 +/- 6.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 33.9       |
|    mean_reward          | 715        |
| time/                   |            |
|    total_timesteps      | 20500      |
| train/                  |            |
|    approx_kl            | 0.00479189 |
|    clip_fraction        | 0.0798     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.962     |
|    explained_variance   | 0.00097    |
|    learning_rate        | 0.0001     |
|    loss                 | 5.9e+03    |
|    n_updates            | 208        |
|    policy_gradient_loss | -0.00488   |
|    value_loss           | 9.76e+03   |
----------------------------------------
Eval num_timesteps=21000, episode_reward=767.52 +/- 673.00
Episode length: 33.80 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=778.88 +/- 600.51
Episode length: 35.76 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=854.90 +/- 685.21
Episode length: 35.28 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=852.85 +/- 682.24
Episode length: 35.40 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.4     |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 11       |
|    time_elapsed    | 76       |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=23000, episode_reward=779.50 +/- 633.13
Episode length: 35.42 +/- 6.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 780         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.012604728 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.799      |
|    explained_variance   | 0.00198     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.69e+03    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.000564   |
|    value_loss           | 1.31e+04    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=755.74 +/- 691.85
Episode length: 33.80 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=921.66 +/- 759.49
Episode length: 35.24 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=619.48 +/- 607.01
Episode length: 32.68 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.7     |
|    mean_reward     | 619      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.4     |
|    ep_rew_mean     | 141      |
| time/              |          |
|    fps             | 294      |
|    iterations      | 12       |
|    time_elapsed    | 83       |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=25000, episode_reward=972.81 +/- 669.95
Episode length: 37.20 +/- 5.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.2         |
|    mean_reward          | 973          |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0027241881 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.721       |
|    explained_variance   | 0.00181      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.66e+03     |
|    n_updates            | 211          |
|    policy_gradient_loss | 0.000545     |
|    value_loss           | 1.56e+04     |
------------------------------------------
Eval num_timesteps=25500, episode_reward=861.12 +/- 677.92
Episode length: 35.68 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=776.91 +/- 696.83
Episode length: 34.62 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=885.40 +/- 728.20
Episode length: 35.30 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.6     |
|    ep_rew_mean     | 210      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 13       |
|    time_elapsed    | 90       |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=27000, episode_reward=685.41 +/- 659.73
Episode length: 33.02 +/- 7.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33          |
|    mean_reward          | 685         |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.004723794 |
|    clip_fraction        | 0.057       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.00176     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.18e+03    |
|    n_updates            | 212         |
|    policy_gradient_loss | -0.0051     |
|    value_loss           | 1.49e+04    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=862.46 +/- 720.25
Episode length: 35.16 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=794.32 +/- 692.61
Episode length: 34.90 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=767.87 +/- 630.24
Episode length: 35.10 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.1     |
|    ep_rew_mean     | 304      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 14       |
|    time_elapsed    | 96       |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=29000, episode_reward=822.36 +/- 671.26
Episode length: 34.78 +/- 6.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 822          |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0045564026 |
|    clip_fraction        | 0.0503       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.343       |
|    explained_variance   | 0.000736     |
|    learning_rate        | 0.0001       |
|    loss                 | 7.92e+03     |
|    n_updates            | 216          |
|    policy_gradient_loss | -0.00309     |
|    value_loss           | 2.49e+04     |
------------------------------------------
Eval num_timesteps=29500, episode_reward=761.92 +/- 633.44
Episode length: 34.80 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=700.14 +/- 625.66
Episode length: 33.70 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=838.16 +/- 726.18
Episode length: 34.94 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33       |
|    ep_rew_mean     | 480      |
| time/              |          |
|    fps             | 297      |
|    iterations      | 15       |
|    time_elapsed    | 103      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=31000, episode_reward=787.46 +/- 686.39
Episode length: 34.30 +/- 6.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.3        |
|    mean_reward          | 787         |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.011674704 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | -0.000136   |
|    learning_rate        | 0.0001      |
|    loss                 | 1.44e+04    |
|    n_updates            | 219         |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 4.29e+04    |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=823.56 +/- 723.88
Episode length: 34.22 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=692.31 +/- 637.29
Episode length: 33.42 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=892.88 +/- 742.52
Episode length: 34.96 +/- 8.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 623      |
| time/              |          |
|    fps             | 297      |
|    iterations      | 16       |
|    time_elapsed    | 109      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=33000, episode_reward=898.39 +/- 741.77
Episode length: 35.50 +/- 7.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 898          |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0053085396 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | -0.000131    |
|    learning_rate        | 0.0001       |
|    loss                 | 1.15e+04     |
|    n_updates            | 225          |
|    policy_gradient_loss | -0.000786    |
|    value_loss           | 4.56e+04     |
------------------------------------------
Eval num_timesteps=33500, episode_reward=879.37 +/- 679.43
Episode length: 36.46 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=839.94 +/- 701.56
Episode length: 34.80 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=921.76 +/- 694.06
Episode length: 36.38 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 663      |
| time/              |          |
|    fps             | 297      |
|    iterations      | 17       |
|    time_elapsed    | 117      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=35000, episode_reward=886.67 +/- 735.68
Episode length: 34.58 +/- 7.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.6        |
|    mean_reward          | 887         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.002367105 |
|    clip_fraction        | 0.00901     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0701     |
|    explained_variance   | -0.000262   |
|    learning_rate        | 0.0001      |
|    loss                 | 1.83e+04    |
|    n_updates            | 226         |
|    policy_gradient_loss | -6.89e-05   |
|    value_loss           | 4.04e+04    |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=837.75 +/- 745.53
Episode length: 34.52 +/- 7.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=1049.80 +/- 691.76
Episode length: 37.76 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=920.75 +/- 745.87
Episode length: 35.48 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 664      |
| time/              |          |
|    fps             | 297      |
|    iterations      | 18       |
|    time_elapsed    | 123      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=37000, episode_reward=675.66 +/- 602.38
Episode length: 34.10 +/- 6.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.1         |
|    mean_reward          | 676          |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0034157773 |
|    clip_fraction        | 0.00438      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0202      |
|    explained_variance   | -7.03e-05    |
|    learning_rate        | 0.0001       |
|    loss                 | 2e+04        |
|    n_updates            | 228          |
|    policy_gradient_loss | -0.000718    |
|    value_loss           | 5.18e+04     |
------------------------------------------
Eval num_timesteps=37500, episode_reward=820.35 +/- 703.15
Episode length: 34.76 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=688.86 +/- 600.25
Episode length: 34.22 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=972.10 +/- 745.83
Episode length: 36.58 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 19       |
|    time_elapsed    | 130      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=39000, episode_reward=798.69 +/- 646.67
Episode length: 35.08 +/- 6.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 799          |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0011591573 |
|    clip_fraction        | 0.00258      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00777     |
|    explained_variance   | -4.83e-05    |
|    learning_rate        | 0.0001       |
|    loss                 | 3.08e+04     |
|    n_updates            | 234          |
|    policy_gradient_loss | -0.000148    |
|    value_loss           | 7.07e+04     |
------------------------------------------
Eval num_timesteps=39500, episode_reward=786.17 +/- 648.62
Episode length: 34.94 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=1088.18 +/- 780.05
Episode length: 36.30 +/- 8.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
Eval num_timesteps=40500, episode_reward=842.60 +/- 719.14
Episode length: 34.54 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 807      |
| time/              |          |
|    fps             | 297      |
|    iterations      | 20       |
|    time_elapsed    | 137      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=872.61 +/- 718.82
Episode length: 35.30 +/- 7.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 873           |
| time/                   |               |
|    total_timesteps      | 41000         |
| train/                  |               |
|    approx_kl            | 3.1272066e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00675      |
|    explained_variance   | 0.000121      |
|    learning_rate        | 0.0001        |
|    loss                 | 1.37e+04      |
|    n_updates            | 244           |
|    policy_gradient_loss | -7.09e-06     |
|    value_loss           | 5.88e+04      |
-------------------------------------------
Eval num_timesteps=41500, episode_reward=752.19 +/- 657.88
Episode length: 34.54 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=882.51 +/- 690.48
Episode length: 36.52 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=737.36 +/- 646.03
Episode length: 34.44 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=797.29 +/- 651.06
Episode length: 35.12 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 294      |
|    iterations      | 21       |
|    time_elapsed    | 145      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=43500, episode_reward=846.90 +/- 680.54
Episode length: 35.28 +/- 6.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 847           |
| time/                   |               |
|    total_timesteps      | 43500         |
| train/                  |               |
|    approx_kl            | 0.00088877557 |
|    clip_fraction        | 0.00185       |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00592      |
|    explained_variance   | 0.000218      |
|    learning_rate        | 0.0001        |
|    loss                 | 3.3e+04       |
|    n_updates            | 246           |
|    policy_gradient_loss | -0.000191     |
|    value_loss           | 4.66e+04      |
-------------------------------------------
Eval num_timesteps=44000, episode_reward=686.12 +/- 589.15
Episode length: 33.68 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=840.97 +/- 684.89
Episode length: 35.34 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=725.11 +/- 624.15
Episode length: 33.68 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 22       |
|    time_elapsed    | 152      |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.03
Eval num_timesteps=45500, episode_reward=869.00 +/- 691.33
Episode length: 35.50 +/- 6.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 869          |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 0.0019057139 |
|    clip_fraction        | 0.000922     |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00243     |
|    explained_variance   | 0.000688     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.69e+04     |
|    n_updates            | 256          |
|    policy_gradient_loss | -7.64e-05    |
|    value_loss           | 6e+04        |
------------------------------------------
Eval num_timesteps=46000, episode_reward=888.40 +/- 688.33
Episode length: 35.88 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=859.44 +/- 745.47
Episode length: 34.52 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=877.92 +/- 717.61
Episode length: 35.06 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 294      |
|    iterations      | 23       |
|    time_elapsed    | 159      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=730.82 +/- 673.57
Episode length: 33.52 +/- 7.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 731       |
| time/                   |           |
|    total_timesteps      | 47500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.000379 |
|    explained_variance   | 0.00115   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.23e+04  |
|    n_updates            | 266       |
|    policy_gradient_loss | 3.58e-06  |
|    value_loss           | 6.8e+04   |
---------------------------------------
Eval num_timesteps=48000, episode_reward=761.11 +/- 630.05
Episode length: 34.66 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=930.71 +/- 664.23
Episode length: 36.26 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=886.36 +/- 674.76
Episode length: 35.98 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 294      |
|    iterations      | 24       |
|    time_elapsed    | 166      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=838.93 +/- 669.54
Episode length: 35.46 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 49500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.42e-05 |
|    explained_variance   | 0.00101   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.42e+04  |
|    n_updates            | 276       |
|    policy_gradient_loss | 7.26e-07  |
|    value_loss           | 4.66e+04  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=716.61 +/- 670.45
Episode length: 33.56 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=971.68 +/- 709.00
Episode length: 36.18 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=875.99 +/- 640.12
Episode length: 36.88 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 25       |
|    time_elapsed    | 174      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=748.76 +/- 611.83
Episode length: 34.98 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 749       |
| time/                   |           |
|    total_timesteps      | 51500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.55e-06 |
|    explained_variance   | 0.00226   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.53e+04  |
|    n_updates            | 286       |
|    policy_gradient_loss | 1.74e-07  |
|    value_loss           | 5.89e+04  |
---------------------------------------
Eval num_timesteps=52000, episode_reward=664.47 +/- 526.76
Episode length: 34.66 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 664      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=920.38 +/- 718.58
Episode length: 35.42 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=642.15 +/- 601.46
Episode length: 32.72 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.7     |
|    mean_reward     | 642      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 870      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 26       |
|    time_elapsed    | 181      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=846.85 +/- 692.58
Episode length: 35.74 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 847       |
| time/                   |           |
|    total_timesteps      | 53500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.02e-08 |
|    explained_variance   | 0.00299   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.59e+04  |
|    n_updates            | 296       |
|    policy_gradient_loss | 6.71e-10  |
|    value_loss           | 5.99e+04  |
---------------------------------------
Eval num_timesteps=54000, episode_reward=887.99 +/- 745.80
Episode length: 35.06 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=793.92 +/- 667.35
Episode length: 34.94 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=808.26 +/- 677.63
Episode length: 34.80 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 900      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 27       |
|    time_elapsed    | 188      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=741.71 +/- 613.36
Episode length: 35.12 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 742       |
| time/                   |           |
|    total_timesteps      | 55500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.98e-10 |
|    explained_variance   | 0.00351   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.25e+04  |
|    n_updates            | 306       |
|    policy_gradient_loss | 2.07e-10  |
|    value_loss           | 4.9e+04   |
---------------------------------------
Eval num_timesteps=56000, episode_reward=753.11 +/- 618.55
Episode length: 34.80 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=756.75 +/- 628.69
Episode length: 35.48 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=1026.24 +/- 757.72
Episode length: 35.90 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 936      |
| time/              |          |
|    fps             | 292      |
|    iterations      | 28       |
|    time_elapsed    | 195      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=708.80 +/- 635.67
Episode length: 34.32 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 709       |
| time/                   |           |
|    total_timesteps      | 57500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.54e-13 |
|    explained_variance   | 0.00636   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.37e+04  |
|    n_updates            | 316       |
|    policy_gradient_loss | -1.02e-09 |
|    value_loss           | 5.28e+04  |
---------------------------------------
Eval num_timesteps=58000, episode_reward=894.24 +/- 721.10
Episode length: 35.42 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=831.23 +/- 673.62
Episode length: 35.48 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=909.76 +/- 712.08
Episode length: 35.50 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 809      |
| time/              |          |
|    fps             | 292      |
|    iterations      | 29       |
|    time_elapsed    | 203      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=841.57 +/- 664.19
Episode length: 36.16 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 842       |
| time/                   |           |
|    total_timesteps      | 59500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.42e-17 |
|    explained_variance   | 0.0036    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.97e+04  |
|    n_updates            | 326       |
|    policy_gradient_loss | 9.46e-10  |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=60000, episode_reward=834.40 +/- 654.15
Episode length: 35.86 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=675.30 +/- 609.24
Episode length: 33.82 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=797.32 +/- 629.06
Episode length: 35.84 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 812      |
| time/              |          |
|    fps             | 292      |
|    iterations      | 30       |
|    time_elapsed    | 210      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=648.42 +/- 589.98
Episode length: 33.88 +/- 6.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 648       |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.95e-15 |
|    explained_variance   | 0.00853   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.29e+04  |
|    n_updates            | 336       |
|    policy_gradient_loss | 1.12e-09  |
|    value_loss           | 3.7e+04   |
---------------------------------------
Eval num_timesteps=62000, episode_reward=913.93 +/- 698.54
Episode length: 36.12 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=769.06 +/- 670.71
Episode length: 34.38 +/- 7.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=916.07 +/- 662.92
Episode length: 37.32 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 291      |
|    iterations      | 31       |
|    time_elapsed    | 217      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=926.55 +/- 725.54
Episode length: 35.06 +/- 7.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 927       |
| time/                   |           |
|    total_timesteps      | 63500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.41e-17 |
|    explained_variance   | 0.00752   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.95e+04  |
|    n_updates            | 346       |
|    policy_gradient_loss | 1.15e-09  |
|    value_loss           | 4.46e+04  |
---------------------------------------
Eval num_timesteps=64000, episode_reward=959.96 +/- 743.80
Episode length: 36.26 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=713.89 +/- 594.92
Episode length: 34.46 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=869.83 +/- 732.21
Episode length: 35.08 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=834.02 +/- 699.02
Episode length: 35.46 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 887      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 32       |
|    time_elapsed    | 226      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=816.52 +/- 688.77
Episode length: 35.26 +/- 6.42
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.3     |
|    mean_reward          | 817      |
| time/                   |          |
|    total_timesteps      | 66000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -7.5e-22 |
|    explained_variance   | 0.0103   |
|    learning_rate        | 0.0001   |
|    loss                 | 2.3e+04  |
|    n_updates            | 356      |
|    policy_gradient_loss | 1.73e-09 |
|    value_loss           | 3.8e+04  |
--------------------------------------
Eval num_timesteps=66500, episode_reward=723.62 +/- 647.93
Episode length: 34.06 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=914.56 +/- 697.34
Episode length: 36.16 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=927.57 +/- 697.71
Episode length: 35.96 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 927      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 33       |
|    time_elapsed    | 233      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=801.75 +/- 663.11
Episode length: 35.46 +/- 6.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 802       |
| time/                   |           |
|    total_timesteps      | 68000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.74e-19 |
|    explained_variance   | 0.0108    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.72e+04  |
|    n_updates            | 366       |
|    policy_gradient_loss | -2.3e-10  |
|    value_loss           | 4.37e+04  |
---------------------------------------
Eval num_timesteps=68500, episode_reward=775.87 +/- 678.81
Episode length: 34.38 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=922.78 +/- 665.91
Episode length: 36.64 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=703.72 +/- 697.41
Episode length: 32.76 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 911      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 34       |
|    time_elapsed    | 240      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=861.07 +/- 684.04
Episode length: 35.58 +/- 5.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 861       |
| time/                   |           |
|    total_timesteps      | 70000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.92e-22 |
|    explained_variance   | 0.0116    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.51e+04  |
|    n_updates            | 376       |
|    policy_gradient_loss | -6.2e-10  |
|    value_loss           | 4.26e+04  |
---------------------------------------
Eval num_timesteps=70500, episode_reward=893.68 +/- 684.37
Episode length: 35.88 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=822.92 +/- 661.75
Episode length: 34.82 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=789.45 +/- 720.99
Episode length: 34.18 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 35       |
|    time_elapsed    | 247      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=796.31 +/- 633.96
Episode length: 35.76 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 796       |
| time/                   |           |
|    total_timesteps      | 72000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.28e-19 |
|    explained_variance   | 0.00855   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.26e+04  |
|    n_updates            | 386       |
|    policy_gradient_loss | -1.16e-09 |
|    value_loss           | 4.54e+04  |
---------------------------------------
Eval num_timesteps=72500, episode_reward=877.77 +/- 717.16
Episode length: 35.46 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=866.58 +/- 690.68
Episode length: 35.86 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=827.76 +/- 697.19
Episode length: 34.80 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 892      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 36       |
|    time_elapsed    | 255      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=763.91 +/- 661.39
Episode length: 34.28 +/- 6.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 764       |
| time/                   |           |
|    total_timesteps      | 74000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.82e-14 |
|    explained_variance   | 0.012     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.75e+04  |
|    n_updates            | 396       |
|    policy_gradient_loss | 4.96e-10  |
|    value_loss           | 3.84e+04  |
---------------------------------------
Eval num_timesteps=74500, episode_reward=659.24 +/- 575.66
Episode length: 34.12 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=955.94 +/- 722.78
Episode length: 36.54 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=956.59 +/- 718.71
Episode length: 36.10 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 855      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 37       |
|    time_elapsed    | 262      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=853.55 +/- 730.58
Episode length: 35.20 +/- 7.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 854       |
| time/                   |           |
|    total_timesteps      | 76000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.96e-16 |
|    explained_variance   | 0.00817   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.31e+04  |
|    n_updates            | 406       |
|    policy_gradient_loss | 6.69e-10  |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=76500, episode_reward=782.95 +/- 629.43
Episode length: 35.80 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=873.99 +/- 744.63
Episode length: 34.68 +/- 8.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=787.51 +/- 709.65
Episode length: 34.14 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 38       |
|    time_elapsed    | 269      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=682.22 +/- 570.29
Episode length: 34.22 +/- 6.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 682       |
| time/                   |           |
|    total_timesteps      | 78000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.29e-14 |
|    explained_variance   | 0.00725   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.28e+04  |
|    n_updates            | 416       |
|    policy_gradient_loss | -5.08e-10 |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=78500, episode_reward=907.34 +/- 679.74
Episode length: 36.36 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=915.08 +/- 644.30
Episode length: 37.06 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=691.86 +/- 576.00
Episode length: 34.50 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 39       |
|    time_elapsed    | 276      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=852.92 +/- 693.75
Episode length: 35.94 +/- 6.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 853       |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.35e-16 |
|    explained_variance   | 0.00667   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.54e+04  |
|    n_updates            | 426       |
|    policy_gradient_loss | -1.48e-09 |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=80500, episode_reward=808.57 +/- 676.06
Episode length: 35.24 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=804.84 +/- 660.44
Episode length: 35.20 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=886.92 +/- 703.85
Episode length: 35.98 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 40       |
|    time_elapsed    | 284      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=947.07 +/- 650.38
Episode length: 37.00 +/- 4.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 947       |
| time/                   |           |
|    total_timesteps      | 82000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.71e-14 |
|    explained_variance   | 0.0102    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.07e+04  |
|    n_updates            | 436       |
|    policy_gradient_loss | 4.18e-10  |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=82500, episode_reward=786.27 +/- 654.33
Episode length: 34.80 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=824.38 +/- 695.71
Episode length: 34.62 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=895.65 +/- 723.99
Episode length: 35.76 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 908      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 41       |
|    time_elapsed    | 291      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=864.31 +/- 699.67
Episode length: 35.52 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 864       |
| time/                   |           |
|    total_timesteps      | 84000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.57e-16 |
|    explained_variance   | 0.0085    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.44e+04  |
|    n_updates            | 446       |
|    policy_gradient_loss | 7.22e-10  |
|    value_loss           | 4.72e+04  |
---------------------------------------
Eval num_timesteps=84500, episode_reward=834.70 +/- 683.05
Episode length: 34.86 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=788.13 +/- 656.92
Episode length: 35.44 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=902.79 +/- 647.05
Episode length: 36.60 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=1125.54 +/- 724.32
Episode length: 37.48 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 908      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 42       |
|    time_elapsed    | 300      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=1051.43 +/- 726.71
Episode length: 37.36 +/- 5.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.4      |
|    mean_reward          | 1.05e+03  |
| time/                   |           |
|    total_timesteps      | 86500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.21e-14 |
|    explained_variance   | 0.00764   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.28e+04  |
|    n_updates            | 456       |
|    policy_gradient_loss | -1.17e-09 |
|    value_loss           | 3.39e+04  |
---------------------------------------
Eval num_timesteps=87000, episode_reward=761.81 +/- 627.36
Episode length: 34.84 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=693.04 +/- 588.03
Episode length: 34.96 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=739.49 +/- 632.74
Episode length: 34.32 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 825      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 43       |
|    time_elapsed    | 307      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=824.50 +/- 720.45
Episode length: 34.96 +/- 6.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 825       |
| time/                   |           |
|    total_timesteps      | 88500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.52e-16 |
|    explained_variance   | 0.00821   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.79e+04  |
|    n_updates            | 466       |
|    policy_gradient_loss | 3.38e-10  |
|    value_loss           | 4.22e+04  |
---------------------------------------
Eval num_timesteps=89000, episode_reward=844.17 +/- 681.71
Episode length: 35.14 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=716.77 +/- 636.33
Episode length: 34.24 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=788.42 +/- 691.83
Episode length: 34.16 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 44       |
|    time_elapsed    | 314      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=852.85 +/- 723.71
Episode length: 35.26 +/- 6.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 853       |
| time/                   |           |
|    total_timesteps      | 90500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.57e-14 |
|    explained_variance   | 0.00836   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.77e+04  |
|    n_updates            | 476       |
|    policy_gradient_loss | 6.4e-11   |
|    value_loss           | 3.68e+04  |
---------------------------------------
Eval num_timesteps=91000, episode_reward=810.23 +/- 658.24
Episode length: 35.10 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=835.02 +/- 725.76
Episode length: 34.88 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=842.84 +/- 716.70
Episode length: 35.04 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 799      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 45       |
|    time_elapsed    | 321      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=805.48 +/- 655.04
Episode length: 35.30 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 805       |
| time/                   |           |
|    total_timesteps      | 92500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.95e-16 |
|    explained_variance   | 0.00895   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.79e+04  |
|    n_updates            | 486       |
|    policy_gradient_loss | -1.27e-09 |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=93000, episode_reward=800.05 +/- 681.80
Episode length: 35.10 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=944.64 +/- 719.73
Episode length: 36.12 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 945      |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=766.52 +/- 648.90
Episode length: 34.84 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 46       |
|    time_elapsed    | 328      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=686.87 +/- 609.53
Episode length: 34.10 +/- 7.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 687       |
| time/                   |           |
|    total_timesteps      | 94500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.02e-14 |
|    explained_variance   | 0.00727   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.09e+04  |
|    n_updates            | 496       |
|    policy_gradient_loss | 7.63e-10  |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=95000, episode_reward=742.68 +/- 677.91
Episode length: 33.46 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=782.26 +/- 620.83
Episode length: 35.22 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=856.38 +/- 680.86
Episode length: 36.14 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 47       |
|    time_elapsed    | 336      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=726.15 +/- 624.68
Episode length: 34.74 +/- 6.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 726       |
| time/                   |           |
|    total_timesteps      | 96500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.41e-16 |
|    explained_variance   | 0.00878   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.65e+04  |
|    n_updates            | 506       |
|    policy_gradient_loss | 1.41e-09  |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=97000, episode_reward=831.07 +/- 634.57
Episode length: 36.14 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=858.24 +/- 703.13
Episode length: 35.22 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=901.96 +/- 727.37
Episode length: 35.20 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 48       |
|    time_elapsed    | 343      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=795.69 +/- 622.30
Episode length: 35.68 +/- 6.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 796       |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.66e-14 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.57e+04  |
|    n_updates            | 516       |
|    policy_gradient_loss | 1.15e-09  |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=99000, episode_reward=822.55 +/- 679.27
Episode length: 35.20 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=864.58 +/- 681.99
Episode length: 35.42 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=693.34 +/- 551.21
Episode length: 34.86 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 881      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 49       |
|    time_elapsed    | 350      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=848.94 +/- 678.97
Episode length: 35.54 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 849       |
| time/                   |           |
|    total_timesteps      | 100500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.53e-16 |
|    explained_variance   | 0.00904   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.01e+04  |
|    n_updates            | 526       |
|    policy_gradient_loss | 1.82e-09  |
|    value_loss           | 3.93e+04  |
---------------------------------------
Eval num_timesteps=101000, episode_reward=773.43 +/- 639.88
Episode length: 35.02 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=762.71 +/- 623.33
Episode length: 35.42 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=762.17 +/- 673.76
Episode length: 34.26 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 896      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 50       |
|    time_elapsed    | 357      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=817.03 +/- 681.57
Episode length: 34.64 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.63e-21 |
|    explained_variance   | 0.0124    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.38e+04  |
|    n_updates            | 536       |
|    policy_gradient_loss | 9.11e-10  |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=1053.38 +/- 766.95
Episode length: 35.40 +/- 8.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=649.79 +/- 658.06
Episode length: 32.56 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 650      |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=786.24 +/- 687.70
Episode length: 34.84 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 883      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 51       |
|    time_elapsed    | 365      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=876.99 +/- 721.45
Episode length: 35.20 +/- 7.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 877       |
| time/                   |           |
|    total_timesteps      | 104500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.42e-19 |
|    explained_variance   | 0.00547   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.79e+04  |
|    n_updates            | 546       |
|    policy_gradient_loss | -6.37e-10 |
|    value_loss           | 5.02e+04  |
---------------------------------------
Eval num_timesteps=105000, episode_reward=1003.39 +/- 705.98
Episode length: 36.80 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=771.45 +/- 649.99
Episode length: 34.18 +/- 7.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=996.31 +/- 718.52
Episode length: 36.52 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 770      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 52       |
|    time_elapsed    | 372      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=700.75 +/- 664.86
Episode length: 33.34 +/- 7.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.3      |
|    mean_reward          | 701       |
| time/                   |           |
|    total_timesteps      | 106500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.32e-14 |
|    explained_variance   | 0.00822   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.28e+04  |
|    n_updates            | 556       |
|    policy_gradient_loss | -3.68e-10 |
|    value_loss           | 4.07e+04  |
---------------------------------------
Eval num_timesteps=107000, episode_reward=757.35 +/- 629.94
Episode length: 34.50 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=667.83 +/- 614.06
Episode length: 33.50 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=946.87 +/- 713.01
Episode length: 35.70 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=874.73 +/- 710.43
Episode length: 35.10 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 53       |
|    time_elapsed    | 380      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=838.07 +/- 664.73
Episode length: 35.50 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 109000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.89e-16 |
|    explained_variance   | 0.011     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.27e+04  |
|    n_updates            | 566       |
|    policy_gradient_loss | -1.8e-09  |
|    value_loss           | 4.5e+04   |
---------------------------------------
Eval num_timesteps=109500, episode_reward=749.95 +/- 617.56
Episode length: 35.06 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=682.34 +/- 584.24
Episode length: 34.24 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=685.82 +/- 598.99
Episode length: 33.96 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 903      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 54       |
|    time_elapsed    | 387      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=941.67 +/- 726.63
Episode length: 35.66 +/- 6.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 942       |
| time/                   |           |
|    total_timesteps      | 111000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.69e-21 |
|    explained_variance   | 0.00721   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.07e+04  |
|    n_updates            | 576       |
|    policy_gradient_loss | -9.02e-10 |
|    value_loss           | 3.93e+04  |
---------------------------------------
Eval num_timesteps=111500, episode_reward=917.64 +/- 677.04
Episode length: 36.42 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=894.63 +/- 679.39
Episode length: 35.80 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=783.61 +/- 616.46
Episode length: 35.68 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 827      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 55       |
|    time_elapsed    | 395      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=785.30 +/- 670.09
Episode length: 34.92 +/- 6.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 785       |
| time/                   |           |
|    total_timesteps      | 113000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.67e-19 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.78e+04  |
|    n_updates            | 586       |
|    policy_gradient_loss | -5.18e-10 |
|    value_loss           | 4.71e+04  |
---------------------------------------
Eval num_timesteps=113500, episode_reward=930.49 +/- 726.70
Episode length: 35.44 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 930      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=650.22 +/- 658.34
Episode length: 32.54 +/- 7.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.5     |
|    mean_reward     | 650      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=874.20 +/- 702.05
Episode length: 35.26 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 56       |
|    time_elapsed    | 402      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=730.19 +/- 613.01
Episode length: 34.80 +/- 7.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 730       |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.07e-21 |
|    explained_variance   | 0.0116    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.81e+04  |
|    n_updates            | 596       |
|    policy_gradient_loss | 1.75e-11  |
|    value_loss           | 3.75e+04  |
---------------------------------------
Eval num_timesteps=115500, episode_reward=673.04 +/- 590.03
Episode length: 34.24 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=840.02 +/- 724.14
Episode length: 34.66 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=716.40 +/- 685.42
Episode length: 33.64 +/- 8.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 870      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 57       |
|    time_elapsed    | 409      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=843.98 +/- 666.58
Episode length: 35.90 +/- 5.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 844       |
| time/                   |           |
|    total_timesteps      | 117000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.61e-19 |
|    explained_variance   | 0.0119    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.85e+04  |
|    n_updates            | 606       |
|    policy_gradient_loss | 4.16e-10  |
|    value_loss           | 4.67e+04  |
---------------------------------------
Eval num_timesteps=117500, episode_reward=883.50 +/- 707.03
Episode length: 35.58 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=818.92 +/- 698.94
Episode length: 35.04 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=882.25 +/- 749.94
Episode length: 35.26 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 910      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 58       |
|    time_elapsed    | 416      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=861.36 +/- 684.78
Episode length: 35.70 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 861       |
| time/                   |           |
|    total_timesteps      | 119000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.14e-21 |
|    explained_variance   | 0.00875   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.53e+04  |
|    n_updates            | 616       |
|    policy_gradient_loss | -3.03e-10 |
|    value_loss           | 3.49e+04  |
---------------------------------------
Eval num_timesteps=119500, episode_reward=693.80 +/- 581.09
Episode length: 34.30 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=731.75 +/- 581.77
Episode length: 35.46 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=1026.21 +/- 789.24
Episode length: 35.84 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 59       |
|    time_elapsed    | 424      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=733.59 +/- 645.62
Episode length: 34.04 +/- 7.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 734       |
| time/                   |           |
|    total_timesteps      | 121000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.29e-19 |
|    explained_variance   | 0.0121    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.79e+04  |
|    n_updates            | 626       |
|    policy_gradient_loss | -4.39e-10 |
|    value_loss           | 4.86e+04  |
---------------------------------------
Eval num_timesteps=121500, episode_reward=876.42 +/- 706.32
Episode length: 35.68 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=762.98 +/- 621.41
Episode length: 35.26 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=793.64 +/- 636.95
Episode length: 35.32 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 60       |
|    time_elapsed    | 431      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=984.39 +/- 715.73
Episode length: 36.12 +/- 7.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 984       |
| time/                   |           |
|    total_timesteps      | 123000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-13 |
|    explained_variance   | 0.00564   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.24e+04  |
|    n_updates            | 636       |
|    policy_gradient_loss | 3.64e-11  |
|    value_loss           | 3.59e+04  |
---------------------------------------
Eval num_timesteps=123500, episode_reward=879.17 +/- 676.50
Episode length: 36.04 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=932.09 +/- 742.19
Episode length: 36.16 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=755.41 +/- 624.07
Episode length: 34.44 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 61       |
|    time_elapsed    | 438      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=894.88 +/- 677.00
Episode length: 36.74 +/- 5.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 895       |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.07e-16 |
|    explained_variance   | 0.00705   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.18e+04  |
|    n_updates            | 646       |
|    policy_gradient_loss | -6.93e-10 |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=125500, episode_reward=740.68 +/- 681.49
Episode length: 33.66 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=800.76 +/- 689.75
Episode length: 34.64 +/- 7.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=980.42 +/- 748.40
Episode length: 35.96 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 846      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 62       |
|    time_elapsed    | 445      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=716.20 +/- 637.80
Episode length: 33.88 +/- 7.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 716       |
| time/                   |           |
|    total_timesteps      | 127000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-13 |
|    explained_variance   | 0.0117    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.18e+04  |
|    n_updates            | 656       |
|    policy_gradient_loss | -9.31e-11 |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=127500, episode_reward=895.45 +/- 657.95
Episode length: 36.04 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=1013.91 +/- 708.19
Episode length: 36.60 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=746.67 +/- 643.95
Episode length: 34.22 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=786.79 +/- 646.61
Episode length: 35.08 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 63       |
|    time_elapsed    | 454      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=762.29 +/- 672.94
Episode length: 34.24 +/- 7.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 762       |
| time/                   |           |
|    total_timesteps      | 129500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.78e-16 |
|    explained_variance   | 0.00591   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.74e+04  |
|    n_updates            | 666       |
|    policy_gradient_loss | -9.17e-11 |
|    value_loss           | 3.81e+04  |
---------------------------------------
Eval num_timesteps=130000, episode_reward=820.16 +/- 657.30
Episode length: 35.88 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=748.62 +/- 639.43
Episode length: 34.46 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=623.36 +/- 639.93
Episode length: 32.20 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 623      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 870      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 64       |
|    time_elapsed    | 461      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=758.01 +/- 667.21
Episode length: 34.04 +/- 6.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 758       |
| time/                   |           |
|    total_timesteps      | 131500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.72e-14 |
|    explained_variance   | 0.0124    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.31e+04  |
|    n_updates            | 676       |
|    policy_gradient_loss | 0         |
|    value_loss           | 3.89e+04  |
---------------------------------------
Eval num_timesteps=132000, episode_reward=813.58 +/- 700.32
Episode length: 34.82 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=869.04 +/- 691.74
Episode length: 35.52 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=878.08 +/- 673.16
Episode length: 36.12 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 895      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 65       |
|    time_elapsed    | 468      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=722.50 +/- 611.53
Episode length: 34.46 +/- 5.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 723       |
| time/                   |           |
|    total_timesteps      | 133500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.66e-16 |
|    explained_variance   | 0.0103    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.23e+04  |
|    n_updates            | 686       |
|    policy_gradient_loss | -7.17e-10 |
|    value_loss           | 4.04e+04  |
---------------------------------------
Eval num_timesteps=134000, episode_reward=913.06 +/- 697.23
Episode length: 35.52 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=764.55 +/- 701.68
Episode length: 33.54 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=733.04 +/- 651.50
Episode length: 34.34 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 876      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 66       |
|    time_elapsed    | 475      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=815.13 +/- 634.57
Episode length: 35.92 +/- 5.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 815       |
| time/                   |           |
|    total_timesteps      | 135500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.51e-21 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.42e+04  |
|    n_updates            | 696       |
|    policy_gradient_loss | -4.83e-10 |
|    value_loss           | 3.85e+04  |
---------------------------------------
Eval num_timesteps=136000, episode_reward=735.47 +/- 580.69
Episode length: 34.88 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=818.54 +/- 682.75
Episode length: 35.08 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=820.55 +/- 645.49
Episode length: 35.82 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 648      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 67       |
|    time_elapsed    | 482      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=627.14 +/- 610.93
Episode length: 32.78 +/- 6.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.8      |
|    mean_reward          | 627       |
| time/                   |           |
|    total_timesteps      | 137500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.35e-19 |
|    explained_variance   | -0.00156  |
|    learning_rate        | 0.0001    |
|    loss                 | 3.58e+04  |
|    n_updates            | 706       |
|    policy_gradient_loss | 2.76e-10  |
|    value_loss           | 5.44e+04  |
---------------------------------------
Eval num_timesteps=138000, episode_reward=818.38 +/- 649.94
Episode length: 35.66 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=890.33 +/- 723.09
Episode length: 36.00 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=873.40 +/- 757.63
Episode length: 34.44 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.4     |
|    ep_rew_mean     | 582      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 68       |
|    time_elapsed    | 490      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=915.41 +/- 645.12
Episode length: 37.70 +/- 4.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.7      |
|    mean_reward          | 915       |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-13 |
|    explained_variance   | 0.00131   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.23e+04  |
|    n_updates            | 716       |
|    policy_gradient_loss | 1.24e-09  |
|    value_loss           | 3.6e+04   |
---------------------------------------
Eval num_timesteps=140000, episode_reward=803.83 +/- 629.18
Episode length: 35.54 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=895.95 +/- 656.59
Episode length: 36.82 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=1008.99 +/- 711.80
Episode length: 36.82 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 69       |
|    time_elapsed    | 497      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=967.81 +/- 686.29
Episode length: 37.36 +/- 5.86
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 37.4     |
|    mean_reward          | 968      |
| time/                   |          |
|    total_timesteps      | 141500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6.3e-16 |
|    explained_variance   | 0.00987  |
|    learning_rate        | 0.0001   |
|    loss                 | 1.27e+04 |
|    n_updates            | 726      |
|    policy_gradient_loss | 1.53e-09 |
|    value_loss           | 3.56e+04 |
--------------------------------------
Eval num_timesteps=142000, episode_reward=977.65 +/- 739.20
Episode length: 35.90 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 978      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=736.90 +/- 635.56
Episode length: 34.18 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=643.64 +/- 562.56
Episode length: 33.78 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 912      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 70       |
|    time_elapsed    | 504      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=1007.35 +/- 745.25
Episode length: 36.18 +/- 6.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 143500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-13 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.89e+04  |
|    n_updates            | 736       |
|    policy_gradient_loss | -8.54e-10 |
|    value_loss           | 4.23e+04  |
---------------------------------------
Eval num_timesteps=144000, episode_reward=839.71 +/- 699.56
Episode length: 34.76 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=799.51 +/- 655.91
Episode length: 35.04 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=757.25 +/- 649.50
Episode length: 34.48 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 71       |
|    time_elapsed    | 512      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=906.47 +/- 711.05
Episode length: 36.02 +/- 6.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 906       |
| time/                   |           |
|    total_timesteps      | 145500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.43e-16 |
|    explained_variance   | 0.00344   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.06e+04  |
|    n_updates            | 746       |
|    policy_gradient_loss | -7.7e-10  |
|    value_loss           | 2.84e+04  |
---------------------------------------
Eval num_timesteps=146000, episode_reward=775.97 +/- 603.42
Episode length: 36.14 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=861.35 +/- 640.88
Episode length: 35.84 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=829.12 +/- 661.09
Episode length: 35.90 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 807      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 72       |
|    time_elapsed    | 519      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=781.05 +/- 626.71
Episode length: 35.46 +/- 6.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 781       |
| time/                   |           |
|    total_timesteps      | 147500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.78e-21 |
|    explained_variance   | 0.0175    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.41e+04  |
|    n_updates            | 756       |
|    policy_gradient_loss | 1.36e-09  |
|    value_loss           | 3.97e+04  |
---------------------------------------
Eval num_timesteps=148000, episode_reward=934.58 +/- 755.60
Episode length: 35.38 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=819.14 +/- 686.86
Episode length: 35.30 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=925.06 +/- 692.40
Episode length: 36.02 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=867.70 +/- 742.99
Episode length: 34.28 +/- 7.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 720      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 73       |
|    time_elapsed    | 527      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=786.48 +/- 652.59
Episode length: 34.48 +/- 7.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 786       |
| time/                   |           |
|    total_timesteps      | 150000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-18 |
|    explained_variance   | 0.0015    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.28e+04  |
|    n_updates            | 766       |
|    policy_gradient_loss | -2.58e-09 |
|    value_loss           | 5.21e+04  |
---------------------------------------
Eval num_timesteps=150500, episode_reward=972.71 +/- 751.62
Episode length: 35.82 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=665.23 +/- 574.44
Episode length: 33.92 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=852.20 +/- 717.99
Episode length: 34.92 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 786      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 74       |
|    time_elapsed    | 535      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=1018.13 +/- 719.59
Episode length: 36.36 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 1.02e+03  |
| time/                   |           |
|    total_timesteps      | 152000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.69e-13 |
|    explained_variance   | 0.00785   |
|    learning_rate        | 0.0001    |
|    loss                 | 9.64e+03  |
|    n_updates            | 776       |
|    policy_gradient_loss | 8.32e-10  |
|    value_loss           | 3.53e+04  |
---------------------------------------
Eval num_timesteps=152500, episode_reward=825.67 +/- 673.65
Episode length: 35.28 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=756.91 +/- 621.79
Episode length: 35.52 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=814.16 +/- 648.75
Episode length: 35.20 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 825      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 75       |
|    time_elapsed    | 542      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=863.39 +/- 687.67
Episode length: 35.70 +/- 6.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 863       |
| time/                   |           |
|    total_timesteps      | 154000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.73e-16 |
|    explained_variance   | 0.00826   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.13e+04  |
|    n_updates            | 786       |
|    policy_gradient_loss | 2.04e-11  |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=154500, episode_reward=840.98 +/- 647.83
Episode length: 35.58 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=981.70 +/- 737.58
Episode length: 36.50 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=793.25 +/- 663.34
Episode length: 34.52 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 76       |
|    time_elapsed    | 549      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=918.08 +/- 737.93
Episode length: 36.22 +/- 6.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 918       |
| time/                   |           |
|    total_timesteps      | 156000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.44e-21 |
|    explained_variance   | 0.0102    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.11e+04  |
|    n_updates            | 796       |
|    policy_gradient_loss | 5.18e-10  |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=156500, episode_reward=902.62 +/- 695.29
Episode length: 36.34 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=958.53 +/- 663.13
Episode length: 36.48 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=726.69 +/- 624.93
Episode length: 34.24 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 77       |
|    time_elapsed    | 556      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=782.29 +/- 611.62
Episode length: 35.44 +/- 5.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 782       |
| time/                   |           |
|    total_timesteps      | 158000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-18 |
|    explained_variance   | 0.0129    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.34e+04  |
|    n_updates            | 806       |
|    policy_gradient_loss | -1.2e-09  |
|    value_loss           | 4.63e+04  |
---------------------------------------
Eval num_timesteps=158500, episode_reward=966.60 +/- 714.75
Episode length: 36.38 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=796.29 +/- 628.71
Episode length: 36.24 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=810.76 +/- 665.97
Episode length: 35.04 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 816      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 78       |
|    time_elapsed    | 564      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=816.39 +/- 717.82
Episode length: 34.84 +/- 7.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 816       |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.6e-13  |
|    explained_variance   | 0.00505   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.5e+04   |
|    n_updates            | 816       |
|    policy_gradient_loss | -3.75e-10 |
|    value_loss           | 3.3e+04   |
---------------------------------------
Eval num_timesteps=160500, episode_reward=720.49 +/- 662.13
Episode length: 34.32 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=965.13 +/- 702.64
Episode length: 36.68 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=832.27 +/- 683.92
Episode length: 34.98 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 690      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 79       |
|    time_elapsed    | 571      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=587.31 +/- 453.23
Episode length: 34.34 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 587       |
| time/                   |           |
|    total_timesteps      | 162000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.67e-09 |
|    explained_variance   | 0.00274   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.33e+04  |
|    n_updates            | 826       |
|    policy_gradient_loss | 3.06e-10  |
|    value_loss           | 2.7e+04   |
---------------------------------------
Eval num_timesteps=162500, episode_reward=717.03 +/- 626.83
Episode length: 34.52 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=807.26 +/- 722.59
Episode length: 34.44 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=1011.73 +/- 713.03
Episode length: 37.06 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 80       |
|    time_elapsed    | 578      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=914.25 +/- 735.62
Episode length: 35.24 +/- 6.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 914       |
| time/                   |           |
|    total_timesteps      | 164000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.92e-12 |
|    explained_variance   | 0.00716   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.67e+04  |
|    n_updates            | 836       |
|    policy_gradient_loss | 1.95e-09  |
|    value_loss           | 4.48e+04  |
---------------------------------------
Eval num_timesteps=164500, episode_reward=646.49 +/- 585.97
Episode length: 33.24 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 646      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=892.76 +/- 679.36
Episode length: 35.62 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=801.37 +/- 727.21
Episode length: 33.68 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 81       |
|    time_elapsed    | 585      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=878.79 +/- 697.83
Episode length: 35.88 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 879       |
| time/                   |           |
|    total_timesteps      | 166000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.8e-16  |
|    explained_variance   | 0.00938   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.51e+04  |
|    n_updates            | 846       |
|    policy_gradient_loss | -2.15e-10 |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=166500, episode_reward=888.90 +/- 743.15
Episode length: 34.94 +/- 7.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=777.93 +/- 669.13
Episode length: 34.34 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=775.83 +/- 678.58
Episode length: 34.50 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 82       |
|    time_elapsed    | 592      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=844.11 +/- 696.70
Episode length: 34.84 +/- 6.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 844       |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.44e-21 |
|    explained_variance   | 0.01      |
|    learning_rate        | 0.0001    |
|    loss                 | 2.17e+04  |
|    n_updates            | 856       |
|    policy_gradient_loss | -5.33e-10 |
|    value_loss           | 3.75e+04  |
---------------------------------------
Eval num_timesteps=168500, episode_reward=777.27 +/- 691.38
Episode length: 34.44 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=747.89 +/- 667.09
Episode length: 33.98 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=770.70 +/- 595.22
Episode length: 35.14 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 808      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 83       |
|    time_elapsed    | 599      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=733.42 +/- 649.51
Episode length: 33.70 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 733       |
| time/                   |           |
|    total_timesteps      | 170000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.03e-18 |
|    explained_variance   | 0.00967   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.1e+04   |
|    n_updates            | 866       |
|    policy_gradient_loss | 1.27e-09  |
|    value_loss           | 4.68e+04  |
---------------------------------------
Eval num_timesteps=170500, episode_reward=852.69 +/- 679.95
Episode length: 35.28 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=776.50 +/- 620.03
Episode length: 35.60 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=915.85 +/- 677.92
Episode length: 36.60 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=945.86 +/- 741.88
Episode length: 35.64 +/- 7.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 894      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 84       |
|    time_elapsed    | 608      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=768.56 +/- 662.51
Episode length: 34.54 +/- 6.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 769       |
| time/                   |           |
|    total_timesteps      | 172500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.89e-20 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.44e+04  |
|    n_updates            | 876       |
|    policy_gradient_loss | 9.49e-10  |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=173000, episode_reward=855.10 +/- 680.64
Episode length: 35.46 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=884.82 +/- 738.75
Episode length: 34.72 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=879.63 +/- 722.32
Episode length: 35.08 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 842      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 85       |
|    time_elapsed    | 615      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=899.71 +/- 729.36
Episode length: 35.64 +/- 6.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 900       |
| time/                   |           |
|    total_timesteps      | 174500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-17 |
|    explained_variance   | 0.00986   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.26e+04  |
|    n_updates            | 886       |
|    policy_gradient_loss | -1.66e-10 |
|    value_loss           | 4.69e+04  |
---------------------------------------
Eval num_timesteps=175000, episode_reward=739.13 +/- 717.88
Episode length: 33.42 +/- 8.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=735.40 +/- 622.88
Episode length: 35.22 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=838.11 +/- 610.89
Episode length: 37.00 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 880      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 86       |
|    time_elapsed    | 622      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=610.89 +/- 505.73
Episode length: 33.70 +/- 6.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 611       |
| time/                   |           |
|    total_timesteps      | 176500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-14 |
|    explained_variance   | 0.0143    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.08e+04  |
|    n_updates            | 896       |
|    policy_gradient_loss | -5.27e-10 |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=177000, episode_reward=800.28 +/- 692.38
Episode length: 35.08 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=931.51 +/- 700.28
Episode length: 35.94 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=806.62 +/- 603.04
Episode length: 36.16 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 905      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 87       |
|    time_elapsed    | 630      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=940.28 +/- 751.90
Episode length: 35.60 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 940       |
| time/                   |           |
|    total_timesteps      | 178500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.04e-17 |
|    explained_variance   | 0.012     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.4e+04   |
|    n_updates            | 906       |
|    policy_gradient_loss | 1.15e-09  |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=179000, episode_reward=835.43 +/- 654.88
Episode length: 36.06 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=701.26 +/- 662.39
Episode length: 33.62 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=826.69 +/- 666.73
Episode length: 35.68 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 88       |
|    time_elapsed    | 637      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=755.55 +/- 672.39
Episode length: 34.30 +/- 6.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 756       |
| time/                   |           |
|    total_timesteps      | 180500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-14 |
|    explained_variance   | 0.000688  |
|    learning_rate        | 0.0001    |
|    loss                 | 3.11e+04  |
|    n_updates            | 916       |
|    policy_gradient_loss | -7.33e-10 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=181000, episode_reward=903.36 +/- 704.46
Episode length: 35.90 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=851.13 +/- 719.02
Episode length: 35.00 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=858.30 +/- 701.50
Episode length: 34.74 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 759      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 89       |
|    time_elapsed    | 644      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=844.46 +/- 686.76
Episode length: 35.48 +/- 7.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 844       |
| time/                   |           |
|    total_timesteps      | 182500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.1e-17  |
|    explained_variance   | 0.0124    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.21e+04  |
|    n_updates            | 926       |
|    policy_gradient_loss | -5.89e-10 |
|    value_loss           | 4.1e+04   |
---------------------------------------
Eval num_timesteps=183000, episode_reward=775.31 +/- 656.82
Episode length: 34.88 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=903.87 +/- 691.30
Episode length: 36.16 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=872.83 +/- 705.47
Episode length: 35.20 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 943      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 90       |
|    time_elapsed    | 651      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=854.54 +/- 742.28
Episode length: 34.46 +/- 7.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 855       |
| time/                   |           |
|    total_timesteps      | 184500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.75e-21 |
|    explained_variance   | 0.0205    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.11e+04  |
|    n_updates            | 936       |
|    policy_gradient_loss | -1.53e-10 |
|    value_loss           | 4.17e+04  |
---------------------------------------
Eval num_timesteps=185000, episode_reward=777.85 +/- 634.23
Episode length: 35.50 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=975.46 +/- 758.14
Episode length: 36.08 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=710.73 +/- 641.75
Episode length: 33.58 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 888      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 91       |
|    time_elapsed    | 658      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=744.27 +/- 587.24
Episode length: 35.32 +/- 6.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 744       |
| time/                   |           |
|    total_timesteps      | 186500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-18 |
|    explained_variance   | 0.00563   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.4e+04   |
|    n_updates            | 946       |
|    policy_gradient_loss | 8.29e-11  |
|    value_loss           | 4.71e+04  |
---------------------------------------
Eval num_timesteps=187000, episode_reward=874.47 +/- 683.82
Episode length: 35.84 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=948.99 +/- 731.12
Episode length: 35.82 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=672.81 +/- 566.85
Episode length: 34.18 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 887      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 92       |
|    time_elapsed    | 666      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=827.70 +/- 646.05
Episode length: 36.16 +/- 4.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 828       |
| time/                   |           |
|    total_timesteps      | 188500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.49e-21 |
|    explained_variance   | 0.015     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.79e+04  |
|    n_updates            | 956       |
|    policy_gradient_loss | -7.51e-10 |
|    value_loss           | 4.15e+04  |
---------------------------------------
Eval num_timesteps=189000, episode_reward=753.04 +/- 673.98
Episode length: 33.94 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=774.54 +/- 628.07
Episode length: 35.16 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=745.13 +/- 596.11
Episode length: 35.26 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 93       |
|    time_elapsed    | 673      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=686.89 +/- 631.96
Episode length: 34.04 +/- 6.89
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34       |
|    mean_reward          | 687      |
| time/                   |          |
|    total_timesteps      | 190500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.3e-18 |
|    explained_variance   | 0.00875  |
|    learning_rate        | 0.0001   |
|    loss                 | 3.03e+04 |
|    n_updates            | 966      |
|    policy_gradient_loss | 3.67e-10 |
|    value_loss           | 5.54e+04 |
--------------------------------------
Eval num_timesteps=191000, episode_reward=959.56 +/- 714.44
Episode length: 36.54 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=1034.59 +/- 752.22
Episode length: 36.60 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=780.69 +/- 664.43
Episode length: 34.74 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=928.20 +/- 742.35
Episode length: 35.46 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 94       |
|    time_elapsed    | 681      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=903.53 +/- 692.59
Episode length: 35.76 +/- 6.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 904       |
| time/                   |           |
|    total_timesteps      | 193000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.03e-21 |
|    explained_variance   | 0.0107    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.13e+04  |
|    n_updates            | 976       |
|    policy_gradient_loss | 4.95e-11  |
|    value_loss           | 3.82e+04  |
---------------------------------------
Eval num_timesteps=193500, episode_reward=962.43 +/- 718.74
Episode length: 36.50 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=873.51 +/- 743.14
Episode length: 35.32 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=733.67 +/- 627.23
Episode length: 34.78 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 95       |
|    time_elapsed    | 689      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=642.80 +/- 579.88
Episode length: 33.52 +/- 6.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 643       |
| time/                   |           |
|    total_timesteps      | 195000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.68e-18 |
|    explained_variance   | 0.0156    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.92e+04  |
|    n_updates            | 986       |
|    policy_gradient_loss | 8.02e-10  |
|    value_loss           | 4.94e+04  |
---------------------------------------
Eval num_timesteps=195500, episode_reward=901.13 +/- 743.02
Episode length: 35.06 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=808.10 +/- 648.10
Episode length: 35.76 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=756.68 +/- 630.59
Episode length: 35.08 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 798      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 96       |
|    time_elapsed    | 696      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=857.34 +/- 725.42
Episode length: 34.66 +/- 8.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 857       |
| time/                   |           |
|    total_timesteps      | 197000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.26e-13 |
|    explained_variance   | 0.0128    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.58e+04  |
|    n_updates            | 996       |
|    policy_gradient_loss | 2.59e-10  |
|    value_loss           | 3.73e+04  |
---------------------------------------
Eval num_timesteps=197500, episode_reward=962.39 +/- 692.45
Episode length: 36.98 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=931.76 +/- 701.74
Episode length: 36.14 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=857.60 +/- 670.80
Episode length: 35.96 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 845      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 97       |
|    time_elapsed    | 703      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=736.66 +/- 625.40
Episode length: 34.54 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 737       |
| time/                   |           |
|    total_timesteps      | 199000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.14e-15 |
|    explained_variance   | 0.0134    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.42e+04  |
|    n_updates            | 1006      |
|    policy_gradient_loss | -1.09e-09 |
|    value_loss           | 3.49e+04  |
---------------------------------------
Eval num_timesteps=199500, episode_reward=664.58 +/- 577.47
Episode length: 34.10 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=938.38 +/- 694.56
Episode length: 36.50 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=683.47 +/- 586.75
Episode length: 34.78 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 98       |
|    time_elapsed    | 711      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=810.52 +/- 655.93
Episode length: 35.82 +/- 6.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 811       |
| time/                   |           |
|    total_timesteps      | 201000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.27e-13 |
|    explained_variance   | 0.00972   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.47e+04  |
|    n_updates            | 1016      |
|    policy_gradient_loss | 8.05e-10  |
|    value_loss           | 3.43e+04  |
---------------------------------------
Eval num_timesteps=201500, episode_reward=734.19 +/- 650.21
Episode length: 34.16 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=754.44 +/- 650.54
Episode length: 34.70 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=1038.00 +/- 739.85
Episode length: 36.38 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 99       |
|    time_elapsed    | 718      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=898.15 +/- 701.95
Episode length: 36.60 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 898       |
| time/                   |           |
|    total_timesteps      | 203000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12e-15 |
|    explained_variance   | 0.00855   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.39e+04  |
|    n_updates            | 1026      |
|    policy_gradient_loss | 8.79e-10  |
|    value_loss           | 3.37e+04  |
---------------------------------------
Eval num_timesteps=203500, episode_reward=825.11 +/- 718.71
Episode length: 35.48 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=753.08 +/- 768.42
Episode length: 32.64 +/- 8.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=797.82 +/- 644.14
Episode length: 35.58 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 100      |
|    time_elapsed    | 725      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=814.42 +/- 634.52
Episode length: 36.04 +/- 5.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 814       |
| time/                   |           |
|    total_timesteps      | 205000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.26e-13 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.48e+04  |
|    n_updates            | 1036      |
|    policy_gradient_loss | 5.76e-10  |
|    value_loss           | 3.92e+04  |
---------------------------------------
Eval num_timesteps=205500, episode_reward=783.47 +/- 677.88
Episode length: 35.04 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=933.78 +/- 730.19
Episode length: 35.28 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=774.89 +/- 650.00
Episode length: 34.74 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 731      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 101      |
|    time_elapsed    | 732      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=1003.03 +/- 779.23
Episode length: 36.08 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 207000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.15e-09 |
|    explained_variance   | 0.00361   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.85e+04  |
|    n_updates            | 1046      |
|    policy_gradient_loss | 3.06e-11  |
|    value_loss           | 3.17e+04  |
---------------------------------------
Eval num_timesteps=207500, episode_reward=854.04 +/- 699.59
Episode length: 35.24 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=726.44 +/- 611.77
Episode length: 34.72 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=800.27 +/- 698.60
Episode length: 33.82 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 102      |
|    time_elapsed    | 739      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=675.97 +/- 601.29
Episode length: 34.16 +/- 6.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 676       |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.52e-11 |
|    explained_variance   | 0.00915   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.84e+04  |
|    n_updates            | 1056      |
|    policy_gradient_loss | 6.98e-11  |
|    value_loss           | 4.05e+04  |
---------------------------------------
Eval num_timesteps=209500, episode_reward=990.68 +/- 699.36
Episode length: 37.54 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=748.35 +/- 628.49
Episode length: 34.56 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=897.63 +/- 716.70
Episode length: 35.96 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 103      |
|    time_elapsed    | 747      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=939.54 +/- 739.42
Episode length: 35.22 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 940       |
| time/                   |           |
|    total_timesteps      | 211000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.31e-15 |
|    explained_variance   | 0.0134    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.39e+04  |
|    n_updates            | 1066      |
|    policy_gradient_loss | 1.6e-09   |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=211500, episode_reward=991.79 +/- 750.61
Episode length: 35.60 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 992      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=879.56 +/- 679.07
Episode length: 35.42 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=604.11 +/- 476.69
Episode length: 34.20 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 604      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 860      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 104      |
|    time_elapsed    | 754      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=718.38 +/- 651.83
Episode length: 34.34 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 718       |
| time/                   |           |
|    total_timesteps      | 213000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.39e-13 |
|    explained_variance   | 0.0179    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.08e+04  |
|    n_updates            | 1076      |
|    policy_gradient_loss | -1.1e-09  |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=213500, episode_reward=951.28 +/- 694.40
Episode length: 36.48 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=880.17 +/- 664.45
Episode length: 36.20 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=946.53 +/- 750.45
Episode length: 35.84 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=939.06 +/- 688.45
Episode length: 37.16 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 105      |
|    time_elapsed    | 762      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=654.19 +/- 627.57
Episode length: 33.04 +/- 7.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33        |
|    mean_reward          | 654       |
| time/                   |           |
|    total_timesteps      | 215500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.35e-15 |
|    explained_variance   | 0.0113    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.02e+04  |
|    n_updates            | 1086      |
|    policy_gradient_loss | 2.47e-11  |
|    value_loss           | 3.62e+04  |
---------------------------------------
Eval num_timesteps=216000, episode_reward=952.93 +/- 705.59
Episode length: 37.10 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 953      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=820.85 +/- 654.43
Episode length: 35.44 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=883.35 +/- 692.59
Episode length: 36.40 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.3     |
|    ep_rew_mean     | 705      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 106      |
|    time_elapsed    | 770      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=730.15 +/- 611.79
Episode length: 34.34 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 730       |
| time/                   |           |
|    total_timesteps      | 217500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.42e-13 |
|    explained_variance   | 0.00884   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.03e+04  |
|    n_updates            | 1096      |
|    policy_gradient_loss | -1.12e-09 |
|    value_loss           | 3.82e+04  |
---------------------------------------
Eval num_timesteps=218000, episode_reward=857.91 +/- 737.24
Episode length: 35.06 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=859.77 +/- 686.22
Episode length: 35.56 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=1029.30 +/- 752.11
Episode length: 36.30 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 858      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 107      |
|    time_elapsed    | 777      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=1063.26 +/- 730.03
Episode length: 37.14 +/- 6.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.1      |
|    mean_reward          | 1.06e+03  |
| time/                   |           |
|    total_timesteps      | 219500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-15 |
|    explained_variance   | 0.0143    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.4e+04   |
|    n_updates            | 1106      |
|    policy_gradient_loss | 5.86e-10  |
|    value_loss           | 4.17e+04  |
---------------------------------------
Eval num_timesteps=220000, episode_reward=878.11 +/- 738.05
Episode length: 35.24 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=949.69 +/- 718.72
Episode length: 36.06 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=743.92 +/- 614.33
Episode length: 34.68 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 968      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 108      |
|    time_elapsed    | 784      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=947.93 +/- 743.20
Episode length: 35.70 +/- 6.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 948       |
| time/                   |           |
|    total_timesteps      | 221500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.74e-21 |
|    explained_variance   | 0.0207    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.64e+04  |
|    n_updates            | 1116      |
|    policy_gradient_loss | 7.86e-11  |
|    value_loss           | 3.74e+04  |
---------------------------------------
Eval num_timesteps=222000, episode_reward=710.92 +/- 633.53
Episode length: 34.06 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=829.20 +/- 647.41
Episode length: 35.86 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=775.84 +/- 689.38
Episode length: 34.02 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 877      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 109      |
|    time_elapsed    | 792      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=879.16 +/- 677.88
Episode length: 35.66 +/- 7.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 879       |
| time/                   |           |
|    total_timesteps      | 223500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.25e-18 |
|    explained_variance   | 0.00713   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.76e+04  |
|    n_updates            | 1126      |
|    policy_gradient_loss | -9.08e-10 |
|    value_loss           | 5.06e+04  |
---------------------------------------
Eval num_timesteps=224000, episode_reward=719.79 +/- 635.15
Episode length: 34.34 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=817.65 +/- 690.81
Episode length: 34.56 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=685.99 +/- 691.87
Episode length: 32.84 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 817      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 110      |
|    time_elapsed    | 799      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=883.36 +/- 698.37
Episode length: 35.68 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 883       |
| time/                   |           |
|    total_timesteps      | 225500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.68e-13 |
|    explained_variance   | 0.0174    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.08e+04  |
|    n_updates            | 1136      |
|    policy_gradient_loss | -5.25e-10 |
|    value_loss           | 3.97e+04  |
---------------------------------------
Eval num_timesteps=226000, episode_reward=801.69 +/- 661.10
Episode length: 35.42 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=839.16 +/- 655.11
Episode length: 35.64 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=646.85 +/- 618.97
Episode length: 33.64 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 881      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 111      |
|    time_elapsed    | 806      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=695.58 +/- 591.04
Episode length: 35.04 +/- 6.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 696       |
| time/                   |           |
|    total_timesteps      | 227500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.69e-15 |
|    explained_variance   | 0.015     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.81e+04  |
|    n_updates            | 1146      |
|    policy_gradient_loss | 4.35e-10  |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=228000, episode_reward=951.81 +/- 711.31
Episode length: 36.30 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=941.18 +/- 735.07
Episode length: 35.78 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=903.57 +/- 695.94
Episode length: 36.42 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.1     |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    fps             | 281      |
|    iterations      | 112      |
|    time_elapsed    | 814      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=712.85 +/- 537.45
Episode length: 35.36 +/- 5.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 713       |
| time/                   |           |
|    total_timesteps      | 229500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-20 |
|    explained_variance   | 0.0272    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.11e+04  |
|    n_updates            | 1156      |
|    policy_gradient_loss | 1.01e-09  |
|    value_loss           | 4.52e+04  |
---------------------------------------
Eval num_timesteps=230000, episode_reward=931.72 +/- 750.54
Episode length: 35.72 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=838.10 +/- 674.07
Episode length: 35.58 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=918.25 +/- 703.27
Episode length: 36.18 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 892      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 113      |
|    time_elapsed    | 821      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=611.12 +/- 549.23
Episode length: 33.44 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.4      |
|    mean_reward          | 611       |
| time/                   |           |
|    total_timesteps      | 231500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.24e-18 |
|    explained_variance   | 0.00284   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.28e+04  |
|    n_updates            | 1166      |
|    policy_gradient_loss | 4.13e-10  |
|    value_loss           | 5.5e+04   |
---------------------------------------
Eval num_timesteps=232000, episode_reward=859.72 +/- 625.55
Episode length: 36.76 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=753.21 +/- 616.90
Episode length: 34.66 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=871.09 +/- 736.67
Episode length: 34.62 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 114      |
|    time_elapsed    | 828      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=812.19 +/- 661.39
Episode length: 35.22 +/- 7.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.47e-13 |
|    explained_variance   | 0.0151    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.44e+04  |
|    n_updates            | 1176      |
|    policy_gradient_loss | -4.26e-10 |
|    value_loss           | 3.52e+04  |
---------------------------------------
Eval num_timesteps=234000, episode_reward=793.79 +/- 631.23
Episode length: 35.14 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=857.52 +/- 683.01
Episode length: 35.66 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=896.42 +/- 722.74
Episode length: 35.20 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=913.79 +/- 709.98
Episode length: 35.90 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 115      |
|    time_elapsed    | 837      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=867.43 +/- 699.82
Episode length: 35.54 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 867       |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.45e-15 |
|    explained_variance   | 0.0115    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.66e+04  |
|    n_updates            | 1186      |
|    policy_gradient_loss | 3.64e-11  |
|    value_loss           | 3.49e+04  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=717.59 +/- 600.94
Episode length: 34.10 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=813.05 +/- 682.53
Episode length: 35.32 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=949.69 +/- 736.60
Episode length: 36.06 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 797      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 116      |
|    time_elapsed    | 845      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=670.58 +/- 553.91
Episode length: 34.10 +/- 6.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 671       |
| time/                   |           |
|    total_timesteps      | 238000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.5e-13  |
|    explained_variance   | 0.0123    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.49e+04  |
|    n_updates            | 1196      |
|    policy_gradient_loss | -4.37e-11 |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=238500, episode_reward=985.30 +/- 712.33
Episode length: 36.62 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 985      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=775.14 +/- 616.50
Episode length: 35.44 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=872.75 +/- 690.44
Episode length: 35.80 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 117      |
|    time_elapsed    | 852      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=962.22 +/- 755.36
Episode length: 35.56 +/- 7.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 962       |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39e-15 |
|    explained_variance   | 0.0151    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.87e+04  |
|    n_updates            | 1206      |
|    policy_gradient_loss | 1.31e-09  |
|    value_loss           | 3.62e+04  |
---------------------------------------
Eval num_timesteps=240500, episode_reward=687.21 +/- 603.66
Episode length: 34.00 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=738.48 +/- 645.52
Episode length: 34.66 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=753.26 +/- 605.41
Episode length: 35.24 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 900      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 118      |
|    time_elapsed    | 859      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=856.96 +/- 671.40
Episode length: 36.24 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 857       |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.28e-20 |
|    explained_variance   | 0.0171    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.17e+04  |
|    n_updates            | 1216      |
|    policy_gradient_loss | 9.95e-10  |
|    value_loss           | 3.92e+04  |
---------------------------------------
Eval num_timesteps=242500, episode_reward=730.29 +/- 656.58
Episode length: 34.50 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=752.66 +/- 633.41
Episode length: 34.94 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=712.99 +/- 657.86
Episode length: 33.18 +/- 7.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 846      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 119      |
|    time_elapsed    | 866      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=876.29 +/- 678.36
Episode length: 35.78 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 876       |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.79e-18 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.92e+04  |
|    n_updates            | 1226      |
|    policy_gradient_loss | 7e-10     |
|    value_loss           | 5.58e+04  |
---------------------------------------
Eval num_timesteps=244500, episode_reward=715.98 +/- 615.21
Episode length: 34.30 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=797.86 +/- 666.20
Episode length: 35.24 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=959.58 +/- 713.34
Episode length: 36.68 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 120      |
|    time_elapsed    | 874      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=792.48 +/- 656.32
Episode length: 35.62 +/- 7.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 792       |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.82e-13 |
|    explained_variance   | 0.0164    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.14e+04  |
|    n_updates            | 1236      |
|    policy_gradient_loss | 3.49e-10  |
|    value_loss           | 4.22e+04  |
---------------------------------------
Eval num_timesteps=246500, episode_reward=663.64 +/- 624.93
Episode length: 33.12 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 664      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=800.35 +/- 706.40
Episode length: 34.46 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=940.15 +/- 797.31
Episode length: 34.50 +/- 7.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 940      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 874      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 121      |
|    time_elapsed    | 881      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=885.57 +/- 700.85
Episode length: 35.90 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 886       |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-15 |
|    explained_variance   | 0.0188    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.78e+04  |
|    n_updates            | 1246      |
|    policy_gradient_loss | 9.11e-10  |
|    value_loss           | 4.01e+04  |
---------------------------------------
Eval num_timesteps=248500, episode_reward=850.66 +/- 668.06
Episode length: 35.60 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=803.27 +/- 677.38
Episode length: 34.62 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=754.13 +/- 645.57
Episode length: 34.50 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 810      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 122      |
|    time_elapsed    | 888      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=787.15 +/- 688.02
Episode length: 33.96 +/- 7.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 787       |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.64e-13 |
|    explained_variance   | 0.0175    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.15e+04  |
|    n_updates            | 1256      |
|    policy_gradient_loss | 1.92e-10  |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=876.30 +/- 676.60
Episode length: 36.24 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=903.26 +/- 691.02
Episode length: 36.50 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=819.65 +/- 680.04
Episode length: 35.24 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 827      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 123      |
|    time_elapsed    | 895      |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=925.71 +/- 688.40
Episode length: 36.08 +/- 6.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 926       |
| time/                   |           |
|    total_timesteps      | 252000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.31e-16 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.23e+04  |
|    n_updates            | 1266      |
|    policy_gradient_loss | 4.22e-10  |
|    value_loss           | 3.39e+04  |
---------------------------------------
Eval num_timesteps=252500, episode_reward=798.77 +/- 644.34
Episode length: 35.24 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=748.56 +/- 642.65
Episode length: 34.44 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=819.97 +/- 705.73
Episode length: 34.70 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 124      |
|    time_elapsed    | 902      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=883.40 +/- 664.44
Episode length: 36.22 +/- 5.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 883       |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.69e-13 |
|    explained_variance   | 0.00474   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.87e+04  |
|    n_updates            | 1276      |
|    policy_gradient_loss | -2.97e-10 |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=254500, episode_reward=818.94 +/- 671.16
Episode length: 34.92 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=788.53 +/- 656.39
Episode length: 35.58 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=833.77 +/- 695.33
Episode length: 34.48 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=700.35 +/- 593.88
Episode length: 34.12 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.4     |
|    ep_rew_mean     | 656      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 125      |
|    time_elapsed    | 911      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=756.94 +/- 665.52
Episode length: 34.34 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 757       |
| time/                   |           |
|    total_timesteps      | 256500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-09 |
|    explained_variance   | 0.00806   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.74e+04  |
|    n_updates            | 1286      |
|    policy_gradient_loss | 8.15e-10  |
|    value_loss           | 3.37e+04  |
---------------------------------------
Eval num_timesteps=257000, episode_reward=804.95 +/- 645.35
Episode length: 35.84 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=746.88 +/- 609.66
Episode length: 35.12 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=892.27 +/- 720.66
Episode length: 35.62 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 716      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 126      |
|    time_elapsed    | 918      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=734.00 +/- 634.75
Episode length: 35.04 +/- 5.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 734       |
| time/                   |           |
|    total_timesteps      | 258500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.99e-12 |
|    explained_variance   | 0.00996   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.61e+04  |
|    n_updates            | 1296      |
|    policy_gradient_loss | -1.51e-09 |
|    value_loss           | 3.58e+04  |
---------------------------------------
Eval num_timesteps=259000, episode_reward=889.83 +/- 735.61
Episode length: 35.00 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=796.64 +/- 691.50
Episode length: 34.78 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=949.96 +/- 743.19
Episode length: 36.32 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 815      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 127      |
|    time_elapsed    | 925      |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=827.89 +/- 708.97
Episode length: 34.66 +/- 7.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 828       |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.75e-16 |
|    explained_variance   | 0.0125    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.64e+04  |
|    n_updates            | 1306      |
|    policy_gradient_loss | 6.26e-10  |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=261000, episode_reward=959.60 +/- 720.47
Episode length: 35.76 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=745.88 +/- 636.47
Episode length: 35.10 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=685.98 +/- 631.28
Episode length: 33.76 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 128      |
|    time_elapsed    | 933      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=718.52 +/- 568.46
Episode length: 35.68 +/- 5.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 719       |
| time/                   |           |
|    total_timesteps      | 262500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.46e-13 |
|    explained_variance   | 0.0185    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.27e+04  |
|    n_updates            | 1316      |
|    policy_gradient_loss | 1.02e-10  |
|    value_loss           | 4.07e+04  |
---------------------------------------
Eval num_timesteps=263000, episode_reward=1018.97 +/- 790.94
Episode length: 36.22 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=846.81 +/- 686.15
Episode length: 35.14 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=855.68 +/- 725.77
Episode length: 34.64 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 129      |
|    time_elapsed    | 940      |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=760.86 +/- 646.99
Episode length: 35.14 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 761       |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.33e-16 |
|    explained_variance   | 0.0141    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.53e+04  |
|    n_updates            | 1326      |
|    policy_gradient_loss | -1.16e-11 |
|    value_loss           | 3.55e+04  |
---------------------------------------
Eval num_timesteps=265000, episode_reward=777.86 +/- 683.75
Episode length: 34.66 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=850.63 +/- 711.13
Episode length: 34.98 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=862.37 +/- 667.37
Episode length: 35.80 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 869      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 130      |
|    time_elapsed    | 947      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=886.97 +/- 654.30
Episode length: 36.80 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 887       |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.9e-13  |
|    explained_variance   | 0.0171    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.3e+04   |
|    n_updates            | 1336      |
|    policy_gradient_loss | -6.34e-10 |
|    value_loss           | 3.7e+04   |
---------------------------------------
Eval num_timesteps=267000, episode_reward=637.40 +/- 589.28
Episode length: 33.60 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=684.07 +/- 582.81
Episode length: 34.50 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=856.34 +/- 728.94
Episode length: 35.04 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 926      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 131      |
|    time_elapsed    | 955      |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=768.96 +/- 657.19
Episode length: 34.54 +/- 6.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.5     |
|    mean_reward          | 769      |
| time/                   |          |
|    total_timesteps      | 268500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -9.5e-16 |
|    explained_variance   | 0.0148   |
|    learning_rate        | 0.0001   |
|    loss                 | 2.06e+04 |
|    n_updates            | 1346     |
|    policy_gradient_loss | 2.36e-10 |
|    value_loss           | 4.09e+04 |
--------------------------------------
Eval num_timesteps=269000, episode_reward=717.94 +/- 623.23
Episode length: 33.76 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=675.92 +/- 597.22
Episode length: 33.86 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 676      |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=849.94 +/- 676.86
Episode length: 35.46 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 132      |
|    time_elapsed    | 962      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=863.55 +/- 671.37
Episode length: 36.54 +/- 5.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 864       |
| time/                   |           |
|    total_timesteps      | 270500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.93e-13 |
|    explained_variance   | 0.0102    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.01e+04  |
|    n_updates            | 1356      |
|    policy_gradient_loss | 1.49e-09  |
|    value_loss           | 3.85e+04  |
---------------------------------------
Eval num_timesteps=271000, episode_reward=839.89 +/- 690.35
Episode length: 34.88 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=833.58 +/- 635.10
Episode length: 35.74 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=918.29 +/- 713.50
Episode length: 36.48 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 133      |
|    time_elapsed    | 969      |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=929.87 +/- 679.93
Episode length: 36.64 +/- 6.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 930       |
| time/                   |           |
|    total_timesteps      | 272500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.65e-16 |
|    explained_variance   | 0.0131    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.81e+04  |
|    n_updates            | 1366      |
|    policy_gradient_loss | 4.73e-10  |
|    value_loss           | 4.11e+04  |
---------------------------------------
Eval num_timesteps=273000, episode_reward=825.36 +/- 698.73
Episode length: 35.18 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=805.18 +/- 673.36
Episode length: 35.26 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=859.37 +/- 720.93
Episode length: 34.52 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 811      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 134      |
|    time_elapsed    | 976      |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=805.98 +/- 664.20
Episode length: 35.00 +/- 7.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 806       |
| time/                   |           |
|    total_timesteps      | 274500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.84e-13 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.65e+04  |
|    n_updates            | 1376      |
|    policy_gradient_loss | 4.93e-10  |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=275000, episode_reward=734.53 +/- 617.73
Episode length: 34.18 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=766.04 +/- 693.07
Episode length: 33.82 +/- 7.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=672.07 +/- 581.13
Episode length: 34.60 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 135      |
|    time_elapsed    | 984      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=1001.54 +/- 826.37
Episode length: 35.04 +/- 8.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 276500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.16e-15 |
|    explained_variance   | 0.014     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.7e+04   |
|    n_updates            | 1386      |
|    policy_gradient_loss | 5.38e-10  |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=277000, episode_reward=734.69 +/- 606.95
Episode length: 34.92 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=864.54 +/- 733.24
Episode length: 34.56 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=753.74 +/- 636.68
Episode length: 34.88 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=960.70 +/- 730.04
Episode length: 35.84 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 961      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 136      |
|    time_elapsed    | 992      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=723.14 +/- 586.48
Episode length: 34.82 +/- 5.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 723       |
| time/                   |           |
|    total_timesteps      | 279000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.76e-13 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.28e+04  |
|    n_updates            | 1396      |
|    policy_gradient_loss | 1.51e-09  |
|    value_loss           | 3.86e+04  |
---------------------------------------
Eval num_timesteps=279500, episode_reward=1051.31 +/- 720.97
Episode length: 37.60 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.6     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=1078.66 +/- 717.84
Episode length: 37.46 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=714.25 +/- 602.28
Episode length: 35.20 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 770      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 137      |
|    time_elapsed    | 999      |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=838.54 +/- 649.49
Episode length: 36.56 +/- 6.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-15 |
|    explained_variance   | 0.0158    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.77e+04  |
|    n_updates            | 1406      |
|    policy_gradient_loss | -2.76e-10 |
|    value_loss           | 4.03e+04  |
---------------------------------------
Eval num_timesteps=281500, episode_reward=750.83 +/- 644.44
Episode length: 34.30 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=836.50 +/- 644.93
Episode length: 35.66 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=868.75 +/- 720.41
Episode length: 35.54 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 138      |
|    time_elapsed    | 1007     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=849.03 +/- 658.70
Episode length: 36.34 +/- 5.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 849       |
| time/                   |           |
|    total_timesteps      | 283000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.81e-13 |
|    explained_variance   | 0.0156    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.43e+04  |
|    n_updates            | 1416      |
|    policy_gradient_loss | 1.06e-09  |
|    value_loss           | 3.71e+04  |
---------------------------------------
Eval num_timesteps=283500, episode_reward=958.76 +/- 723.61
Episode length: 36.16 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=831.84 +/- 701.87
Episode length: 35.58 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=885.82 +/- 672.86
Episode length: 36.06 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 799      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 139      |
|    time_elapsed    | 1014     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=890.09 +/- 684.26
Episode length: 36.18 +/- 5.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 890       |
| time/                   |           |
|    total_timesteps      | 285000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.67e-16 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.82e+04  |
|    n_updates            | 1426      |
|    policy_gradient_loss | 1.04e-09  |
|    value_loss           | 3.6e+04   |
---------------------------------------
Eval num_timesteps=285500, episode_reward=890.56 +/- 688.97
Episode length: 36.18 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=842.45 +/- 681.51
Episode length: 35.52 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=860.78 +/- 686.46
Episode length: 35.40 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 713      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 140      |
|    time_elapsed    | 1021     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=887.21 +/- 707.96
Episode length: 36.02 +/- 6.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 887       |
| time/                   |           |
|    total_timesteps      | 287000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.83e-13 |
|    explained_variance   | 0.0101    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.11e+04  |
|    n_updates            | 1436      |
|    policy_gradient_loss | 1.19e-09  |
|    value_loss           | 3.84e+04  |
---------------------------------------
Eval num_timesteps=287500, episode_reward=568.69 +/- 424.90
Episode length: 34.56 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 569      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=1044.50 +/- 755.08
Episode length: 36.58 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=916.51 +/- 742.65
Episode length: 35.20 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 141      |
|    time_elapsed    | 1029     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=757.15 +/- 670.72
Episode length: 33.98 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 757       |
| time/                   |           |
|    total_timesteps      | 289000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.95e-16 |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.57e+04  |
|    n_updates            | 1446      |
|    policy_gradient_loss | 1.62e-09  |
|    value_loss           | 3.88e+04  |
---------------------------------------
Eval num_timesteps=289500, episode_reward=832.90 +/- 683.82
Episode length: 35.68 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=767.04 +/- 626.76
Episode length: 35.60 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=624.47 +/- 531.08
Episode length: 33.96 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 142      |
|    time_elapsed    | 1036     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=718.84 +/- 639.04
Episode length: 34.22 +/- 6.97
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.2     |
|    mean_reward          | 719      |
| time/                   |          |
|    total_timesteps      | 291000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.8e-13 |
|    explained_variance   | 0.0147   |
|    learning_rate        | 0.0001   |
|    loss                 | 3.11e+04 |
|    n_updates            | 1456     |
|    policy_gradient_loss | 5.97e-10 |
|    value_loss           | 3.85e+04 |
--------------------------------------
Eval num_timesteps=291500, episode_reward=895.44 +/- 698.87
Episode length: 36.28 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=851.48 +/- 673.08
Episode length: 35.84 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=790.04 +/- 635.19
Episode length: 35.84 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 835      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 143      |
|    time_elapsed    | 1043     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=938.67 +/- 728.77
Episode length: 35.44 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 939       |
| time/                   |           |
|    total_timesteps      | 293000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.05e-15 |
|    explained_variance   | 0.015     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.49e+04  |
|    n_updates            | 1466      |
|    policy_gradient_loss | -9.9e-11  |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=293500, episode_reward=821.87 +/- 691.87
Episode length: 34.94 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=930.08 +/- 744.89
Episode length: 35.52 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 930      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=1181.51 +/- 766.57
Episode length: 37.18 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 144      |
|    time_elapsed    | 1051     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=842.17 +/- 641.09
Episode length: 36.24 +/- 5.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 842       |
| time/                   |           |
|    total_timesteps      | 295000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.84e-13 |
|    explained_variance   | 0.0185    |
|    learning_rate        | 0.0001    |
|    loss                 | 2e+04     |
|    n_updates            | 1476      |
|    policy_gradient_loss | -8.96e-10 |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=295500, episode_reward=763.92 +/- 612.31
Episode length: 35.36 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=818.21 +/- 688.45
Episode length: 34.96 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=971.39 +/- 787.36
Episode length: 35.28 +/- 8.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 838      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 145      |
|    time_elapsed    | 1058     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=847.83 +/- 678.38
Episode length: 36.02 +/- 6.95
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36       |
|    mean_reward          | 848      |
| time/                   |          |
|    total_timesteps      | 297000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.1e-15 |
|    explained_variance   | 0.0146   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.48e+04 |
|    n_updates            | 1486     |
|    policy_gradient_loss | 2.12e-10 |
|    value_loss           | 3.71e+04 |
--------------------------------------
Eval num_timesteps=297500, episode_reward=795.46 +/- 640.49
Episode length: 35.30 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=867.59 +/- 701.62
Episode length: 35.52 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=829.13 +/- 667.95
Episode length: 35.54 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=891.29 +/- 734.25
Episode length: 35.60 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 883      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 146      |
|    time_elapsed    | 1067     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=778.16 +/- 667.33
Episode length: 34.56 +/- 6.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 778       |
| time/                   |           |
|    total_timesteps      | 299500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.86e-13 |
|    explained_variance   | 0.0182    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.51e+04  |
|    n_updates            | 1496      |
|    policy_gradient_loss | 1.78e-10  |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=300000, episode_reward=804.61 +/- 661.55
Episode length: 35.20 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=1059.33 +/- 736.87
Episode length: 36.76 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=723.79 +/- 572.90
Episode length: 35.28 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 885      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 147      |
|    time_elapsed    | 1074     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=726.07 +/- 620.48
Episode length: 34.68 +/- 6.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 726       |
| time/                   |           |
|    total_timesteps      | 301500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-15 |
|    explained_variance   | 0.0134    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.71e+04  |
|    n_updates            | 1506      |
|    policy_gradient_loss | -6.84e-11 |
|    value_loss           | 3.44e+04  |
---------------------------------------
Eval num_timesteps=302000, episode_reward=864.42 +/- 711.81
Episode length: 34.92 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=884.49 +/- 754.11
Episode length: 35.02 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=773.44 +/- 657.98
Episode length: 35.00 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 148      |
|    time_elapsed    | 1081     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=785.01 +/- 666.69
Episode length: 35.00 +/- 7.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 785       |
| time/                   |           |
|    total_timesteps      | 303500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.12e-13 |
|    explained_variance   | 0.0153    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.03e+04  |
|    n_updates            | 1516      |
|    policy_gradient_loss | 1.03e-09  |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=304000, episode_reward=714.59 +/- 681.59
Episode length: 33.04 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=822.54 +/- 726.11
Episode length: 34.42 +/- 7.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=782.44 +/- 694.64
Episode length: 34.38 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 149      |
|    time_elapsed    | 1089     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=917.58 +/- 725.25
Episode length: 36.00 +/- 6.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 918       |
| time/                   |           |
|    total_timesteps      | 305500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-15 |
|    explained_variance   | 0.0114    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.52e+04  |
|    n_updates            | 1526      |
|    policy_gradient_loss | 1.56e-10  |
|    value_loss           | 3.51e+04  |
---------------------------------------
Eval num_timesteps=306000, episode_reward=696.90 +/- 611.73
Episode length: 34.58 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 697      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=890.31 +/- 716.38
Episode length: 35.38 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=846.69 +/- 670.38
Episode length: 35.32 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 900      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 150      |
|    time_elapsed    | 1096     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=959.00 +/- 741.49
Episode length: 36.16 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 959       |
| time/                   |           |
|    total_timesteps      | 307500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.08e-13 |
|    explained_variance   | 0.0184    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.25e+04  |
|    n_updates            | 1536      |
|    policy_gradient_loss | -3.84e-10 |
|    value_loss           | 3.79e+04  |
---------------------------------------
Eval num_timesteps=308000, episode_reward=785.09 +/- 616.37
Episode length: 35.58 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=672.29 +/- 577.78
Episode length: 34.38 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=790.73 +/- 665.66
Episode length: 34.94 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 903      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 151      |
|    time_elapsed    | 1103     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=766.50 +/- 649.72
Episode length: 34.34 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 766       |
| time/                   |           |
|    total_timesteps      | 309500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-15 |
|    explained_variance   | 0.0178    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.65e+04  |
|    n_updates            | 1546      |
|    policy_gradient_loss | -1.76e-10 |
|    value_loss           | 4.02e+04  |
---------------------------------------
Eval num_timesteps=310000, episode_reward=898.69 +/- 705.74
Episode length: 35.62 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=1031.04 +/- 728.88
Episode length: 36.12 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=801.72 +/- 620.51
Episode length: 36.24 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 862      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 152      |
|    time_elapsed    | 1110     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=953.48 +/- 709.48
Episode length: 36.86 +/- 5.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 953       |
| time/                   |           |
|    total_timesteps      | 311500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.11e-13 |
|    explained_variance   | 0.0199    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.52e+04  |
|    n_updates            | 1556      |
|    policy_gradient_loss | 5.15e-10  |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=312000, episode_reward=716.64 +/- 593.22
Episode length: 35.22 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=809.66 +/- 616.70
Episode length: 36.34 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=880.57 +/- 721.94
Episode length: 35.42 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 878      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 153      |
|    time_elapsed    | 1118     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=784.20 +/- 665.71
Episode length: 34.72 +/- 7.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 784       |
| time/                   |           |
|    total_timesteps      | 313500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-15 |
|    explained_variance   | 0.0139    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.64e+04  |
|    n_updates            | 1566      |
|    policy_gradient_loss | 2.76e-10  |
|    value_loss           | 3.69e+04  |
---------------------------------------
Eval num_timesteps=314000, episode_reward=723.46 +/- 602.32
Episode length: 34.78 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=766.59 +/- 622.14
Episode length: 35.00 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=873.93 +/- 685.81
Episode length: 36.20 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 154      |
|    time_elapsed    | 1125     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=726.42 +/- 651.41
Episode length: 34.54 +/- 7.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 726       |
| time/                   |           |
|    total_timesteps      | 315500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.25e-13 |
|    explained_variance   | 0.021     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.03e+04  |
|    n_updates            | 1576      |
|    policy_gradient_loss | -4.92e-10 |
|    value_loss           | 4.12e+04  |
---------------------------------------
Eval num_timesteps=316000, episode_reward=732.35 +/- 642.57
Episode length: 34.88 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=787.48 +/- 698.30
Episode length: 34.84 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=758.43 +/- 600.92
Episode length: 35.10 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 155      |
|    time_elapsed    | 1132     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=739.12 +/- 647.62
Episode length: 34.80 +/- 6.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 739       |
| time/                   |           |
|    total_timesteps      | 317500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-15 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.38e+04  |
|    n_updates            | 1586      |
|    policy_gradient_loss | 8.18e-10  |
|    value_loss           | 3.36e+04  |
---------------------------------------
Eval num_timesteps=318000, episode_reward=864.51 +/- 673.32
Episode length: 36.14 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=861.15 +/- 692.39
Episode length: 35.66 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=638.41 +/- 489.10
Episode length: 34.30 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 638      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 886      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 156      |
|    time_elapsed    | 1139     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=818.67 +/- 664.53
Episode length: 35.52 +/- 5.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 819       |
| time/                   |           |
|    total_timesteps      | 319500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.04e-13 |
|    explained_variance   | 0.0271    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.97e+04  |
|    n_updates            | 1596      |
|    policy_gradient_loss | 1.14e-09  |
|    value_loss           | 4.33e+04  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=924.25 +/- 698.20
Episode length: 36.10 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 924      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=774.84 +/- 668.06
Episode length: 34.64 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=827.08 +/- 720.44
Episode length: 34.92 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=806.38 +/- 685.87
Episode length: 34.42 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 943      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 157      |
|    time_elapsed    | 1148     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=892.12 +/- 752.95
Episode length: 34.94 +/- 8.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 892       |
| time/                   |           |
|    total_timesteps      | 322000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.33e-15 |
|    explained_variance   | 0.0179    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.51e+04  |
|    n_updates            | 1606      |
|    policy_gradient_loss | -6.24e-10 |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=322500, episode_reward=829.43 +/- 690.60
Episode length: 35.38 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=846.04 +/- 734.85
Episode length: 34.38 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=635.20 +/- 548.91
Episode length: 33.96 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 880      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 158      |
|    time_elapsed    | 1155     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=800.04 +/- 623.23
Episode length: 35.40 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 800       |
| time/                   |           |
|    total_timesteps      | 324000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.1e-13  |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.34e+04  |
|    n_updates            | 1616      |
|    policy_gradient_loss | -6.66e-10 |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=324500, episode_reward=724.02 +/- 607.82
Episode length: 34.62 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=1015.94 +/- 729.59
Episode length: 36.70 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=756.63 +/- 666.17
Episode length: 33.40 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 159      |
|    time_elapsed    | 1162     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=728.56 +/- 652.06
Episode length: 33.44 +/- 7.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.4      |
|    mean_reward          | 729       |
| time/                   |           |
|    total_timesteps      | 326000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.26e-15 |
|    explained_variance   | 0.00539   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.1e+04   |
|    n_updates            | 1626      |
|    policy_gradient_loss | -5.75e-10 |
|    value_loss           | 3.05e+04  |
---------------------------------------
Eval num_timesteps=326500, episode_reward=817.17 +/- 745.31
Episode length: 33.84 +/- 7.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=832.82 +/- 651.98
Episode length: 36.10 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=796.86 +/- 689.28
Episode length: 34.18 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 160      |
|    time_elapsed    | 1169     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=722.53 +/- 689.59
Episode length: 33.52 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 723       |
| time/                   |           |
|    total_timesteps      | 328000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.35e-20 |
|    explained_variance   | 0.0183    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.06e+04  |
|    n_updates            | 1636      |
|    policy_gradient_loss | -7.31e-10 |
|    value_loss           | 3.67e+04  |
---------------------------------------
Eval num_timesteps=328500, episode_reward=722.48 +/- 551.75
Episode length: 35.20 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=692.55 +/- 623.40
Episode length: 33.56 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=681.97 +/- 577.80
Episode length: 34.02 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 711      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 161      |
|    time_elapsed    | 1177     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=868.24 +/- 700.46
Episode length: 34.96 +/- 7.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 868       |
| time/                   |           |
|    total_timesteps      | 330000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.76e-18 |
|    explained_variance   | 0.00423   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.05e+04  |
|    n_updates            | 1646      |
|    policy_gradient_loss | -2.44e-09 |
|    value_loss           | 5.46e+04  |
---------------------------------------
Eval num_timesteps=330500, episode_reward=973.19 +/- 667.67
Episode length: 37.66 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.7     |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=758.54 +/- 668.55
Episode length: 34.16 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=733.06 +/- 564.04
Episode length: 34.82 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 162      |
|    time_elapsed    | 1184     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=772.89 +/- 627.26
Episode length: 35.26 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 773       |
| time/                   |           |
|    total_timesteps      | 332000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.99e-13 |
|    explained_variance   | 0.0142    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.69e+04  |
|    n_updates            | 1656      |
|    policy_gradient_loss | -1.09e-09 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=332500, episode_reward=736.22 +/- 628.39
Episode length: 34.64 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=746.57 +/- 635.98
Episode length: 34.78 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=830.00 +/- 610.24
Episode length: 36.70 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 163      |
|    time_elapsed    | 1191     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=1011.47 +/- 746.48
Episode length: 36.12 +/- 6.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 334000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.75e-15 |
|    explained_variance   | 0.018     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.59e+04  |
|    n_updates            | 1666      |
|    policy_gradient_loss | -2.62e-10 |
|    value_loss           | 3.49e+04  |
---------------------------------------
Eval num_timesteps=334500, episode_reward=764.72 +/- 687.10
Episode length: 34.54 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=779.91 +/- 661.45
Episode length: 35.08 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=825.73 +/- 658.88
Episode length: 35.58 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 164      |
|    time_elapsed    | 1198     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=705.36 +/- 600.32
Episode length: 34.60 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 705       |
| time/                   |           |
|    total_timesteps      | 336000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.04e-13 |
|    explained_variance   | 0.0166    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.82e+04  |
|    n_updates            | 1676      |
|    policy_gradient_loss | 7.45e-10  |
|    value_loss           | 3.98e+04  |
---------------------------------------
Eval num_timesteps=336500, episode_reward=940.44 +/- 687.78
Episode length: 36.74 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 940      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=743.73 +/- 586.75
Episode length: 35.16 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=781.19 +/- 655.37
Episode length: 35.26 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 165      |
|    time_elapsed    | 1206     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=1031.51 +/- 815.44
Episode length: 35.92 +/- 7.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 1.03e+03  |
| time/                   |           |
|    total_timesteps      | 338000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.11e-15 |
|    explained_variance   | 0.0129    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.5e+04   |
|    n_updates            | 1686      |
|    policy_gradient_loss | 2.46e-10  |
|    value_loss           | 3.31e+04  |
---------------------------------------
Eval num_timesteps=338500, episode_reward=881.51 +/- 732.29
Episode length: 35.24 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=721.97 +/- 611.25
Episode length: 34.68 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=895.62 +/- 689.37
Episode length: 35.68 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 725      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 166      |
|    time_elapsed    | 1213     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=831.39 +/- 689.63
Episode length: 35.30 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 831       |
| time/                   |           |
|    total_timesteps      | 340000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.75e-13 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.59e+04  |
|    n_updates            | 1696      |
|    policy_gradient_loss | 5.78e-10  |
|    value_loss           | 4.21e+04  |
---------------------------------------
Eval num_timesteps=340500, episode_reward=844.88 +/- 739.67
Episode length: 35.26 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=743.14 +/- 625.36
Episode length: 34.98 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=843.23 +/- 687.78
Episode length: 35.30 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=849.24 +/- 732.33
Episode length: 34.32 +/- 7.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 167      |
|    time_elapsed    | 1221     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=896.93 +/- 741.57
Episode length: 35.74 +/- 6.88
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.7     |
|    mean_reward          | 897      |
| time/                   |          |
|    total_timesteps      | 342500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -4.2e-15 |
|    explained_variance   | 0.0128   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.42e+04 |
|    n_updates            | 1706     |
|    policy_gradient_loss | 9.17e-10 |
|    value_loss           | 3.58e+04 |
--------------------------------------
Eval num_timesteps=343000, episode_reward=860.29 +/- 720.56
Episode length: 35.16 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=871.07 +/- 725.69
Episode length: 35.22 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=989.61 +/- 751.11
Episode length: 36.16 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 726      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 168      |
|    time_elapsed    | 1229     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=838.82 +/- 693.72
Episode length: 35.02 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 344500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.76e-13 |
|    explained_variance   | 0.00209   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.44e+04  |
|    n_updates            | 1716      |
|    policy_gradient_loss | -9.4e-10  |
|    value_loss           | 3.76e+04  |
---------------------------------------
Eval num_timesteps=345000, episode_reward=834.58 +/- 664.28
Episode length: 35.56 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=858.87 +/- 706.36
Episode length: 35.12 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=635.97 +/- 579.30
Episode length: 33.38 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 636      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 169      |
|    time_elapsed    | 1236     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=847.56 +/- 701.41
Episode length: 34.70 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 848       |
| time/                   |           |
|    total_timesteps      | 346500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.74e-15 |
|    explained_variance   | 0.0136    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.59e+04  |
|    n_updates            | 1726      |
|    policy_gradient_loss | -2.46e-10 |
|    value_loss           | 3.59e+04  |
---------------------------------------
Eval num_timesteps=347000, episode_reward=877.01 +/- 712.50
Episode length: 34.94 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=782.41 +/- 619.33
Episode length: 35.12 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=930.84 +/- 717.06
Episode length: 36.28 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 897      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 170      |
|    time_elapsed    | 1243     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=811.73 +/- 700.22
Episode length: 34.66 +/- 6.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 348500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.66e-13 |
|    explained_variance   | 0.0179    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.91e+04  |
|    n_updates            | 1736      |
|    policy_gradient_loss | 1.12e-09  |
|    value_loss           | 3.78e+04  |
---------------------------------------
Eval num_timesteps=349000, episode_reward=812.69 +/- 624.13
Episode length: 36.00 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=705.41 +/- 619.28
Episode length: 33.96 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=766.35 +/- 611.78
Episode length: 35.78 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 171      |
|    time_elapsed    | 1250     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=739.84 +/- 674.15
Episode length: 33.86 +/- 7.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 740       |
| time/                   |           |
|    total_timesteps      | 350500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.54e-15 |
|    explained_variance   | 0.0169    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.21e+04  |
|    n_updates            | 1746      |
|    policy_gradient_loss | 2.74e-10  |
|    value_loss           | 3.62e+04  |
---------------------------------------
Eval num_timesteps=351000, episode_reward=930.49 +/- 705.17
Episode length: 36.32 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 930      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=844.80 +/- 713.24
Episode length: 35.06 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=920.12 +/- 704.17
Episode length: 36.00 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 845      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 172      |
|    time_elapsed    | 1257     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=876.97 +/- 679.73
Episode length: 35.96 +/- 5.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 877       |
| time/                   |           |
|    total_timesteps      | 352500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.42e-13 |
|    explained_variance   | 0.00835   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.77e+04  |
|    n_updates            | 1756      |
|    policy_gradient_loss | -2.97e-10 |
|    value_loss           | 3.26e+04  |
---------------------------------------
Eval num_timesteps=353000, episode_reward=821.35 +/- 688.05
Episode length: 35.10 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=755.51 +/- 640.94
Episode length: 34.76 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=904.83 +/- 753.21
Episode length: 34.82 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 173      |
|    time_elapsed    | 1265     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=865.81 +/- 689.10
Episode length: 35.94 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 866       |
| time/                   |           |
|    total_timesteps      | 354500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.16e-15 |
|    explained_variance   | 0.017     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.33e+04  |
|    n_updates            | 1766      |
|    policy_gradient_loss | -3.22e-10 |
|    value_loss           | 3.84e+04  |
---------------------------------------
Eval num_timesteps=355000, episode_reward=813.56 +/- 663.82
Episode length: 35.22 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=795.17 +/- 725.40
Episode length: 34.12 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=988.08 +/- 699.37
Episode length: 37.30 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 897      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 174      |
|    time_elapsed    | 1272     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=733.81 +/- 599.37
Episode length: 35.60 +/- 5.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 734       |
| time/                   |           |
|    total_timesteps      | 356500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.41e-13 |
|    explained_variance   | 0.0192    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.67e+04  |
|    n_updates            | 1776      |
|    policy_gradient_loss | -2.5e-10  |
|    value_loss           | 3.98e+04  |
---------------------------------------
Eval num_timesteps=357000, episode_reward=920.64 +/- 728.23
Episode length: 35.88 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=846.62 +/- 734.09
Episode length: 34.24 +/- 7.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=884.55 +/- 745.93
Episode length: 35.22 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 808      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 175      |
|    time_elapsed    | 1280     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=678.03 +/- 607.09
Episode length: 33.32 +/- 6.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.3      |
|    mean_reward          | 678       |
| time/                   |           |
|    total_timesteps      | 358500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.55e-15 |
|    explained_variance   | 0.0189    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.88e+04  |
|    n_updates            | 1786      |
|    policy_gradient_loss | 3.22e-10  |
|    value_loss           | 3.71e+04  |
---------------------------------------
Eval num_timesteps=359000, episode_reward=775.54 +/- 654.49
Episode length: 35.28 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=788.08 +/- 666.90
Episode length: 34.54 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=813.08 +/- 695.29
Episode length: 34.74 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.2     |
|    ep_rew_mean     | 681      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 176      |
|    time_elapsed    | 1287     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=797.54 +/- 674.18
Episode length: 34.70 +/- 6.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 798       |
| time/                   |           |
|    total_timesteps      | 360500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.85e-13 |
|    explained_variance   | 0.00852   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.53e+04  |
|    n_updates            | 1796      |
|    policy_gradient_loss | 1.46e-10  |
|    value_loss           | 4.13e+04  |
---------------------------------------
Eval num_timesteps=361000, episode_reward=932.47 +/- 765.02
Episode length: 35.04 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=812.12 +/- 700.15
Episode length: 34.76 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=907.68 +/- 726.94
Episode length: 35.72 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 726      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 177      |
|    time_elapsed    | 1294     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=1140.65 +/- 800.76
Episode length: 36.48 +/- 6.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 1.14e+03  |
| time/                   |           |
|    total_timesteps      | 362500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.33e-15 |
|    explained_variance   | 0.0185    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.85e+04  |
|    n_updates            | 1806      |
|    policy_gradient_loss | -3.61e-10 |
|    value_loss           | 3.58e+04  |
---------------------------------------
Eval num_timesteps=363000, episode_reward=920.26 +/- 742.42
Episode length: 35.36 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=907.15 +/- 754.80
Episode length: 35.18 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=818.60 +/- 684.90
Episode length: 34.76 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=939.21 +/- 750.68
Episode length: 35.06 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 712      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 178      |
|    time_elapsed    | 1302     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=941.88 +/- 738.49
Episode length: 36.14 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 942       |
| time/                   |           |
|    total_timesteps      | 365000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.12e-13 |
|    explained_variance   | 0.00926   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.25e+04  |
|    n_updates            | 1816      |
|    policy_gradient_loss | 4.53e-10  |
|    value_loss           | 3.73e+04  |
---------------------------------------
Eval num_timesteps=365500, episode_reward=761.61 +/- 664.68
Episode length: 34.40 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=823.49 +/- 660.05
Episode length: 35.98 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=659.59 +/- 696.68
Episode length: 32.60 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 660      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 725      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 179      |
|    time_elapsed    | 1310     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=882.09 +/- 662.41
Episode length: 36.32 +/- 6.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 882       |
| time/                   |           |
|    total_timesteps      | 367000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.53e-09 |
|    explained_variance   | 0.0172    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.03e+04  |
|    n_updates            | 1826      |
|    policy_gradient_loss | -6.84e-10 |
|    value_loss           | 3.65e+04  |
---------------------------------------
Eval num_timesteps=367500, episode_reward=636.71 +/- 517.26
Episode length: 34.42 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=713.73 +/- 586.32
Episode length: 34.54 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=827.45 +/- 626.72
Episode length: 35.88 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 180      |
|    time_elapsed    | 1317     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=759.59 +/- 675.17
Episode length: 34.78 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 760       |
| time/                   |           |
|    total_timesteps      | 369000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.49e-11 |
|    explained_variance   | 0.0121    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.81e+04  |
|    n_updates            | 1836      |
|    policy_gradient_loss | -5.59e-10 |
|    value_loss           | 3.55e+04  |
---------------------------------------
Eval num_timesteps=369500, episode_reward=853.03 +/- 682.01
Episode length: 35.32 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=792.58 +/- 732.14
Episode length: 33.76 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=900.58 +/- 724.27
Episode length: 35.46 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 817      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 181      |
|    time_elapsed    | 1324     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=838.66 +/- 650.12
Episode length: 36.18 +/- 6.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 371000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.39e-09 |
|    explained_variance   | 0.0178    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.92e+04  |
|    n_updates            | 1846      |
|    policy_gradient_loss | 4.87e-10  |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=371500, episode_reward=768.75 +/- 745.91
Episode length: 33.24 +/- 7.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=786.49 +/- 646.34
Episode length: 34.88 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=863.68 +/- 681.89
Episode length: 35.56 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 182      |
|    time_elapsed    | 1331     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=807.41 +/- 724.27
Episode length: 34.52 +/- 7.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 807       |
| time/                   |           |
|    total_timesteps      | 373000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.34e-11 |
|    explained_variance   | 0.00731   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.23e+04  |
|    n_updates            | 1856      |
|    policy_gradient_loss | 5.69e-10  |
|    value_loss           | 3.04e+04  |
---------------------------------------
Eval num_timesteps=373500, episode_reward=831.32 +/- 666.01
Episode length: 35.66 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=866.85 +/- 651.31
Episode length: 36.50 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=779.92 +/- 682.75
Episode length: 34.46 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 819      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 183      |
|    time_elapsed    | 1338     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=713.78 +/- 629.07
Episode length: 34.20 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 714       |
| time/                   |           |
|    total_timesteps      | 375000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.02e-15 |
|    explained_variance   | 0.0265    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.35e+04  |
|    n_updates            | 1866      |
|    policy_gradient_loss | 3.49e-10  |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=375500, episode_reward=695.67 +/- 611.30
Episode length: 34.02 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 696      |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=822.40 +/- 687.45
Episode length: 34.44 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=848.64 +/- 683.97
Episode length: 35.06 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 898      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 184      |
|    time_elapsed    | 1345     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=837.72 +/- 683.23
Episode length: 34.74 +/- 6.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 377000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.77e-13 |
|    explained_variance   | 0.0187    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.69e+04  |
|    n_updates            | 1876      |
|    policy_gradient_loss | -4.98e-10 |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=377500, episode_reward=841.20 +/- 700.66
Episode length: 35.34 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=947.73 +/- 717.46
Episode length: 35.76 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=713.20 +/- 685.13
Episode length: 32.98 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 887      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 185      |
|    time_elapsed    | 1353     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=874.61 +/- 704.58
Episode length: 35.40 +/- 6.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 379000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.2e-15  |
|    explained_variance   | 0.0219    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.66e+04  |
|    n_updates            | 1886      |
|    policy_gradient_loss | -2.14e-09 |
|    value_loss           | 4.15e+04  |
---------------------------------------
Eval num_timesteps=379500, episode_reward=717.64 +/- 616.46
Episode length: 34.40 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=721.93 +/- 603.34
Episode length: 34.10 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=968.05 +/- 739.72
Episode length: 35.72 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 934      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 186      |
|    time_elapsed    | 1360     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=818.17 +/- 734.40
Episode length: 34.68 +/- 7.28
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.7     |
|    mean_reward          | 818      |
| time/                   |          |
|    total_timesteps      | 381000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -8.9e-13 |
|    explained_variance   | 0.0251   |
|    learning_rate        | 0.0001   |
|    loss                 | 2.18e+04 |
|    n_updates            | 1896     |
|    policy_gradient_loss | 1.1e-09  |
|    value_loss           | 4.23e+04 |
--------------------------------------
Eval num_timesteps=381500, episode_reward=898.66 +/- 693.76
Episode length: 35.92 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=954.53 +/- 746.59
Episode length: 35.70 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=790.91 +/- 674.51
Episode length: 35.24 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 825      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 187      |
|    time_elapsed    | 1367     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=729.74 +/- 663.93
Episode length: 33.64 +/- 6.17
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 33.6     |
|    mean_reward          | 730      |
| time/                   |          |
|    total_timesteps      | 383000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.6e-09 |
|    explained_variance   | 0.0155   |
|    learning_rate        | 0.0001   |
|    loss                 | 2.08e+04 |
|    n_updates            | 1906     |
|    policy_gradient_loss | 2.59e-10 |
|    value_loss           | 3.81e+04 |
--------------------------------------
Eval num_timesteps=383500, episode_reward=780.08 +/- 682.39
Episode length: 34.48 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=817.98 +/- 671.33
Episode length: 35.04 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=784.88 +/- 662.58
Episode length: 35.10 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=897.95 +/- 765.27
Episode length: 34.96 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 188      |
|    time_elapsed    | 1375     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=871.41 +/- 668.05
Episode length: 36.32 +/- 5.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 871       |
| time/                   |           |
|    total_timesteps      | 385500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.51e-11 |
|    explained_variance   | 0.0131    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.43e+04  |
|    n_updates            | 1916      |
|    policy_gradient_loss | -2.5e-10  |
|    value_loss           | 3.81e+04  |
---------------------------------------
Eval num_timesteps=386000, episode_reward=899.68 +/- 701.12
Episode length: 35.78 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 900      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=968.66 +/- 759.48
Episode length: 35.82 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=806.90 +/- 677.53
Episode length: 35.26 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 882      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 189      |
|    time_elapsed    | 1382     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=729.96 +/- 579.77
Episode length: 35.24 +/- 6.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 730       |
| time/                   |           |
|    total_timesteps      | 387500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.33e-15 |
|    explained_variance   | 0.0197    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.33e+04  |
|    n_updates            | 1926      |
|    policy_gradient_loss | -1.1e-09  |
|    value_loss           | 3.85e+04  |
---------------------------------------
Eval num_timesteps=388000, episode_reward=819.47 +/- 658.66
Episode length: 35.70 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=894.96 +/- 672.09
Episode length: 36.04 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=927.22 +/- 775.02
Episode length: 35.30 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 190      |
|    time_elapsed    | 1390     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=958.64 +/- 745.64
Episode length: 36.40 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 959       |
| time/                   |           |
|    total_timesteps      | 389500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.14e-13 |
|    explained_variance   | 0.0143    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.78e+04  |
|    n_updates            | 1936      |
|    policy_gradient_loss | -1.51e-10 |
|    value_loss           | 3.65e+04  |
---------------------------------------
Eval num_timesteps=390000, episode_reward=898.49 +/- 732.56
Episode length: 35.54 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=934.88 +/- 716.36
Episode length: 36.56 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=1120.35 +/- 760.23
Episode length: 36.94 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 805      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 191      |
|    time_elapsed    | 1397     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=915.57 +/- 665.42
Episode length: 36.46 +/- 5.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 916       |
| time/                   |           |
|    total_timesteps      | 391500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.36e-09 |
|    explained_variance   | 0.0176    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.94e+04  |
|    n_updates            | 1946      |
|    policy_gradient_loss | -5.7e-10  |
|    value_loss           | 3.41e+04  |
---------------------------------------
Eval num_timesteps=392000, episode_reward=855.86 +/- 699.21
Episode length: 35.84 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=855.77 +/- 701.95
Episode length: 35.36 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=943.40 +/- 703.57
Episode length: 36.34 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 867      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 192      |
|    time_elapsed    | 1404     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=763.89 +/- 621.21
Episode length: 35.22 +/- 6.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 764       |
| time/                   |           |
|    total_timesteps      | 393500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.27e-11 |
|    explained_variance   | 0.0155    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.41e+04  |
|    n_updates            | 1956      |
|    policy_gradient_loss | 5.97e-11  |
|    value_loss           | 4.71e+04  |
---------------------------------------
Eval num_timesteps=394000, episode_reward=898.14 +/- 699.67
Episode length: 36.10 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=892.87 +/- 732.80
Episode length: 35.70 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=807.59 +/- 653.95
Episode length: 35.48 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 953      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 193      |
|    time_elapsed    | 1412     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=871.56 +/- 624.51
Episode length: 37.14 +/- 5.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.1      |
|    mean_reward          | 872       |
| time/                   |           |
|    total_timesteps      | 395500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.13e-09 |
|    explained_variance   | 0.0243    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.04e+04  |
|    n_updates            | 1966      |
|    policy_gradient_loss | 1.3e-09   |
|    value_loss           | 4.48e+04  |
---------------------------------------
Eval num_timesteps=396000, episode_reward=847.22 +/- 684.16
Episode length: 35.10 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=936.72 +/- 768.93
Episode length: 35.10 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 937      |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=757.90 +/- 620.82
Episode length: 34.50 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 837      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 194      |
|    time_elapsed    | 1419     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=810.28 +/- 683.45
Episode length: 35.42 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 810       |
| time/                   |           |
|    total_timesteps      | 397500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.57e-11 |
|    explained_variance   | 0.0117    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.16e+04  |
|    n_updates            | 1976      |
|    policy_gradient_loss | -6.56e-10 |
|    value_loss           | 3.1e+04   |
---------------------------------------
Eval num_timesteps=398000, episode_reward=963.76 +/- 722.90
Episode length: 36.50 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 964      |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=751.33 +/- 578.23
Episode length: 35.52 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=781.55 +/- 689.49
Episode length: 34.96 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 703      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 195      |
|    time_elapsed    | 1426     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=835.24 +/- 682.43
Episode length: 35.98 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 835       |
| time/                   |           |
|    total_timesteps      | 399500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.82e-15 |
|    explained_variance   | 0.0108    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.62e+04  |
|    n_updates            | 1986      |
|    policy_gradient_loss | -1.07e-09 |
|    value_loss           | 3.31e+04  |
---------------------------------------
Eval num_timesteps=400000, episode_reward=683.56 +/- 533.98
Episode length: 35.78 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=638.99 +/- 542.43
Episode length: 34.50 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 639      |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=703.53 +/- 614.22
Episode length: 34.16 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.9     |
|    ep_rew_mean     | 656      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 196      |
|    time_elapsed    | 1433     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=749.20 +/- 628.05
Episode length: 35.06 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 749       |
| time/                   |           |
|    total_timesteps      | 401500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3e-12    |
|    explained_variance   | 0.0176    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.31e+04  |
|    n_updates            | 1996      |
|    policy_gradient_loss | -6.98e-10 |
|    value_loss           | 4.04e+04  |
---------------------------------------
Eval num_timesteps=402000, episode_reward=720.65 +/- 660.12
Episode length: 34.24 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=877.26 +/- 765.39
Episode length: 34.68 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=862.33 +/- 692.20
Episode length: 35.52 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 725      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 197      |
|    time_elapsed    | 1440     |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=989.26 +/- 710.31
Episode length: 36.98 +/- 5.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 989       |
| time/                   |           |
|    total_timesteps      | 403500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.36e-09 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.29e+04  |
|    n_updates            | 2006      |
|    policy_gradient_loss | -1.83e-10 |
|    value_loss           | 3.38e+04  |
---------------------------------------
Eval num_timesteps=404000, episode_reward=811.32 +/- 675.91
Episode length: 34.84 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=737.91 +/- 584.67
Episode length: 35.28 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=991.62 +/- 745.42
Episode length: 36.52 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 992      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=711.93 +/- 548.32
Episode length: 35.18 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 198      |
|    time_elapsed    | 1449     |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=860.24 +/- 667.07
Episode length: 35.70 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 860       |
| time/                   |           |
|    total_timesteps      | 406000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.69e-11 |
|    explained_variance   | 0.0116    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.84e+04  |
|    n_updates            | 2016      |
|    policy_gradient_loss | -8.72e-10 |
|    value_loss           | 3.45e+04  |
---------------------------------------
Eval num_timesteps=406500, episode_reward=888.60 +/- 670.56
Episode length: 36.60 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=913.82 +/- 729.46
Episode length: 35.72 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=900.95 +/- 714.06
Episode length: 35.28 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 199      |
|    time_elapsed    | 1456     |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=861.15 +/- 693.37
Episode length: 35.52 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 861       |
| time/                   |           |
|    total_timesteps      | 408000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7e-09    |
|    explained_variance   | 0.0161    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.08e+04  |
|    n_updates            | 2026      |
|    policy_gradient_loss | -8.95e-10 |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=408500, episode_reward=832.93 +/- 685.84
Episode length: 35.16 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=748.60 +/- 680.25
Episode length: 34.00 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=773.44 +/- 642.07
Episode length: 34.50 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 200      |
|    time_elapsed    | 1463     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=809.96 +/- 635.79
Episode length: 35.78 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 810       |
| time/                   |           |
|    total_timesteps      | 410000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.24e-11 |
|    explained_variance   | 0.0146    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.33e+04  |
|    n_updates            | 2036      |
|    policy_gradient_loss | -2.27e-10 |
|    value_loss           | 4.17e+04  |
---------------------------------------
Eval num_timesteps=410500, episode_reward=752.70 +/- 631.01
Episode length: 34.88 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=846.33 +/- 672.40
Episode length: 35.56 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=839.77 +/- 660.58
Episode length: 35.32 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 706      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 201      |
|    time_elapsed    | 1471     |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=902.65 +/- 695.14
Episode length: 35.98 +/- 5.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 903       |
| time/                   |           |
|    total_timesteps      | 412000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39e-14 |
|    explained_variance   | 0.0139    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.64e+04  |
|    n_updates            | 2046      |
|    policy_gradient_loss | 1.44e-10  |
|    value_loss           | 3.38e+04  |
---------------------------------------
Eval num_timesteps=412500, episode_reward=638.89 +/- 635.31
Episode length: 33.08 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 639      |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=881.13 +/- 704.95
Episode length: 35.16 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=865.81 +/- 686.52
Episode length: 35.56 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 877      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 202      |
|    time_elapsed    | 1478     |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=692.29 +/- 587.80
Episode length: 34.20 +/- 7.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 692       |
| time/                   |           |
|    total_timesteps      | 414000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.29e-19 |
|    explained_variance   | 0.0304    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.08e+04  |
|    n_updates            | 2056      |
|    policy_gradient_loss | 1.48e-10  |
|    value_loss           | 4.09e+04  |
---------------------------------------
Eval num_timesteps=414500, episode_reward=771.12 +/- 631.33
Episode length: 35.14 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=735.65 +/- 681.79
Episode length: 33.90 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=774.84 +/- 665.95
Episode length: 34.80 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.4     |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    fps             | 279      |
|    iterations      | 203      |
|    time_elapsed    | 1485     |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=953.53 +/- 729.32
Episode length: 36.46 +/- 6.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 954       |
| time/                   |           |
|    total_timesteps      | 416000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.01e-16 |
|    explained_variance   | 0.0305    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.09e+04  |
|    n_updates            | 2066      |
|    policy_gradient_loss | -4.39e-10 |
|    value_loss           | 4.37e+04  |
---------------------------------------
Eval num_timesteps=416500, episode_reward=858.34 +/- 630.40
Episode length: 36.78 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=790.50 +/- 654.30
Episode length: 35.44 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=895.83 +/- 673.48
Episode length: 36.18 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 844      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 204      |
|    time_elapsed    | 1492     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=837.68 +/- 677.89
Episode length: 35.40 +/- 6.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 418000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.96e-12 |
|    explained_variance   | 0.011     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.9e+04   |
|    n_updates            | 2076      |
|    policy_gradient_loss | -1.29e-09 |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=418500, episode_reward=888.03 +/- 704.27
Episode length: 36.26 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=851.59 +/- 708.34
Episode length: 34.80 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=953.28 +/- 717.20
Episode length: 36.88 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 953      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 205      |
|    time_elapsed    | 1500     |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=886.86 +/- 712.29
Episode length: 35.36 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 887       |
| time/                   |           |
|    total_timesteps      | 420000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.9e-14  |
|    explained_variance   | 0.0181    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.65e+04  |
|    n_updates            | 2086      |
|    policy_gradient_loss | -6.69e-10 |
|    value_loss           | 3.6e+04   |
---------------------------------------
Eval num_timesteps=420500, episode_reward=790.66 +/- 672.19
Episode length: 34.94 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=909.43 +/- 722.41
Episode length: 35.92 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=669.83 +/- 578.20
Episode length: 34.00 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 206      |
|    time_elapsed    | 1507     |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=674.98 +/- 491.69
Episode length: 35.30 +/- 4.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 675       |
| time/                   |           |
|    total_timesteps      | 422000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.86e-12 |
|    explained_variance   | 0.00965   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.56e+04  |
|    n_updates            | 2096      |
|    policy_gradient_loss | 2.81e-10  |
|    value_loss           | 3.78e+04  |
---------------------------------------
Eval num_timesteps=422500, episode_reward=713.53 +/- 676.62
Episode length: 33.24 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=730.64 +/- 623.82
Episode length: 34.92 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=845.80 +/- 683.28
Episode length: 35.66 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 207      |
|    time_elapsed    | 1514     |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=972.95 +/- 683.87
Episode length: 36.88 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 973       |
| time/                   |           |
|    total_timesteps      | 424000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.72e-14 |
|    explained_variance   | 0.0192    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.78e+04  |
|    n_updates            | 2106      |
|    policy_gradient_loss | 5.73e-10  |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=424500, episode_reward=758.41 +/- 582.63
Episode length: 35.36 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=869.88 +/- 711.08
Episode length: 35.14 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=836.53 +/- 744.55
Episode length: 34.26 +/- 7.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 208      |
|    time_elapsed    | 1521     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=860.04 +/- 692.72
Episode length: 35.42 +/- 7.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 860       |
| time/                   |           |
|    total_timesteps      | 426000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.88e-12 |
|    explained_variance   | 0.0156    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.66e+04  |
|    n_updates            | 2116      |
|    policy_gradient_loss | 1.98e-09  |
|    value_loss           | 3.97e+04  |
---------------------------------------
Eval num_timesteps=426500, episode_reward=827.27 +/- 643.45
Episode length: 36.22 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=795.23 +/- 681.01
Episode length: 35.14 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=859.93 +/- 655.90
Episode length: 36.58 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=771.06 +/- 644.55
Episode length: 35.06 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 209      |
|    time_elapsed    | 1530     |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=817.56 +/- 693.24
Episode length: 35.30 +/- 5.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 818       |
| time/                   |           |
|    total_timesteps      | 428500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.09e-09 |
|    explained_variance   | 0.0193    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.86e+04  |
|    n_updates            | 2126      |
|    policy_gradient_loss | -4.1e-10  |
|    value_loss           | 3.98e+04  |
---------------------------------------
Eval num_timesteps=429000, episode_reward=717.90 +/- 642.10
Episode length: 34.28 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=687.60 +/- 545.47
Episode length: 34.64 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 688      |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=800.01 +/- 638.08
Episode length: 35.22 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 838      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 210      |
|    time_elapsed    | 1537     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=809.04 +/- 657.91
Episode length: 35.14 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 809       |
| time/                   |           |
|    total_timesteps      | 430500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.65e-11 |
|    explained_variance   | 0.0187    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.05e+04  |
|    n_updates            | 2136      |
|    policy_gradient_loss | -1.01e-09 |
|    value_loss           | 4.87e+04  |
---------------------------------------
Eval num_timesteps=431000, episode_reward=867.44 +/- 741.08
Episode length: 34.66 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=794.09 +/- 620.98
Episode length: 35.72 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=822.79 +/- 704.87
Episode length: 34.64 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 211      |
|    time_elapsed    | 1544     |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=929.01 +/- 713.21
Episode length: 35.98 +/- 7.22
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36       |
|    mean_reward          | 929      |
| time/                   |          |
|    total_timesteps      | 432500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.2e-14 |
|    explained_variance   | 0.0152   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.48e+04 |
|    n_updates            | 2146     |
|    policy_gradient_loss | 7.16e-10 |
|    value_loss           | 3.38e+04 |
--------------------------------------
Eval num_timesteps=433000, episode_reward=688.62 +/- 597.93
Episode length: 34.14 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=742.41 +/- 650.28
Episode length: 34.56 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=716.16 +/- 641.35
Episode length: 33.74 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 826      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 212      |
|    time_elapsed    | 1551     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=794.97 +/- 650.66
Episode length: 34.86 +/- 6.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 795       |
| time/                   |           |
|    total_timesteps      | 434500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.48e-12 |
|    explained_variance   | 0.0174    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.62e+04  |
|    n_updates            | 2156      |
|    policy_gradient_loss | 3.23e-10  |
|    value_loss           | 3.89e+04  |
---------------------------------------
Eval num_timesteps=435000, episode_reward=748.24 +/- 622.29
Episode length: 34.52 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=782.78 +/- 709.96
Episode length: 34.26 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=904.14 +/- 736.05
Episode length: 35.62 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 711      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 213      |
|    time_elapsed    | 1558     |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=998.61 +/- 716.91
Episode length: 37.26 +/- 6.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.3      |
|    mean_reward          | 999       |
| time/                   |           |
|    total_timesteps      | 436500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.04e-08 |
|    explained_variance   | 0.00319   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.9e+04   |
|    n_updates            | 2166      |
|    policy_gradient_loss | -4.5e-10  |
|    value_loss           | 2.87e+04  |
---------------------------------------
Eval num_timesteps=437000, episode_reward=767.38 +/- 644.24
Episode length: 34.84 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=768.75 +/- 605.55
Episode length: 35.64 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=813.61 +/- 716.81
Episode length: 35.08 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 695      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 214      |
|    time_elapsed    | 1565     |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=734.76 +/- 638.36
Episode length: 34.84 +/- 6.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 735       |
| time/                   |           |
|    total_timesteps      | 438500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.12e-10 |
|    explained_variance   | 0.0124    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.63e+04  |
|    n_updates            | 2176      |
|    policy_gradient_loss | -7.1e-10  |
|    value_loss           | 3.61e+04  |
---------------------------------------
Eval num_timesteps=439000, episode_reward=865.00 +/- 714.61
Episode length: 35.18 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=688.51 +/- 644.76
Episode length: 33.62 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=814.91 +/- 699.24
Episode length: 34.72 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 215      |
|    time_elapsed    | 1573     |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=721.00 +/- 660.76
Episode length: 33.68 +/- 7.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 721       |
| time/                   |           |
|    total_timesteps      | 440500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.62e-13 |
|    explained_variance   | 0.0144    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.14e+04  |
|    n_updates            | 2186      |
|    policy_gradient_loss | 2.98e-10  |
|    value_loss           | 3.54e+04  |
---------------------------------------
Eval num_timesteps=441000, episode_reward=716.15 +/- 619.34
Episode length: 34.82 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=746.98 +/- 586.67
Episode length: 35.76 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=702.94 +/- 620.29
Episode length: 33.70 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 805      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 216      |
|    time_elapsed    | 1580     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=844.48 +/- 714.13
Episode length: 35.50 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 844       |
| time/                   |           |
|    total_timesteps      | 442500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.59e-13 |
|    explained_variance   | 0.0159    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.75e+04  |
|    n_updates            | 2196      |
|    policy_gradient_loss | -4.37e-12 |
|    value_loss           | 3.74e+04  |
---------------------------------------
Eval num_timesteps=443000, episode_reward=878.56 +/- 674.91
Episode length: 35.72 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=605.53 +/- 513.89
Episode length: 33.80 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=818.40 +/- 701.91
Episode length: 34.66 +/- 7.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 807      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 217      |
|    time_elapsed    | 1587     |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=837.46 +/- 710.19
Episode length: 34.76 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 837       |
| time/                   |           |
|    total_timesteps      | 444500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.51e-15 |
|    explained_variance   | 0.0235    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.98e+04  |
|    n_updates            | 2206      |
|    policy_gradient_loss | -9.75e-10 |
|    value_loss           | 4.07e+04  |
---------------------------------------
Eval num_timesteps=445000, episode_reward=943.49 +/- 715.10
Episode length: 36.54 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=831.41 +/- 668.11
Episode length: 35.44 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=887.16 +/- 699.12
Episode length: 35.94 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 843      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 218      |
|    time_elapsed    | 1594     |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=772.69 +/- 615.55
Episode length: 35.14 +/- 6.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 773       |
| time/                   |           |
|    total_timesteps      | 446500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.83e-13 |
|    explained_variance   | 0.0276    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.06e+04  |
|    n_updates            | 2216      |
|    policy_gradient_loss | -1.79e-10 |
|    value_loss           | 4.07e+04  |
---------------------------------------
Eval num_timesteps=447000, episode_reward=807.32 +/- 747.31
Episode length: 33.80 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=800.45 +/- 682.75
Episode length: 34.74 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=836.80 +/- 637.57
Episode length: 36.10 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=868.23 +/- 650.38
Episode length: 36.54 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 944      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 219      |
|    time_elapsed    | 1603     |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=1055.52 +/- 787.50
Episode length: 36.50 +/- 7.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 1.06e+03  |
| time/                   |           |
|    total_timesteps      | 449000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.59e-15 |
|    explained_variance   | 0.0228    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.95e+04  |
|    n_updates            | 2226      |
|    policy_gradient_loss | 7.9e-10   |
|    value_loss           | 4.04e+04  |
---------------------------------------
Eval num_timesteps=449500, episode_reward=650.59 +/- 581.37
Episode length: 33.16 +/- 7.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=888.47 +/- 746.26
Episode length: 34.72 +/- 7.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=775.78 +/- 662.10
Episode length: 34.68 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 886      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 220      |
|    time_elapsed    | 1610     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=709.99 +/- 657.32
Episode length: 33.76 +/- 7.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 710       |
| time/                   |           |
|    total_timesteps      | 451000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.65e-13 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.0001    |
|    loss                 | 2e+04     |
|    n_updates            | 2236      |
|    policy_gradient_loss | -2.03e-09 |
|    value_loss           | 3.98e+04  |
---------------------------------------
Eval num_timesteps=451500, episode_reward=1034.99 +/- 704.51
Episode length: 36.82 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=771.00 +/- 669.44
Episode length: 34.84 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=820.04 +/- 711.25
Episode length: 34.62 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 221      |
|    time_elapsed    | 1617     |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=865.57 +/- 675.65
Episode length: 36.28 +/- 5.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 866       |
| time/                   |           |
|    total_timesteps      | 453000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.36e-09 |
|    explained_variance   | 0.0274    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.83e+04  |
|    n_updates            | 2246      |
|    policy_gradient_loss | 4.95e-11  |
|    value_loss           | 4.29e+04  |
---------------------------------------
Eval num_timesteps=453500, episode_reward=744.69 +/- 610.45
Episode length: 35.18 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=833.86 +/- 734.47
Episode length: 34.36 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=744.90 +/- 678.86
Episode length: 34.06 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    fps             | 279      |
|    iterations      | 222      |
|    time_elapsed    | 1624     |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=809.76 +/- 658.14
Episode length: 34.78 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 810       |
| time/                   |           |
|    total_timesteps      | 455000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.6e-12  |
|    explained_variance   | 0.0248    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.84e+04  |
|    n_updates            | 2256      |
|    policy_gradient_loss | -1.19e-10 |
|    value_loss           | 5.21e+04  |
---------------------------------------
Eval num_timesteps=455500, episode_reward=913.73 +/- 716.20
Episode length: 35.76 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=783.70 +/- 693.05
Episode length: 34.32 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=868.08 +/- 679.27
Episode length: 35.76 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 900      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 223      |
|    time_elapsed    | 1631     |
|    total_timesteps | 456704   |
---------------------------------
Eval num_timesteps=457000, episode_reward=783.06 +/- 702.45
Episode length: 34.30 +/- 7.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 783       |
| time/                   |           |
|    total_timesteps      | 457000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.45e-09 |
|    explained_variance   | 0.021     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.49e+04  |
|    n_updates            | 2266      |
|    policy_gradient_loss | 0         |
|    value_loss           | 3.75e+04  |
---------------------------------------
Eval num_timesteps=457500, episode_reward=906.27 +/- 728.21
Episode length: 35.30 +/- 7.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=907.08 +/- 703.62
Episode length: 35.78 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=760.57 +/- 697.45
Episode length: 33.94 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 869      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 224      |
|    time_elapsed    | 1639     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459000, episode_reward=951.91 +/- 747.37
Episode length: 36.16 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 952       |
| time/                   |           |
|    total_timesteps      | 459000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-11 |
|    explained_variance   | 0.0166    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.62e+04  |
|    n_updates            | 2276      |
|    policy_gradient_loss | 1.03e-09  |
|    value_loss           | 4.1e+04   |
---------------------------------------
Eval num_timesteps=459500, episode_reward=913.94 +/- 650.92
Episode length: 36.44 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=897.73 +/- 615.87
Episode length: 36.50 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=864.18 +/- 740.82
Episode length: 34.84 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 956      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 225      |
|    time_elapsed    | 1646     |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=461000, episode_reward=1029.13 +/- 752.43
Episode length: 36.60 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 1.03e+03  |
| time/                   |           |
|    total_timesteps      | 461000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.15e-10 |
|    explained_variance   | 0.0236    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.71e+04  |
|    n_updates            | 2286      |
|    policy_gradient_loss | -1.98e-10 |
|    value_loss           | 4.16e+04  |
---------------------------------------
Eval num_timesteps=461500, episode_reward=965.75 +/- 707.72
Episode length: 36.64 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 966      |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=788.14 +/- 682.81
Episode length: 33.98 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=605.07 +/- 475.70
Episode length: 34.32 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 605      |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 797      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 226      |
|    time_elapsed    | 1653     |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=787.20 +/- 648.99
Episode length: 35.12 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 787       |
| time/                   |           |
|    total_timesteps      | 463000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.64e-07 |
|    explained_variance   | 0.0182    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.43e+04  |
|    n_updates            | 2296      |
|    policy_gradient_loss | -1.34e-10 |
|    value_loss           | 3.65e+04  |
---------------------------------------
Eval num_timesteps=463500, episode_reward=833.70 +/- 672.84
Episode length: 35.56 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=784.27 +/- 661.14
Episode length: 34.34 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=609.25 +/- 522.61
Episode length: 33.80 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 609      |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 733      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 227      |
|    time_elapsed    | 1660     |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=851.42 +/- 729.51
Episode length: 34.62 +/- 6.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 851       |
| time/                   |           |
|    total_timesteps      | 465000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.24e-09 |
|    explained_variance   | 0.0118    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.98e+04  |
|    n_updates            | 2306      |
|    policy_gradient_loss | 9.6e-11   |
|    value_loss           | 3.93e+04  |
---------------------------------------
Eval num_timesteps=465500, episode_reward=931.12 +/- 699.38
Episode length: 36.54 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=1010.45 +/- 733.32
Episode length: 36.98 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=814.29 +/- 682.91
Episode length: 34.88 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 228      |
|    time_elapsed    | 1668     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=903.90 +/- 653.97
Episode length: 36.34 +/- 5.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 904       |
| time/                   |           |
|    total_timesteps      | 467000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.5e-12  |
|    explained_variance   | 0.0252    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.54e+04  |
|    n_updates            | 2316      |
|    policy_gradient_loss | -3.94e-10 |
|    value_loss           | 4.03e+04  |
---------------------------------------
Eval num_timesteps=467500, episode_reward=841.38 +/- 713.88
Episode length: 35.14 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=754.27 +/- 669.09
Episode length: 34.50 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=822.77 +/- 678.52
Episode length: 35.54 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 229      |
|    time_elapsed    | 1675     |
|    total_timesteps | 468992   |
---------------------------------
Eval num_timesteps=469000, episode_reward=855.20 +/- 663.43
Episode length: 35.58 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 855       |
| time/                   |           |
|    total_timesteps      | 469000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.18e-16 |
|    explained_variance   | 0.0226    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.25e+04  |
|    n_updates            | 2326      |
|    policy_gradient_loss | 4.98e-10  |
|    value_loss           | 3.58e+04  |
---------------------------------------
Eval num_timesteps=469500, episode_reward=731.39 +/- 642.02
Episode length: 34.08 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=807.22 +/- 683.52
Episode length: 34.88 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=952.12 +/- 763.94
Episode length: 35.72 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=778.80 +/- 666.07
Episode length: 34.50 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 230      |
|    time_elapsed    | 1683     |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=806.73 +/- 641.46
Episode length: 35.28 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 807       |
| time/                   |           |
|    total_timesteps      | 471500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.82e-13 |
|    explained_variance   | 0.0226    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.41e+04  |
|    n_updates            | 2336      |
|    policy_gradient_loss | 2.3e-10   |
|    value_loss           | 3.89e+04  |
---------------------------------------
Eval num_timesteps=472000, episode_reward=739.90 +/- 625.77
Episode length: 35.06 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=716.04 +/- 629.69
Episode length: 34.04 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=663.56 +/- 597.02
Episode length: 33.94 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 664      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 231      |
|    time_elapsed    | 1690     |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=645.52 +/- 588.02
Episode length: 33.60 +/- 6.74
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 33.6     |
|    mean_reward          | 646      |
| time/                   |          |
|    total_timesteps      | 473500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.4e-09 |
|    explained_variance   | 0.0258   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.31e+04 |
|    n_updates            | 2346     |
|    policy_gradient_loss | 1.16e-10 |
|    value_loss           | 3.68e+04 |
--------------------------------------
Eval num_timesteps=474000, episode_reward=828.88 +/- 702.42
Episode length: 34.68 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=990.94 +/- 751.94
Episode length: 35.80 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=792.84 +/- 663.31
Episode length: 34.98 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 232      |
|    time_elapsed    | 1698     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=752.99 +/- 643.62
Episode length: 34.62 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 753       |
| time/                   |           |
|    total_timesteps      | 475500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.41e-11 |
|    explained_variance   | 0.023     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.34e+04  |
|    n_updates            | 2356      |
|    policy_gradient_loss | 1.39e-09  |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=476000, episode_reward=577.74 +/- 476.40
Episode length: 33.14 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 578      |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=804.12 +/- 648.79
Episode length: 35.40 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=1081.97 +/- 688.32
Episode length: 38.66 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.7     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 233      |
|    time_elapsed    | 1705     |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=689.80 +/- 535.85
Episode length: 35.26 +/- 5.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 690       |
| time/                   |           |
|    total_timesteps      | 477500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.08e-15 |
|    explained_variance   | 0.0197    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.94e+04  |
|    n_updates            | 2366      |
|    policy_gradient_loss | 1.89e-10  |
|    value_loss           | 3.67e+04  |
---------------------------------------
Eval num_timesteps=478000, episode_reward=677.54 +/- 639.64
Episode length: 33.44 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=838.13 +/- 719.13
Episode length: 34.10 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=1082.78 +/- 709.68
Episode length: 37.46 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 234      |
|    time_elapsed    | 1712     |
|    total_timesteps | 479232   |
---------------------------------
Eval num_timesteps=479500, episode_reward=902.66 +/- 718.07
Episode length: 35.96 +/- 7.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 903       |
| time/                   |           |
|    total_timesteps      | 479500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.62e-13 |
|    explained_variance   | 0.0205    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.73e+04  |
|    n_updates            | 2376      |
|    policy_gradient_loss | 2.01e-10  |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=480000, episode_reward=969.31 +/- 734.86
Episode length: 36.32 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=904.79 +/- 757.85
Episode length: 34.68 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=840.36 +/- 700.60
Episode length: 35.18 +/- 7.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 235      |
|    time_elapsed    | 1719     |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=814.72 +/- 713.33
Episode length: 35.04 +/- 6.72
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35       |
|    mean_reward          | 815      |
| time/                   |          |
|    total_timesteps      | 481500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.1e-09 |
|    explained_variance   | 0.0193   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.34e+04 |
|    n_updates            | 2386     |
|    policy_gradient_loss | 5.53e-11 |
|    value_loss           | 3.48e+04 |
--------------------------------------
Eval num_timesteps=482000, episode_reward=742.62 +/- 565.76
Episode length: 35.66 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=1002.29 +/- 743.71
Episode length: 36.76 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=872.06 +/- 696.16
Episode length: 35.86 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 236      |
|    time_elapsed    | 1727     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=789.78 +/- 619.32
Episode length: 35.32 +/- 5.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 790       |
| time/                   |           |
|    total_timesteps      | 483500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.18e-11 |
|    explained_variance   | 0.0137    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.83e+04  |
|    n_updates            | 2396      |
|    policy_gradient_loss | 9.76e-10  |
|    value_loss           | 3.13e+04  |
---------------------------------------
Eval num_timesteps=484000, episode_reward=669.34 +/- 614.57
Episode length: 32.64 +/- 7.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=969.15 +/- 680.05
Episode length: 37.00 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=875.90 +/- 655.66
Episode length: 35.90 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 735      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 237      |
|    time_elapsed    | 1734     |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=939.75 +/- 689.24
Episode length: 36.72 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 940       |
| time/                   |           |
|    total_timesteps      | 485500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.45e-15 |
|    explained_variance   | 0.0172    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.58e+04  |
|    n_updates            | 2406      |
|    policy_gradient_loss | -2.71e-10 |
|    value_loss           | 3.03e+04  |
---------------------------------------
Eval num_timesteps=486000, episode_reward=805.37 +/- 695.43
Episode length: 34.54 +/- 8.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=718.81 +/- 621.42
Episode length: 34.16 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=890.93 +/- 659.27
Episode length: 36.80 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 238      |
|    time_elapsed    | 1741     |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=947.13 +/- 736.73
Episode length: 36.00 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 947       |
| time/                   |           |
|    total_timesteps      | 487500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.06e-13 |
|    explained_variance   | 0.0238    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.23e+04  |
|    n_updates            | 2416      |
|    policy_gradient_loss | 2.71e-10  |
|    value_loss           | 4.08e+04  |
---------------------------------------
Eval num_timesteps=488000, episode_reward=849.26 +/- 624.93
Episode length: 36.02 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=900.71 +/- 724.82
Episode length: 35.80 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=781.93 +/- 645.06
Episode length: 35.00 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 239      |
|    time_elapsed    | 1748     |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=947.36 +/- 743.58
Episode length: 35.78 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 947       |
| time/                   |           |
|    total_timesteps      | 489500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.88e-15 |
|    explained_variance   | 0.023     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.68e+04  |
|    n_updates            | 2426      |
|    policy_gradient_loss | -3.17e-10 |
|    value_loss           | 3.42e+04  |
---------------------------------------
Eval num_timesteps=490000, episode_reward=833.17 +/- 632.54
Episode length: 36.24 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=803.26 +/- 651.00
Episode length: 35.40 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=825.64 +/- 720.50
Episode length: 35.00 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=897.01 +/- 710.96
Episode length: 35.88 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 240      |
|    time_elapsed    | 1757     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=752.06 +/- 629.96
Episode length: 34.94 +/- 5.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 752       |
| time/                   |           |
|    total_timesteps      | 492000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.65e-12 |
|    explained_variance   | 0.00913   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.42e+04  |
|    n_updates            | 2436      |
|    policy_gradient_loss | -1.17e-09 |
|    value_loss           | 4.21e+04  |
---------------------------------------
Eval num_timesteps=492500, episode_reward=875.85 +/- 690.72
Episode length: 35.46 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=830.87 +/- 662.43
Episode length: 35.42 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=817.94 +/- 710.17
Episode length: 34.78 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 714      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 241      |
|    time_elapsed    | 1764     |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=764.61 +/- 649.68
Episode length: 35.00 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 765       |
| time/                   |           |
|    total_timesteps      | 494000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.95e-09 |
|    explained_variance   | 0.0207    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.88e+04  |
|    n_updates            | 2446      |
|    policy_gradient_loss | -3.75e-10 |
|    value_loss           | 3.26e+04  |
---------------------------------------
Eval num_timesteps=494500, episode_reward=913.92 +/- 735.24
Episode length: 35.88 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=866.29 +/- 686.76
Episode length: 35.36 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=731.08 +/- 658.47
Episode length: 34.04 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 242      |
|    time_elapsed    | 1771     |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=871.97 +/- 677.70
Episode length: 35.62 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 872       |
| time/                   |           |
|    total_timesteps      | 496000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.47e-11 |
|    explained_variance   | 0.0262    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.7e+04   |
|    n_updates            | 2456      |
|    policy_gradient_loss | -1.41e-09 |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=496500, episode_reward=790.62 +/- 656.57
Episode length: 34.62 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=907.16 +/- 725.50
Episode length: 35.46 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=729.93 +/- 578.08
Episode length: 35.38 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 865      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 243      |
|    time_elapsed    | 1779     |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=759.66 +/- 628.78
Episode length: 34.94 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 760       |
| time/                   |           |
|    total_timesteps      | 498000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-14 |
|    explained_variance   | 0.0243    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.71e+04  |
|    n_updates            | 2466      |
|    policy_gradient_loss | -1.46e-12 |
|    value_loss           | 3.93e+04  |
---------------------------------------
Eval num_timesteps=498500, episode_reward=946.27 +/- 709.76
Episode length: 35.94 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=836.99 +/- 638.05
Episode length: 36.16 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=829.67 +/- 678.43
Episode length: 34.68 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 916      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 244      |
|    time_elapsed    | 1786     |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=601.43 +/- 480.04
Episode length: 34.20 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 601       |
| time/                   |           |
|    total_timesteps      | 500000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.62e-12 |
|    explained_variance   | 0.0343    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.69e+04  |
|    n_updates            | 2476      |
|    policy_gradient_loss | 2.46e-10  |
|    value_loss           | 4.15e+04  |
---------------------------------------
Eval num_timesteps=500500, episode_reward=871.70 +/- 749.34
Episode length: 35.14 +/- 7.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=661.06 +/- 529.29
Episode length: 34.52 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=866.06 +/- 679.86
Episode length: 34.82 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 968      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 245      |
|    time_elapsed    | 1793     |
|    total_timesteps | 501760   |
---------------------------------
