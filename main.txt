/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-419.65 +/- 72.91
Episode length: 68.98 +/- 71.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-375.55 +/- 83.24
Episode length: 121.40 +/- 163.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=-386.63 +/- 80.05
Episode length: 94.44 +/- 128.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-366.63 +/- 92.12
Episode length: 132.46 +/- 172.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | -367     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | -317     |
| time/              |          |
|    fps             | 91       |
|    iterations      | 1        |
|    time_elapsed    | 22       |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=2500, episode_reward=-427.77 +/- 70.54
Episode length: 51.62 +/- 20.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.6         |
|    mean_reward          | -428         |
| time/                   |              |
|    total_timesteps      | 2500         |
| train/                  |              |
|    approx_kl            | 0.0086747445 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | 0.000187     |
|    learning_rate        | 0.001        |
|    loss                 | 478          |
|    n_updates            | 1            |
|    policy_gradient_loss | -0.000181    |
|    value_loss           | 767          |
------------------------------------------
Eval num_timesteps=3000, episode_reward=-434.91 +/- 67.15
Episode length: 50.12 +/- 14.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-436.92 +/- 69.03
Episode length: 52.06 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-421.19 +/- 80.41
Episode length: 50.42 +/- 19.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | -342     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 2        |
|    time_elapsed    | 34       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=-418.27 +/- 75.85
Episode length: 51.90 +/- 17.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.9       |
|    mean_reward          | -418       |
| time/                   |            |
|    total_timesteps      | 4500       |
| train/                  |            |
|    approx_kl            | 0.00883813 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.000639   |
|    learning_rate        | 0.001      |
|    loss                 | 1.14e+03   |
|    n_updates            | 2          |
|    policy_gradient_loss | 0.00131    |
|    value_loss           | 2.18e+03   |
----------------------------------------
Eval num_timesteps=5000, episode_reward=-420.03 +/- 81.31
Episode length: 50.02 +/- 19.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-410.13 +/- 85.82
Episode length: 45.74 +/- 14.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-429.10 +/- 67.64
Episode length: 54.14 +/- 16.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | -329     |
| time/              |          |
|    fps             | 143      |
|    iterations      | 3        |
|    time_elapsed    | 42       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=-439.49 +/- 61.53
Episode length: 52.78 +/- 18.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.8        |
|    mean_reward          | -439        |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.011364025 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.0029      |
|    learning_rate        | 0.001       |
|    loss                 | 73.8        |
|    n_updates            | 3           |
|    policy_gradient_loss | 0.0175      |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=-435.05 +/- 62.01
Episode length: 53.42 +/- 15.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-407.71 +/- 78.84
Episode length: 49.14 +/- 14.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-449.37 +/- 53.90
Episode length: 50.84 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -449     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | -340     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 4        |
|    time_elapsed    | 51       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=8500, episode_reward=-431.61 +/- 63.09
Episode length: 54.04 +/- 17.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54          |
|    mean_reward          | -432        |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.022422817 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.00422     |
|    learning_rate        | 0.001       |
|    loss                 | 529         |
|    n_updates            | 4           |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 1.64e+03    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=-402.07 +/- 82.39
Episode length: 49.18 +/- 19.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-415.89 +/- 79.11
Episode length: 48.96 +/- 14.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-410.39 +/- 83.92
Episode length: 46.94 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | -359     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 5        |
|    time_elapsed    | 60       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=10500, episode_reward=-431.89 +/- 65.97
Episode length: 54.40 +/- 16.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.4        |
|    mean_reward          | -432        |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.033106767 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.88       |
|    explained_variance   | 0.00171     |
|    learning_rate        | 0.001       |
|    loss                 | 844         |
|    n_updates            | 5           |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 2.67e+03    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=-438.53 +/- 71.41
Episode length: 50.94 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -439     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-413.19 +/- 84.16
Episode length: 50.06 +/- 19.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-439.59 +/- 80.23
Episode length: 52.02 +/- 19.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -440     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.9     |
|    ep_rew_mean     | -380     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 6        |
|    time_elapsed    | 68       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=12500, episode_reward=-426.41 +/- 79.77
Episode length: 49.02 +/- 17.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49          |
|    mean_reward          | -426        |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.026286945 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.742      |
|    explained_variance   | 0.00204     |
|    learning_rate        | 0.001       |
|    loss                 | 1.62e+03    |
|    n_updates            | 6           |
|    policy_gradient_loss | 0.00565     |
|    value_loss           | 3.93e+03    |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=-447.00 +/- 65.97
Episode length: 51.54 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -447     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-424.43 +/- 74.38
Episode length: 49.24 +/- 20.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-420.97 +/- 77.87
Episode length: 50.38 +/- 16.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 185      |
|    iterations      | 7        |
|    time_elapsed    | 77       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=14500, episode_reward=-415.99 +/- 78.06
Episode length: 48.82 +/- 13.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.8        |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.015891187 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.683      |
|    explained_variance   | 0.00175     |
|    learning_rate        | 0.001       |
|    loss                 | 2.48e+03    |
|    n_updates            | 7           |
|    policy_gradient_loss | 0.000973    |
|    value_loss           | 5.29e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-414.79 +/- 67.13
Episode length: 48.68 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-401.00 +/- 60.15
Episode length: 50.80 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-396.79 +/- 84.50
Episode length: 49.66 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.8     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 190      |
|    iterations      | 8        |
|    time_elapsed    | 85       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.17
Eval num_timesteps=16500, episode_reward=-429.80 +/- 63.90
Episode length: 51.78 +/- 17.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.8       |
|    mean_reward          | -430       |
| time/                   |            |
|    total_timesteps      | 16500      |
| train/                  |            |
|    approx_kl            | 0.08741897 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.579     |
|    explained_variance   | -0.00121   |
|    learning_rate        | 0.001      |
|    loss                 | 2.74e+03   |
|    n_updates            | 8          |
|    policy_gradient_loss | 0.015      |
|    value_loss           | 4.23e+03   |
----------------------------------------
Eval num_timesteps=17000, episode_reward=-418.99 +/- 62.09
Episode length: 47.68 +/- 17.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-423.80 +/- 59.04
Episode length: 49.50 +/- 14.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-426.79 +/- 69.81
Episode length: 51.48 +/- 18.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 194      |
|    iterations      | 9        |
|    time_elapsed    | 94       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.16
Eval num_timesteps=18500, episode_reward=-420.79 +/- 83.52
Episode length: 50.70 +/- 14.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.7        |
|    mean_reward          | -421        |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.081897736 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.321      |
|    explained_variance   | -0.00778    |
|    learning_rate        | 0.001       |
|    loss                 | 1.12e+03    |
|    n_updates            | 9           |
|    policy_gradient_loss | 0.0298      |
|    value_loss           | 2.59e+03    |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=-426.20 +/- 70.15
Episode length: 51.52 +/- 17.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-437.59 +/- 68.15
Episode length: 53.44 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-402.19 +/- 71.37
Episode length: 48.16 +/- 18.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 197      |
|    iterations      | 10       |
|    time_elapsed    | 103      |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=20500, episode_reward=-418.39 +/- 64.67
Episode length: 50.06 +/- 12.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.1       |
|    mean_reward          | -418       |
| time/                   |            |
|    total_timesteps      | 20500      |
| train/                  |            |
|    approx_kl            | 0.04180234 |
|    clip_fraction        | 0.0391     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.107     |
|    explained_variance   | -0.00454   |
|    learning_rate        | 0.001      |
|    loss                 | 1.54e+03   |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00125   |
|    value_loss           | 3.04e+03   |
----------------------------------------
Eval num_timesteps=21000, episode_reward=-418.39 +/- 71.03
Episode length: 50.00 +/- 18.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-402.80 +/- 74.93
Episode length: 48.34 +/- 19.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-419.60 +/- 61.77
Episode length: 50.98 +/- 14.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-430.99 +/- 66.13
Episode length: 51.54 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 198      |
|    iterations      | 11       |
|    time_elapsed    | 113      |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=23000, episode_reward=-421.39 +/- 73.39
Episode length: 51.66 +/- 17.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.7        |
|    mean_reward          | -421        |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.043138977 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0161     |
|    explained_variance   | -0.00676    |
|    learning_rate        | 0.001       |
|    loss                 | 2.2e+03     |
|    n_updates            | 11          |
|    policy_gradient_loss | 0.00311     |
|    value_loss           | 4.73e+03    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=-417.19 +/- 74.27
Episode length: 51.66 +/- 20.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-414.19 +/- 74.96
Episode length: 51.54 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-420.79 +/- 69.65
Episode length: 46.48 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 200      |
|    iterations      | 12       |
|    time_elapsed    | 122      |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.23
Eval num_timesteps=25000, episode_reward=-396.19 +/- 71.92
Episode length: 48.66 +/- 18.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.7       |
|    mean_reward          | -396       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01345953 |
|    clip_fraction        | 0.000919   |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.000118  |
|    explained_variance   | -0.00717   |
|    learning_rate        | 0.001      |
|    loss                 | 2.81e+03   |
|    n_updates            | 12         |
|    policy_gradient_loss | -7.36e-06  |
|    value_loss           | 4.41e+03   |
----------------------------------------
Eval num_timesteps=25500, episode_reward=-411.19 +/- 78.10
Episode length: 48.28 +/- 15.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-413.00 +/- 84.83
Episode length: 49.16 +/- 14.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-409.99 +/- 78.70
Episode length: 50.90 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 13       |
|    time_elapsed    | 130      |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=-422.59 +/- 71.81
Episode length: 48.66 +/- 16.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.7      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 27000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.75e-07 |
|    explained_variance   | -0.0057   |
|    learning_rate        | 0.001     |
|    loss                 | 3e+03     |
|    n_updates            | 22        |
|    policy_gradient_loss | -7.51e-10 |
|    value_loss           | 6.98e+03  |
---------------------------------------
Eval num_timesteps=27500, episode_reward=-408.79 +/- 68.82
Episode length: 50.34 +/- 14.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-414.79 +/- 63.54
Episode length: 51.26 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-420.19 +/- 64.03
Episode length: 49.12 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 14       |
|    time_elapsed    | 139      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=29000, episode_reward=-415.39 +/- 64.40
Episode length: 47.16 +/- 14.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.2         |
|    mean_reward          | -415         |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0024460596 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | -0.00464     |
|    learning_rate        | 0.001        |
|    loss                 | 1.59e+03     |
|    n_updates            | 23           |
|    policy_gradient_loss | -0.000118    |
|    value_loss           | 4.17e+03     |
------------------------------------------
Eval num_timesteps=29500, episode_reward=-421.99 +/- 69.31
Episode length: 50.88 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-418.99 +/- 67.11
Episode length: 47.24 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-416.59 +/- 80.33
Episode length: 55.72 +/- 19.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.9     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 206      |
|    iterations      | 15       |
|    time_elapsed    | 148      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.28
Eval num_timesteps=31000, episode_reward=-431.21 +/- 66.38
Episode length: 49.48 +/- 15.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | -431        |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.097283095 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.001       |
|    learning_rate        | 0.001       |
|    loss                 | 2.19e+03    |
|    n_updates            | 24          |
|    policy_gradient_loss | 0.0715      |
|    value_loss           | 3.98e+03    |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=-422.89 +/- 67.94
Episode length: 49.46 +/- 13.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-420.18 +/- 76.91
Episode length: 51.96 +/- 17.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-424.39 +/- 72.40
Episode length: 50.58 +/- 19.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 208      |
|    iterations      | 16       |
|    time_elapsed    | 156      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.34
Eval num_timesteps=33000, episode_reward=-424.31 +/- 74.83
Episode length: 50.04 +/- 16.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -424      |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 0.1700933 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.544    |
|    explained_variance   | -0.00598  |
|    learning_rate        | 0.001     |
|    loss                 | 2.19e+03  |
|    n_updates            | 25        |
|    policy_gradient_loss | 0.139     |
|    value_loss           | 3.25e+03  |
---------------------------------------
Eval num_timesteps=33500, episode_reward=-441.43 +/- 71.43
Episode length: 51.68 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-432.99 +/- 57.68
Episode length: 47.58 +/- 13.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-418.13 +/- 91.37
Episode length: 51.28 +/- 17.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 210      |
|    iterations      | 17       |
|    time_elapsed    | 165      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.28
Eval num_timesteps=35000, episode_reward=-424.01 +/- 66.11
Episode length: 49.28 +/- 15.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.3       |
|    mean_reward          | -424       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.14013296 |
|    clip_fraction        | 0.0703     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.215     |
|    explained_variance   | -0.00391   |
|    learning_rate        | 0.001      |
|    loss                 | 1.02e+03   |
|    n_updates            | 26         |
|    policy_gradient_loss | 0.0277     |
|    value_loss           | 2.34e+03   |
----------------------------------------
Eval num_timesteps=35500, episode_reward=-421.73 +/- 61.66
Episode length: 51.50 +/- 18.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-442.05 +/- 61.70
Episode length: 48.62 +/- 19.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-421.10 +/- 65.43
Episode length: 53.82 +/- 19.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 211      |
|    iterations      | 18       |
|    time_elapsed    | 174      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.26
Eval num_timesteps=37000, episode_reward=-419.01 +/- 75.52
Episode length: 52.04 +/- 19.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52        |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 37000     |
| train/                  |           |
|    approx_kl            | 0.0859052 |
|    clip_fraction        | 0.00521   |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.0119   |
|    explained_variance   | -0.00846  |
|    learning_rate        | 0.001     |
|    loss                 | 2.16e+03  |
|    n_updates            | 27        |
|    policy_gradient_loss | 0.00502   |
|    value_loss           | 3.56e+03  |
---------------------------------------
Eval num_timesteps=37500, episode_reward=-417.27 +/- 78.69
Episode length: 47.06 +/- 13.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-418.43 +/- 86.01
Episode length: 47.76 +/- 13.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-433.51 +/- 64.84
Episode length: 52.72 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -431     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 19       |
|    time_elapsed    | 182      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=-426.35 +/- 84.85
Episode length: 45.88 +/- 15.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.9      |
|    mean_reward          | -426      |
| time/                   |           |
|    total_timesteps      | 39000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.76e-10 |
|    explained_variance   | -0.0624   |
|    learning_rate        | 0.001     |
|    loss                 | 2.75e+03  |
|    n_updates            | 37        |
|    policy_gradient_loss | -2.54e-09 |
|    value_loss           | 5.82e+03  |
---------------------------------------
Eval num_timesteps=39500, episode_reward=-443.45 +/- 63.31
Episode length: 50.46 +/- 15.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -443     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-425.87 +/- 66.98
Episode length: 47.22 +/- 16.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-416.11 +/- 67.74
Episode length: 51.02 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 20       |
|    time_elapsed    | 191      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=-425.04 +/- 62.65
Episode length: 49.82 +/- 16.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -425      |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.58e-32 |
|    explained_variance   | -0.678    |
|    learning_rate        | 0.001     |
|    loss                 | 1.01e+05  |
|    n_updates            | 47        |
|    policy_gradient_loss | -9.87e-09 |
|    value_loss           | 1.84e+05  |
---------------------------------------
Eval num_timesteps=41500, episode_reward=-425.56 +/- 78.60
Episode length: 50.26 +/- 13.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-422.97 +/- 69.67
Episode length: 48.00 +/- 16.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-419.62 +/- 68.76
Episode length: 48.76 +/- 14.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-422.31 +/- 76.30
Episode length: 45.90 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.7     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 21       |
|    time_elapsed    | 202      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.20
Eval num_timesteps=43500, episode_reward=-411.55 +/- 74.93
Episode length: 47.38 +/- 17.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.4        |
|    mean_reward          | -412        |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.012697123 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0199     |
|    explained_variance   | -0.14       |
|    learning_rate        | 0.001       |
|    loss                 | 4.68e+03    |
|    n_updates            | 49          |
|    policy_gradient_loss | 0.0014      |
|    value_loss           | 1.06e+04    |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=-425.89 +/- 77.47
Episode length: 49.42 +/- 16.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-415.71 +/- 84.25
Episode length: 51.52 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-416.65 +/- 79.67
Episode length: 49.36 +/- 19.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 22       |
|    time_elapsed    | 211      |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=45500, episode_reward=-411.78 +/- 90.98
Episode length: 48.98 +/- 15.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49          |
|    mean_reward          | -412        |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.056168392 |
|    clip_fraction        | 0.5         |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.000481    |
|    learning_rate        | 0.001       |
|    loss                 | 1.6e+03     |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.011       |
|    value_loss           | 3.88e+03    |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=-417.19 +/- 75.00
Episode length: 46.82 +/- 15.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-414.20 +/- 72.77
Episode length: 54.66 +/- 14.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-416.59 +/- 66.34
Episode length: 52.50 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 23       |
|    time_elapsed    | 220      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=47500, episode_reward=-402.79 +/- 82.70
Episode length: 49.96 +/- 17.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | -403       |
| time/                   |            |
|    total_timesteps      | 47500      |
| train/                  |            |
|    approx_kl            | 0.01245202 |
|    clip_fraction        | 0.0859     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.545     |
|    explained_variance   | 0.000448   |
|    learning_rate        | 0.001      |
|    loss                 | 3.01e+03   |
|    n_updates            | 51         |
|    policy_gradient_loss | 0.00306    |
|    value_loss           | 4.81e+03   |
----------------------------------------
Eval num_timesteps=48000, episode_reward=-404.59 +/- 71.56
Episode length: 49.28 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-409.40 +/- 79.10
Episode length: 51.66 +/- 15.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-415.39 +/- 81.21
Episode length: 49.68 +/- 16.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 24       |
|    time_elapsed    | 228      |
|    total_timesteps | 49152    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=49500, episode_reward=-425.00 +/- 66.62
Episode length: 53.76 +/- 21.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.8        |
|    mean_reward          | -425        |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.013815325 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.41       |
|    explained_variance   | 0.00214     |
|    learning_rate        | 0.001       |
|    loss                 | 3.52e+03    |
|    n_updates            | 52          |
|    policy_gradient_loss | 0.0191      |
|    value_loss           | 5.43e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-415.40 +/- 76.89
Episode length: 53.06 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-405.19 +/- 65.08
Episode length: 51.28 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-423.19 +/- 68.68
Episode length: 54.34 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -429     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 25       |
|    time_elapsed    | 237      |
|    total_timesteps | 51200    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=51500, episode_reward=-408.79 +/- 74.59
Episode length: 46.18 +/- 16.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.2        |
|    mean_reward          | -409        |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.018407349 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.223      |
|    explained_variance   | -0.00336    |
|    learning_rate        | 0.001       |
|    loss                 | 1.42e+03    |
|    n_updates            | 53          |
|    policy_gradient_loss | 0.00396     |
|    value_loss           | 2.67e+03    |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=-413.59 +/- 66.00
Episode length: 52.70 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-417.19 +/- 77.82
Episode length: 47.54 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-424.40 +/- 60.47
Episode length: 49.36 +/- 12.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 26       |
|    time_elapsed    | 245      |
|    total_timesteps | 53248    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=53500, episode_reward=-434.59 +/- 52.74
Episode length: 51.82 +/- 17.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.8       |
|    mean_reward          | -435       |
| time/                   |            |
|    total_timesteps      | 53500      |
| train/                  |            |
|    approx_kl            | 0.03887898 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.062     |
|    explained_variance   | -0.00176   |
|    learning_rate        | 0.001      |
|    loss                 | 2.15e+03   |
|    n_updates            | 54         |
|    policy_gradient_loss | 0.0139     |
|    value_loss           | 3.76e+03   |
----------------------------------------
Eval num_timesteps=54000, episode_reward=-409.39 +/- 78.87
Episode length: 53.90 +/- 19.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-440.00 +/- 62.94
Episode length: 55.56 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | -440     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-402.19 +/- 81.28
Episode length: 46.20 +/- 14.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 27       |
|    time_elapsed    | 254      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=-418.97 +/- 85.41
Episode length: 51.12 +/- 16.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.1          |
|    mean_reward          | -419          |
| time/                   |               |
|    total_timesteps      | 55500         |
| train/                  |               |
|    approx_kl            | 3.4214463e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -2.16e-05     |
|    explained_variance   | -0.00156      |
|    learning_rate        | 0.001         |
|    loss                 | 2.78e+03      |
|    n_updates            | 64            |
|    policy_gradient_loss | 1.52e-05      |
|    value_loss           | 5.5e+03       |
-------------------------------------------
Eval num_timesteps=56000, episode_reward=-421.39 +/- 61.95
Episode length: 49.08 +/- 17.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-414.79 +/- 68.98
Episode length: 46.68 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-424.40 +/- 75.31
Episode length: 49.10 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 28       |
|    time_elapsed    | 263      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=-417.19 +/- 78.77
Episode length: 51.40 +/- 18.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 57500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.1e-07  |
|    explained_variance   | -0.0108   |
|    learning_rate        | 0.001     |
|    loss                 | 1.46e+04  |
|    n_updates            | 74        |
|    policy_gradient_loss | -6.55e-09 |
|    value_loss           | 2.79e+04  |
---------------------------------------
Eval num_timesteps=58000, episode_reward=-420.19 +/- 65.97
Episode length: 53.38 +/- 18.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-402.19 +/- 75.77
Episode length: 45.30 +/- 13.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-408.79 +/- 71.39
Episode length: 51.68 +/- 18.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 29       |
|    time_elapsed    | 272      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=-416.60 +/- 75.95
Episode length: 49.70 +/- 18.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 59500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.59e-10 |
|    explained_variance   | 0.00128   |
|    learning_rate        | 0.001     |
|    loss                 | 1.3e+03   |
|    n_updates            | 84        |
|    policy_gradient_loss | -6.14e-10 |
|    value_loss           | 3.42e+03  |
---------------------------------------
Eval num_timesteps=60000, episode_reward=-422.59 +/- 75.24
Episode length: 48.44 +/- 19.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-432.79 +/- 58.16
Episode length: 49.30 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-424.39 +/- 67.23
Episode length: 50.94 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 30       |
|    time_elapsed    | 281      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=-414.79 +/- 75.22
Episode length: 49.16 +/- 17.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.2      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.39e-08 |
|    explained_variance   | -0.0117   |
|    learning_rate        | 0.001     |
|    loss                 | 2.2e+04   |
|    n_updates            | 94        |
|    policy_gradient_loss | 2.03e-09  |
|    value_loss           | 4.22e+04  |
---------------------------------------
Eval num_timesteps=62000, episode_reward=-410.59 +/- 72.81
Episode length: 49.06 +/- 17.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-414.19 +/- 76.15
Episode length: 50.22 +/- 14.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-403.99 +/- 89.17
Episode length: 51.16 +/- 18.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 31       |
|    time_elapsed    | 291      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=-416.59 +/- 73.30
Episode length: 48.66 +/- 15.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.7      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 63500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.52e-10 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.001     |
|    loss                 | 1.23e+03  |
|    n_updates            | 104       |
|    policy_gradient_loss | 1.59e-09  |
|    value_loss           | 3.57e+03  |
---------------------------------------
Eval num_timesteps=64000, episode_reward=-411.19 +/- 69.31
Episode length: 53.58 +/- 20.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-414.19 +/- 57.89
Episode length: 49.28 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-423.80 +/- 59.34
Episode length: 50.70 +/- 16.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-393.79 +/- 85.66
Episode length: 49.08 +/- 20.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 32       |
|    time_elapsed    | 301      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-424.39 +/- 62.80
Episode length: 53.50 +/- 18.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.5          |
|    mean_reward          | -424          |
| time/                   |               |
|    total_timesteps      | 66000         |
| train/                  |               |
|    approx_kl            | -1.580338e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -3.55e-05     |
|    explained_variance   | 0.0183        |
|    learning_rate        | 0.001         |
|    loss                 | 2.24e+04      |
|    n_updates            | 114           |
|    policy_gradient_loss | 1.69e-07      |
|    value_loss           | 4.37e+04      |
-------------------------------------------
Eval num_timesteps=66500, episode_reward=-407.59 +/- 75.18
Episode length: 52.04 +/- 18.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-423.19 +/- 78.24
Episode length: 51.58 +/- 18.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-390.79 +/- 98.37
Episode length: 43.98 +/- 14.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44       |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 33       |
|    time_elapsed    | 311      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-412.39 +/- 72.93
Episode length: 48.68 +/- 14.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.7      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 68000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.21e-08 |
|    explained_variance   | 0.00851   |
|    learning_rate        | 0.001     |
|    loss                 | 1.88e+03  |
|    n_updates            | 124       |
|    policy_gradient_loss | -5.13e-08 |
|    value_loss           | 3.45e+03  |
---------------------------------------
Eval num_timesteps=68500, episode_reward=-403.39 +/- 80.81
Episode length: 49.56 +/- 16.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-408.79 +/- 78.36
Episode length: 50.54 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-415.39 +/- 72.05
Episode length: 51.08 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 34       |
|    time_elapsed    | 320      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-414.19 +/- 70.76
Episode length: 51.34 +/- 17.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.3      |
|    mean_reward          | -414      |
| time/                   |           |
|    total_timesteps      | 70000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4e-07    |
|    explained_variance   | 0.00868   |
|    learning_rate        | 0.001     |
|    loss                 | 2.23e+04  |
|    n_updates            | 134       |
|    policy_gradient_loss | -6.35e-09 |
|    value_loss           | 4.61e+04  |
---------------------------------------
Eval num_timesteps=70500, episode_reward=-421.99 +/- 77.17
Episode length: 52.92 +/- 18.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-420.79 +/- 61.12
Episode length: 53.12 +/- 17.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-429.20 +/- 74.90
Episode length: 49.88 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 35       |
|    time_elapsed    | 329      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=-431.60 +/- 65.45
Episode length: 49.58 +/- 12.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.6      |
|    mean_reward          | -432      |
| time/                   |           |
|    total_timesteps      | 72000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-09 |
|    explained_variance   | 0.00514   |
|    learning_rate        | 0.001     |
|    loss                 | 1.51e+03  |
|    n_updates            | 144       |
|    policy_gradient_loss | -1.91e-09 |
|    value_loss           | 3.37e+03  |
---------------------------------------
Eval num_timesteps=72500, episode_reward=-425.00 +/- 67.69
Episode length: 50.90 +/- 14.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-421.39 +/- 63.10
Episode length: 46.30 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-410.00 +/- 64.07
Episode length: 51.74 +/- 17.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 36       |
|    time_elapsed    | 338      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=-418.39 +/- 59.14
Episode length: 54.30 +/- 15.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.3      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 74000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.94e-07 |
|    explained_variance   | 0.00378   |
|    learning_rate        | 0.001     |
|    loss                 | 2.39e+04  |
|    n_updates            | 154       |
|    policy_gradient_loss | 8.18e-10  |
|    value_loss           | 4.77e+04  |
---------------------------------------
Eval num_timesteps=74500, episode_reward=-413.59 +/- 68.67
Episode length: 53.16 +/- 20.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-416.59 +/- 67.15
Episode length: 48.04 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-410.59 +/- 92.03
Episode length: 51.34 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 37       |
|    time_elapsed    | 347      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=-418.99 +/- 66.57
Episode length: 50.20 +/- 18.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.2      |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 76000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.42e-09 |
|    explained_variance   | 0.00526   |
|    learning_rate        | 0.001     |
|    loss                 | 2.33e+03  |
|    n_updates            | 164       |
|    policy_gradient_loss | -1.09e-09 |
|    value_loss           | 3.62e+03  |
---------------------------------------
Eval num_timesteps=76500, episode_reward=-423.19 +/- 66.28
Episode length: 48.08 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-412.40 +/- 75.36
Episode length: 51.02 +/- 15.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-411.20 +/- 73.84
Episode length: 49.34 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.7     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 38       |
|    time_elapsed    | 356      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-414.19 +/- 68.17
Episode length: 52.98 +/- 16.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53        |
|    mean_reward          | -414      |
| time/                   |           |
|    total_timesteps      | 78000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-06 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.001     |
|    loss                 | 2.15e+04  |
|    n_updates            | 174       |
|    policy_gradient_loss | 3.39e-09  |
|    value_loss           | 4.64e+04  |
---------------------------------------
Eval num_timesteps=78500, episode_reward=-423.19 +/- 68.16
Episode length: 49.24 +/- 18.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-427.99 +/- 52.53
Episode length: 53.78 +/- 20.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-420.79 +/- 73.18
Episode length: 45.90 +/- 14.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 39       |
|    time_elapsed    | 366      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-409.39 +/- 67.57
Episode length: 48.70 +/- 17.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.7      |
|    mean_reward          | -409      |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.84e-09 |
|    explained_variance   | 0.00465   |
|    learning_rate        | 0.001     |
|    loss                 | 2.63e+03  |
|    n_updates            | 184       |
|    policy_gradient_loss | -3.78e-10 |
|    value_loss           | 3.05e+03  |
---------------------------------------
Eval num_timesteps=80500, episode_reward=-420.19 +/- 61.45
Episode length: 47.94 +/- 15.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-417.79 +/- 56.67
Episode length: 49.70 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-405.80 +/- 72.75
Episode length: 50.94 +/- 18.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 40       |
|    time_elapsed    | 375      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=-425.59 +/- 67.08
Episode length: 52.22 +/- 15.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -426      |
| time/                   |           |
|    total_timesteps      | 82000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.84e-06 |
|    explained_variance   | 0.0076    |
|    learning_rate        | 0.001     |
|    loss                 | 2.08e+04  |
|    n_updates            | 194       |
|    policy_gradient_loss | -7.63e-09 |
|    value_loss           | 4.25e+04  |
---------------------------------------
Eval num_timesteps=82500, episode_reward=-423.80 +/- 73.19
Episode length: 56.24 +/- 20.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-417.19 +/- 74.52
Episode length: 50.50 +/- 19.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-415.40 +/- 75.47
Episode length: 50.78 +/- 16.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 41       |
|    time_elapsed    | 385      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=-422.59 +/- 76.66
Episode length: 51.96 +/- 18.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52        |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 84000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.83e-09 |
|    explained_variance   | 0.00438   |
|    learning_rate        | 0.001     |
|    loss                 | 2.33e+03  |
|    n_updates            | 204       |
|    policy_gradient_loss | 1.16e-09  |
|    value_loss           | 3.32e+03  |
---------------------------------------
Eval num_timesteps=84500, episode_reward=-419.59 +/- 64.34
Episode length: 51.16 +/- 21.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-414.80 +/- 59.14
Episode length: 52.90 +/- 20.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-424.40 +/- 65.33
Episode length: 53.76 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-411.20 +/- 67.73
Episode length: 51.44 +/- 17.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 42       |
|    time_elapsed    | 397      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=-430.99 +/- 62.21
Episode length: 53.24 +/- 18.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.2      |
|    mean_reward          | -431      |
| time/                   |           |
|    total_timesteps      | 86500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.6e-07  |
|    explained_variance   | 0.0117    |
|    learning_rate        | 0.001     |
|    loss                 | 1.85e+04  |
|    n_updates            | 214       |
|    policy_gradient_loss | -5.88e-09 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=87000, episode_reward=-406.99 +/- 64.54
Episode length: 49.44 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-407.59 +/- 77.77
Episode length: 52.72 +/- 14.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-418.99 +/- 66.30
Episode length: 52.76 +/- 14.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 43       |
|    time_elapsed    | 407      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=-399.79 +/- 69.30
Episode length: 47.74 +/- 14.84
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 47.7     |
|    mean_reward          | -400     |
| time/                   |          |
|    total_timesteps      | 88500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.8e-09 |
|    explained_variance   | 0.00468  |
|    learning_rate        | 0.001    |
|    loss                 | 3.02e+03 |
|    n_updates            | 224      |
|    policy_gradient_loss | 2.47e-10 |
|    value_loss           | 3.17e+03 |
--------------------------------------
Eval num_timesteps=89000, episode_reward=-397.99 +/- 76.99
Episode length: 49.86 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-413.59 +/- 81.17
Episode length: 48.78 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-417.19 +/- 68.47
Episode length: 49.86 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.4     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 44       |
|    time_elapsed    | 417      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=-414.20 +/- 66.03
Episode length: 50.46 +/- 17.88
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 50.5     |
|    mean_reward          | -414     |
| time/                   |          |
|    total_timesteps      | 90500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.7e-06 |
|    explained_variance   | 0.0134   |
|    learning_rate        | 0.001    |
|    loss                 | 1.71e+04 |
|    n_updates            | 234      |
|    policy_gradient_loss | 3.65e-09 |
|    value_loss           | 3.11e+04 |
--------------------------------------
Eval num_timesteps=91000, episode_reward=-398.59 +/- 80.11
Episode length: 49.90 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-429.79 +/- 59.22
Episode length: 55.08 +/- 15.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-403.39 +/- 72.10
Episode length: 45.34 +/- 15.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 45       |
|    time_elapsed    | 426      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=-405.20 +/- 79.29
Episode length: 46.12 +/- 16.13
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 46.1     |
|    mean_reward          | -405     |
| time/                   |          |
|    total_timesteps      | 92500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5.5e-09 |
|    explained_variance   | 0.0058   |
|    learning_rate        | 0.001    |
|    loss                 | 1.48e+03 |
|    n_updates            | 244      |
|    policy_gradient_loss | 6.77e-10 |
|    value_loss           | 3.93e+03 |
--------------------------------------
Eval num_timesteps=93000, episode_reward=-432.20 +/- 58.94
Episode length: 54.20 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-400.39 +/- 94.29
Episode length: 48.18 +/- 18.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-418.99 +/- 70.25
Episode length: 51.72 +/- 18.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 46       |
|    time_elapsed    | 435      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=-411.79 +/- 69.09
Episode length: 52.44 +/- 16.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.4      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 94500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.42e-06 |
|    explained_variance   | 0.00613   |
|    learning_rate        | 0.001     |
|    loss                 | 1.28e+04  |
|    n_updates            | 254       |
|    policy_gradient_loss | -5.24e-09 |
|    value_loss           | 2.55e+04  |
---------------------------------------
Eval num_timesteps=95000, episode_reward=-421.99 +/- 66.93
Episode length: 49.52 +/- 20.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-414.79 +/- 69.50
Episode length: 47.18 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-410.00 +/- 67.09
Episode length: 51.84 +/- 18.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 47       |
|    time_elapsed    | 444      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=-412.40 +/- 70.42
Episode length: 51.72 +/- 15.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 96500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.17e-09 |
|    explained_variance   | 0.00519   |
|    learning_rate        | 0.001     |
|    loss                 | 2.67e+03  |
|    n_updates            | 264       |
|    policy_gradient_loss | 3.64e-10  |
|    value_loss           | 3.07e+03  |
---------------------------------------
Eval num_timesteps=97000, episode_reward=-408.79 +/- 68.56
Episode length: 48.88 +/- 14.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-404.59 +/- 63.85
Episode length: 51.36 +/- 18.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-425.00 +/- 70.05
Episode length: 49.08 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 48       |
|    time_elapsed    | 454      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=-428.00 +/- 70.65
Episode length: 53.68 +/- 21.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.7      |
|    mean_reward          | -428      |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.08e-06 |
|    explained_variance   | 0.0116    |
|    learning_rate        | 0.001     |
|    loss                 | 9.4e+03   |
|    n_updates            | 274       |
|    policy_gradient_loss | 6.98e-11  |
|    value_loss           | 2.13e+04  |
---------------------------------------
Eval num_timesteps=99000, episode_reward=-418.99 +/- 60.03
Episode length: 48.56 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-401.59 +/- 77.54
Episode length: 48.56 +/- 19.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-428.60 +/- 73.06
Episode length: 51.28 +/- 20.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 49       |
|    time_elapsed    | 464      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=-426.79 +/- 63.88
Episode length: 51.42 +/- 15.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 100500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-08 |
|    explained_variance   | 0.00677   |
|    learning_rate        | 0.001     |
|    loss                 | 2.16e+03  |
|    n_updates            | 284       |
|    policy_gradient_loss | -2.04e-10 |
|    value_loss           | 3.59e+03  |
---------------------------------------
Eval num_timesteps=101000, episode_reward=-418.39 +/- 78.28
Episode length: 47.04 +/- 17.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-424.99 +/- 70.56
Episode length: 51.34 +/- 18.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-419.59 +/- 71.75
Episode length: 48.70 +/- 14.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 50       |
|    time_elapsed    | 473      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=-428.00 +/- 56.17
Episode length: 47.88 +/- 13.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.9      |
|    mean_reward          | -428      |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.8e-07  |
|    explained_variance   | 0.0053    |
|    learning_rate        | 0.001     |
|    loss                 | 8.51e+03  |
|    n_updates            | 294       |
|    policy_gradient_loss | -2.11e-09 |
|    value_loss           | 1.96e+04  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=-402.79 +/- 87.97
Episode length: 52.40 +/- 18.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-420.79 +/- 64.28
Episode length: 53.46 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-408.79 +/- 73.87
Episode length: 52.54 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 51       |
|    time_elapsed    | 483      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-421.99 +/- 76.94
Episode length: 53.42 +/- 23.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.4      |
|    mean_reward          | -422      |
| time/                   |           |
|    total_timesteps      | 104500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.57e-09 |
|    explained_variance   | 0.00573   |
|    learning_rate        | 0.001     |
|    loss                 | 1.52e+03  |
|    n_updates            | 304       |
|    policy_gradient_loss | -4.45e-10 |
|    value_loss           | 3.75e+03  |
---------------------------------------
Eval num_timesteps=105000, episode_reward=-410.00 +/- 68.68
Episode length: 51.66 +/- 18.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-422.60 +/- 65.52
Episode length: 52.88 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-414.79 +/- 70.78
Episode length: 51.16 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 52       |
|    time_elapsed    | 492      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=-421.99 +/- 65.02
Episode length: 46.08 +/- 13.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.1      |
|    mean_reward          | -422      |
| time/                   |           |
|    total_timesteps      | 106500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.05e-06 |
|    explained_variance   | 0.00974   |
|    learning_rate        | 0.001     |
|    loss                 | 9.07e+03  |
|    n_updates            | 314       |
|    policy_gradient_loss | -3.81e-10 |
|    value_loss           | 1.7e+04   |
---------------------------------------
Eval num_timesteps=107000, episode_reward=-430.99 +/- 62.21
Episode length: 47.24 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-391.99 +/- 87.04
Episode length: 50.46 +/- 16.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-404.59 +/- 71.06
Episode length: 50.92 +/- 17.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-417.19 +/- 76.19
Episode length: 48.24 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 53       |
|    time_elapsed    | 503      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-398.59 +/- 79.66
Episode length: 49.80 +/- 22.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -399      |
| time/                   |           |
|    total_timesteps      | 109000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.37e-09 |
|    explained_variance   | 0.00448   |
|    learning_rate        | 0.001     |
|    loss                 | 784       |
|    n_updates            | 324       |
|    policy_gradient_loss | 1.2e-09   |
|    value_loss           | 2.91e+03  |
---------------------------------------
Eval num_timesteps=109500, episode_reward=-417.19 +/- 66.07
Episode length: 51.10 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-417.19 +/- 65.24
Episode length: 47.60 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-413.59 +/- 62.06
Episode length: 54.52 +/- 23.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 54       |
|    time_elapsed    | 512      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-420.80 +/- 75.84
Episode length: 53.04 +/- 18.84
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 53       |
|    mean_reward          | -421     |
| time/                   |          |
|    total_timesteps      | 111000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.2e-06 |
|    explained_variance   | 0.0112   |
|    learning_rate        | 0.001    |
|    loss                 | 7.48e+03 |
|    n_updates            | 334      |
|    policy_gradient_loss | 4.69e-10 |
|    value_loss           | 1.42e+04 |
--------------------------------------
Eval num_timesteps=111500, episode_reward=-418.39 +/- 71.29
Episode length: 47.26 +/- 15.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-421.40 +/- 68.57
Episode length: 49.84 +/- 13.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-405.79 +/- 71.75
Episode length: 48.66 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 55       |
|    time_elapsed    | 522      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-419.59 +/- 61.77
Episode length: 49.48 +/- 15.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.5      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 113000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.59e-09 |
|    explained_variance   | 0.0061    |
|    learning_rate        | 0.001     |
|    loss                 | 867       |
|    n_updates            | 344       |
|    policy_gradient_loss | 3.46e-10  |
|    value_loss           | 3.03e+03  |
---------------------------------------
Eval num_timesteps=113500, episode_reward=-418.99 +/- 67.91
Episode length: 49.70 +/- 21.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-429.79 +/- 49.63
Episode length: 55.34 +/- 20.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-411.19 +/- 73.59
Episode length: 54.46 +/- 18.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 56       |
|    time_elapsed    | 531      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-412.39 +/- 64.28
Episode length: 52.10 +/- 17.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.1      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.96e-06 |
|    explained_variance   | 0.0148    |
|    learning_rate        | 0.001     |
|    loss                 | 7.84e+03  |
|    n_updates            | 354       |
|    policy_gradient_loss | 6.58e-09  |
|    value_loss           | 1.33e+04  |
---------------------------------------
Eval num_timesteps=115500, episode_reward=-396.79 +/- 76.69
Episode length: 48.78 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-411.79 +/- 73.14
Episode length: 52.56 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-405.76 +/- 83.72
Episode length: 46.72 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 57       |
|    time_elapsed    | 540      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-426.80 +/- 55.11
Episode length: 54.90 +/- 16.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.9      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 117000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.15e-09 |
|    explained_variance   | 0.00395   |
|    learning_rate        | 0.001     |
|    loss                 | 764       |
|    n_updates            | 364       |
|    policy_gradient_loss | 1.22e-10  |
|    value_loss           | 2.54e+03  |
---------------------------------------
Eval num_timesteps=117500, episode_reward=-420.80 +/- 76.31
Episode length: 51.06 +/- 18.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-408.20 +/- 72.82
Episode length: 48.60 +/- 16.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-414.79 +/- 76.41
Episode length: 52.32 +/- 19.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.6     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 58       |
|    time_elapsed    | 550      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-421.99 +/- 56.11
Episode length: 53.16 +/- 19.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.2      |
|    mean_reward          | -422      |
| time/                   |           |
|    total_timesteps      | 119000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.06e-06 |
|    explained_variance   | 0.00928   |
|    learning_rate        | 0.001     |
|    loss                 | 5e+03     |
|    n_updates            | 374       |
|    policy_gradient_loss | 8.67e-10  |
|    value_loss           | 1.11e+04  |
---------------------------------------
Eval num_timesteps=119500, episode_reward=-409.39 +/- 69.41
Episode length: 49.66 +/- 15.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-414.79 +/- 67.12
Episode length: 48.32 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-419.59 +/- 58.48
Episode length: 46.76 +/- 13.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 59       |
|    time_elapsed    | 559      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-417.19 +/- 58.25
Episode length: 49.78 +/- 14.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 121000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.37e-05 |
|    explained_variance   | 0.00461   |
|    learning_rate        | 0.001     |
|    loss                 | 1.23e+03  |
|    n_updates            | 384       |
|    policy_gradient_loss | 3.92e-07  |
|    value_loss           | 3.56e+03  |
---------------------------------------
Eval num_timesteps=121500, episode_reward=-419.59 +/- 59.09
Episode length: 53.88 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-415.99 +/- 71.05
Episode length: 48.42 +/- 14.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-408.20 +/- 72.82
Episode length: 50.78 +/- 13.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 60       |
|    time_elapsed    | 568      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-412.39 +/- 71.44
Episode length: 53.36 +/- 18.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.4      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 123000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.65e-08 |
|    explained_variance   | 0.00187   |
|    learning_rate        | 0.001     |
|    loss                 | 2.13e+03  |
|    n_updates            | 394       |
|    policy_gradient_loss | -9.04e-09 |
|    value_loss           | 4.75e+03  |
---------------------------------------
Eval num_timesteps=123500, episode_reward=-429.19 +/- 55.59
Episode length: 51.46 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-405.19 +/- 80.86
Episode length: 46.42 +/- 13.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-417.79 +/- 80.10
Episode length: 53.00 +/- 20.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 61       |
|    time_elapsed    | 577      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-420.19 +/- 68.12
Episode length: 50.48 +/- 18.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.5      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.37e-05 |
|    explained_variance   | 0.0118    |
|    learning_rate        | 0.001     |
|    loss                 | 1.67e+03  |
|    n_updates            | 404       |
|    policy_gradient_loss | 2.25e-07  |
|    value_loss           | 3.08e+03  |
---------------------------------------
Eval num_timesteps=125500, episode_reward=-412.97 +/- 96.28
Episode length: 53.98 +/- 19.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-415.39 +/- 66.33
Episode length: 51.28 +/- 16.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-415.99 +/- 61.55
Episode length: 49.84 +/- 15.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 62       |
|    time_elapsed    | 586      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-406.39 +/- 84.50
Episode length: 50.44 +/- 19.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -406      |
| time/                   |           |
|    total_timesteps      | 127000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.65e-08 |
|    explained_variance   | 0.00311   |
|    learning_rate        | 0.001     |
|    loss                 | 2.23e+03  |
|    n_updates            | 414       |
|    policy_gradient_loss | -9.91e-09 |
|    value_loss           | 4.55e+03  |
---------------------------------------
Eval num_timesteps=127500, episode_reward=-419.60 +/- 69.97
Episode length: 50.98 +/- 15.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-423.79 +/- 55.58
Episode length: 49.64 +/- 12.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-424.39 +/- 58.96
Episode length: 48.72 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-417.19 +/- 73.54
Episode length: 49.18 +/- 15.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 63       |
|    time_elapsed    | 597      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=-429.20 +/- 49.78
Episode length: 48.48 +/- 18.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 129500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.29e-05 |
|    explained_variance   | 0.011     |
|    learning_rate        | 0.001     |
|    loss                 | 682       |
|    n_updates            | 424       |
|    policy_gradient_loss | 1.76e-07  |
|    value_loss           | 3.11e+03  |
---------------------------------------
Eval num_timesteps=130000, episode_reward=-427.39 +/- 62.07
Episode length: 50.24 +/- 15.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-427.40 +/- 66.00
Episode length: 51.46 +/- 14.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-398.59 +/- 86.58
Episode length: 46.98 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 64       |
|    time_elapsed    | 606      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=-408.19 +/- 85.97
Episode length: 46.28 +/- 16.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.3      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 131500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-08 |
|    explained_variance   | 0.00505   |
|    learning_rate        | 0.001     |
|    loss                 | 2.7e+03   |
|    n_updates            | 434       |
|    policy_gradient_loss | -1.99e-08 |
|    value_loss           | 4.1e+03   |
---------------------------------------
Eval num_timesteps=132000, episode_reward=-426.19 +/- 65.09
Episode length: 54.00 +/- 18.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-421.99 +/- 65.30
Episode length: 50.52 +/- 17.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-421.39 +/- 72.90
Episode length: 47.86 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 65       |
|    time_elapsed    | 615      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=-429.19 +/- 61.14
Episode length: 50.24 +/- 16.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.2      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 133500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0156    |
|    learning_rate        | 0.001     |
|    loss                 | 1.39e+03  |
|    n_updates            | 444       |
|    policy_gradient_loss | -9.84e-10 |
|    value_loss           | 3.16e+03  |
---------------------------------------
Eval num_timesteps=134000, episode_reward=-418.39 +/- 76.17
Episode length: 51.66 +/- 17.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-401.59 +/- 74.70
Episode length: 45.54 +/- 15.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-432.19 +/- 61.92
Episode length: 52.74 +/- 18.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 66       |
|    time_elapsed    | 625      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=-443.60 +/- 51.27
Episode length: 55.84 +/- 18.43
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 55.8     |
|    mean_reward          | -444     |
| time/                   |          |
|    total_timesteps      | 135500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0333   |
|    learning_rate        | 0.001    |
|    loss                 | 3.39e+04 |
|    n_updates            | 454      |
|    policy_gradient_loss | 1.18e-09 |
|    value_loss           | 6.63e+04 |
--------------------------------------
Eval num_timesteps=136000, episode_reward=-414.19 +/- 78.02
Episode length: 51.16 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-428.00 +/- 71.66
Episode length: 53.92 +/- 13.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-430.99 +/- 61.04
Episode length: 50.60 +/- 14.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 67       |
|    time_elapsed    | 634      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=-399.79 +/- 92.64
Episode length: 50.02 +/- 16.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -400      |
| time/                   |           |
|    total_timesteps      | 137500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.05e-09 |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.001     |
|    loss                 | 1.12e+03  |
|    n_updates            | 464       |
|    policy_gradient_loss | -4.28e-10 |
|    value_loss           | 2.58e+03  |
---------------------------------------
Eval num_timesteps=138000, episode_reward=-432.80 +/- 55.95
Episode length: 54.42 +/- 19.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-403.39 +/- 85.57
Episode length: 48.44 +/- 15.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-411.19 +/- 71.61
Episode length: 50.54 +/- 15.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.5     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 68       |
|    time_elapsed    | 643      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-406.99 +/- 77.00
Episode length: 51.78 +/- 20.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.8      |
|    mean_reward          | -407      |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-11 |
|    explained_variance   | 0.00397   |
|    learning_rate        | 0.001     |
|    loss                 | 1.6e+03   |
|    n_updates            | 474       |
|    policy_gradient_loss | 1.02e-09  |
|    value_loss           | 3.36e+03  |
---------------------------------------
Eval num_timesteps=140000, episode_reward=-406.39 +/- 67.98
Episode length: 52.84 +/- 16.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-433.99 +/- 49.06
Episode length: 52.54 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-424.99 +/- 63.58
Episode length: 50.68 +/- 14.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.2     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 69       |
|    time_elapsed    | 653      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-419.59 +/- 69.46
Episode length: 50.94 +/- 16.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.9      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 141500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.13e-33 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.001     |
|    loss                 | 1.01e+03  |
|    n_updates            | 484       |
|    policy_gradient_loss | -2.88e-10 |
|    value_loss           | 3.62e+03  |
---------------------------------------
Eval num_timesteps=142000, episode_reward=-418.39 +/- 67.13
Episode length: 49.42 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-423.19 +/- 66.56
Episode length: 46.80 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-419.00 +/- 55.35
Episode length: 50.98 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 70       |
|    time_elapsed    | 662      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-423.20 +/- 61.79
Episode length: 51.48 +/- 17.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.5      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 143500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.36e-30 |
|    explained_variance   | 0.0453    |
|    learning_rate        | 0.001     |
|    loss                 | 5.34e+03  |
|    n_updates            | 494       |
|    policy_gradient_loss | -2.84e-09 |
|    value_loss           | 1.02e+04  |
---------------------------------------
Eval num_timesteps=144000, episode_reward=-427.39 +/- 64.07
Episode length: 47.50 +/- 20.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-420.79 +/- 67.29
Episode length: 54.74 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-431.00 +/- 62.50
Episode length: 48.98 +/- 18.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.1     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 71       |
|    time_elapsed    | 671      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=-426.20 +/- 83.72
Episode length: 48.76 +/- 16.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.8      |
|    mean_reward          | -426      |
| time/                   |           |
|    total_timesteps      | 145500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.91e-33 |
|    explained_variance   | 0.0114    |
|    learning_rate        | 0.001     |
|    loss                 | 1.35e+03  |
|    n_updates            | 504       |
|    policy_gradient_loss | 3.09e-10  |
|    value_loss           | 3.61e+03  |
---------------------------------------
Eval num_timesteps=146000, episode_reward=-399.79 +/- 76.47
Episode length: 49.82 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-407.59 +/- 80.27
Episode length: 52.44 +/- 19.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-416.59 +/- 66.61
Episode length: 48.92 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 72       |
|    time_elapsed    | 680      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-400.99 +/- 68.01
Episode length: 48.26 +/- 18.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.3      |
|    mean_reward          | -401      |
| time/                   |           |
|    total_timesteps      | 147500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.31e-29 |
|    explained_variance   | 0.0301    |
|    learning_rate        | 0.001     |
|    loss                 | 3.53e+03  |
|    n_updates            | 514       |
|    policy_gradient_loss | -1.56e-09 |
|    value_loss           | 6.65e+03  |
---------------------------------------
Eval num_timesteps=148000, episode_reward=-429.20 +/- 59.65
Episode length: 50.34 +/- 15.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-426.80 +/- 59.81
Episode length: 55.22 +/- 17.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-421.99 +/- 58.31
Episode length: 52.22 +/- 21.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-405.79 +/- 73.49
Episode length: 48.28 +/- 15.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 73       |
|    time_elapsed    | 691      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-407.59 +/- 70.74
Episode length: 50.80 +/- 15.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.8      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 150000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.63e-32 |
|    explained_variance   | 0.014     |
|    learning_rate        | 0.001     |
|    loss                 | 1.86e+03  |
|    n_updates            | 524       |
|    policy_gradient_loss | -9.26e-10 |
|    value_loss           | 3.26e+03  |
---------------------------------------
Eval num_timesteps=150500, episode_reward=-426.80 +/- 67.18
Episode length: 48.78 +/- 18.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-423.19 +/- 62.37
Episode length: 47.92 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-423.79 +/- 54.93
Episode length: 47.82 +/- 15.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 74       |
|    time_elapsed    | 700      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-417.80 +/- 61.25
Episode length: 52.52 +/- 17.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.5      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 152000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.39e-28 |
|    explained_variance   | 0.0316    |
|    learning_rate        | 0.001     |
|    loss                 | 2.77e+03  |
|    n_updates            | 534       |
|    policy_gradient_loss | 2.96e-09  |
|    value_loss           | 5.51e+03  |
---------------------------------------
Eval num_timesteps=152500, episode_reward=-417.19 +/- 76.90
Episode length: 53.80 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-424.39 +/- 59.87
Episode length: 54.86 +/- 15.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-415.99 +/- 54.08
Episode length: 51.30 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 75       |
|    time_elapsed    | 710      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-426.80 +/- 73.08
Episode length: 54.96 +/- 14.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 55        |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 154000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.65e-31 |
|    explained_variance   | 0.0119    |
|    learning_rate        | 0.001     |
|    loss                 | 2.81e+03  |
|    n_updates            | 544       |
|    policy_gradient_loss | -1.74e-09 |
|    value_loss           | 3.38e+03  |
---------------------------------------
Eval num_timesteps=154500, episode_reward=-401.59 +/- 74.46
Episode length: 48.68 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-418.39 +/- 68.45
Episode length: 49.00 +/- 14.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-414.19 +/- 73.51
Episode length: 51.70 +/- 19.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 76       |
|    time_elapsed    | 719      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-415.99 +/- 69.26
Episode length: 47.82 +/- 16.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.8      |
|    mean_reward          | -416      |
| time/                   |           |
|    total_timesteps      | 156000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.36e-27 |
|    explained_variance   | 0.0273    |
|    learning_rate        | 0.001     |
|    loss                 | 1.73e+03  |
|    n_updates            | 554       |
|    policy_gradient_loss | 8.98e-10  |
|    value_loss           | 4.92e+03  |
---------------------------------------
Eval num_timesteps=156500, episode_reward=-414.19 +/- 79.62
Episode length: 46.88 +/- 14.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-401.59 +/- 71.25
Episode length: 49.68 +/- 17.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-418.40 +/- 68.72
Episode length: 53.02 +/- 19.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 77       |
|    time_elapsed    | 728      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-410.59 +/- 55.72
Episode length: 50.06 +/- 17.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 158000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.63e-30 |
|    explained_variance   | 0.00823   |
|    learning_rate        | 0.001     |
|    loss                 | 1.27e+03  |
|    n_updates            | 564       |
|    policy_gradient_loss | 8.72e-10  |
|    value_loss           | 3.53e+03  |
---------------------------------------
Eval num_timesteps=158500, episode_reward=-417.79 +/- 61.25
Episode length: 50.46 +/- 13.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-433.99 +/- 53.95
Episode length: 53.68 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-411.19 +/- 72.86
Episode length: 49.72 +/- 18.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 78       |
|    time_elapsed    | 737      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-417.79 +/- 81.67
Episode length: 52.26 +/- 18.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.3      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.68e-27 |
|    explained_variance   | 0.02      |
|    learning_rate        | 0.001     |
|    loss                 | 2.03e+03  |
|    n_updates            | 574       |
|    policy_gradient_loss | -1.23e-09 |
|    value_loss           | 4.51e+03  |
---------------------------------------
Eval num_timesteps=160500, episode_reward=-418.39 +/- 65.77
Episode length: 52.00 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-421.40 +/- 71.90
Episode length: 55.94 +/- 18.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-412.99 +/- 62.90
Episode length: 49.50 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 79       |
|    time_elapsed    | 747      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-426.19 +/- 67.53
Episode length: 53.56 +/- 19.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.6      |
|    mean_reward          | -426      |
| time/                   |           |
|    total_timesteps      | 162000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.56e-30 |
|    explained_variance   | 0.00763   |
|    learning_rate        | 0.001     |
|    loss                 | 1.49e+03  |
|    n_updates            | 584       |
|    policy_gradient_loss | 7.01e-10  |
|    value_loss           | 3.77e+03  |
---------------------------------------
Eval num_timesteps=162500, episode_reward=-441.20 +/- 48.91
Episode length: 51.26 +/- 18.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-426.19 +/- 73.16
Episode length: 50.86 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-397.99 +/- 80.87
Episode length: 44.94 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.9     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 80       |
|    time_elapsed    | 756      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-434.59 +/- 52.74
Episode length: 48.74 +/- 17.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.7      |
|    mean_reward          | -435      |
| time/                   |           |
|    total_timesteps      | 164000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.09e-26 |
|    explained_variance   | 0.0222    |
|    learning_rate        | 0.001     |
|    loss                 | 2.2e+03   |
|    n_updates            | 594       |
|    policy_gradient_loss | -2.62e-10 |
|    value_loss           | 3.8e+03   |
---------------------------------------
Eval num_timesteps=164500, episode_reward=-427.40 +/- 78.00
Episode length: 54.86 +/- 17.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.9     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-429.19 +/- 65.41
Episode length: 50.50 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-427.39 +/- 78.92
Episode length: 52.12 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 81       |
|    time_elapsed    | 765      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-418.40 +/- 68.45
Episode length: 52.54 +/- 17.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.5      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 166000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18e-28 |
|    explained_variance   | 0.00576   |
|    learning_rate        | 0.001     |
|    loss                 | 818       |
|    n_updates            | 604       |
|    policy_gradient_loss | -7.25e-10 |
|    value_loss           | 3.54e+03  |
---------------------------------------
Eval num_timesteps=166500, episode_reward=-422.59 +/- 69.52
Episode length: 51.26 +/- 16.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-420.19 +/- 68.64
Episode length: 47.88 +/- 14.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-417.20 +/- 73.05
Episode length: 50.66 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 82       |
|    time_elapsed    | 774      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-412.99 +/- 78.21
Episode length: 56.34 +/- 21.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 56.3      |
|    mean_reward          | -413      |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.83e-26 |
|    explained_variance   | 0.0201    |
|    learning_rate        | 0.001     |
|    loss                 | 2.53e+03  |
|    n_updates            | 614       |
|    policy_gradient_loss | -2.79e-10 |
|    value_loss           | 3.93e+03  |
---------------------------------------
Eval num_timesteps=168500, episode_reward=-429.80 +/- 55.77
Episode length: 49.68 +/- 18.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-414.79 +/- 58.84
Episode length: 48.00 +/- 14.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-414.19 +/- 67.91
Episode length: 46.38 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 83       |
|    time_elapsed    | 783      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-418.39 +/- 62.11
Episode length: 44.78 +/- 13.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 44.8      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 170000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.72e-29 |
|    explained_variance   | 0.00836   |
|    learning_rate        | 0.001     |
|    loss                 | 1.62e+03  |
|    n_updates            | 624       |
|    policy_gradient_loss | -1.46e-12 |
|    value_loss           | 3.2e+03   |
---------------------------------------
Eval num_timesteps=170500, episode_reward=-413.59 +/- 73.49
Episode length: 46.98 +/- 17.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-394.39 +/- 87.48
Episode length: 46.74 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-411.19 +/- 66.93
Episode length: 47.86 +/- 16.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-434.00 +/- 59.95
Episode length: 53.06 +/- 14.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 84       |
|    time_elapsed    | 794      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=-403.39 +/- 71.35
Episode length: 51.46 +/- 16.96
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 51.5     |
|    mean_reward          | -403     |
| time/                   |          |
|    total_timesteps      | 172500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -7.1e-26 |
|    explained_variance   | 0.0218   |
|    learning_rate        | 0.001    |
|    loss                 | 1.96e+03 |
|    n_updates            | 634      |
|    policy_gradient_loss | -5.7e-10 |
|    value_loss           | 4.15e+03 |
--------------------------------------
Eval num_timesteps=173000, episode_reward=-402.79 +/- 79.82
Episode length: 50.98 +/- 16.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-422.59 +/- 59.78
Episode length: 46.20 +/- 15.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-420.19 +/- 61.74
Episode length: 51.30 +/- 15.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 85       |
|    time_elapsed    | 803      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-422.59 +/- 59.47
Episode length: 50.00 +/- 18.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 174500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.55e-28 |
|    explained_variance   | 0.0113    |
|    learning_rate        | 0.001     |
|    loss                 | 2.24e+03  |
|    n_updates            | 644       |
|    policy_gradient_loss | 5.38e-10  |
|    value_loss           | 2.96e+03  |
---------------------------------------
Eval num_timesteps=175000, episode_reward=-411.19 +/- 67.47
Episode length: 48.14 +/- 13.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-428.00 +/- 67.79
Episode length: 49.44 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-422.59 +/- 64.13
Episode length: 49.42 +/- 12.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 86       |
|    time_elapsed    | 812      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-423.19 +/- 62.08
Episode length: 49.08 +/- 16.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.1      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 176500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.71e-25 |
|    explained_variance   | 0.0271    |
|    learning_rate        | 0.001     |
|    loss                 | 1.92e+03  |
|    n_updates            | 654       |
|    policy_gradient_loss | 1.36e-09  |
|    value_loss           | 3.59e+03  |
---------------------------------------
Eval num_timesteps=177000, episode_reward=-418.99 +/- 68.43
Episode length: 49.22 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-417.19 +/- 87.62
Episode length: 48.32 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-411.79 +/- 76.04
Episode length: 52.74 +/- 17.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 87       |
|    time_elapsed    | 821      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-410.60 +/- 75.00
Episode length: 50.72 +/- 15.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 178500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.83e-27 |
|    explained_variance   | 0.00964   |
|    learning_rate        | 0.001     |
|    loss                 | 2.56e+03  |
|    n_updates            | 664       |
|    policy_gradient_loss | 3.43e-10  |
|    value_loss           | 3.72e+03  |
---------------------------------------
Eval num_timesteps=179000, episode_reward=-422.00 +/- 75.76
Episode length: 53.58 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-400.40 +/- 75.89
Episode length: 50.26 +/- 14.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-413.59 +/- 75.66
Episode length: 51.92 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 88       |
|    time_elapsed    | 830      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-408.19 +/- 82.11
Episode length: 51.10 +/- 14.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.1      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 180500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.14e-24 |
|    explained_variance   | 0.0199    |
|    learning_rate        | 0.001     |
|    loss                 | 1.69e+03  |
|    n_updates            | 674       |
|    policy_gradient_loss | 6.17e-10  |
|    value_loss           | 3.42e+03  |
---------------------------------------
Eval num_timesteps=181000, episode_reward=-429.19 +/- 60.55
Episode length: 50.12 +/- 17.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-422.59 +/- 64.41
Episode length: 51.38 +/- 16.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-430.39 +/- 63.49
Episode length: 51.12 +/- 19.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 89       |
|    time_elapsed    | 840      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-414.79 +/- 64.11
Episode length: 46.00 +/- 16.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46        |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 182500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.02e-27 |
|    explained_variance   | 0.00823   |
|    learning_rate        | 0.001     |
|    loss                 | 2.14e+03  |
|    n_updates            | 684       |
|    policy_gradient_loss | 8.96e-10  |
|    value_loss           | 3.5e+03   |
---------------------------------------
Eval num_timesteps=183000, episode_reward=-404.60 +/- 77.36
Episode length: 50.62 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-419.59 +/- 76.84
Episode length: 49.14 +/- 14.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-407.00 +/- 77.47
Episode length: 52.10 +/- 16.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 90       |
|    time_elapsed    | 849      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=-414.20 +/- 70.76
Episode length: 51.88 +/- 16.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.9      |
|    mean_reward          | -414      |
| time/                   |           |
|    total_timesteps      | 184500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.43e-25 |
|    explained_variance   | 0.0221    |
|    learning_rate        | 0.001     |
|    loss                 | 1.81e+03  |
|    n_updates            | 694       |
|    policy_gradient_loss | -1.66e-10 |
|    value_loss           | 3.58e+03  |
---------------------------------------
Eval num_timesteps=185000, episode_reward=-442.99 +/- 53.29
Episode length: 51.28 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -443     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-404.59 +/- 81.66
Episode length: 46.94 +/- 15.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-427.39 +/- 60.90
Episode length: 48.06 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 91       |
|    time_elapsed    | 858      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-422.60 +/- 57.00
Episode length: 53.36 +/- 16.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.4      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 186500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.99e-27 |
|    explained_variance   | 0.00904   |
|    learning_rate        | 0.001     |
|    loss                 | 1.87e+03  |
|    n_updates            | 704       |
|    policy_gradient_loss | -3.9e-10  |
|    value_loss           | 3.2e+03   |
---------------------------------------
Eval num_timesteps=187000, episode_reward=-413.00 +/- 75.63
Episode length: 48.88 +/- 13.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-411.79 +/- 76.75
Episode length: 51.12 +/- 18.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-404.59 +/- 90.05
Episode length: 50.74 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 92       |
|    time_elapsed    | 867      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=-440.59 +/- 52.05
Episode length: 45.88 +/- 13.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.9      |
|    mean_reward          | -441      |
| time/                   |           |
|    total_timesteps      | 188500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.41e-25 |
|    explained_variance   | 0.0149    |
|    learning_rate        | 0.001     |
|    loss                 | 2.3e+03   |
|    n_updates            | 714       |
|    policy_gradient_loss | 3.29e-10  |
|    value_loss           | 3.72e+03  |
---------------------------------------
Eval num_timesteps=189000, episode_reward=-419.59 +/- 59.40
Episode length: 52.46 +/- 15.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-423.19 +/- 67.09
Episode length: 48.86 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-396.79 +/- 81.47
Episode length: 50.44 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 93       |
|    time_elapsed    | 876      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=-412.40 +/- 63.15
Episode length: 52.52 +/- 15.61
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 52.5     |
|    mean_reward          | -412     |
| time/                   |          |
|    total_timesteps      | 190500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.4e-27 |
|    explained_variance   | 0.00798  |
|    learning_rate        | 0.001    |
|    loss                 | 1.62e+03 |
|    n_updates            | 724      |
|    policy_gradient_loss | 6.8e-10  |
|    value_loss           | 3.57e+03 |
--------------------------------------
Eval num_timesteps=191000, episode_reward=-413.60 +/- 63.50
Episode length: 51.52 +/- 17.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-426.79 +/- 59.81
Episode length: 50.06 +/- 18.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-434.60 +/- 67.95
Episode length: 52.14 +/- 15.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-403.39 +/- 85.15
Episode length: 47.36 +/- 14.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 94       |
|    time_elapsed    | 887      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-392.59 +/- 100.09
Episode length: 51.72 +/- 15.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -393      |
| time/                   |           |
|    total_timesteps      | 193000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.95e-24 |
|    explained_variance   | 0.0209    |
|    learning_rate        | 0.001     |
|    loss                 | 1.49e+03  |
|    n_updates            | 734       |
|    policy_gradient_loss | 1.07e-09  |
|    value_loss           | 3.83e+03  |
---------------------------------------
Eval num_timesteps=193500, episode_reward=-387.19 +/- 86.79
Episode length: 48.46 +/- 15.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-410.00 +/- 68.95
Episode length: 51.22 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-421.99 +/- 64.75
Episode length: 51.82 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 95       |
|    time_elapsed    | 896      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-405.79 +/- 82.49
Episode length: 48.58 +/- 15.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.6      |
|    mean_reward          | -406      |
| time/                   |           |
|    total_timesteps      | 195000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.31e-26 |
|    explained_variance   | 0.00913   |
|    learning_rate        | 0.001     |
|    loss                 | 1.28e+03  |
|    n_updates            | 744       |
|    policy_gradient_loss | 1.19e-10  |
|    value_loss           | 3.32e+03  |
---------------------------------------
Eval num_timesteps=195500, episode_reward=-412.99 +/- 76.34
Episode length: 48.00 +/- 16.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-411.19 +/- 56.75
Episode length: 47.66 +/- 18.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-419.59 +/- 74.94
Episode length: 49.74 +/- 18.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 96       |
|    time_elapsed    | 905      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-401.59 +/- 74.94
Episode length: 50.18 +/- 15.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.2      |
|    mean_reward          | -402      |
| time/                   |           |
|    total_timesteps      | 197000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.36e-24 |
|    explained_variance   | 0.0169    |
|    learning_rate        | 0.001     |
|    loss                 | 1.42e+03  |
|    n_updates            | 754       |
|    policy_gradient_loss | 8.79e-10  |
|    value_loss           | 3.64e+03  |
---------------------------------------
Eval num_timesteps=197500, episode_reward=-427.39 +/- 60.90
Episode length: 48.24 +/- 13.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-393.79 +/- 82.23
Episode length: 47.74 +/- 16.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-418.99 +/- 65.20
Episode length: 50.70 +/- 14.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 97       |
|    time_elapsed    | 914      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-402.79 +/- 76.59
Episode length: 48.32 +/- 17.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.3      |
|    mean_reward          | -403      |
| time/                   |           |
|    total_timesteps      | 199000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1e-26    |
|    explained_variance   | 0.00804   |
|    learning_rate        | 0.001     |
|    loss                 | 1.59e+03  |
|    n_updates            | 764       |
|    policy_gradient_loss | -3.58e-10 |
|    value_loss           | 2.96e+03  |
---------------------------------------
Eval num_timesteps=199500, episode_reward=-399.19 +/- 73.69
Episode length: 48.08 +/- 15.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-415.98 +/- 56.03
Episode length: 50.90 +/- 21.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-428.59 +/- 57.94
Episode length: 51.00 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 98       |
|    time_elapsed    | 924      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-420.19 +/- 65.42
Episode length: 57.40 +/- 18.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 57.4      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 201000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.66e-23 |
|    explained_variance   | 0.0225    |
|    learning_rate        | 0.001     |
|    loss                 | 1.84e+03  |
|    n_updates            | 774       |
|    policy_gradient_loss | 1.54e-10  |
|    value_loss           | 3.56e+03  |
---------------------------------------
Eval num_timesteps=201500, episode_reward=-426.79 +/- 60.41
Episode length: 51.06 +/- 14.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-410.59 +/- 63.85
Episode length: 49.86 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-421.38 +/- 82.66
Episode length: 49.42 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 99       |
|    time_elapsed    | 933      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=-432.19 +/- 75.30
Episode length: 50.12 +/- 15.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -432      |
| time/                   |           |
|    total_timesteps      | 203000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.61e-26 |
|    explained_variance   | 0.0128    |
|    learning_rate        | 0.001     |
|    loss                 | 1.87e+03  |
|    n_updates            | 784       |
|    policy_gradient_loss | -1.57e-10 |
|    value_loss           | 3.03e+03  |
---------------------------------------
Eval num_timesteps=203500, episode_reward=-414.79 +/- 64.67
Episode length: 47.24 +/- 14.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-431.00 +/- 61.33
Episode length: 56.88 +/- 19.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.9     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-412.99 +/- 79.35
Episode length: 50.66 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 100      |
|    time_elapsed    | 943      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=-421.99 +/- 65.30
Episode length: 49.78 +/- 16.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -422      |
| time/                   |           |
|    total_timesteps      | 205000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.24e-23 |
|    explained_variance   | 0.0186    |
|    learning_rate        | 0.001     |
|    loss                 | 1.85e+03  |
|    n_updates            | 794       |
|    policy_gradient_loss | -9.79e-10 |
|    value_loss           | 3.67e+03  |
---------------------------------------
Eval num_timesteps=205500, episode_reward=-406.39 +/- 75.03
Episode length: 48.60 +/- 16.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-423.19 +/- 71.51
Episode length: 51.86 +/- 16.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-397.39 +/- 78.23
Episode length: 45.06 +/- 13.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 101      |
|    time_elapsed    | 952      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=-429.79 +/- 75.04
Episode length: 50.64 +/- 16.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.6      |
|    mean_reward          | -430      |
| time/                   |           |
|    total_timesteps      | 207000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.34e-26 |
|    explained_variance   | 0.0108    |
|    learning_rate        | 0.001     |
|    loss                 | 1.38e+03  |
|    n_updates            | 804       |
|    policy_gradient_loss | 4.1e-10   |
|    value_loss           | 3.6e+03   |
---------------------------------------
Eval num_timesteps=207500, episode_reward=-431.00 +/- 69.32
Episode length: 50.98 +/- 16.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-426.79 +/- 55.76
Episode length: 49.56 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-406.39 +/- 76.92
Episode length: 45.62 +/- 16.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 102      |
|    time_elapsed    | 961      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=-434.59 +/- 57.00
Episode length: 52.24 +/- 15.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -435      |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.78e-21 |
|    explained_variance   | 0.027     |
|    learning_rate        | 0.001     |
|    loss                 | 2.34e+03  |
|    n_updates            | 814       |
|    policy_gradient_loss | 5.36e-10  |
|    value_loss           | 3.82e+03  |
---------------------------------------
Eval num_timesteps=209500, episode_reward=-398.60 +/- 87.62
Episode length: 51.70 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-414.78 +/- 84.29
Episode length: 50.16 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-426.19 +/- 65.92
Episode length: 49.04 +/- 18.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 103      |
|    time_elapsed    | 970      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-416.59 +/- 67.68
Episode length: 46.18 +/- 15.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.2      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 211000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.16e-23 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.001     |
|    loss                 | 2.85e+03  |
|    n_updates            | 824       |
|    policy_gradient_loss | -4.89e-10 |
|    value_loss           | 3.92e+03  |
---------------------------------------
Eval num_timesteps=211500, episode_reward=-403.99 +/- 78.19
Episode length: 44.76 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-417.79 +/- 69.25
Episode length: 50.46 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-405.79 +/- 81.17
Episode length: 48.36 +/- 14.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 104      |
|    time_elapsed    | 979      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=-421.99 +/- 65.30
Episode length: 47.64 +/- 13.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.6      |
|    mean_reward          | -422      |
| time/                   |           |
|    total_timesteps      | 213000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.05e-22 |
|    explained_variance   | 0.0263    |
|    learning_rate        | 0.001     |
|    loss                 | 1.74e+03  |
|    n_updates            | 834       |
|    policy_gradient_loss | -1.78e-09 |
|    value_loss           | 3.22e+03  |
---------------------------------------
Eval num_timesteps=213500, episode_reward=-421.39 +/- 74.85
Episode length: 52.06 +/- 20.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-423.20 +/- 69.98
Episode length: 52.78 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-430.39 +/- 53.99
Episode length: 51.00 +/- 17.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-431.00 +/- 60.45
Episode length: 52.32 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -427     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 105      |
|    time_elapsed    | 990      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=-415.39 +/- 67.94
Episode length: 53.06 +/- 21.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.1      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 215500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.98e-24 |
|    explained_variance   | 0.0167    |
|    learning_rate        | 0.001     |
|    loss                 | 1.94e+03  |
|    n_updates            | 844       |
|    policy_gradient_loss | 4.37e-10  |
|    value_loss           | 2.98e+03  |
---------------------------------------
Eval num_timesteps=216000, episode_reward=-417.19 +/- 69.52
Episode length: 48.44 +/- 13.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-397.96 +/- 96.81
Episode length: 46.18 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-416.59 +/- 74.76
Episode length: 51.50 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 106      |
|    time_elapsed    | 999      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=-410.59 +/- 67.68
Episode length: 51.00 +/- 18.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 217500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.06e-23 |
|    explained_variance   | 0.0319    |
|    learning_rate        | 0.001     |
|    loss                 | 1.67e+03  |
|    n_updates            | 854       |
|    policy_gradient_loss | 6.93e-10  |
|    value_loss           | 3e+03     |
---------------------------------------
Eval num_timesteps=218000, episode_reward=-424.39 +/- 55.82
Episode length: 47.92 +/- 14.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-440.59 +/- 53.08
Episode length: 56.62 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.6     |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=-415.40 +/- 84.26
Episode length: 49.92 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 107      |
|    time_elapsed    | 1008     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-419.59 +/- 68.67
Episode length: 45.28 +/- 16.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.3      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 219500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.98e-25 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.001     |
|    loss                 | 1.82e+03  |
|    n_updates            | 864       |
|    policy_gradient_loss | -1.45e-09 |
|    value_loss           | 4.3e+03   |
---------------------------------------
Eval num_timesteps=220000, episode_reward=-404.59 +/- 69.00
Episode length: 48.40 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-421.99 +/- 56.43
Episode length: 53.18 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-441.79 +/- 61.07
Episode length: 49.72 +/- 13.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.5     |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 108      |
|    time_elapsed    | 1017     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=-405.19 +/- 77.45
Episode length: 42.98 +/- 15.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 43        |
|    mean_reward          | -405      |
| time/                   |           |
|    total_timesteps      | 221500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.46e-21 |
|    explained_variance   | 0.0274    |
|    learning_rate        | 0.001     |
|    loss                 | 1.39e+03  |
|    n_updates            | 874       |
|    policy_gradient_loss | -1.5e-09  |
|    value_loss           | 3.01e+03  |
---------------------------------------
Eval num_timesteps=222000, episode_reward=-440.00 +/- 64.63
Episode length: 50.56 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -440     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-429.20 +/- 58.12
Episode length: 49.46 +/- 13.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=-397.39 +/- 87.57
Episode length: 52.24 +/- 21.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 109      |
|    time_elapsed    | 1026     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=-424.99 +/- 58.26
Episode length: 50.68 +/- 14.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -425      |
| time/                   |           |
|    total_timesteps      | 223500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.93e-23 |
|    explained_variance   | 0.0137    |
|    learning_rate        | 0.001     |
|    loss                 | 1.94e+03  |
|    n_updates            | 884       |
|    policy_gradient_loss | 8.64e-10  |
|    value_loss           | 3.55e+03  |
---------------------------------------
Eval num_timesteps=224000, episode_reward=-415.99 +/- 80.33
Episode length: 48.10 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-430.39 +/- 54.32
Episode length: 52.02 +/- 15.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-426.19 +/- 73.65
Episode length: 51.80 +/- 15.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 110      |
|    time_elapsed    | 1035     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=-432.20 +/- 57.70
Episode length: 54.08 +/- 18.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.1      |
|    mean_reward          | -432      |
| time/                   |           |
|    total_timesteps      | 225500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.96e-22 |
|    explained_variance   | 0.0305    |
|    learning_rate        | 0.001     |
|    loss                 | 2.38e+03  |
|    n_updates            | 894       |
|    policy_gradient_loss | -3.49e-11 |
|    value_loss           | 3.65e+03  |
---------------------------------------
Eval num_timesteps=226000, episode_reward=-398.00 +/- 70.90
Episode length: 49.20 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-402.79 +/- 67.07
Episode length: 47.94 +/- 13.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=-421.99 +/- 65.57
Episode length: 47.50 +/- 14.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 111      |
|    time_elapsed    | 1044     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=-399.80 +/- 74.07
Episode length: 49.94 +/- 17.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.9      |
|    mean_reward          | -400      |
| time/                   |           |
|    total_timesteps      | 227500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.74e-24 |
|    explained_variance   | 0.0218    |
|    learning_rate        | 0.001     |
|    loss                 | 1.33e+03  |
|    n_updates            | 904       |
|    policy_gradient_loss | -7.86e-11 |
|    value_loss           | 3.28e+03  |
---------------------------------------
Eval num_timesteps=228000, episode_reward=-425.00 +/- 83.20
Episode length: 54.40 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-412.39 +/- 67.02
Episode length: 49.24 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-414.19 +/- 65.75
Episode length: 55.52 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 112      |
|    time_elapsed    | 1054     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=-431.59 +/- 52.31
Episode length: 52.92 +/- 22.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.9      |
|    mean_reward          | -432      |
| time/                   |           |
|    total_timesteps      | 229500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.62e-19 |
|    explained_variance   | 0.0417    |
|    learning_rate        | 0.001     |
|    loss                 | 1.2e+03   |
|    n_updates            | 914       |
|    policy_gradient_loss | -2.83e-09 |
|    value_loss           | 3.24e+03  |
---------------------------------------
Eval num_timesteps=230000, episode_reward=-430.40 +/- 66.53
Episode length: 49.34 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-412.39 +/- 54.58
Episode length: 50.24 +/- 18.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-427.39 +/- 56.61
Episode length: 51.96 +/- 15.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 113      |
|    time_elapsed    | 1063     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=-429.79 +/- 56.10
Episode length: 46.66 +/- 15.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.7      |
|    mean_reward          | -430      |
| time/                   |           |
|    total_timesteps      | 231500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.46e-21 |
|    explained_variance   | 0.0199    |
|    learning_rate        | 0.001     |
|    loss                 | 1.67e+03  |
|    n_updates            | 924       |
|    policy_gradient_loss | 6.81e-10  |
|    value_loss           | 3.65e+03  |
---------------------------------------
Eval num_timesteps=232000, episode_reward=-410.59 +/- 68.21
Episode length: 47.90 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-426.19 +/- 69.63
Episode length: 47.46 +/- 13.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-399.80 +/- 74.32
Episode length: 50.98 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 114      |
|    time_elapsed    | 1072     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=-415.40 +/- 74.99
Episode length: 53.30 +/- 15.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.3      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.76e-21 |
|    explained_variance   | 0.0473    |
|    learning_rate        | 0.001     |
|    loss                 | 1.79e+03  |
|    n_updates            | 934       |
|    policy_gradient_loss | 7.26e-10  |
|    value_loss           | 3.16e+03  |
---------------------------------------
Eval num_timesteps=234000, episode_reward=-412.99 +/- 80.92
Episode length: 49.54 +/- 17.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-409.39 +/- 85.24
Episode length: 50.56 +/- 15.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-437.59 +/- 49.11
Episode length: 52.00 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-396.79 +/- 72.09
Episode length: 48.74 +/- 17.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 115      |
|    time_elapsed    | 1083     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=-423.19 +/- 62.08
Episode length: 51.72 +/- 15.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.85e-23 |
|    explained_variance   | 0.0203    |
|    learning_rate        | 0.001     |
|    loss                 | 1.01e+03  |
|    n_updates            | 944       |
|    policy_gradient_loss | -9.01e-10 |
|    value_loss           | 3.35e+03  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=-404.60 +/- 87.62
Episode length: 50.90 +/- 15.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-432.79 +/- 66.80
Episode length: 52.70 +/- 19.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=-421.40 +/- 63.10
Episode length: 49.92 +/- 15.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 116      |
|    time_elapsed    | 1092     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=-415.39 +/- 58.85
Episode length: 50.56 +/- 13.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.6      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 238000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.04e-20 |
|    explained_variance   | 0.037     |
|    learning_rate        | 0.001     |
|    loss                 | 1.34e+03  |
|    n_updates            | 954       |
|    policy_gradient_loss | 5.36e-10  |
|    value_loss           | 3.15e+03  |
---------------------------------------
Eval num_timesteps=238500, episode_reward=-422.59 +/- 67.95
Episode length: 53.84 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-422.60 +/- 69.00
Episode length: 50.92 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-429.20 +/- 62.88
Episode length: 53.14 +/- 18.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 117      |
|    time_elapsed    | 1102     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-409.39 +/- 77.03
Episode length: 48.66 +/- 14.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.7      |
|    mean_reward          | -409      |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.47e-23 |
|    explained_variance   | 0.0217    |
|    learning_rate        | 0.001     |
|    loss                 | 1.2e+03   |
|    n_updates            | 964       |
|    policy_gradient_loss | -9.84e-10 |
|    value_loss           | 3.52e+03  |
---------------------------------------
Eval num_timesteps=240500, episode_reward=-396.79 +/- 83.22
Episode length: 50.96 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-427.39 +/- 58.48
Episode length: 48.32 +/- 14.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=-389.59 +/- 76.13
Episode length: 47.82 +/- 14.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -390     |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 118      |
|    time_elapsed    | 1111     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-406.39 +/- 65.83
Episode length: 46.64 +/- 15.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.6      |
|    mean_reward          | -406      |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.93e-19 |
|    explained_variance   | 0.0552    |
|    learning_rate        | 0.001     |
|    loss                 | 1.59e+03  |
|    n_updates            | 974       |
|    policy_gradient_loss | -1.53e-09 |
|    value_loss           | 2.72e+03  |
---------------------------------------
Eval num_timesteps=242500, episode_reward=-421.99 +/- 59.83
Episode length: 47.50 +/- 13.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-415.39 +/- 68.47
Episode length: 47.70 +/- 18.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=-410.60 +/- 68.48
Episode length: 47.74 +/- 19.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 119      |
|    time_elapsed    | 1119     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=-437.59 +/- 59.09
Episode length: 53.58 +/- 17.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.6      |
|    mean_reward          | -438      |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.09e-22 |
|    explained_variance   | 0.0192    |
|    learning_rate        | 0.001     |
|    loss                 | 1.61e+03  |
|    n_updates            | 984       |
|    policy_gradient_loss | 1.13e-09  |
|    value_loss           | 3.31e+03  |
---------------------------------------
Eval num_timesteps=244500, episode_reward=-412.38 +/- 85.90
Episode length: 48.42 +/- 15.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-404.00 +/- 78.88
Episode length: 51.28 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=-419.00 +/- 52.68
Episode length: 52.92 +/- 18.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54       |
|    ep_rew_mean     | -429     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 120      |
|    time_elapsed    | 1129     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-415.39 +/- 74.75
Episode length: 46.84 +/- 17.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.8      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.19e-17 |
|    explained_variance   | 0.0478    |
|    learning_rate        | 0.001     |
|    loss                 | 1.97e+03  |
|    n_updates            | 994       |
|    policy_gradient_loss | 6.14e-10  |
|    value_loss           | 3.06e+03  |
---------------------------------------
Eval num_timesteps=246500, episode_reward=-409.39 +/- 81.57
Episode length: 49.24 +/- 16.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-410.60 +/- 65.25
Episode length: 48.64 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=-403.99 +/- 80.01
Episode length: 47.22 +/- 17.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 121      |
|    time_elapsed    | 1138     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=-427.39 +/- 60.30
Episode length: 53.84 +/- 17.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.8      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.28e-19 |
|    explained_variance   | 0.0293    |
|    learning_rate        | 0.001     |
|    loss                 | 1.11e+03  |
|    n_updates            | 1004      |
|    policy_gradient_loss | -9.14e-10 |
|    value_loss           | 3.61e+03  |
---------------------------------------
Eval num_timesteps=248500, episode_reward=-402.79 +/- 85.27
Episode length: 48.94 +/- 20.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-405.78 +/- 95.29
Episode length: 52.88 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=-419.60 +/- 60.30
Episode length: 53.62 +/- 18.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -427     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 122      |
|    time_elapsed    | 1147     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-398.59 +/- 75.00
Episode length: 49.36 +/- 18.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.4      |
|    mean_reward          | -399      |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.64e-17 |
|    explained_variance   | 0.0556    |
|    learning_rate        | 0.001     |
|    loss                 | 1.58e+03  |
|    n_updates            | 1014      |
|    policy_gradient_loss | 4.31e-10  |
|    value_loss           | 2.76e+03  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=-421.39 +/- 63.67
Episode length: 50.58 +/- 18.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-398.59 +/- 79.88
Episode length: 48.84 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=-423.79 +/- 72.95
Episode length: 47.56 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 123      |
|    time_elapsed    | 1156     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=-417.19 +/- 67.41
Episode length: 50.04 +/- 15.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 252000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.65e-20 |
|    explained_variance   | 0.0462    |
|    learning_rate        | 0.001     |
|    loss                 | 3.46e+03  |
|    n_updates            | 1024      |
|    policy_gradient_loss | -7.23e-10 |
|    value_loss           | 3.74e+03  |
---------------------------------------
Eval num_timesteps=252500, episode_reward=-432.20 +/- 60.74
Episode length: 50.82 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-411.80 +/- 75.57
Episode length: 51.88 +/- 18.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=-424.39 +/- 62.52
Episode length: 49.00 +/- 20.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 124      |
|    time_elapsed    | 1165     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=-408.19 +/- 75.49
Episode length: 51.24 +/- 17.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.2      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.63e-17 |
|    explained_variance   | 0.0991    |
|    learning_rate        | 0.001     |
|    loss                 | 2.44e+03  |
|    n_updates            | 1034      |
|    policy_gradient_loss | -1.28e-09 |
|    value_loss           | 3.47e+03  |
---------------------------------------
Eval num_timesteps=254500, episode_reward=-428.60 +/- 62.43
Episode length: 51.16 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-434.00 +/- 62.60
Episode length: 52.94 +/- 15.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=-421.39 +/- 72.15
Episode length: 52.20 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-411.19 +/- 79.02
Episode length: 47.12 +/- 15.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.3     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 125      |
|    time_elapsed    | 1176     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=-430.99 +/- 61.92
Episode length: 51.46 +/- 18.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.5      |
|    mean_reward          | -431      |
| time/                   |           |
|    total_timesteps      | 256500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.11e-20 |
|    explained_variance   | 0.0525    |
|    learning_rate        | 0.001     |
|    loss                 | 2.03e+03  |
|    n_updates            | 1044      |
|    policy_gradient_loss | -1.96e-09 |
|    value_loss           | 3.89e+03  |
---------------------------------------
Eval num_timesteps=257000, episode_reward=-418.99 +/- 66.84
Episode length: 45.92 +/- 15.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-413.59 +/- 66.27
Episode length: 49.82 +/- 19.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-428.59 +/- 67.15
Episode length: 51.04 +/- 15.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 126      |
|    time_elapsed    | 1185     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=-403.39 +/- 80.59
Episode length: 46.26 +/- 15.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.3      |
|    mean_reward          | -403      |
| time/                   |           |
|    total_timesteps      | 258500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.79e-11 |
|    explained_variance   | 0.0846    |
|    learning_rate        | 0.001     |
|    loss                 | 1.11e+03  |
|    n_updates            | 1054      |
|    policy_gradient_loss | -1.24e-10 |
|    value_loss           | 3.16e+03  |
---------------------------------------
Eval num_timesteps=259000, episode_reward=-407.59 +/- 72.00
Episode length: 54.26 +/- 23.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=-407.59 +/- 79.60
Episode length: 49.54 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-388.39 +/- 91.22
Episode length: 46.98 +/- 19.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 127      |
|    time_elapsed    | 1194     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=-420.79 +/- 59.02
Episode length: 53.18 +/- 17.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.2      |
|    mean_reward          | -421      |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.55e-14 |
|    explained_variance   | 0.0785    |
|    learning_rate        | 0.001     |
|    loss                 | 1.35e+03  |
|    n_updates            | 1064      |
|    policy_gradient_loss | -9.87e-10 |
|    value_loss           | 3.49e+03  |
---------------------------------------
Eval num_timesteps=261000, episode_reward=-413.59 +/- 90.00
Episode length: 48.56 +/- 19.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-403.39 +/- 78.33
Episode length: 50.54 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=-424.39 +/- 57.10
Episode length: 52.90 +/- 16.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 128      |
|    time_elapsed    | 1204     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=-421.39 +/- 61.95
Episode length: 48.42 +/- 15.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.4      |
|    mean_reward          | -421      |
| time/                   |           |
|    total_timesteps      | 262500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.52e-11 |
|    explained_variance   | 0.136     |
|    learning_rate        | 0.001     |
|    loss                 | 1.23e+03  |
|    n_updates            | 1074      |
|    policy_gradient_loss | 3.78e-11  |
|    value_loss           | 2.81e+03  |
---------------------------------------
Eval num_timesteps=263000, episode_reward=-417.79 +/- 59.46
Episode length: 47.92 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-429.19 +/- 49.42
Episode length: 49.38 +/- 17.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-406.40 +/- 70.06
Episode length: 47.18 +/- 17.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 129      |
|    time_elapsed    | 1213     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=-418.99 +/- 55.67
Episode length: 51.48 +/- 20.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.5      |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.04e-14 |
|    explained_variance   | 0.0902    |
|    learning_rate        | 0.001     |
|    loss                 | 894       |
|    n_updates            | 1084      |
|    policy_gradient_loss | -1.19e-09 |
|    value_loss           | 3.18e+03  |
---------------------------------------
Eval num_timesteps=265000, episode_reward=-407.60 +/- 62.64
Episode length: 51.44 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-413.59 +/- 72.50
Episode length: 48.14 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=-431.60 +/- 67.88
Episode length: 56.86 +/- 20.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.9     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.7     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 130      |
|    time_elapsed    | 1222     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=-411.79 +/- 79.74
Episode length: 51.68 +/- 13.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.44e-11 |
|    explained_variance   | 0.166     |
|    learning_rate        | 0.001     |
|    loss                 | 991       |
|    n_updates            | 1094      |
|    policy_gradient_loss | 1.46e-09  |
|    value_loss           | 2.41e+03  |
---------------------------------------
Eval num_timesteps=267000, episode_reward=-436.40 +/- 63.32
Episode length: 49.14 +/- 18.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-434.59 +/- 53.75
Episode length: 51.86 +/- 12.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=-431.59 +/- 66.00
Episode length: 52.92 +/- 20.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.1     |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 131      |
|    time_elapsed    | 1231     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=-432.79 +/- 54.32
Episode length: 50.82 +/- 16.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.8      |
|    mean_reward          | -433      |
| time/                   |           |
|    total_timesteps      | 268500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.54e-14 |
|    explained_variance   | 0.0787    |
|    learning_rate        | 0.001     |
|    loss                 | 1.22e+03  |
|    n_updates            | 1104      |
|    policy_gradient_loss | 1.83e-10  |
|    value_loss           | 2.7e+03   |
---------------------------------------
Eval num_timesteps=269000, episode_reward=-410.59 +/- 75.00
Episode length: 51.82 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-419.59 +/- 80.05
Episode length: 53.04 +/- 17.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-410.59 +/- 64.69
Episode length: 49.02 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 132      |
|    time_elapsed    | 1241     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=-413.59 +/- 65.45
Episode length: 48.38 +/- 18.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.4      |
|    mean_reward          | -414      |
| time/                   |           |
|    total_timesteps      | 270500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.46e-31 |
|    explained_variance   | 0.207     |
|    learning_rate        | 0.001     |
|    loss                 | 1.02e+03  |
|    n_updates            | 1114      |
|    policy_gradient_loss | -3.01e-10 |
|    value_loss           | 2.6e+03   |
---------------------------------------
Eval num_timesteps=271000, episode_reward=-436.40 +/- 63.60
Episode length: 52.24 +/- 17.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-424.99 +/- 67.96
Episode length: 49.36 +/- 17.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=-412.39 +/- 73.92
Episode length: 46.38 +/- 15.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 133      |
|    time_elapsed    | 1250     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=-403.99 +/- 74.66
Episode length: 50.30 +/- 16.71
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 50.3     |
|    mean_reward          | -404     |
| time/                   |          |
|    total_timesteps      | 272500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.5e-09 |
|    explained_variance   | 0.289    |
|    learning_rate        | 0.001    |
|    loss                 | 2.57e+03 |
|    n_updates            | 1124     |
|    policy_gradient_loss | 1.87e-09 |
|    value_loss           | 5.08e+03 |
--------------------------------------
Eval num_timesteps=273000, episode_reward=-399.79 +/- 87.86
Episode length: 47.90 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-418.39 +/- 65.77
Episode length: 47.70 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=-427.39 +/- 52.65
Episode length: 50.14 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 134      |
|    time_elapsed    | 1259     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=-447.19 +/- 42.95
Episode length: 53.88 +/- 17.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.9      |
|    mean_reward          | -447      |
| time/                   |           |
|    total_timesteps      | 274500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.7e-12  |
|    explained_variance   | 0.158     |
|    learning_rate        | 0.001     |
|    loss                 | 1.66e+03  |
|    n_updates            | 1134      |
|    policy_gradient_loss | -1.48e-10 |
|    value_loss           | 3.01e+03  |
---------------------------------------
Eval num_timesteps=275000, episode_reward=-396.80 +/- 85.77
Episode length: 50.44 +/- 18.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-408.19 +/- 71.07
Episode length: 48.50 +/- 15.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-422.59 +/- 62.43
Episode length: 49.58 +/- 15.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 135      |
|    time_elapsed    | 1268     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=-433.39 +/- 53.80
Episode length: 53.20 +/- 18.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.2      |
|    mean_reward          | -433      |
| time/                   |           |
|    total_timesteps      | 276500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.92e-11 |
|    explained_variance   | 0.321     |
|    learning_rate        | 0.001     |
|    loss                 | 1.46e+03  |
|    n_updates            | 1144      |
|    policy_gradient_loss | 9.49e-10  |
|    value_loss           | 3.2e+03   |
---------------------------------------
Eval num_timesteps=277000, episode_reward=-386.00 +/- 90.25
Episode length: 48.80 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -386     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-401.00 +/- 68.54
Episode length: 51.92 +/- 18.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-400.99 +/- 81.28
Episode length: 50.52 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=-399.79 +/- 79.01
Episode length: 50.58 +/- 18.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 136      |
|    time_elapsed    | 1279     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=-413.00 +/- 73.95
Episode length: 50.12 +/- 16.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -413      |
| time/                   |           |
|    total_timesteps      | 279000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.25e-15 |
|    explained_variance   | 0.139     |
|    learning_rate        | 0.001     |
|    loss                 | 635       |
|    n_updates            | 1154      |
|    policy_gradient_loss | -5.06e-10 |
|    value_loss           | 3.26e+03  |
---------------------------------------
Eval num_timesteps=279500, episode_reward=-417.19 +/- 74.03
Episode length: 50.08 +/- 21.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-424.99 +/- 67.69
Episode length: 53.54 +/- 15.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=-417.20 +/- 65.24
Episode length: 49.48 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 137      |
|    time_elapsed    | 1288     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=-429.20 +/- 55.59
Episode length: 53.72 +/- 17.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.7      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.69e-10 |
|    explained_variance   | 0.261     |
|    learning_rate        | 0.001     |
|    loss                 | 1.83e+03  |
|    n_updates            | 1164      |
|    policy_gradient_loss | -1.83e-09 |
|    value_loss           | 2.71e+03  |
---------------------------------------
Eval num_timesteps=281500, episode_reward=-411.80 +/- 74.12
Episode length: 50.86 +/- 13.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=-426.79 +/- 74.79
Episode length: 51.48 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=-430.39 +/- 59.99
Episode length: 51.88 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 138      |
|    time_elapsed    | 1298     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=-426.20 +/- 62.27
Episode length: 51.72 +/- 17.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -426      |
| time/                   |           |
|    total_timesteps      | 283000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.32e-13 |
|    explained_variance   | 0.156     |
|    learning_rate        | 0.001     |
|    loss                 | 2.48e+03  |
|    n_updates            | 1174      |
|    policy_gradient_loss | 2.72e-09  |
|    value_loss           | 3.11e+03  |
---------------------------------------
Eval num_timesteps=283500, episode_reward=-420.19 +/- 60.27
Episode length: 51.32 +/- 19.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=-420.19 +/- 70.97
Episode length: 50.26 +/- 16.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=-413.59 +/- 64.62
Episode length: 47.70 +/- 15.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 139      |
|    time_elapsed    | 1307     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=-425.59 +/- 69.20
Episode length: 51.06 +/- 16.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.1          |
|    mean_reward          | -426          |
| time/                   |               |
|    total_timesteps      | 285000        |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -6.75e-07     |
|    explained_variance   | 0.268         |
|    learning_rate        | 0.001         |
|    loss                 | 1.27e+03      |
|    n_updates            | 1184          |
|    policy_gradient_loss | 7.36e-09      |
|    value_loss           | 2.8e+03       |
-------------------------------------------
Eval num_timesteps=285500, episode_reward=-417.79 +/- 62.13
Episode length: 49.36 +/- 18.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=-430.99 +/- 48.93
Episode length: 50.06 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=-432.19 +/- 58.33
Episode length: 50.68 +/- 15.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 140      |
|    time_elapsed    | 1316     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=-408.19 +/- 81.45
Episode length: 47.32 +/- 20.29
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 47.3     |
|    mean_reward          | -408     |
| time/                   |          |
|    total_timesteps      | 287000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.9e-09 |
|    explained_variance   | 0.151    |
|    learning_rate        | 0.001    |
|    loss                 | 1.06e+03 |
|    n_updates            | 1194     |
|    policy_gradient_loss | -1e-08   |
|    value_loss           | 3.07e+03 |
--------------------------------------
Eval num_timesteps=287500, episode_reward=-415.38 +/- 84.72
Episode length: 50.76 +/- 15.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-419.60 +/- 66.54
Episode length: 53.28 +/- 15.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-414.19 +/- 68.96
Episode length: 48.70 +/- 16.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 141      |
|    time_elapsed    | 1325     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=-417.19 +/- 61.26
Episode length: 54.12 +/- 20.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.1      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 289000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.12e-24 |
|    explained_variance   | 0.299     |
|    learning_rate        | 0.001     |
|    loss                 | 1.59e+03  |
|    n_updates            | 1204      |
|    policy_gradient_loss | 6.97e-10  |
|    value_loss           | 2.3e+03   |
---------------------------------------
Eval num_timesteps=289500, episode_reward=-422.59 +/- 70.80
Episode length: 51.36 +/- 16.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-430.99 +/- 67.48
Episode length: 51.58 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=-403.39 +/- 75.04
Episode length: 50.24 +/- 20.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 142      |
|    time_elapsed    | 1335     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=-407.59 +/- 73.73
Episode length: 50.62 +/- 18.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.6      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 291000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-16 |
|    explained_variance   | 0.378     |
|    learning_rate        | 0.001     |
|    loss                 | 4.43e+03  |
|    n_updates            | 1214      |
|    policy_gradient_loss | -3.72e-09 |
|    value_loss           | 7.93e+03  |
---------------------------------------
Eval num_timesteps=291500, episode_reward=-433.39 +/- 51.76
Episode length: 49.42 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=-425.59 +/- 72.75
Episode length: 47.46 +/- 13.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=-425.59 +/- 59.40
Episode length: 48.92 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 143      |
|    time_elapsed    | 1344     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=-420.20 +/- 68.38
Episode length: 51.04 +/- 15.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 293000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.05e-19 |
|    explained_variance   | 0.235     |
|    learning_rate        | 0.001     |
|    loss                 | 1.27e+03  |
|    n_updates            | 1224      |
|    policy_gradient_loss | -1.47e-10 |
|    value_loss           | 2.26e+03  |
---------------------------------------
Eval num_timesteps=293500, episode_reward=-420.80 +/- 63.15
Episode length: 50.56 +/- 18.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-421.99 +/- 74.32
Episode length: 47.92 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=-414.79 +/- 87.60
Episode length: 48.84 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 144      |
|    time_elapsed    | 1353     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=-407.59 +/- 55.32
Episode length: 48.42 +/- 16.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.4      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 295000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.91e-12 |
|    explained_variance   | 0.301     |
|    learning_rate        | 0.001     |
|    loss                 | 2.5e+03   |
|    n_updates            | 1234      |
|    policy_gradient_loss | 8.03e-10  |
|    value_loss           | 5e+03     |
---------------------------------------
Eval num_timesteps=295500, episode_reward=-419.59 +/- 57.55
Episode length: 52.28 +/- 18.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=-386.59 +/- 92.42
Episode length: 45.92 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=-408.20 +/- 67.96
Episode length: 48.62 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 145      |
|    time_elapsed    | 1362     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=-429.19 +/- 60.55
Episode length: 47.68 +/- 16.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.7      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 297000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.24e-15 |
|    explained_variance   | 0.204     |
|    learning_rate        | 0.001     |
|    loss                 | 1.44e+03  |
|    n_updates            | 1244      |
|    policy_gradient_loss | -4.28e-10 |
|    value_loss           | 2.3e+03   |
---------------------------------------
Eval num_timesteps=297500, episode_reward=-426.80 +/- 77.39
Episode length: 53.20 +/- 21.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-404.59 +/- 73.06
Episode length: 49.46 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=-429.19 +/- 63.45
Episode length: 51.76 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-399.79 +/- 75.99
Episode length: 48.70 +/- 14.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 146      |
|    time_elapsed    | 1372     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=-410.59 +/- 71.31
Episode length: 47.56 +/- 15.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.6      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 299500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.07e-10 |
|    explained_variance   | 0.313     |
|    learning_rate        | 0.001     |
|    loss                 | 1.95e+03  |
|    n_updates            | 1254      |
|    policy_gradient_loss | 4.23e-10  |
|    value_loss           | 3.45e+03  |
---------------------------------------
Eval num_timesteps=300000, episode_reward=-438.79 +/- 57.10
Episode length: 56.10 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.1     |
|    mean_reward     | -439     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-411.79 +/- 82.62
Episode length: 48.88 +/- 16.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=-418.40 +/- 63.83
Episode length: 48.20 +/- 15.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 147      |
|    time_elapsed    | 1382     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=-410.59 +/- 75.48
Episode length: 47.74 +/- 18.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.7      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 301500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.64e-13 |
|    explained_variance   | 0.221     |
|    learning_rate        | 0.001     |
|    loss                 | 1.01e+03  |
|    n_updates            | 1264      |
|    policy_gradient_loss | -1.47e-10 |
|    value_loss           | 2.65e+03  |
---------------------------------------
Eval num_timesteps=302000, episode_reward=-415.39 +/- 67.94
Episode length: 49.14 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=-421.40 +/- 74.85
Episode length: 50.76 +/- 16.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=-426.20 +/- 63.41
Episode length: 50.16 +/- 14.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 148      |
|    time_elapsed    | 1391     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=-430.40 +/- 68.66
Episode length: 55.44 +/- 18.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 55.4      |
|    mean_reward          | -430      |
| time/                   |           |
|    total_timesteps      | 303500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.41e-10 |
|    explained_variance   | 0.323     |
|    learning_rate        | 0.001     |
|    loss                 | 1.34e+03  |
|    n_updates            | 1274      |
|    policy_gradient_loss | 2.08e-09  |
|    value_loss           | 2.78e+03  |
---------------------------------------
Eval num_timesteps=304000, episode_reward=-418.39 +/- 70.01
Episode length: 48.40 +/- 14.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=-410.59 +/- 72.56
Episode length: 46.54 +/- 15.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=-431.59 +/- 59.40
Episode length: 49.98 +/- 18.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 149      |
|    time_elapsed    | 1400     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=-425.59 +/- 70.23
Episode length: 50.96 +/- 16.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -426      |
| time/                   |           |
|    total_timesteps      | 305500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.37e-12 |
|    explained_variance   | 0.262     |
|    learning_rate        | 0.001     |
|    loss                 | 1.19e+03  |
|    n_updates            | 1284      |
|    policy_gradient_loss | 1.8e-09   |
|    value_loss           | 1.93e+03  |
---------------------------------------
Eval num_timesteps=306000, episode_reward=-419.59 +/- 57.24
Episode length: 49.98 +/- 16.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=-419.60 +/- 67.08
Episode length: 52.10 +/- 19.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=-423.19 +/- 73.49
Episode length: 50.16 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 150      |
|    time_elapsed    | 1409     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=-403.99 +/- 72.95
Episode length: 49.04 +/- 17.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49        |
|    mean_reward          | -404      |
| time/                   |           |
|    total_timesteps      | 307500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.05e-09 |
|    explained_variance   | 0.31      |
|    learning_rate        | 0.001     |
|    loss                 | 1.13e+03  |
|    n_updates            | 1294      |
|    policy_gradient_loss | -1.11e-09 |
|    value_loss           | 2.47e+03  |
---------------------------------------
Eval num_timesteps=308000, episode_reward=-404.00 +/- 89.37
Episode length: 52.20 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=-410.59 +/- 58.87
Episode length: 46.82 +/- 14.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=-419.59 +/- 78.00
Episode length: 54.20 +/- 20.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 151      |
|    time_elapsed    | 1418     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=-432.19 +/- 60.45
Episode length: 49.02 +/- 17.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49        |
|    mean_reward          | -432      |
| time/                   |           |
|    total_timesteps      | 309500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-11 |
|    explained_variance   | 0.227     |
|    learning_rate        | 0.001     |
|    loss                 | 1.39e+03  |
|    n_updates            | 1304      |
|    policy_gradient_loss | -5.15e-10 |
|    value_loss           | 2.63e+03  |
---------------------------------------
Eval num_timesteps=310000, episode_reward=-424.99 +/- 63.30
Episode length: 49.84 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=-427.39 +/- 68.15
Episode length: 52.76 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=-422.59 +/- 60.08
Episode length: 48.22 +/- 13.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 152      |
|    time_elapsed    | 1428     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=-420.79 +/- 63.15
Episode length: 48.64 +/- 16.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.6          |
|    mean_reward          | -421          |
| time/                   |               |
|    total_timesteps      | 311500        |
| train/                  |               |
|    approx_kl            | -8.731149e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -6.72e-07     |
|    explained_variance   | 0.257         |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+03      |
|    n_updates            | 1314          |
|    policy_gradient_loss | 1.11e-08      |
|    value_loss           | 2.45e+03      |
-------------------------------------------
Eval num_timesteps=312000, episode_reward=-402.20 +/- 81.05
Episode length: 53.02 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=-411.79 +/- 76.04
Episode length: 47.40 +/- 14.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=-417.19 +/- 64.97
Episode length: 49.18 +/- 18.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.9     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 153      |
|    time_elapsed    | 1437     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=-406.39 +/- 71.84
Episode length: 49.82 +/- 15.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -406      |
| time/                   |           |
|    total_timesteps      | 313500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.84e-09 |
|    explained_variance   | 0.229     |
|    learning_rate        | 0.001     |
|    loss                 | 2.62e+03  |
|    n_updates            | 1324      |
|    policy_gradient_loss | -9.99e-09 |
|    value_loss           | 2.37e+03  |
---------------------------------------
Eval num_timesteps=314000, episode_reward=-414.79 +/- 80.31
Episode length: 51.80 +/- 17.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=-420.79 +/- 66.75
Episode length: 49.68 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=-411.79 +/- 82.84
Episode length: 44.82 +/- 16.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.5     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 154      |
|    time_elapsed    | 1446     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=-424.39 +/- 58.35
Episode length: 48.54 +/- 13.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -424      |
| time/                   |           |
|    total_timesteps      | 315500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.44e-08 |
|    explained_variance   | 0.306     |
|    learning_rate        | 0.001     |
|    loss                 | 916       |
|    n_updates            | 1334      |
|    policy_gradient_loss | 4.92e-10  |
|    value_loss           | 2.1e+03   |
---------------------------------------
Eval num_timesteps=316000, episode_reward=-413.59 +/- 76.60
Episode length: 47.88 +/- 14.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=-404.59 +/- 82.76
Episode length: 45.32 +/- 15.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=-434.59 +/- 58.25
Episode length: 44.26 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.3     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 155      |
|    time_elapsed    | 1454     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=-421.40 +/- 62.82
Episode length: 52.82 +/- 18.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.8      |
|    mean_reward          | -421      |
| time/                   |           |
|    total_timesteps      | 317500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.75e-10 |
|    explained_variance   | 0.207     |
|    learning_rate        | 0.001     |
|    loss                 | 1.32e+03  |
|    n_updates            | 1344      |
|    policy_gradient_loss | -2.51e-09 |
|    value_loss           | 3.13e+03  |
---------------------------------------
Eval num_timesteps=318000, episode_reward=-403.40 +/- 72.60
Episode length: 49.26 +/- 13.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=-405.19 +/- 72.91
Episode length: 49.98 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=-410.00 +/- 71.26
Episode length: 50.70 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 156      |
|    time_elapsed    | 1464     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=-417.19 +/- 63.28
Episode length: 46.42 +/- 19.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.4      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 319500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-17 |
|    explained_variance   | 0.31      |
|    learning_rate        | 0.001     |
|    loss                 | 1.19e+03  |
|    n_updates            | 1354      |
|    policy_gradient_loss | -2.79e-10 |
|    value_loss           | 2.01e+03  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=-419.60 +/- 74.46
Episode length: 49.44 +/- 17.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=-432.20 +/- 66.41
Episode length: 51.90 +/- 17.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=-426.79 +/- 59.81
Episode length: 48.92 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=-419.60 +/- 67.08
Episode length: 51.86 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 157      |
|    time_elapsed    | 1475     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=-419.59 +/- 72.00
Episode length: 50.42 +/- 18.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.4          |
|    mean_reward          | -420          |
| time/                   |               |
|    total_timesteps      | 322000        |
| train/                  |               |
|    approx_kl            | -2.910383e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -9.49e-06     |
|    explained_variance   | 0.429         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+03      |
|    n_updates            | 1364          |
|    policy_gradient_loss | 8e-08         |
|    value_loss           | 2.92e+03      |
-------------------------------------------
Eval num_timesteps=322500, episode_reward=-420.19 +/- 68.91
Episode length: 51.66 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=-438.19 +/- 64.42
Episode length: 51.16 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=-413.60 +/- 67.62
Episode length: 48.56 +/- 14.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 158      |
|    time_elapsed    | 1484     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=-411.79 +/- 69.87
Episode length: 51.70 +/- 17.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 324000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.72e-08 |
|    explained_variance   | 0.256     |
|    learning_rate        | 0.001     |
|    loss                 | 940       |
|    n_updates            | 1374      |
|    policy_gradient_loss | -2.68e-07 |
|    value_loss           | 2.54e+03  |
---------------------------------------
Eval num_timesteps=324500, episode_reward=-426.19 +/- 59.30
Episode length: 47.24 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-400.99 +/- 70.61
Episode length: 49.70 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=-411.79 +/- 72.15
Episode length: 50.20 +/- 15.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 159      |
|    time_elapsed    | 1493     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=-416.59 +/- 68.21
Episode length: 51.20 +/- 15.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.2      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 326000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.12e-09 |
|    explained_variance   | 0.433     |
|    learning_rate        | 0.001     |
|    loss                 | 949       |
|    n_updates            | 1384      |
|    policy_gradient_loss | 9.98e-10  |
|    value_loss           | 2.41e+03  |
---------------------------------------
Eval num_timesteps=326500, episode_reward=-412.99 +/- 70.46
Episode length: 53.46 +/- 20.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=-412.99 +/- 53.29
Episode length: 48.78 +/- 18.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=-418.99 +/- 71.02
Episode length: 46.38 +/- 14.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -433     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 160      |
|    time_elapsed    | 1502     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=-411.19 +/- 66.93
Episode length: 49.96 +/- 14.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 328000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.45e-11 |
|    explained_variance   | 0.314     |
|    learning_rate        | 0.001     |
|    loss                 | 1.77e+03  |
|    n_updates            | 1394      |
|    policy_gradient_loss | -2.02e-09 |
|    value_loss           | 2.36e+03  |
---------------------------------------
Eval num_timesteps=328500, episode_reward=-436.99 +/- 54.89
Episode length: 48.68 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=-441.80 +/- 46.32
Episode length: 51.68 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=-396.80 +/- 87.85
Episode length: 49.84 +/- 17.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -431     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 161      |
|    time_elapsed    | 1511     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-401.59 +/- 85.91
Episode length: 43.58 +/- 15.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43.6         |
|    mean_reward          | -402         |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 2.910383e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -4.36e-08    |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.001        |
|    loss                 | 1.97e+03     |
|    n_updates            | 1404         |
|    policy_gradient_loss | 7.31e-10     |
|    value_loss           | 2.19e+03     |
------------------------------------------
Eval num_timesteps=330500, episode_reward=-423.79 +/- 75.61
Episode length: 54.46 +/- 20.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=-421.99 +/- 75.05
Episode length: 48.02 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=-421.99 +/- 57.69
Episode length: 51.60 +/- 15.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 162      |
|    time_elapsed    | 1520     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=-408.20 +/- 81.01
Episode length: 48.48 +/- 16.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 332000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.88e-12 |
|    explained_variance   | 0.309     |
|    learning_rate        | 0.001     |
|    loss                 | 1.28e+03  |
|    n_updates            | 1414      |
|    policy_gradient_loss | -6.81e-10 |
|    value_loss           | 2.23e+03  |
---------------------------------------
Eval num_timesteps=332500, episode_reward=-408.19 +/- 78.76
Episode length: 51.90 +/- 21.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=-441.19 +/- 49.64
Episode length: 51.82 +/- 19.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=-402.19 +/- 68.01
Episode length: 49.08 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 163      |
|    time_elapsed    | 1530     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=-411.20 +/- 81.71
Episode length: 50.34 +/- 18.55
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 50.3     |
|    mean_reward          | -411     |
| time/                   |          |
|    total_timesteps      | 334000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.9e-07 |
|    explained_variance   | 0.345    |
|    learning_rate        | 0.001    |
|    loss                 | 1.47e+03 |
|    n_updates            | 1424     |
|    policy_gradient_loss | 5.97e-11 |
|    value_loss           | 2.73e+03 |
--------------------------------------
Eval num_timesteps=334500, episode_reward=-416.59 +/- 79.66
Episode length: 47.54 +/- 19.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=-423.18 +/- 87.84
Episode length: 49.20 +/- 14.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=-399.19 +/- 78.88
Episode length: 48.00 +/- 17.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 164      |
|    time_elapsed    | 1539     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=-419.60 +/- 73.73
Episode length: 52.66 +/- 16.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.7      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 336000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.77e-12 |
|    explained_variance   | 0.249     |
|    learning_rate        | 0.001     |
|    loss                 | 1.31e+03  |
|    n_updates            | 1434      |
|    policy_gradient_loss | -7.67e-10 |
|    value_loss           | 2.65e+03  |
---------------------------------------
Eval num_timesteps=336500, episode_reward=-425.59 +/- 58.48
Episode length: 51.94 +/- 17.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=-425.00 +/- 64.98
Episode length: 52.14 +/- 15.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=-428.59 +/- 58.86
Episode length: 48.02 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 165      |
|    time_elapsed    | 1548     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=-412.99 +/- 76.11
Episode length: 51.28 +/- 17.23
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 51.3           |
|    mean_reward          | -413           |
| time/                   |                |
|    total_timesteps      | 338000         |
| train/                  |                |
|    approx_kl            | -2.3283064e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -3.76e-07      |
|    explained_variance   | 0.434          |
|    learning_rate        | 0.001          |
|    loss                 | 1.1e+03        |
|    n_updates            | 1444           |
|    policy_gradient_loss | 5.83e-09       |
|    value_loss           | 2.13e+03       |
--------------------------------------------
Eval num_timesteps=338500, episode_reward=-418.39 +/- 57.60
Episode length: 51.20 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=-420.19 +/- 67.32
Episode length: 48.08 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=-387.79 +/- 73.78
Episode length: 49.52 +/- 18.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 166      |
|    time_elapsed    | 1557     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-403.39 +/- 67.73
Episode length: 48.64 +/- 16.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.6      |
|    mean_reward          | -403      |
| time/                   |           |
|    total_timesteps      | 340000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-10 |
|    explained_variance   | 0.35      |
|    learning_rate        | 0.001     |
|    loss                 | 430       |
|    n_updates            | 1454      |
|    policy_gradient_loss | -1.82e-09 |
|    value_loss           | 2.03e+03  |
---------------------------------------
Eval num_timesteps=340500, episode_reward=-406.99 +/- 78.62
Episode length: 47.34 +/- 15.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=-418.99 +/- 71.77
Episode length: 50.92 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=-414.79 +/- 72.54
Episode length: 49.04 +/- 19.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=-411.80 +/- 72.90
Episode length: 52.54 +/- 20.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 167      |
|    time_elapsed    | 1568     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=-421.99 +/- 63.62
Episode length: 45.88 +/- 14.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.9      |
|    mean_reward          | -422      |
| time/                   |           |
|    total_timesteps      | 342500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.99e-08 |
|    explained_variance   | 0.383     |
|    learning_rate        | 0.001     |
|    loss                 | 1.2e+03   |
|    n_updates            | 1464      |
|    policy_gradient_loss | -9.81e-10 |
|    value_loss           | 2.28e+03  |
---------------------------------------
Eval num_timesteps=343000, episode_reward=-423.79 +/- 67.30
Episode length: 50.52 +/- 16.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=-422.00 +/- 83.88
Episode length: 52.96 +/- 19.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-432.19 +/- 59.24
Episode length: 52.40 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 168      |
|    time_elapsed    | 1577     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=-423.19 +/- 56.62
Episode length: 50.74 +/- 15.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 344500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.14e-10 |
|    explained_variance   | 0.281     |
|    learning_rate        | 0.001     |
|    loss                 | 598       |
|    n_updates            | 1474      |
|    policy_gradient_loss | -2.18e-10 |
|    value_loss           | 2.22e+03  |
---------------------------------------
Eval num_timesteps=345000, episode_reward=-412.99 +/- 67.85
Episode length: 48.40 +/- 13.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=-416.00 +/- 76.19
Episode length: 51.16 +/- 18.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=-418.39 +/- 70.27
Episode length: 48.98 +/- 12.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 169      |
|    time_elapsed    | 1586     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=-408.19 +/- 72.57
Episode length: 48.40 +/- 14.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.4          |
|    mean_reward          | -408          |
| time/                   |               |
|    total_timesteps      | 346500        |
| train/                  |               |
|    approx_kl            | 4.3655746e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -5.54e-05     |
|    explained_variance   | 0.388         |
|    learning_rate        | 0.001         |
|    loss                 | 1.19e+03      |
|    n_updates            | 1484          |
|    policy_gradient_loss | 9.78e-07      |
|    value_loss           | 2.36e+03      |
-------------------------------------------
Eval num_timesteps=347000, episode_reward=-417.19 +/- 62.42
Episode length: 49.26 +/- 16.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=-406.39 +/- 82.35
Episode length: 48.14 +/- 17.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=-424.99 +/- 56.38
Episode length: 49.88 +/- 18.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 170      |
|    time_elapsed    | 1595     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=-402.19 +/- 77.66
Episode length: 49.74 +/- 18.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.7         |
|    mean_reward          | -402         |
| time/                   |              |
|    total_timesteps      | 348500       |
| train/                  |              |
|    approx_kl            | 4.208414e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -6.27e-08    |
|    explained_variance   | 0.3          |
|    learning_rate        | 0.001        |
|    loss                 | 1.91e+03     |
|    n_updates            | 1494         |
|    policy_gradient_loss | -2.86e-06    |
|    value_loss           | 2.44e+03     |
------------------------------------------
Eval num_timesteps=349000, episode_reward=-410.59 +/- 71.31
Episode length: 51.56 +/- 18.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=-412.39 +/- 67.29
Episode length: 51.72 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-415.99 +/- 64.13
Episode length: 45.46 +/- 13.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -431     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 171      |
|    time_elapsed    | 1604     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=-408.20 +/- 67.16
Episode length: 48.50 +/- 16.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.5          |
|    mean_reward          | -408          |
| time/                   |               |
|    total_timesteps      | 350500        |
| train/                  |               |
|    approx_kl            | 0.00044371415 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00307      |
|    explained_variance   | 0.4           |
|    learning_rate        | 0.001         |
|    loss                 | 974           |
|    n_updates            | 1504          |
|    policy_gradient_loss | -7.37e-05     |
|    value_loss           | 2.6e+03       |
-------------------------------------------
Eval num_timesteps=351000, episode_reward=-421.99 +/- 69.83
Episode length: 50.78 +/- 14.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=-417.79 +/- 78.28
Episode length: 47.08 +/- 19.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=-416.59 +/- 75.72
Episode length: 48.62 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -433     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 172      |
|    time_elapsed    | 1613     |
|    total_timesteps | 352256   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=352500, episode_reward=-426.19 +/- 63.41
Episode length: 45.62 +/- 14.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.6        |
|    mean_reward          | -426        |
| time/                   |             |
|    total_timesteps      | 352500      |
| train/                  |             |
|    approx_kl            | 0.005931033 |
|    clip_fraction        | 0.00104     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.000326   |
|    explained_variance   | 0.309       |
|    learning_rate        | 0.001       |
|    loss                 | 946         |
|    n_updates            | 1506        |
|    policy_gradient_loss | 4.25e-05    |
|    value_loss           | 2.21e+03    |
-----------------------------------------
Eval num_timesteps=353000, episode_reward=-428.60 +/- 69.78
Episode length: 53.56 +/- 18.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=-421.40 +/- 62.53
Episode length: 48.98 +/- 13.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=-397.99 +/- 79.97
Episode length: 48.72 +/- 18.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.2     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 173      |
|    time_elapsed    | 1622     |
|    total_timesteps | 354304   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=354500, episode_reward=-422.64 +/- 79.24
Episode length: 51.86 +/- 13.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.9        |
|    mean_reward          | -423        |
| time/                   |             |
|    total_timesteps      | 354500      |
| train/                  |             |
|    approx_kl            | 0.006720158 |
|    clip_fraction        | 0.000845    |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00198    |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.001       |
|    loss                 | 1.05e+03    |
|    n_updates            | 1508        |
|    policy_gradient_loss | -5.43e-05   |
|    value_loss           | 2.22e+03    |
-----------------------------------------
Eval num_timesteps=355000, episode_reward=-403.41 +/- 70.86
Episode length: 49.32 +/- 18.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=-410.21 +/- 67.01
Episode length: 46.82 +/- 15.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=-418.47 +/- 75.01
Episode length: 51.38 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.5     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 174      |
|    time_elapsed    | 1631     |
|    total_timesteps | 356352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=356500, episode_reward=-414.28 +/- 71.06
Episode length: 49.62 +/- 14.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.6        |
|    mean_reward          | -414        |
| time/                   |             |
|    total_timesteps      | 356500      |
| train/                  |             |
|    approx_kl            | 0.014780395 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0145     |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.001       |
|    loss                 | 1.75e+03    |
|    n_updates            | 1509        |
|    policy_gradient_loss | 0.000961    |
|    value_loss           | 3.1e+03     |
-----------------------------------------
Eval num_timesteps=357000, episode_reward=-426.90 +/- 58.40
Episode length: 49.46 +/- 15.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=-415.39 +/- 63.84
Episode length: 50.66 +/- 15.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=-425.69 +/- 78.75
Episode length: 48.80 +/- 16.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 175      |
|    time_elapsed    | 1639     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=-417.19 +/- 71.05
Episode length: 45.36 +/- 14.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 45.4          |
|    mean_reward          | -417          |
| time/                   |               |
|    total_timesteps      | 358500        |
| train/                  |               |
|    approx_kl            | 1.7798739e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -5.02e-08     |
|    explained_variance   | 0.363         |
|    learning_rate        | 0.001         |
|    loss                 | 809           |
|    n_updates            | 1519          |
|    policy_gradient_loss | 3.26e-06      |
|    value_loss           | 2.01e+03      |
-------------------------------------------
Eval num_timesteps=359000, episode_reward=-415.38 +/- 74.09
Episode length: 50.78 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=-406.39 +/- 75.27
Episode length: 51.84 +/- 17.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-429.20 +/- 52.60
Episode length: 51.56 +/- 14.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 176      |
|    time_elapsed    | 1648     |
|    total_timesteps | 360448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=360500, episode_reward=-411.19 +/- 74.81
Episode length: 48.40 +/- 15.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.4         |
|    mean_reward          | -411         |
| time/                   |              |
|    total_timesteps      | 360500       |
| train/                  |              |
|    approx_kl            | 0.0024308695 |
|    clip_fraction        | 0.000679     |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.000207    |
|    explained_variance   | 0.466        |
|    learning_rate        | 0.001        |
|    loss                 | 1e+03        |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.00017     |
|    value_loss           | 2.62e+03     |
------------------------------------------
Eval num_timesteps=361000, episode_reward=-402.83 +/- 75.20
Episode length: 49.54 +/- 21.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=-436.99 +/- 57.77
Episode length: 52.64 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=-416.01 +/- 73.56
Episode length: 50.32 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 177      |
|    time_elapsed    | 1657     |
|    total_timesteps | 362496   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=362500, episode_reward=-408.25 +/- 74.35
Episode length: 48.00 +/- 18.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48           |
|    mean_reward          | -408         |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 0.0067970604 |
|    clip_fraction        | 0.000422     |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.000372    |
|    explained_variance   | 0.365        |
|    learning_rate        | 0.001        |
|    loss                 | 1.38e+03     |
|    n_updates            | 1522         |
|    policy_gradient_loss | -4.22e-06    |
|    value_loss           | 2.06e+03     |
------------------------------------------
Eval num_timesteps=363000, episode_reward=-428.02 +/- 62.84
Episode length: 53.54 +/- 20.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=-412.46 +/- 67.62
Episode length: 52.34 +/- 19.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=-420.79 +/- 77.25
Episode length: 47.34 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=-411.85 +/- 78.63
Episode length: 52.18 +/- 16.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 178      |
|    time_elapsed    | 1667     |
|    total_timesteps | 364544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=365000, episode_reward=-423.20 +/- 79.61
Episode length: 49.56 +/- 16.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.6        |
|    mean_reward          | -423        |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.009433233 |
|    clip_fraction        | 0.0012      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.000689   |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.001       |
|    loss                 | 1.93e+03    |
|    n_updates            | 1523        |
|    policy_gradient_loss | 0.000233    |
|    value_loss           | 2.39e+03    |
-----------------------------------------
Eval num_timesteps=365500, episode_reward=-434.59 +/- 71.31
Episode length: 50.82 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=-426.19 +/- 71.42
Episode length: 50.88 +/- 19.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=-402.80 +/- 72.24
Episode length: 56.40 +/- 20.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.4     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 179      |
|    time_elapsed    | 1676     |
|    total_timesteps | 366592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=367000, episode_reward=-417.75 +/- 73.20
Episode length: 48.16 +/- 13.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.2      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 367000    |
| train/                  |           |
|    approx_kl            | 0.0054595 |
|    clip_fraction        | 0.00223   |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.00132  |
|    explained_variance   | 0.368     |
|    learning_rate        | 0.001     |
|    loss                 | 695       |
|    n_updates            | 1524      |
|    policy_gradient_loss | 0.000256  |
|    value_loss           | 2.12e+03  |
---------------------------------------
Eval num_timesteps=367500, episode_reward=-416.15 +/- 79.30
Episode length: 48.60 +/- 19.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=-410.31 +/- 69.78
Episode length: 45.44 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=-421.69 +/- 62.15
Episode length: 51.14 +/- 17.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 180      |
|    time_elapsed    | 1684     |
|    total_timesteps | 368640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=369000, episode_reward=-413.27 +/- 75.52
Episode length: 50.54 +/- 15.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.5        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 369000      |
| train/                  |             |
|    approx_kl            | 0.025170075 |
|    clip_fraction        | 0.00521     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00637    |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.001       |
|    loss                 | 706         |
|    n_updates            | 1525        |
|    policy_gradient_loss | -0.000816   |
|    value_loss           | 2.04e+03    |
-----------------------------------------
Eval num_timesteps=369500, episode_reward=-432.43 +/- 51.71
Episode length: 49.32 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-425.53 +/- 63.53
Episode length: 53.22 +/- 18.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=-424.68 +/- 55.89
Episode length: 53.02 +/- 17.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 181      |
|    time_elapsed    | 1693     |
|    total_timesteps | 370688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=371000, episode_reward=-408.17 +/- 77.91
Episode length: 50.72 +/- 13.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.7       |
|    mean_reward          | -408       |
| time/                   |            |
|    total_timesteps      | 371000     |
| train/                  |            |
|    approx_kl            | 0.03302629 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0266    |
|    explained_variance   | 0.428      |
|    learning_rate        | 0.001      |
|    loss                 | 635        |
|    n_updates            | 1526       |
|    policy_gradient_loss | 0.00132    |
|    value_loss           | 1.83e+03   |
----------------------------------------
Eval num_timesteps=371500, episode_reward=-425.29 +/- 62.28
Episode length: 49.10 +/- 14.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=-416.25 +/- 68.46
Episode length: 50.58 +/- 19.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=-426.97 +/- 61.18
Episode length: 48.26 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 182      |
|    time_elapsed    | 1702     |
|    total_timesteps | 372736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=373000, episode_reward=-419.17 +/- 65.39
Episode length: 48.14 +/- 11.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.1        |
|    mean_reward          | -419        |
| time/                   |             |
|    total_timesteps      | 373000      |
| train/                  |             |
|    approx_kl            | 0.006436903 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0363     |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.001       |
|    loss                 | 1.47e+03    |
|    n_updates            | 1527        |
|    policy_gradient_loss | 0.00252     |
|    value_loss           | 2.5e+03     |
-----------------------------------------
Eval num_timesteps=373500, episode_reward=-426.20 +/- 71.43
Episode length: 55.56 +/- 16.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=-419.59 +/- 68.09
Episode length: 49.66 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=-424.99 +/- 66.44
Episode length: 46.26 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 183      |
|    time_elapsed    | 1710     |
|    total_timesteps | 374784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=375000, episode_reward=-410.25 +/- 83.38
Episode length: 48.74 +/- 19.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -410        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.013139894 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0351     |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.001       |
|    loss                 | 721         |
|    n_updates            | 1528        |
|    policy_gradient_loss | -0.00103    |
|    value_loss           | 1.84e+03    |
-----------------------------------------
Eval num_timesteps=375500, episode_reward=-429.65 +/- 68.47
Episode length: 52.28 +/- 17.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=-408.62 +/- 74.20
Episode length: 48.82 +/- 15.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=-419.79 +/- 73.58
Episode length: 55.22 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 184      |
|    time_elapsed    | 1719     |
|    total_timesteps | 376832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.16
Eval num_timesteps=377000, episode_reward=-407.09 +/- 74.62
Episode length: 52.34 +/- 15.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.3        |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.026245082 |
|    clip_fraction        | 0.0026      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00982    |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.001       |
|    loss                 | 1.01e+03    |
|    n_updates            | 1529        |
|    policy_gradient_loss | -0.00014    |
|    value_loss           | 2.26e+03    |
-----------------------------------------
Eval num_timesteps=377500, episode_reward=-412.79 +/- 78.08
Episode length: 47.72 +/- 14.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=-414.76 +/- 72.46
Episode length: 52.60 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=-425.91 +/- 62.12
Episode length: 49.20 +/- 17.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 185      |
|    time_elapsed    | 1729     |
|    total_timesteps | 378880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=379000, episode_reward=-429.29 +/- 64.42
Episode length: 51.54 +/- 20.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.5        |
|    mean_reward          | -429        |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.008875918 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00847    |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.001       |
|    loss                 | 1.04e+03    |
|    n_updates            | 1530        |
|    policy_gradient_loss | 0.00314     |
|    value_loss           | 1.83e+03    |
-----------------------------------------
Eval num_timesteps=379500, episode_reward=-412.09 +/- 67.98
Episode length: 48.86 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-433.20 +/- 51.03
Episode length: 48.28 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=-411.83 +/- 70.99
Episode length: 52.74 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.7     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 186      |
|    time_elapsed    | 1738     |
|    total_timesteps | 380928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.31
Eval num_timesteps=381000, episode_reward=-436.25 +/- 64.24
Episode length: 54.62 +/- 19.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 54.6       |
|    mean_reward          | -436       |
| time/                   |            |
|    total_timesteps      | 381000     |
| train/                  |            |
|    approx_kl            | 0.15312323 |
|    clip_fraction        | 0.0312     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.025     |
|    explained_variance   | 0.379      |
|    learning_rate        | 0.001      |
|    loss                 | 482        |
|    n_updates            | 1531       |
|    policy_gradient_loss | 0.0268     |
|    value_loss           | 1.57e+03   |
----------------------------------------
Eval num_timesteps=381500, episode_reward=-422.19 +/- 62.52
Episode length: 45.92 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=-410.83 +/- 85.95
Episode length: 47.58 +/- 18.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=-399.63 +/- 100.70
Episode length: 48.50 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 187      |
|    time_elapsed    | 1746     |
|    total_timesteps | 382976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=383000, episode_reward=-416.05 +/- 83.96
Episode length: 52.94 +/- 18.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.9        |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 383000      |
| train/                  |             |
|    approx_kl            | 0.007473238 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0135     |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.001       |
|    loss                 | 874         |
|    n_updates            | 1532        |
|    policy_gradient_loss | 0.000782    |
|    value_loss           | 2.13e+03    |
-----------------------------------------
Eval num_timesteps=383500, episode_reward=-431.95 +/- 63.89
Episode length: 55.48 +/- 23.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.5     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=-407.69 +/- 73.88
Episode length: 47.56 +/- 14.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=-416.15 +/- 64.20
Episode length: 51.36 +/- 19.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=-416.32 +/- 69.14
Episode length: 55.12 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -427     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 188      |
|    time_elapsed    | 1757     |
|    total_timesteps | 385024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=385500, episode_reward=-421.89 +/- 74.75
Episode length: 50.80 +/- 17.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.8        |
|    mean_reward          | -422        |
| time/                   |             |
|    total_timesteps      | 385500      |
| train/                  |             |
|    approx_kl            | 0.015960908 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.013      |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.001       |
|    loss                 | 733         |
|    n_updates            | 1533        |
|    policy_gradient_loss | 0.00317     |
|    value_loss           | 2.11e+03    |
-----------------------------------------
Eval num_timesteps=386000, episode_reward=-428.53 +/- 67.43
Episode length: 47.46 +/- 15.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=-430.21 +/- 60.85
Episode length: 51.28 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=-430.69 +/- 67.05
Episode length: 52.78 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -430     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 189      |
|    time_elapsed    | 1765     |
|    total_timesteps | 387072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=387500, episode_reward=-399.93 +/- 82.61
Episode length: 47.02 +/- 16.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47           |
|    mean_reward          | -400         |
| time/                   |              |
|    total_timesteps      | 387500       |
| train/                  |              |
|    approx_kl            | 0.0060183154 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.013       |
|    explained_variance   | 0.396        |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+03     |
|    n_updates            | 1534         |
|    policy_gradient_loss | 0.000103     |
|    value_loss           | 1.85e+03     |
------------------------------------------
Eval num_timesteps=388000, episode_reward=-410.23 +/- 71.67
Episode length: 49.52 +/- 18.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=-426.92 +/- 63.49
Episode length: 54.62 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=-428.83 +/- 83.09
Episode length: 51.30 +/- 17.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 190      |
|    time_elapsed    | 1774     |
|    total_timesteps | 389120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=389500, episode_reward=-426.47 +/- 64.63
Episode length: 51.42 +/- 13.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.4       |
|    mean_reward          | -426       |
| time/                   |            |
|    total_timesteps      | 389500     |
| train/                  |            |
|    approx_kl            | 0.01233767 |
|    clip_fraction        | 0.0273     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0287    |
|    explained_variance   | 0.444      |
|    learning_rate        | 0.001      |
|    loss                 | 1.07e+03   |
|    n_updates            | 1535       |
|    policy_gradient_loss | 0.0028     |
|    value_loss           | 1.73e+03   |
----------------------------------------
Eval num_timesteps=390000, episode_reward=-404.67 +/- 71.57
Episode length: 50.26 +/- 19.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=-437.40 +/- 64.81
Episode length: 53.58 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=-427.87 +/- 69.05
Episode length: 52.58 +/- 19.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 191      |
|    time_elapsed    | 1783     |
|    total_timesteps | 391168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=391500, episode_reward=-390.49 +/- 71.04
Episode length: 46.84 +/- 16.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.8        |
|    mean_reward          | -390        |
| time/                   |             |
|    total_timesteps      | 391500      |
| train/                  |             |
|    approx_kl            | 0.006470433 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0275     |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.001       |
|    loss                 | 986         |
|    n_updates            | 1536        |
|    policy_gradient_loss | -0.001      |
|    value_loss           | 2.25e+03    |
-----------------------------------------
Eval num_timesteps=392000, episode_reward=-405.11 +/- 76.85
Episode length: 48.38 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=-398.49 +/- 81.86
Episode length: 49.14 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=-405.13 +/- 68.60
Episode length: 50.96 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 192      |
|    time_elapsed    | 1791     |
|    total_timesteps | 393216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=393500, episode_reward=-398.45 +/- 78.55
Episode length: 47.06 +/- 16.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.1        |
|    mean_reward          | -398        |
| time/                   |             |
|    total_timesteps      | 393500      |
| train/                  |             |
|    approx_kl            | 0.006428568 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0336     |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.001       |
|    loss                 | 1.16e+03    |
|    n_updates            | 1537        |
|    policy_gradient_loss | -0.000164   |
|    value_loss           | 2.55e+03    |
-----------------------------------------
Eval num_timesteps=394000, episode_reward=-418.71 +/- 65.35
Episode length: 54.58 +/- 18.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=-422.17 +/- 63.57
Episode length: 48.58 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=-412.33 +/- 64.27
Episode length: 49.38 +/- 15.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 193      |
|    time_elapsed    | 1800     |
|    total_timesteps | 395264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=395500, episode_reward=-394.89 +/- 82.82
Episode length: 52.82 +/- 19.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.8        |
|    mean_reward          | -395        |
| time/                   |             |
|    total_timesteps      | 395500      |
| train/                  |             |
|    approx_kl            | 0.010849934 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0217     |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.001       |
|    loss                 | 1.22e+03    |
|    n_updates            | 1538        |
|    policy_gradient_loss | 0.00192     |
|    value_loss           | 1.96e+03    |
-----------------------------------------
Eval num_timesteps=396000, episode_reward=-420.65 +/- 75.12
Episode length: 47.68 +/- 12.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=-434.36 +/- 50.95
Episode length: 55.68 +/- 18.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.7     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=-412.49 +/- 84.41
Episode length: 48.76 +/- 18.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 194      |
|    time_elapsed    | 1808     |
|    total_timesteps | 397312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=397500, episode_reward=-414.65 +/- 62.71
Episode length: 51.08 +/- 16.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -415        |
| time/                   |             |
|    total_timesteps      | 397500      |
| train/                  |             |
|    approx_kl            | 0.012370969 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0277     |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.001       |
|    loss                 | 784         |
|    n_updates            | 1539        |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 1.56e+03    |
-----------------------------------------
Eval num_timesteps=398000, episode_reward=-418.18 +/- 65.70
Episode length: 52.48 +/- 16.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=-421.65 +/- 70.70
Episode length: 52.96 +/- 18.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=-423.95 +/- 62.52
Episode length: 49.60 +/- 15.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.8     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 195      |
|    time_elapsed    | 1817     |
|    total_timesteps | 399360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=399500, episode_reward=-425.25 +/- 83.31
Episode length: 48.76 +/- 13.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.8        |
|    mean_reward          | -425        |
| time/                   |             |
|    total_timesteps      | 399500      |
| train/                  |             |
|    approx_kl            | 0.001652068 |
|    clip_fraction        | 0.00481     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00957    |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.001       |
|    loss                 | 1.26e+03    |
|    n_updates            | 1540        |
|    policy_gradient_loss | 0.000138    |
|    value_loss           | 1.72e+03    |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=-428.63 +/- 69.56
Episode length: 52.66 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=-411.33 +/- 76.81
Episode length: 54.02 +/- 24.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=-441.80 +/- 56.16
Episode length: 54.98 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.8     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 196      |
|    time_elapsed    | 1826     |
|    total_timesteps | 401408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=401500, episode_reward=-428.60 +/- 60.67
Episode length: 54.10 +/- 18.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.1        |
|    mean_reward          | -429        |
| time/                   |             |
|    total_timesteps      | 401500      |
| train/                  |             |
|    approx_kl            | 0.013528821 |
|    clip_fraction        | 0.00313     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0129     |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.001       |
|    loss                 | 717         |
|    n_updates            | 1541        |
|    policy_gradient_loss | 0.000859    |
|    value_loss           | 1.51e+03    |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=-411.78 +/- 92.54
Episode length: 47.68 +/- 16.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=-399.86 +/- 65.90
Episode length: 50.30 +/- 16.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=-421.39 +/- 62.24
Episode length: 51.52 +/- 19.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.3     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 197      |
|    time_elapsed    | 1834     |
|    total_timesteps | 403456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=403500, episode_reward=-422.63 +/- 56.73
Episode length: 48.26 +/- 13.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.3         |
|    mean_reward          | -423         |
| time/                   |              |
|    total_timesteps      | 403500       |
| train/                  |              |
|    approx_kl            | 0.0068021794 |
|    clip_fraction        | 0.00481      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00978     |
|    explained_variance   | 0.389        |
|    learning_rate        | 0.001        |
|    loss                 | 1.9e+03      |
|    n_updates            | 1542         |
|    policy_gradient_loss | -6.41e-05    |
|    value_loss           | 2.53e+03     |
------------------------------------------
Eval num_timesteps=404000, episode_reward=-426.20 +/- 66.19
Episode length: 51.46 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=-419.05 +/- 65.27
Episode length: 49.34 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=-391.53 +/- 77.40
Episode length: 49.22 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=-407.67 +/- 70.51
Episode length: 52.38 +/- 19.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 198      |
|    time_elapsed    | 1845     |
|    total_timesteps | 405504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=406000, episode_reward=-407.59 +/- 68.67
Episode length: 47.84 +/- 15.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.8         |
|    mean_reward          | -408         |
| time/                   |              |
|    total_timesteps      | 406000       |
| train/                  |              |
|    approx_kl            | 0.0036507964 |
|    clip_fraction        | 0.0026       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00124     |
|    explained_variance   | 0.379        |
|    learning_rate        | 0.001        |
|    loss                 | 942          |
|    n_updates            | 1543         |
|    policy_gradient_loss | -0.000433    |
|    value_loss           | 2e+03        |
------------------------------------------
Eval num_timesteps=406500, episode_reward=-415.41 +/- 64.70
Episode length: 50.16 +/- 15.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=-428.05 +/- 58.68
Episode length: 50.74 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=-413.65 +/- 75.48
Episode length: 45.04 +/- 14.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 199      |
|    time_elapsed    | 1853     |
|    total_timesteps | 407552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.21
Eval num_timesteps=408000, episode_reward=-419.59 +/- 61.19
Episode length: 49.84 +/- 18.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.8        |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.013289601 |
|    clip_fraction        | 0.000651    |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -3.6e-05    |
|    explained_variance   | 0.406       |
|    learning_rate        | 0.001       |
|    loss                 | 773         |
|    n_updates            | 1545        |
|    policy_gradient_loss | -4.32e-06   |
|    value_loss           | 2.24e+03    |
-----------------------------------------
Eval num_timesteps=408500, episode_reward=-423.19 +/- 63.23
Episode length: 49.54 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=-417.20 +/- 60.07
Episode length: 54.42 +/- 17.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=-409.40 +/- 69.67
Episode length: 47.84 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 200      |
|    time_elapsed    | 1862     |
|    total_timesteps | 409600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=410000, episode_reward=-410.65 +/- 76.02
Episode length: 49.20 +/- 15.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.2         |
|    mean_reward          | -411         |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0043493337 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25e-05    |
|    explained_variance   | 0.476        |
|    learning_rate        | 0.001        |
|    loss                 | 1.36e+03     |
|    n_updates            | 1546         |
|    policy_gradient_loss | -0.000101    |
|    value_loss           | 2.47e+03     |
------------------------------------------
Eval num_timesteps=410500, episode_reward=-426.80 +/- 59.20
Episode length: 50.24 +/- 13.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=-401.59 +/- 90.00
Episode length: 45.56 +/- 14.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=-414.19 +/- 73.75
Episode length: 55.16 +/- 19.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 201      |
|    time_elapsed    | 1870     |
|    total_timesteps | 411648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=412000, episode_reward=-413.11 +/- 82.10
Episode length: 54.78 +/- 22.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54.8         |
|    mean_reward          | -413         |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0045133056 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0236      |
|    explained_variance   | 0.442        |
|    learning_rate        | 0.001        |
|    loss                 | 857          |
|    n_updates            | 1547         |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 1.88e+03     |
------------------------------------------
Eval num_timesteps=412500, episode_reward=-414.95 +/- 71.95
Episode length: 51.34 +/- 17.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=-415.05 +/- 70.47
Episode length: 48.28 +/- 20.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=-402.33 +/- 66.83
Episode length: 51.74 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 202      |
|    time_elapsed    | 1879     |
|    total_timesteps | 413696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=414000, episode_reward=-429.97 +/- 60.73
Episode length: 48.06 +/- 15.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.1         |
|    mean_reward          | -430         |
| time/                   |              |
|    total_timesteps      | 414000       |
| train/                  |              |
|    approx_kl            | 0.0039223665 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0173      |
|    explained_variance   | 0.412        |
|    learning_rate        | 0.001        |
|    loss                 | 1.49e+03     |
|    n_updates            | 1548         |
|    policy_gradient_loss | 0.000814     |
|    value_loss           | 2.36e+03     |
------------------------------------------
Eval num_timesteps=414500, episode_reward=-425.45 +/- 65.70
Episode length: 49.60 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=-427.58 +/- 63.35
Episode length: 51.48 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=-422.97 +/- 64.35
Episode length: 47.14 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 203      |
|    time_elapsed    | 1888     |
|    total_timesteps | 415744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=416000, episode_reward=-418.39 +/- 80.31
Episode length: 48.66 +/- 17.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -418        |
| time/                   |             |
|    total_timesteps      | 416000      |
| train/                  |             |
|    approx_kl            | 0.002036283 |
|    clip_fraction        | 0.00335     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00347    |
|    explained_variance   | 0.455       |
|    learning_rate        | 0.001       |
|    loss                 | 763         |
|    n_updates            | 1549        |
|    policy_gradient_loss | -9.8e-07    |
|    value_loss           | 1.71e+03    |
-----------------------------------------
Eval num_timesteps=416500, episode_reward=-420.19 +/- 56.25
Episode length: 47.22 +/- 21.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=-412.39 +/- 78.41
Episode length: 50.64 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=-424.33 +/- 67.20
Episode length: 51.92 +/- 16.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 204      |
|    time_elapsed    | 1896     |
|    total_timesteps | 417792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=418000, episode_reward=-408.91 +/- 75.35
Episode length: 50.88 +/- 14.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.9         |
|    mean_reward          | -409         |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0050124023 |
|    clip_fraction        | 0.00481      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00734     |
|    explained_variance   | 0.478        |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+03     |
|    n_updates            | 1550         |
|    policy_gradient_loss | 0.000171     |
|    value_loss           | 1.73e+03     |
------------------------------------------
Eval num_timesteps=418500, episode_reward=-412.15 +/- 75.25
Episode length: 48.30 +/- 19.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=-423.33 +/- 66.94
Episode length: 48.74 +/- 13.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=-411.39 +/- 72.84
Episode length: 51.48 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 205      |
|    time_elapsed    | 1905     |
|    total_timesteps | 419840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=420000, episode_reward=-422.73 +/- 57.73
Episode length: 49.06 +/- 18.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.1         |
|    mean_reward          | -423         |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0037659833 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.023       |
|    explained_variance   | 0.503        |
|    learning_rate        | 0.001        |
|    loss                 | 686          |
|    n_updates            | 1551         |
|    policy_gradient_loss | 0.000507     |
|    value_loss           | 2.1e+03      |
------------------------------------------
Eval num_timesteps=420500, episode_reward=-411.87 +/- 75.89
Episode length: 46.66 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=-420.79 +/- 65.11
Episode length: 46.28 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=-425.60 +/- 69.20
Episode length: 50.84 +/- 17.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 206      |
|    time_elapsed    | 1913     |
|    total_timesteps | 421888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=422000, episode_reward=-401.31 +/- 73.56
Episode length: 47.12 +/- 18.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.1         |
|    mean_reward          | -401         |
| time/                   |              |
|    total_timesteps      | 422000       |
| train/                  |              |
|    approx_kl            | 0.0050210166 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.035       |
|    explained_variance   | 0.435        |
|    learning_rate        | 0.001        |
|    loss                 | 1.99e+03     |
|    n_updates            | 1552         |
|    policy_gradient_loss | 0.00238      |
|    value_loss           | 2.04e+03     |
------------------------------------------
Eval num_timesteps=422500, episode_reward=-420.84 +/- 65.15
Episode length: 50.12 +/- 17.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=-405.33 +/- 64.29
Episode length: 48.06 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=-415.45 +/- 64.92
Episode length: 53.48 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 207      |
|    time_elapsed    | 1922     |
|    total_timesteps | 423936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=424000, episode_reward=-424.92 +/- 61.06
Episode length: 49.00 +/- 17.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49          |
|    mean_reward          | -425        |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.005299017 |
|    clip_fraction        | 0.00868     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0231     |
|    explained_variance   | 0.417       |
|    learning_rate        | 0.001       |
|    loss                 | 662         |
|    n_updates            | 1553        |
|    policy_gradient_loss | 0.000358    |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=424500, episode_reward=-415.99 +/- 62.71
Episode length: 48.24 +/- 15.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=-410.86 +/- 69.40
Episode length: 49.70 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=-434.59 +/- 66.61
Episode length: 52.58 +/- 17.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 208      |
|    time_elapsed    | 1930     |
|    total_timesteps | 425984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=426000, episode_reward=-419.63 +/- 76.41
Episode length: 48.40 +/- 15.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.4        |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 426000      |
| train/                  |             |
|    approx_kl            | 0.021011325 |
|    clip_fraction        | 0.0067      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00251    |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.001       |
|    loss                 | 671         |
|    n_updates            | 1554        |
|    policy_gradient_loss | -0.000218   |
|    value_loss           | 2.11e+03    |
-----------------------------------------
Eval num_timesteps=426500, episode_reward=-411.19 +/- 72.61
Episode length: 52.04 +/- 17.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=-405.79 +/- 89.20
Episode length: 50.66 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=-418.39 +/- 68.98
Episode length: 50.86 +/- 18.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=-411.80 +/- 66.98
Episode length: 50.18 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 209      |
|    time_elapsed    | 1941     |
|    total_timesteps | 428032   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=428500, episode_reward=-425.28 +/- 58.49
Episode length: 53.46 +/- 20.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.5         |
|    mean_reward          | -425         |
| time/                   |              |
|    total_timesteps      | 428500       |
| train/                  |              |
|    approx_kl            | 0.0023886112 |
|    clip_fraction        | 0.00124      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00249     |
|    explained_variance   | 0.436        |
|    learning_rate        | 0.001        |
|    loss                 | 612          |
|    n_updates            | 1556         |
|    policy_gradient_loss | -0.000168    |
|    value_loss           | 1.68e+03     |
------------------------------------------
Eval num_timesteps=429000, episode_reward=-421.71 +/- 63.51
Episode length: 50.92 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=-415.07 +/- 69.86
Episode length: 49.88 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-429.79 +/- 55.45
Episode length: 53.50 +/- 18.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.7     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 210      |
|    time_elapsed    | 1950     |
|    total_timesteps | 430080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=430500, episode_reward=-407.59 +/- 74.46
Episode length: 48.08 +/- 15.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.1         |
|    mean_reward          | -408         |
| time/                   |              |
|    total_timesteps      | 430500       |
| train/                  |              |
|    approx_kl            | 0.0072936467 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0276      |
|    explained_variance   | 0.444        |
|    learning_rate        | 0.001        |
|    loss                 | 746          |
|    n_updates            | 1557         |
|    policy_gradient_loss | 0.000415     |
|    value_loss           | 1.39e+03     |
------------------------------------------
Eval num_timesteps=431000, episode_reward=-434.00 +/- 64.86
Episode length: 52.96 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=-417.81 +/- 82.54
Episode length: 46.80 +/- 17.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=-416.00 +/- 70.55
Episode length: 47.56 +/- 15.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.4     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 211      |
|    time_elapsed    | 1958     |
|    total_timesteps | 432128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=432500, episode_reward=-424.49 +/- 73.61
Episode length: 52.20 +/- 20.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.2        |
|    mean_reward          | -424        |
| time/                   |             |
|    total_timesteps      | 432500      |
| train/                  |             |
|    approx_kl            | 0.009807885 |
|    clip_fraction        | 0.0179      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0189     |
|    explained_variance   | 0.429       |
|    learning_rate        | 0.001       |
|    loss                 | 1.08e+03    |
|    n_updates            | 1558        |
|    policy_gradient_loss | 2.19e-05    |
|    value_loss           | 2.02e+03    |
-----------------------------------------
Eval num_timesteps=433000, episode_reward=-407.00 +/- 71.42
Episode length: 45.64 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=-411.79 +/- 72.15
Episode length: 49.76 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=-400.41 +/- 73.73
Episode length: 49.00 +/- 15.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 212      |
|    time_elapsed    | 1966     |
|    total_timesteps | 434176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=434500, episode_reward=-426.49 +/- 71.24
Episode length: 53.64 +/- 17.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.6         |
|    mean_reward          | -426         |
| time/                   |              |
|    total_timesteps      | 434500       |
| train/                  |              |
|    approx_kl            | 0.0035919107 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0319      |
|    explained_variance   | 0.383        |
|    learning_rate        | 0.001        |
|    loss                 | 1.26e+03     |
|    n_updates            | 1559         |
|    policy_gradient_loss | 0.000157     |
|    value_loss           | 2.18e+03     |
------------------------------------------
Eval num_timesteps=435000, episode_reward=-419.00 +/- 60.62
Episode length: 51.76 +/- 22.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=-417.53 +/- 64.81
Episode length: 48.92 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=-428.03 +/- 62.83
Episode length: 50.46 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 213      |
|    time_elapsed    | 1975     |
|    total_timesteps | 436224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=436500, episode_reward=-426.85 +/- 74.32
Episode length: 51.00 +/- 14.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51          |
|    mean_reward          | -427        |
| time/                   |             |
|    total_timesteps      | 436500      |
| train/                  |             |
|    approx_kl            | 0.006100693 |
|    clip_fraction        | 0.0179      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0293     |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.001       |
|    loss                 | 572         |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00065    |
|    value_loss           | 2.14e+03    |
-----------------------------------------
Eval num_timesteps=437000, episode_reward=-405.23 +/- 69.40
Episode length: 47.36 +/- 19.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=-424.08 +/- 70.28
Episode length: 47.94 +/- 13.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=-406.47 +/- 79.29
Episode length: 47.84 +/- 14.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 214      |
|    time_elapsed    | 1984     |
|    total_timesteps | 438272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=438500, episode_reward=-409.66 +/- 77.85
Episode length: 51.28 +/- 19.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.3        |
|    mean_reward          | -410        |
| time/                   |             |
|    total_timesteps      | 438500      |
| train/                  |             |
|    approx_kl            | 0.002201463 |
|    clip_fraction        | 0.0071      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0149     |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.001       |
|    loss                 | 821         |
|    n_updates            | 1561        |
|    policy_gradient_loss | 0.000384    |
|    value_loss           | 1.81e+03    |
-----------------------------------------
Eval num_timesteps=439000, episode_reward=-381.79 +/- 99.25
Episode length: 44.18 +/- 13.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.2     |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=-409.11 +/- 68.56
Episode length: 49.46 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-406.67 +/- 76.35
Episode length: 47.18 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 215      |
|    time_elapsed    | 1992     |
|    total_timesteps | 440320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.26
Eval num_timesteps=440500, episode_reward=-421.07 +/- 63.25
Episode length: 48.84 +/- 15.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.8        |
|    mean_reward          | -421        |
| time/                   |             |
|    total_timesteps      | 440500      |
| train/                  |             |
|    approx_kl            | 0.044906493 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00655    |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.001       |
|    loss                 | 1.04e+03    |
|    n_updates            | 1562        |
|    policy_gradient_loss | -0.000381   |
|    value_loss           | 1.92e+03    |
-----------------------------------------
Eval num_timesteps=441000, episode_reward=-386.29 +/- 87.32
Episode length: 48.34 +/- 15.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -386     |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=-416.25 +/- 67.53
Episode length: 50.62 +/- 18.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=-428.33 +/- 64.98
Episode length: 53.80 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 216      |
|    time_elapsed    | 2000     |
|    total_timesteps | 442368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=442500, episode_reward=-428.67 +/- 59.04
Episode length: 57.84 +/- 23.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 57.8       |
|    mean_reward          | -429       |
| time/                   |            |
|    total_timesteps      | 442500     |
| train/                  |            |
|    approx_kl            | 0.00805221 |
|    clip_fraction        | 0.00937    |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.015     |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.001      |
|    loss                 | 961        |
|    n_updates            | 1563       |
|    policy_gradient_loss | 0.0022     |
|    value_loss           | 2e+03      |
----------------------------------------
Eval num_timesteps=443000, episode_reward=-406.99 +/- 71.06
Episode length: 49.22 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=-407.16 +/- 80.61
Episode length: 52.80 +/- 20.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=-403.06 +/- 82.32
Episode length: 49.08 +/- 15.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 217      |
|    time_elapsed    | 2009     |
|    total_timesteps | 444416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=444500, episode_reward=-416.57 +/- 60.58
Episode length: 48.72 +/- 17.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -417        |
| time/                   |             |
|    total_timesteps      | 444500      |
| train/                  |             |
|    approx_kl            | 0.004205736 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0356     |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.001       |
|    loss                 | 640         |
|    n_updates            | 1564        |
|    policy_gradient_loss | 0.000671    |
|    value_loss           | 1.61e+03    |
-----------------------------------------
Eval num_timesteps=445000, episode_reward=-398.35 +/- 64.51
Episode length: 47.42 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=-403.49 +/- 85.24
Episode length: 50.42 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=-407.57 +/- 66.01
Episode length: 48.26 +/- 14.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.2     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 218      |
|    time_elapsed    | 2018     |
|    total_timesteps | 446464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=446500, episode_reward=-417.09 +/- 58.65
Episode length: 45.22 +/- 16.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 45.2         |
|    mean_reward          | -417         |
| time/                   |              |
|    total_timesteps      | 446500       |
| train/                  |              |
|    approx_kl            | 0.0060829236 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.021       |
|    explained_variance   | 0.427        |
|    learning_rate        | 0.001        |
|    loss                 | 1.6e+03      |
|    n_updates            | 1565         |
|    policy_gradient_loss | 0.00123      |
|    value_loss           | 2.07e+03     |
------------------------------------------
Eval num_timesteps=447000, episode_reward=-390.07 +/- 76.55
Episode length: 48.50 +/- 13.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -390     |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=-413.55 +/- 65.50
Episode length: 47.94 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=-409.69 +/- 54.81
Episode length: 50.40 +/- 18.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=-405.60 +/- 67.91
Episode length: 50.42 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 219      |
|    time_elapsed    | 2028     |
|    total_timesteps | 448512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=449000, episode_reward=-410.51 +/- 59.28
Episode length: 54.32 +/- 19.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.3        |
|    mean_reward          | -411        |
| time/                   |             |
|    total_timesteps      | 449000      |
| train/                  |             |
|    approx_kl            | 0.009719259 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0433     |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.001       |
|    loss                 | 999         |
|    n_updates            | 1566        |
|    policy_gradient_loss | 0.00456     |
|    value_loss           | 2.21e+03    |
-----------------------------------------
Eval num_timesteps=449500, episode_reward=-408.63 +/- 65.43
Episode length: 53.34 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-422.47 +/- 56.97
Episode length: 52.40 +/- 17.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=-405.69 +/- 66.89
Episode length: 48.52 +/- 19.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 220      |
|    time_elapsed    | 2037     |
|    total_timesteps | 450560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=451000, episode_reward=-445.40 +/- 55.28
Episode length: 51.02 +/- 16.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51         |
|    mean_reward          | -445       |
| time/                   |            |
|    total_timesteps      | 451000     |
| train/                  |            |
|    approx_kl            | 0.02013973 |
|    clip_fraction        | 0.013      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0269    |
|    explained_variance   | 0.453      |
|    learning_rate        | 0.001      |
|    loss                 | 877        |
|    n_updates            | 1567       |
|    policy_gradient_loss | 0.000786   |
|    value_loss           | 1.9e+03    |
----------------------------------------
Eval num_timesteps=451500, episode_reward=-411.35 +/- 85.71
Episode length: 53.88 +/- 22.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=-405.04 +/- 74.20
Episode length: 51.18 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=-409.64 +/- 75.80
Episode length: 52.74 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 221      |
|    time_elapsed    | 2045     |
|    total_timesteps | 452608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=453000, episode_reward=-405.63 +/- 84.36
Episode length: 51.12 +/- 20.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 453000      |
| train/                  |             |
|    approx_kl            | 0.003397224 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0118     |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.001       |
|    loss                 | 1.05e+03    |
|    n_updates            | 1568        |
|    policy_gradient_loss | 0.000164    |
|    value_loss           | 1.77e+03    |
-----------------------------------------
Eval num_timesteps=453500, episode_reward=-414.42 +/- 80.21
Episode length: 51.08 +/- 14.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=-426.31 +/- 69.93
Episode length: 53.62 +/- 15.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=-428.53 +/- 61.27
Episode length: 54.20 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 222      |
|    time_elapsed    | 2054     |
|    total_timesteps | 454656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=455000, episode_reward=-404.95 +/- 82.93
Episode length: 50.98 +/- 16.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51          |
|    mean_reward          | -405        |
| time/                   |             |
|    total_timesteps      | 455000      |
| train/                  |             |
|    approx_kl            | 0.008100215 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00359    |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.001       |
|    loss                 | 1.08e+03    |
|    n_updates            | 1569        |
|    policy_gradient_loss | 5.83e-05    |
|    value_loss           | 1.99e+03    |
-----------------------------------------
Eval num_timesteps=455500, episode_reward=-402.90 +/- 81.47
Episode length: 50.88 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=-428.31 +/- 72.30
Episode length: 52.02 +/- 19.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=-420.03 +/- 70.43
Episode length: 49.00 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 223      |
|    time_elapsed    | 2063     |
|    total_timesteps | 456704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=457000, episode_reward=-423.04 +/- 66.77
Episode length: 51.06 +/- 15.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -423        |
| time/                   |             |
|    total_timesteps      | 457000      |
| train/                  |             |
|    approx_kl            | 0.012983999 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00841    |
|    explained_variance   | 0.498       |
|    learning_rate        | 0.001       |
|    loss                 | 737         |
|    n_updates            | 1570        |
|    policy_gradient_loss | 0.00153     |
|    value_loss           | 2.04e+03    |
-----------------------------------------
Eval num_timesteps=457500, episode_reward=-420.38 +/- 69.32
Episode length: 52.02 +/- 14.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=-417.53 +/- 71.21
Episode length: 50.70 +/- 16.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=-428.51 +/- 61.96
Episode length: 56.28 +/- 22.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.3     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 224      |
|    time_elapsed    | 2072     |
|    total_timesteps | 458752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=459000, episode_reward=-418.69 +/- 72.08
Episode length: 49.40 +/- 17.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.4         |
|    mean_reward          | -419         |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.0077058836 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0.548        |
|    learning_rate        | 0.001        |
|    loss                 | 814          |
|    n_updates            | 1571         |
|    policy_gradient_loss | -0.000741    |
|    value_loss           | 1.66e+03     |
------------------------------------------
Eval num_timesteps=459500, episode_reward=-435.34 +/- 70.01
Episode length: 52.52 +/- 19.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-428.89 +/- 67.63
Episode length: 50.86 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=-424.19 +/- 58.95
Episode length: 52.44 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 225      |
|    time_elapsed    | 2081     |
|    total_timesteps | 460800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=461000, episode_reward=-420.59 +/- 68.52
Episode length: 50.66 +/- 17.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.7       |
|    mean_reward          | -421       |
| time/                   |            |
|    total_timesteps      | 461000     |
| train/                  |            |
|    approx_kl            | 0.01026338 |
|    clip_fraction        | 0.00781    |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0109    |
|    explained_variance   | 0.445      |
|    learning_rate        | 0.001      |
|    loss                 | 1.28e+03   |
|    n_updates            | 1572       |
|    policy_gradient_loss | -0.000145  |
|    value_loss           | 2.49e+03   |
----------------------------------------
Eval num_timesteps=461500, episode_reward=-424.16 +/- 64.75
Episode length: 51.62 +/- 16.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=-429.15 +/- 73.34
Episode length: 50.10 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=-432.23 +/- 65.48
Episode length: 54.58 +/- 19.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 226      |
|    time_elapsed    | 2090     |
|    total_timesteps | 462848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=463000, episode_reward=-406.93 +/- 75.05
Episode length: 52.00 +/- 22.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 463000      |
| train/                  |             |
|    approx_kl            | 0.007127113 |
|    clip_fraction        | 0.0298      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0593     |
|    explained_variance   | 0.486       |
|    learning_rate        | 0.001       |
|    loss                 | 1e+03       |
|    n_updates            | 1573        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 1.59e+03    |
-----------------------------------------
Eval num_timesteps=463500, episode_reward=-407.45 +/- 66.14
Episode length: 50.96 +/- 16.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=-407.89 +/- 80.41
Episode length: 49.02 +/- 16.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=-398.53 +/- 65.09
Episode length: 47.90 +/- 18.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 227      |
|    time_elapsed    | 2098     |
|    total_timesteps | 464896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=465000, episode_reward=-404.83 +/- 71.58
Episode length: 51.58 +/- 14.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.6        |
|    mean_reward          | -405        |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.015359749 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.048      |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.001       |
|    loss                 | 656         |
|    n_updates            | 1574        |
|    policy_gradient_loss | 0.00296     |
|    value_loss           | 1.44e+03    |
-----------------------------------------
Eval num_timesteps=465500, episode_reward=-427.99 +/- 61.15
Episode length: 51.66 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=-415.52 +/- 71.36
Episode length: 52.12 +/- 18.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=-415.59 +/- 72.01
Episode length: 50.98 +/- 19.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -397     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 228      |
|    time_elapsed    | 2107     |
|    total_timesteps | 466944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=467000, episode_reward=-409.93 +/- 53.91
Episode length: 51.72 +/- 18.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.7        |
|    mean_reward          | -410        |
| time/                   |             |
|    total_timesteps      | 467000      |
| train/                  |             |
|    approx_kl            | 0.011430292 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0213     |
|    explained_variance   | 0.477       |
|    learning_rate        | 0.001       |
|    loss                 | 1.01e+03    |
|    n_updates            | 1575        |
|    policy_gradient_loss | 0.00339     |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=467500, episode_reward=-407.73 +/- 74.23
Episode length: 48.56 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=-412.15 +/- 82.45
Episode length: 47.54 +/- 15.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=-419.99 +/- 60.32
Episode length: 49.14 +/- 15.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -394     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 229      |
|    time_elapsed    | 2115     |
|    total_timesteps | 468992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=469000, episode_reward=-420.99 +/- 61.36
Episode length: 49.84 +/- 16.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.8        |
|    mean_reward          | -421        |
| time/                   |             |
|    total_timesteps      | 469000      |
| train/                  |             |
|    approx_kl            | 0.006223602 |
|    clip_fraction        | 0.00852     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0369     |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.001       |
|    loss                 | 1.05e+03    |
|    n_updates            | 1576        |
|    policy_gradient_loss | 0.000146    |
|    value_loss           | 1.78e+03    |
-----------------------------------------
Eval num_timesteps=469500, episode_reward=-407.16 +/- 55.40
Episode length: 54.16 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-411.35 +/- 77.85
Episode length: 51.02 +/- 15.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=-404.29 +/- 79.52
Episode length: 43.86 +/- 14.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=-415.15 +/- 64.45
Episode length: 50.20 +/- 16.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -392     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 230      |
|    time_elapsed    | 2126     |
|    total_timesteps | 471040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=471500, episode_reward=-423.86 +/- 64.07
Episode length: 53.30 +/- 15.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.3         |
|    mean_reward          | -424         |
| time/                   |              |
|    total_timesteps      | 471500       |
| train/                  |              |
|    approx_kl            | 0.0050255656 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0219      |
|    explained_variance   | 0.399        |
|    learning_rate        | 0.001        |
|    loss                 | 667          |
|    n_updates            | 1577         |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 1.31e+03     |
------------------------------------------
Eval num_timesteps=472000, episode_reward=-415.29 +/- 63.06
Episode length: 51.48 +/- 19.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=-419.03 +/- 69.50
Episode length: 52.62 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=-436.08 +/- 57.59
Episode length: 53.66 +/- 18.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 231      |
|    time_elapsed    | 2134     |
|    total_timesteps | 473088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=473500, episode_reward=-410.59 +/- 75.24
Episode length: 49.54 +/- 17.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | -411        |
| time/                   |             |
|    total_timesteps      | 473500      |
| train/                  |             |
|    approx_kl            | 0.007000087 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0194     |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.001       |
|    loss                 | 472         |
|    n_updates            | 1578        |
|    policy_gradient_loss | 0.00105     |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=474000, episode_reward=-435.79 +/- 62.93
Episode length: 50.16 +/- 16.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=-430.39 +/- 70.22
Episode length: 51.12 +/- 18.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=-425.59 +/- 71.75
Episode length: 47.50 +/- 14.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 232      |
|    time_elapsed    | 2143     |
|    total_timesteps | 475136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.31
Eval num_timesteps=475500, episode_reward=-415.39 +/- 72.55
Episode length: 48.70 +/- 15.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -415        |
| time/                   |             |
|    total_timesteps      | 475500      |
| train/                  |             |
|    approx_kl            | 0.027324079 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0262     |
|    explained_variance   | 0.516       |
|    learning_rate        | 0.001       |
|    loss                 | 809         |
|    n_updates            | 1579        |
|    policy_gradient_loss | -0.000292   |
|    value_loss           | 1.76e+03    |
-----------------------------------------
Eval num_timesteps=476000, episode_reward=-427.39 +/- 57.87
Episode length: 50.48 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=-401.59 +/- 74.46
Episode length: 51.62 +/- 21.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=-432.19 +/- 58.32
Episode length: 55.02 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 233      |
|    time_elapsed    | 2152     |
|    total_timesteps | 477184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=477500, episode_reward=-414.19 +/- 63.81
Episode length: 51.96 +/- 15.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52           |
|    mean_reward          | -414         |
| time/                   |              |
|    total_timesteps      | 477500       |
| train/                  |              |
|    approx_kl            | 0.0055375206 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0291      |
|    explained_variance   | 0.52         |
|    learning_rate        | 0.001        |
|    loss                 | 736          |
|    n_updates            | 1580         |
|    policy_gradient_loss | 0.000546     |
|    value_loss           | 2e+03        |
------------------------------------------
Eval num_timesteps=478000, episode_reward=-411.19 +/- 73.35
Episode length: 49.12 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=-396.19 +/- 89.94
Episode length: 48.14 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=-435.19 +/- 54.54
Episode length: 53.84 +/- 20.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 234      |
|    time_elapsed    | 2160     |
|    total_timesteps | 479232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=479500, episode_reward=-427.39 +/- 61.48
Episode length: 49.52 +/- 17.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.5         |
|    mean_reward          | -427         |
| time/                   |              |
|    total_timesteps      | 479500       |
| train/                  |              |
|    approx_kl            | 0.0045553655 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0237      |
|    explained_variance   | 0.442        |
|    learning_rate        | 0.001        |
|    loss                 | 613          |
|    n_updates            | 1581         |
|    policy_gradient_loss | 0.00165      |
|    value_loss           | 1.79e+03     |
------------------------------------------
Eval num_timesteps=480000, episode_reward=-412.39 +/- 71.44
Episode length: 45.98 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=-404.59 +/- 88.64
Episode length: 47.44 +/- 22.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=-394.40 +/- 91.50
Episode length: 47.72 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 235      |
|    time_elapsed    | 2169     |
|    total_timesteps | 481280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=481500, episode_reward=-411.79 +/- 79.51
Episode length: 50.56 +/- 16.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.6         |
|    mean_reward          | -412         |
| time/                   |              |
|    total_timesteps      | 481500       |
| train/                  |              |
|    approx_kl            | 0.0055241953 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0243      |
|    explained_variance   | 0.498        |
|    learning_rate        | 0.001        |
|    loss                 | 511          |
|    n_updates            | 1582         |
|    policy_gradient_loss | 0.000723     |
|    value_loss           | 1.96e+03     |
------------------------------------------
Eval num_timesteps=482000, episode_reward=-405.79 +/- 73.73
Episode length: 53.38 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=-420.79 +/- 70.42
Episode length: 49.54 +/- 14.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=-437.59 +/- 54.99
Episode length: 50.20 +/- 18.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 236      |
|    time_elapsed    | 2177     |
|    total_timesteps | 483328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=483500, episode_reward=-418.99 +/- 82.94
Episode length: 50.94 +/- 17.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.9        |
|    mean_reward          | -419        |
| time/                   |             |
|    total_timesteps      | 483500      |
| train/                  |             |
|    approx_kl            | 0.003180888 |
|    clip_fraction        | 0.01        |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0263     |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.001       |
|    loss                 | 860         |
|    n_updates            | 1583        |
|    policy_gradient_loss | 0.000433    |
|    value_loss           | 2e+03       |
-----------------------------------------
Eval num_timesteps=484000, episode_reward=-410.59 +/- 76.90
Episode length: 45.10 +/- 16.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=-410.59 +/- 74.52
Episode length: 47.10 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=-439.39 +/- 66.76
Episode length: 51.10 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -439     |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 237      |
|    time_elapsed    | 2186     |
|    total_timesteps | 485376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=485500, episode_reward=-423.20 +/- 60.91
Episode length: 56.94 +/- 19.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.9        |
|    mean_reward          | -423        |
| time/                   |             |
|    total_timesteps      | 485500      |
| train/                  |             |
|    approx_kl            | 0.007711433 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0366     |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.001       |
|    loss                 | 659         |
|    n_updates            | 1584        |
|    policy_gradient_loss | 0.00141     |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=486000, episode_reward=-411.80 +/- 77.21
Episode length: 46.56 +/- 15.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=-426.19 +/- 61.10
Episode length: 49.98 +/- 14.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=-424.39 +/- 82.83
Episode length: 49.38 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 238      |
|    time_elapsed    | 2194     |
|    total_timesteps | 487424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=487500, episode_reward=-405.19 +/- 77.92
Episode length: 48.18 +/- 17.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.2        |
|    mean_reward          | -405        |
| time/                   |             |
|    total_timesteps      | 487500      |
| train/                  |             |
|    approx_kl            | 0.004946054 |
|    clip_fraction        | 0.00937     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0209     |
|    explained_variance   | 0.396       |
|    learning_rate        | 0.001       |
|    loss                 | 943         |
|    n_updates            | 1585        |
|    policy_gradient_loss | 0.00212     |
|    value_loss           | 1.69e+03    |
-----------------------------------------
Eval num_timesteps=488000, episode_reward=-417.79 +/- 57.93
Episode length: 48.74 +/- 14.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=-410.59 +/- 77.83
Episode length: 55.00 +/- 21.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=-419.01 +/- 72.04
Episode length: 51.86 +/- 19.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 239      |
|    time_elapsed    | 2203     |
|    total_timesteps | 489472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=489500, episode_reward=-404.57 +/- 88.76
Episode length: 49.58 +/- 15.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.6       |
|    mean_reward          | -405       |
| time/                   |            |
|    total_timesteps      | 489500     |
| train/                  |            |
|    approx_kl            | 0.00548799 |
|    clip_fraction        | 0.0122     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0257    |
|    explained_variance   | 0.42       |
|    learning_rate        | 0.001      |
|    loss                 | 1.63e+03   |
|    n_updates            | 1586       |
|    policy_gradient_loss | -3.76e-05  |
|    value_loss           | 2.22e+03   |
----------------------------------------
Eval num_timesteps=490000, episode_reward=-413.01 +/- 76.82
Episode length: 50.06 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=-421.99 +/- 71.61
Episode length: 50.56 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=-404.00 +/- 66.77
Episode length: 50.80 +/- 18.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=-421.39 +/- 55.84
Episode length: 47.00 +/- 13.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 240      |
|    time_elapsed    | 2213     |
|    total_timesteps | 491520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=492000, episode_reward=-417.79 +/- 69.77
Episode length: 50.70 +/- 15.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.7         |
|    mean_reward          | -418         |
| time/                   |              |
|    total_timesteps      | 492000       |
| train/                  |              |
|    approx_kl            | 0.0027154088 |
|    clip_fraction        | 0.00916      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0223      |
|    explained_variance   | 0.511        |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+03     |
|    n_updates            | 1587         |
|    policy_gradient_loss | 0.00026      |
|    value_loss           | 1.94e+03     |
------------------------------------------
Eval num_timesteps=492500, episode_reward=-414.20 +/- 63.52
Episode length: 51.88 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=-424.99 +/- 63.30
Episode length: 48.40 +/- 16.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=-436.42 +/- 60.43
Episode length: 55.02 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.1     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 241      |
|    time_elapsed    | 2222     |
|    total_timesteps | 493568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=494000, episode_reward=-426.79 +/- 50.33
Episode length: 47.54 +/- 12.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.5        |
|    mean_reward          | -427        |
| time/                   |             |
|    total_timesteps      | 494000      |
| train/                  |             |
|    approx_kl            | 0.005909164 |
|    clip_fraction        | 0.0142      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0198     |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.001       |
|    loss                 | 846         |
|    n_updates            | 1588        |
|    policy_gradient_loss | 0.00105     |
|    value_loss           | 2.06e+03    |
-----------------------------------------
Eval num_timesteps=494500, episode_reward=-427.40 +/- 66.00
Episode length: 49.96 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=-420.83 +/- 71.71
Episode length: 50.78 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=-396.07 +/- 77.14
Episode length: 50.76 +/- 16.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 242      |
|    time_elapsed    | 2231     |
|    total_timesteps | 495616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=496000, episode_reward=-415.99 +/- 68.21
Episode length: 46.68 +/- 14.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.7      |
|    mean_reward          | -416      |
| time/                   |           |
|    total_timesteps      | 496000    |
| train/                  |           |
|    approx_kl            | 0.0044429 |
|    clip_fraction        | 0.0104    |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.0489   |
|    explained_variance   | 0.576     |
|    learning_rate        | 0.001     |
|    loss                 | 632       |
|    n_updates            | 1589      |
|    policy_gradient_loss | 0.00168   |
|    value_loss           | 1.61e+03  |
---------------------------------------
Eval num_timesteps=496500, episode_reward=-403.39 +/- 85.36
Episode length: 48.74 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=-425.81 +/- 57.05
Episode length: 48.20 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=-421.39 +/- 61.66
Episode length: 48.14 +/- 15.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 243      |
|    time_elapsed    | 2239     |
|    total_timesteps | 497664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=498000, episode_reward=-405.83 +/- 69.74
Episode length: 48.60 +/- 15.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.6        |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.011041126 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0583     |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.001       |
|    loss                 | 811         |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.000981   |
|    value_loss           | 1.74e+03    |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=-412.99 +/- 62.90
Episode length: 50.18 +/- 15.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=-430.07 +/- 60.77
Episode length: 48.00 +/- 14.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=-404.67 +/- 67.16
Episode length: 46.46 +/- 18.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 244      |
|    time_elapsed    | 2247     |
|    total_timesteps | 499712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=500000, episode_reward=-429.82 +/- 62.78
Episode length: 55.48 +/- 24.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.5        |
|    mean_reward          | -430        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.012268677 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0315     |
|    explained_variance   | 0.464       |
|    learning_rate        | 0.001       |
|    loss                 | 1.21e+03    |
|    n_updates            | 1591        |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 1.99e+03    |
-----------------------------------------
Eval num_timesteps=500500, episode_reward=-422.63 +/- 58.91
Episode length: 45.84 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=-434.03 +/- 64.88
Episode length: 53.18 +/- 16.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=-409.99 +/- 75.19
Episode length: 47.22 +/- 16.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 245      |
|    time_elapsed    | 2256     |
|    total_timesteps | 501760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=502000, episode_reward=-413.85 +/- 67.98
Episode length: 50.62 +/- 18.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.6         |
|    mean_reward          | -414         |
| time/                   |              |
|    total_timesteps      | 502000       |
| train/                  |              |
|    approx_kl            | 0.0039865156 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0542      |
|    explained_variance   | 0.537        |
|    learning_rate        | 0.001        |
|    loss                 | 937          |
|    n_updates            | 1592         |
|    policy_gradient_loss | 0.000372     |
|    value_loss           | 1.76e+03     |
------------------------------------------
Eval num_timesteps=502500, episode_reward=-423.19 +/- 61.20
Episode length: 49.26 +/- 15.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
Eval num_timesteps=503000, episode_reward=-416.61 +/- 75.97
Episode length: 45.34 +/- 14.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=503500, episode_reward=-405.87 +/- 75.24
Episode length: 46.02 +/- 15.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 246      |
|    time_elapsed    | 2264     |
|    total_timesteps | 503808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=504000, episode_reward=-401.29 +/- 76.29
Episode length: 49.18 +/- 17.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.2         |
|    mean_reward          | -401         |
| time/                   |              |
|    total_timesteps      | 504000       |
| train/                  |              |
|    approx_kl            | 0.0022200225 |
|    clip_fraction        | 0.00729      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0365      |
|    explained_variance   | 0.525        |
|    learning_rate        | 0.001        |
|    loss                 | 439          |
|    n_updates            | 1593         |
|    policy_gradient_loss | 0.0003       |
|    value_loss           | 1.59e+03     |
------------------------------------------
Eval num_timesteps=504500, episode_reward=-416.87 +/- 66.18
Episode length: 47.70 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
Eval num_timesteps=505000, episode_reward=-431.89 +/- 58.98
Episode length: 51.36 +/- 18.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=505500, episode_reward=-410.99 +/- 74.95
Episode length: 45.50 +/- 13.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 247      |
|    time_elapsed    | 2272     |
|    total_timesteps | 505856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=506000, episode_reward=-431.60 +/- 50.20
Episode length: 48.70 +/- 13.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -432        |
| time/                   |             |
|    total_timesteps      | 506000      |
| train/                  |             |
|    approx_kl            | 0.004228043 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0525     |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.001       |
|    loss                 | 444         |
|    n_updates            | 1594        |
|    policy_gradient_loss | -0.000358   |
|    value_loss           | 1.78e+03    |
-----------------------------------------
Eval num_timesteps=506500, episode_reward=-397.99 +/- 81.31
Episode length: 47.14 +/- 17.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
Eval num_timesteps=507000, episode_reward=-393.79 +/- 82.23
Episode length: 45.50 +/- 14.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=507500, episode_reward=-416.63 +/- 74.54
Episode length: 49.88 +/- 16.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 248      |
|    time_elapsed    | 2281     |
|    total_timesteps | 507904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=508000, episode_reward=-414.79 +/- 81.87
Episode length: 49.00 +/- 13.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49          |
|    mean_reward          | -415        |
| time/                   |             |
|    total_timesteps      | 508000      |
| train/                  |             |
|    approx_kl            | 0.011379792 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0122     |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.001       |
|    loss                 | 1.05e+03    |
|    n_updates            | 1595        |
|    policy_gradient_loss | 0.00753     |
|    value_loss           | 1.94e+03    |
-----------------------------------------
Eval num_timesteps=508500, episode_reward=-431.00 +/- 66.68
Episode length: 48.70 +/- 13.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
Eval num_timesteps=509000, episode_reward=-405.79 +/- 78.46
Episode length: 49.34 +/- 17.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=509500, episode_reward=-416.02 +/- 65.26
Episode length: 47.78 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 249      |
|    time_elapsed    | 2289     |
|    total_timesteps | 509952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=510000, episode_reward=-441.19 +/- 52.11
Episode length: 51.64 +/- 17.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.6         |
|    mean_reward          | -441         |
| time/                   |              |
|    total_timesteps      | 510000       |
| train/                  |              |
|    approx_kl            | 0.0037960378 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0147      |
|    explained_variance   | 0.512        |
|    learning_rate        | 0.001        |
|    loss                 | 487          |
|    n_updates            | 1596         |
|    policy_gradient_loss | -0.000389    |
|    value_loss           | 1.59e+03     |
------------------------------------------
Eval num_timesteps=510500, episode_reward=-430.99 +/- 63.64
Episode length: 50.92 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
Eval num_timesteps=511000, episode_reward=-425.60 +/- 65.18
Episode length: 51.46 +/- 18.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=511500, episode_reward=-400.99 +/- 76.01
Episode length: 50.80 +/- 18.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=-407.59 +/- 71.50
Episode length: 45.38 +/- 15.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 250      |
|    time_elapsed    | 2299     |
|    total_timesteps | 512000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=512500, episode_reward=-416.59 +/- 84.91
Episode length: 52.36 +/- 17.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.4         |
|    mean_reward          | -417         |
| time/                   |              |
|    total_timesteps      | 512500       |
| train/                  |              |
|    approx_kl            | 0.0039660996 |
|    clip_fraction        | 0.00417      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00683     |
|    explained_variance   | 0.466        |
|    learning_rate        | 0.001        |
|    loss                 | 987          |
|    n_updates            | 1597         |
|    policy_gradient_loss | 0.000735     |
|    value_loss           | 1.93e+03     |
------------------------------------------
Eval num_timesteps=513000, episode_reward=-403.99 +/- 73.93
Episode length: 50.72 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
Eval num_timesteps=513500, episode_reward=-423.79 +/- 70.69
Episode length: 48.26 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 513500   |
---------------------------------
Eval num_timesteps=514000, episode_reward=-422.59 +/- 69.26
Episode length: 48.34 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 251      |
|    time_elapsed    | 2308     |
|    total_timesteps | 514048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=514500, episode_reward=-410.00 +/- 72.01
Episode length: 50.88 +/- 17.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.9         |
|    mean_reward          | -410         |
| time/                   |              |
|    total_timesteps      | 514500       |
| train/                  |              |
|    approx_kl            | 0.0023704914 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0109      |
|    explained_variance   | 0.518        |
|    learning_rate        | 0.001        |
|    loss                 | 926          |
|    n_updates            | 1598         |
|    policy_gradient_loss | 0.000354     |
|    value_loss           | 1.85e+03     |
------------------------------------------
Eval num_timesteps=515000, episode_reward=-422.00 +/- 60.43
Episode length: 51.82 +/- 17.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
Eval num_timesteps=515500, episode_reward=-409.96 +/- 101.26
Episode length: 51.96 +/- 17.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 515500   |
---------------------------------
Eval num_timesteps=516000, episode_reward=-400.99 +/- 77.88
Episode length: 47.72 +/- 18.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 252      |
|    time_elapsed    | 2316     |
|    total_timesteps | 516096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=516500, episode_reward=-422.00 +/- 62.19
Episode length: 55.66 +/- 17.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 55.7         |
|    mean_reward          | -422         |
| time/                   |              |
|    total_timesteps      | 516500       |
| train/                  |              |
|    approx_kl            | 0.0044580903 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00988     |
|    explained_variance   | 0.5          |
|    learning_rate        | 0.001        |
|    loss                 | 694          |
|    n_updates            | 1599         |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 1.69e+03     |
------------------------------------------
Eval num_timesteps=517000, episode_reward=-420.21 +/- 80.26
Episode length: 47.86 +/- 15.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
Eval num_timesteps=517500, episode_reward=-422.62 +/- 75.73
Episode length: 53.04 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 517500   |
---------------------------------
Eval num_timesteps=518000, episode_reward=-424.99 +/- 67.16
Episode length: 51.40 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 253      |
|    time_elapsed    | 2325     |
|    total_timesteps | 518144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=518500, episode_reward=-423.81 +/- 78.22
Episode length: 53.12 +/- 18.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.1        |
|    mean_reward          | -424        |
| time/                   |             |
|    total_timesteps      | 518500      |
| train/                  |             |
|    approx_kl            | 0.013775124 |
|    clip_fraction        | 0.029       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0546     |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.001       |
|    loss                 | 877         |
|    n_updates            | 1600        |
|    policy_gradient_loss | 0.00178     |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=519000, episode_reward=-401.60 +/- 98.22
Episode length: 48.06 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
Eval num_timesteps=519500, episode_reward=-396.19 +/- 77.47
Episode length: 47.26 +/- 18.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 519500   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-426.20 +/- 55.22
Episode length: 52.82 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 254      |
|    time_elapsed    | 2334     |
|    total_timesteps | 520192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=520500, episode_reward=-402.79 +/- 83.56
Episode length: 50.18 +/- 14.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.2         |
|    mean_reward          | -403         |
| time/                   |              |
|    total_timesteps      | 520500       |
| train/                  |              |
|    approx_kl            | 0.0048055663 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.038       |
|    explained_variance   | 0.409        |
|    learning_rate        | 0.001        |
|    loss                 | 965          |
|    n_updates            | 1601         |
|    policy_gradient_loss | 0.000565     |
|    value_loss           | 1.7e+03      |
------------------------------------------
Eval num_timesteps=521000, episode_reward=-408.83 +/- 74.88
Episode length: 51.72 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
Eval num_timesteps=521500, episode_reward=-415.39 +/- 58.85
Episode length: 53.40 +/- 20.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 521500   |
---------------------------------
Eval num_timesteps=522000, episode_reward=-423.25 +/- 62.13
Episode length: 47.40 +/- 13.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -396     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 255      |
|    time_elapsed    | 2342     |
|    total_timesteps | 522240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=522500, episode_reward=-420.19 +/- 67.05
Episode length: 50.64 +/- 20.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.6        |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 522500      |
| train/                  |             |
|    approx_kl            | 0.004761536 |
|    clip_fraction        | 0.0112      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0203     |
|    explained_variance   | 0.433       |
|    learning_rate        | 0.001       |
|    loss                 | 974         |
|    n_updates            | 1602        |
|    policy_gradient_loss | -0.000262   |
|    value_loss           | 1.66e+03    |
-----------------------------------------
Eval num_timesteps=523000, episode_reward=-418.99 +/- 84.66
Episode length: 49.76 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
Eval num_timesteps=523500, episode_reward=-435.19 +/- 61.38
Episode length: 50.18 +/- 15.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 523500   |
---------------------------------
Eval num_timesteps=524000, episode_reward=-412.39 +/- 79.09
Episode length: 51.06 +/- 18.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 256      |
|    time_elapsed    | 2351     |
|    total_timesteps | 524288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=524500, episode_reward=-425.59 +/- 66.54
Episode length: 51.12 +/- 17.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -426        |
| time/                   |             |
|    total_timesteps      | 524500      |
| train/                  |             |
|    approx_kl            | 0.006493299 |
|    clip_fraction        | 0.0071      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00987    |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.001       |
|    loss                 | 944         |
|    n_updates            | 1603        |
|    policy_gradient_loss | 8.47e-06    |
|    value_loss           | 1.69e+03    |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=-421.39 +/- 55.84
Episode length: 50.98 +/- 21.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
Eval num_timesteps=525500, episode_reward=-421.99 +/- 68.79
Episode length: 46.32 +/- 13.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 525500   |
---------------------------------
Eval num_timesteps=526000, episode_reward=-417.79 +/- 70.28
Episode length: 50.44 +/- 19.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 257      |
|    time_elapsed    | 2359     |
|    total_timesteps | 526336   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=526500, episode_reward=-429.19 +/- 63.17
Episode length: 48.94 +/- 14.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.9         |
|    mean_reward          | -429         |
| time/                   |              |
|    total_timesteps      | 526500       |
| train/                  |              |
|    approx_kl            | 0.0047067995 |
|    clip_fraction        | 0.00497      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00526     |
|    explained_variance   | 0.495        |
|    learning_rate        | 0.001        |
|    loss                 | 1.47e+03     |
|    n_updates            | 1605         |
|    policy_gradient_loss | 0.0001       |
|    value_loss           | 2.27e+03     |
------------------------------------------
Eval num_timesteps=527000, episode_reward=-416.59 +/- 61.56
Episode length: 47.14 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
Eval num_timesteps=527500, episode_reward=-411.80 +/- 68.83
Episode length: 47.00 +/- 14.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 527500   |
---------------------------------
Eval num_timesteps=528000, episode_reward=-423.19 +/- 63.79
Episode length: 47.94 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 258      |
|    time_elapsed    | 2368     |
|    total_timesteps | 528384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=528500, episode_reward=-420.80 +/- 72.69
Episode length: 62.62 +/- 20.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 62.6         |
|    mean_reward          | -421         |
| time/                   |              |
|    total_timesteps      | 528500       |
| train/                  |              |
|    approx_kl            | 0.0019926538 |
|    clip_fraction        | 0.0026       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0063      |
|    explained_variance   | 0.514        |
|    learning_rate        | 0.001        |
|    loss                 | 904          |
|    n_updates            | 1606         |
|    policy_gradient_loss | 3.4e-05      |
|    value_loss           | 1.81e+03     |
------------------------------------------
Eval num_timesteps=529000, episode_reward=-412.39 +/- 77.71
Episode length: 45.10 +/- 14.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
Eval num_timesteps=529500, episode_reward=-423.80 +/- 63.44
Episode length: 47.66 +/- 14.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 529500   |
---------------------------------
Eval num_timesteps=530000, episode_reward=-409.99 +/- 72.76
Episode length: 53.38 +/- 18.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 259      |
|    time_elapsed    | 2376     |
|    total_timesteps | 530432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=530500, episode_reward=-406.39 +/- 69.03
Episode length: 47.60 +/- 15.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.6         |
|    mean_reward          | -406         |
| time/                   |              |
|    total_timesteps      | 530500       |
| train/                  |              |
|    approx_kl            | 0.0029721013 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0135      |
|    explained_variance   | 0.487        |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+03     |
|    n_updates            | 1607         |
|    policy_gradient_loss | -0.000163    |
|    value_loss           | 1.67e+03     |
------------------------------------------
Eval num_timesteps=531000, episode_reward=-423.79 +/- 57.80
Episode length: 53.94 +/- 18.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
Eval num_timesteps=531500, episode_reward=-419.59 +/- 54.99
Episode length: 55.78 +/- 21.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 531500   |
---------------------------------
Eval num_timesteps=532000, episode_reward=-409.39 +/- 82.44
Episode length: 48.20 +/- 15.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 260      |
|    time_elapsed    | 2385     |
|    total_timesteps | 532480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=532500, episode_reward=-400.99 +/- 64.20
Episode length: 51.56 +/- 18.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.6         |
|    mean_reward          | -401         |
| time/                   |              |
|    total_timesteps      | 532500       |
| train/                  |              |
|    approx_kl            | 0.0016271293 |
|    clip_fraction        | 0.00289      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.029       |
|    explained_variance   | 0.472        |
|    learning_rate        | 0.001        |
|    loss                 | 824          |
|    n_updates            | 1608         |
|    policy_gradient_loss | 0.000494     |
|    value_loss           | 2e+03        |
------------------------------------------
Eval num_timesteps=533000, episode_reward=-416.59 +/- 61.26
Episode length: 50.00 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=-423.79 +/- 86.91
Episode length: 49.78 +/- 19.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
Eval num_timesteps=534000, episode_reward=-399.19 +/- 80.69
Episode length: 50.08 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=534500, episode_reward=-400.99 +/- 82.81
Episode length: 48.72 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.7     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 261      |
|    time_elapsed    | 2395     |
|    total_timesteps | 534528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=535000, episode_reward=-418.99 +/- 68.70
Episode length: 50.54 +/- 16.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.5         |
|    mean_reward          | -419         |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 0.0041721496 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0108      |
|    explained_variance   | 0.448        |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+03     |
|    n_updates            | 1609         |
|    policy_gradient_loss | -4.68e-05    |
|    value_loss           | 2.02e+03     |
------------------------------------------
Eval num_timesteps=535500, episode_reward=-422.00 +/- 62.48
Episode length: 57.38 +/- 20.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.4     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
Eval num_timesteps=536000, episode_reward=-411.79 +/- 75.09
Episode length: 50.42 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=536500, episode_reward=-417.20 +/- 76.66
Episode length: 48.10 +/- 16.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 262      |
|    time_elapsed    | 2404     |
|    total_timesteps | 536576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=537000, episode_reward=-412.39 +/- 65.39
Episode length: 48.90 +/- 18.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.9         |
|    mean_reward          | -412         |
| time/                   |              |
|    total_timesteps      | 537000       |
| train/                  |              |
|    approx_kl            | 0.0039365534 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0243      |
|    explained_variance   | 0.506        |
|    learning_rate        | 0.001        |
|    loss                 | 748          |
|    n_updates            | 1610         |
|    policy_gradient_loss | 0.000153     |
|    value_loss           | 1.39e+03     |
------------------------------------------
Eval num_timesteps=537500, episode_reward=-408.79 +/- 74.35
Episode length: 45.28 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
Eval num_timesteps=538000, episode_reward=-423.79 +/- 59.94
Episode length: 52.02 +/- 20.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=538500, episode_reward=-395.58 +/- 96.06
Episode length: 51.40 +/- 16.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 263      |
|    time_elapsed    | 2413     |
|    total_timesteps | 538624   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=539000, episode_reward=-425.59 +/- 65.18
Episode length: 48.90 +/- 19.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.9       |
|    mean_reward          | -426       |
| time/                   |            |
|    total_timesteps      | 539000     |
| train/                  |            |
|    approx_kl            | 0.00606098 |
|    clip_fraction        | 0.015      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0357    |
|    explained_variance   | 0.398      |
|    learning_rate        | 0.001      |
|    loss                 | 1.15e+03   |
|    n_updates            | 1612       |
|    policy_gradient_loss | 0.000229   |
|    value_loss           | 1.76e+03   |
----------------------------------------
Eval num_timesteps=539500, episode_reward=-404.59 +/- 76.43
Episode length: 46.16 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-413.60 +/- 73.73
Episode length: 51.94 +/- 14.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=540500, episode_reward=-398.59 +/- 89.65
Episode length: 52.56 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 264      |
|    time_elapsed    | 2421     |
|    total_timesteps | 540672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=541000, episode_reward=-420.19 +/- 64.03
Episode length: 48.56 +/- 14.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.6        |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 541000      |
| train/                  |             |
|    approx_kl            | 0.011090826 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0353     |
|    explained_variance   | 0.408       |
|    learning_rate        | 0.001       |
|    loss                 | 812         |
|    n_updates            | 1613        |
|    policy_gradient_loss | 0.000522    |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=541500, episode_reward=-425.59 +/- 67.88
Episode length: 55.90 +/- 19.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.9     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
Eval num_timesteps=542000, episode_reward=-414.17 +/- 85.41
Episode length: 50.22 +/- 19.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=542500, episode_reward=-408.19 +/- 76.67
Episode length: 51.66 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 265      |
|    time_elapsed    | 2430     |
|    total_timesteps | 542720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=543000, episode_reward=-419.59 +/- 76.84
Episode length: 51.92 +/- 16.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.9        |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 543000      |
| train/                  |             |
|    approx_kl            | 0.005704424 |
|    clip_fraction        | 0.0112      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0306     |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.001       |
|    loss                 | 695         |
|    n_updates            | 1614        |
|    policy_gradient_loss | -0.000183   |
|    value_loss           | 1.91e+03    |
-----------------------------------------
Eval num_timesteps=543500, episode_reward=-411.19 +/- 79.47
Episode length: 52.54 +/- 21.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
Eval num_timesteps=544000, episode_reward=-413.59 +/- 68.67
Episode length: 51.60 +/- 14.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=544500, episode_reward=-409.39 +/- 70.95
Episode length: 49.70 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.2     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 266      |
|    time_elapsed    | 2439     |
|    total_timesteps | 544768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=545000, episode_reward=-408.79 +/- 74.59
Episode length: 51.36 +/- 15.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.4         |
|    mean_reward          | -409         |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 0.0027757222 |
|    clip_fraction        | 0.00911      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0222      |
|    explained_variance   | 0.457        |
|    learning_rate        | 0.001        |
|    loss                 | 909          |
|    n_updates            | 1615         |
|    policy_gradient_loss | -0.000306    |
|    value_loss           | 1.31e+03     |
------------------------------------------
Eval num_timesteps=545500, episode_reward=-423.19 +/- 68.95
Episode length: 48.24 +/- 15.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
Eval num_timesteps=546000, episode_reward=-413.59 +/- 69.46
Episode length: 51.12 +/- 17.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=546500, episode_reward=-413.02 +/- 73.23
Episode length: 53.16 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 267      |
|    time_elapsed    | 2447     |
|    total_timesteps | 546816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=547000, episode_reward=-407.00 +/- 79.98
Episode length: 52.28 +/- 15.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.3        |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 547000      |
| train/                  |             |
|    approx_kl            | 0.004576761 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0269     |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.001       |
|    loss                 | 735         |
|    n_updates            | 1616        |
|    policy_gradient_loss | 0.000875    |
|    value_loss           | 1.42e+03    |
-----------------------------------------
Eval num_timesteps=547500, episode_reward=-419.31 +/- 71.51
Episode length: 49.26 +/- 16.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
Eval num_timesteps=548000, episode_reward=-418.39 +/- 86.98
Episode length: 53.80 +/- 22.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=548500, episode_reward=-430.39 +/- 55.30
Episode length: 52.30 +/- 15.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 268      |
|    time_elapsed    | 2456     |
|    total_timesteps | 548864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=549000, episode_reward=-418.66 +/- 73.12
Episode length: 53.14 +/- 17.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.1        |
|    mean_reward          | -419        |
| time/                   |             |
|    total_timesteps      | 549000      |
| train/                  |             |
|    approx_kl            | 0.004586859 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0355     |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.001       |
|    loss                 | 974         |
|    n_updates            | 1617        |
|    policy_gradient_loss | 0.00203     |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=549500, episode_reward=-409.72 +/- 65.55
Episode length: 48.76 +/- 15.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-414.05 +/- 71.34
Episode length: 49.04 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=550500, episode_reward=-391.39 +/- 78.37
Episode length: 46.68 +/- 17.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 269      |
|    time_elapsed    | 2465     |
|    total_timesteps | 550912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=551000, episode_reward=-408.19 +/- 74.53
Episode length: 51.90 +/- 15.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.9        |
|    mean_reward          | -408        |
| time/                   |             |
|    total_timesteps      | 551000      |
| train/                  |             |
|    approx_kl            | 0.005233545 |
|    clip_fraction        | 0.00937     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0362     |
|    explained_variance   | 0.524       |
|    learning_rate        | 0.001       |
|    loss                 | 633         |
|    n_updates            | 1618        |
|    policy_gradient_loss | -0.000666   |
|    value_loss           | 1.41e+03    |
-----------------------------------------
Eval num_timesteps=551500, episode_reward=-403.99 +/- 73.69
Episode length: 49.62 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
Eval num_timesteps=552000, episode_reward=-435.19 +/- 54.87
Episode length: 49.76 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=552500, episode_reward=-414.19 +/- 71.27
Episode length: 48.40 +/- 14.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 270      |
|    time_elapsed    | 2473     |
|    total_timesteps | 552960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=553000, episode_reward=-402.22 +/- 80.29
Episode length: 47.20 +/- 16.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.2         |
|    mean_reward          | -402         |
| time/                   |              |
|    total_timesteps      | 553000       |
| train/                  |              |
|    approx_kl            | 0.0071381303 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.05        |
|    explained_variance   | 0.558        |
|    learning_rate        | 0.001        |
|    loss                 | 674          |
|    n_updates            | 1619         |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 1.71e+03     |
------------------------------------------
Eval num_timesteps=553500, episode_reward=-415.39 +/- 65.24
Episode length: 47.78 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
Eval num_timesteps=554000, episode_reward=-411.79 +/- 66.44
Episode length: 47.22 +/- 16.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=554500, episode_reward=-437.29 +/- 53.29
Episode length: 55.20 +/- 19.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=-409.27 +/- 72.87
Episode length: 53.48 +/- 18.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 271      |
|    time_elapsed    | 2484     |
|    total_timesteps | 555008   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=555500, episode_reward=-413.03 +/- 67.35
Episode length: 42.34 +/- 18.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 555500      |
| train/                  |             |
|    approx_kl            | 0.011340105 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.074      |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.001       |
|    loss                 | 746         |
|    n_updates            | 1620        |
|    policy_gradient_loss | 0.000938    |
|    value_loss           | 1.68e+03    |
-----------------------------------------
Eval num_timesteps=556000, episode_reward=-415.35 +/- 77.01
Episode length: 52.14 +/- 19.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
Eval num_timesteps=556500, episode_reward=-426.73 +/- 76.40
Episode length: 50.28 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 556500   |
---------------------------------
Eval num_timesteps=557000, episode_reward=-395.51 +/- 86.49
Episode length: 48.60 +/- 18.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 272      |
|    time_elapsed    | 2492     |
|    total_timesteps | 557056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=557500, episode_reward=-409.65 +/- 73.78
Episode length: 49.94 +/- 16.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.9         |
|    mean_reward          | -410         |
| time/                   |              |
|    total_timesteps      | 557500       |
| train/                  |              |
|    approx_kl            | 0.0046544448 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0841      |
|    explained_variance   | 0.531        |
|    learning_rate        | 0.001        |
|    loss                 | 1.68e+03     |
|    n_updates            | 1621         |
|    policy_gradient_loss | 0.000172     |
|    value_loss           | 1.7e+03      |
------------------------------------------
Eval num_timesteps=558000, episode_reward=-436.39 +/- 55.44
Episode length: 48.32 +/- 17.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=558500, episode_reward=-423.08 +/- 59.08
Episode length: 50.06 +/- 15.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 558500   |
---------------------------------
Eval num_timesteps=559000, episode_reward=-414.61 +/- 78.29
Episode length: 47.84 +/- 15.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 273      |
|    time_elapsed    | 2500     |
|    total_timesteps | 559104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=559500, episode_reward=-421.93 +/- 73.55
Episode length: 47.42 +/- 16.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.4         |
|    mean_reward          | -422         |
| time/                   |              |
|    total_timesteps      | 559500       |
| train/                  |              |
|    approx_kl            | 0.0022954838 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0434      |
|    explained_variance   | 0.428        |
|    learning_rate        | 0.001        |
|    loss                 | 380          |
|    n_updates            | 1622         |
|    policy_gradient_loss | 0.000522     |
|    value_loss           | 1.96e+03     |
------------------------------------------
Eval num_timesteps=560000, episode_reward=-415.73 +/- 62.56
Episode length: 50.86 +/- 14.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=560500, episode_reward=-407.59 +/- 83.36
Episode length: 52.46 +/- 18.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 560500   |
---------------------------------
Eval num_timesteps=561000, episode_reward=-417.81 +/- 63.86
Episode length: 49.50 +/- 13.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 274      |
|    time_elapsed    | 2509     |
|    total_timesteps | 561152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=561500, episode_reward=-411.30 +/- 61.97
Episode length: 48.18 +/- 12.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.2         |
|    mean_reward          | -411         |
| time/                   |              |
|    total_timesteps      | 561500       |
| train/                  |              |
|    approx_kl            | 0.0049598957 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0566      |
|    explained_variance   | 0.526        |
|    learning_rate        | 0.001        |
|    loss                 | 445          |
|    n_updates            | 1623         |
|    policy_gradient_loss | -0.000608    |
|    value_loss           | 1.63e+03     |
------------------------------------------
Eval num_timesteps=562000, episode_reward=-434.65 +/- 57.06
Episode length: 53.34 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
Eval num_timesteps=562500, episode_reward=-409.16 +/- 74.74
Episode length: 49.76 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 562500   |
---------------------------------
Eval num_timesteps=563000, episode_reward=-401.63 +/- 71.05
Episode length: 49.42 +/- 15.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 275      |
|    time_elapsed    | 2518     |
|    total_timesteps | 563200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=563500, episode_reward=-422.09 +/- 61.44
Episode length: 49.60 +/- 16.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.6         |
|    mean_reward          | -422         |
| time/                   |              |
|    total_timesteps      | 563500       |
| train/                  |              |
|    approx_kl            | 0.0040003625 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0368      |
|    explained_variance   | 0.504        |
|    learning_rate        | 0.001        |
|    loss                 | 879          |
|    n_updates            | 1624         |
|    policy_gradient_loss | -6.55e-05    |
|    value_loss           | 1.8e+03      |
------------------------------------------
Eval num_timesteps=564000, episode_reward=-415.39 +/- 75.71
Episode length: 50.10 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=564500, episode_reward=-415.46 +/- 90.49
Episode length: 47.68 +/- 16.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 564500   |
---------------------------------
Eval num_timesteps=565000, episode_reward=-414.79 +/- 72.97
Episode length: 51.30 +/- 14.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 276      |
|    time_elapsed    | 2526     |
|    total_timesteps | 565248   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=565500, episode_reward=-408.81 +/- 81.31
Episode length: 47.16 +/- 13.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.2        |
|    mean_reward          | -409        |
| time/                   |             |
|    total_timesteps      | 565500      |
| train/                  |             |
|    approx_kl            | 0.010699795 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0212     |
|    explained_variance   | 0.495       |
|    learning_rate        | 0.001       |
|    loss                 | 1.22e+03    |
|    n_updates            | 1625        |
|    policy_gradient_loss | -0.000344   |
|    value_loss           | 1.95e+03    |
-----------------------------------------
Eval num_timesteps=566000, episode_reward=-410.55 +/- 59.53
Episode length: 47.52 +/- 14.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
Eval num_timesteps=566500, episode_reward=-416.00 +/- 69.52
Episode length: 51.50 +/- 15.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 566500   |
---------------------------------
Eval num_timesteps=567000, episode_reward=-407.97 +/- 70.68
Episode length: 53.52 +/- 21.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 277      |
|    time_elapsed    | 2535     |
|    total_timesteps | 567296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=567500, episode_reward=-428.59 +/- 58.87
Episode length: 47.76 +/- 19.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.8         |
|    mean_reward          | -429         |
| time/                   |              |
|    total_timesteps      | 567500       |
| train/                  |              |
|    approx_kl            | 0.0023882634 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0212      |
|    explained_variance   | 0.529        |
|    learning_rate        | 0.001        |
|    loss                 | 967          |
|    n_updates            | 1626         |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 1.94e+03     |
------------------------------------------
Eval num_timesteps=568000, episode_reward=-397.39 +/- 85.28
Episode length: 50.80 +/- 20.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=568500, episode_reward=-413.59 +/- 70.48
Episode length: 46.00 +/- 13.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 568500   |
---------------------------------
Eval num_timesteps=569000, episode_reward=-415.39 +/- 60.66
Episode length: 48.70 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 278      |
|    time_elapsed    | 2543     |
|    total_timesteps | 569344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=569500, episode_reward=-405.80 +/- 66.55
Episode length: 50.16 +/- 16.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.2         |
|    mean_reward          | -406         |
| time/                   |              |
|    total_timesteps      | 569500       |
| train/                  |              |
|    approx_kl            | 0.0051710447 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0649      |
|    explained_variance   | 0.422        |
|    learning_rate        | 0.001        |
|    loss                 | 891          |
|    n_updates            | 1627         |
|    policy_gradient_loss | 0.000751     |
|    value_loss           | 1.54e+03     |
------------------------------------------
Eval num_timesteps=570000, episode_reward=-423.79 +/- 61.13
Episode length: 52.88 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
Eval num_timesteps=570500, episode_reward=-411.27 +/- 69.11
Episode length: 45.64 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 570500   |
---------------------------------
Eval num_timesteps=571000, episode_reward=-434.27 +/- 65.45
Episode length: 54.78 +/- 18.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.8     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 279      |
|    time_elapsed    | 2552     |
|    total_timesteps | 571392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=571500, episode_reward=-413.59 +/- 64.90
Episode length: 48.22 +/- 14.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.2         |
|    mean_reward          | -414         |
| time/                   |              |
|    total_timesteps      | 571500       |
| train/                  |              |
|    approx_kl            | 0.0021644619 |
|    clip_fraction        | 0.00744      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0449      |
|    explained_variance   | 0.486        |
|    learning_rate        | 0.001        |
|    loss                 | 929          |
|    n_updates            | 1628         |
|    policy_gradient_loss | -0.00016     |
|    value_loss           | 1.64e+03     |
------------------------------------------
Eval num_timesteps=572000, episode_reward=-407.59 +/- 71.25
Episode length: 48.72 +/- 16.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=572500, episode_reward=-413.60 +/- 61.77
Episode length: 51.56 +/- 13.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 572500   |
---------------------------------
Eval num_timesteps=573000, episode_reward=-422.59 +/- 58.25
Episode length: 52.60 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 280      |
|    time_elapsed    | 2560     |
|    total_timesteps | 573440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=573500, episode_reward=-418.39 +/- 66.32
Episode length: 52.10 +/- 20.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.1         |
|    mean_reward          | -418         |
| time/                   |              |
|    total_timesteps      | 573500       |
| train/                  |              |
|    approx_kl            | 0.0029775463 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0165      |
|    explained_variance   | 0.494        |
|    learning_rate        | 0.001        |
|    loss                 | 613          |
|    n_updates            | 1629         |
|    policy_gradient_loss | -8.87e-05    |
|    value_loss           | 1.57e+03     |
------------------------------------------
Eval num_timesteps=574000, episode_reward=-431.99 +/- 74.51
Episode length: 52.66 +/- 16.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
Eval num_timesteps=574500, episode_reward=-424.99 +/- 67.43
Episode length: 48.50 +/- 15.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 574500   |
---------------------------------
Eval num_timesteps=575000, episode_reward=-426.79 +/- 54.78
Episode length: 51.22 +/- 18.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 281      |
|    time_elapsed    | 2569     |
|    total_timesteps | 575488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=575500, episode_reward=-426.81 +/- 60.43
Episode length: 56.62 +/- 22.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 56.6         |
|    mean_reward          | -427         |
| time/                   |              |
|    total_timesteps      | 575500       |
| train/                  |              |
|    approx_kl            | 0.0045608697 |
|    clip_fraction        | 0.00521      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0219      |
|    explained_variance   | 0.535        |
|    learning_rate        | 0.001        |
|    loss                 | 936          |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.000535    |
|    value_loss           | 1.98e+03     |
------------------------------------------
Eval num_timesteps=576000, episode_reward=-411.20 +/- 69.05
Episode length: 51.92 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=-395.03 +/- 86.61
Episode length: 45.56 +/- 16.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
Eval num_timesteps=577000, episode_reward=-412.99 +/- 71.22
Episode length: 46.98 +/- 14.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=577500, episode_reward=-417.24 +/- 79.24
Episode length: 54.12 +/- 20.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 282      |
|    time_elapsed    | 2579     |
|    total_timesteps | 577536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=578000, episode_reward=-424.43 +/- 69.38
Episode length: 49.84 +/- 17.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.8         |
|    mean_reward          | -424         |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0029639038 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0251      |
|    explained_variance   | 0.472        |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+03     |
|    n_updates            | 1631         |
|    policy_gradient_loss | 0.00021      |
|    value_loss           | 1.8e+03      |
------------------------------------------
Eval num_timesteps=578500, episode_reward=-418.73 +/- 62.26
Episode length: 49.22 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
Eval num_timesteps=579000, episode_reward=-414.79 +/- 73.77
Episode length: 50.26 +/- 18.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=579500, episode_reward=-425.59 +/- 66.27
Episode length: 48.14 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 283      |
|    time_elapsed    | 2588     |
|    total_timesteps | 579584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=580000, episode_reward=-426.30 +/- 54.29
Episode length: 54.10 +/- 19.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54.1         |
|    mean_reward          | -426         |
| time/                   |              |
|    total_timesteps      | 580000       |
| train/                  |              |
|    approx_kl            | 0.0022820283 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0386      |
|    explained_variance   | 0.496        |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+03     |
|    n_updates            | 1632         |
|    policy_gradient_loss | -8.88e-05    |
|    value_loss           | 1.54e+03     |
------------------------------------------
Eval num_timesteps=580500, episode_reward=-415.99 +/- 71.05
Episode length: 50.78 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
Eval num_timesteps=581000, episode_reward=-413.05 +/- 57.51
Episode length: 50.02 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=581500, episode_reward=-423.55 +/- 62.18
Episode length: 50.10 +/- 18.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 284      |
|    time_elapsed    | 2597     |
|    total_timesteps | 581632   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=582000, episode_reward=-421.40 +/- 68.31
Episode length: 49.30 +/- 13.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.3         |
|    mean_reward          | -421         |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 0.0038266345 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0799      |
|    explained_variance   | 0.487        |
|    learning_rate        | 0.001        |
|    loss                 | 851          |
|    n_updates            | 1633         |
|    policy_gradient_loss | -0.00031     |
|    value_loss           | 2.35e+03     |
------------------------------------------
Eval num_timesteps=582500, episode_reward=-408.25 +/- 67.49
Episode length: 47.42 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
Eval num_timesteps=583000, episode_reward=-400.99 +/- 83.89
Episode length: 50.94 +/- 21.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=583500, episode_reward=-407.01 +/- 84.17
Episode length: 50.20 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 285      |
|    time_elapsed    | 2605     |
|    total_timesteps | 583680   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=584000, episode_reward=-414.19 +/- 76.86
Episode length: 50.36 +/- 17.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.4        |
|    mean_reward          | -414        |
| time/                   |             |
|    total_timesteps      | 584000      |
| train/                  |             |
|    approx_kl            | 0.005872426 |
|    clip_fraction        | 0.00174     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0464     |
|    explained_variance   | 0.436       |
|    learning_rate        | 0.001       |
|    loss                 | 1.21e+03    |
|    n_updates            | 1634        |
|    policy_gradient_loss | 0.000165    |
|    value_loss           | 1.82e+03    |
-----------------------------------------
Eval num_timesteps=584500, episode_reward=-399.19 +/- 69.67
Episode length: 47.14 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
Eval num_timesteps=585000, episode_reward=-426.19 +/- 57.45
Episode length: 47.32 +/- 14.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=585500, episode_reward=-415.39 +/- 79.19
Episode length: 44.66 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 286      |
|    time_elapsed    | 2613     |
|    total_timesteps | 585728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=586000, episode_reward=-417.79 +/- 72.05
Episode length: 50.50 +/- 18.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.5        |
|    mean_reward          | -418        |
| time/                   |             |
|    total_timesteps      | 586000      |
| train/                  |             |
|    approx_kl            | 0.004299965 |
|    clip_fraction        | 0.0132      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0373     |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.001       |
|    loss                 | 429         |
|    n_updates            | 1635        |
|    policy_gradient_loss | 0.00161     |
|    value_loss           | 2.1e+03     |
-----------------------------------------
Eval num_timesteps=586500, episode_reward=-423.79 +/- 60.84
Episode length: 53.14 +/- 19.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
Eval num_timesteps=587000, episode_reward=-432.19 +/- 56.44
Episode length: 55.16 +/- 17.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=587500, episode_reward=-416.60 +/- 83.62
Episode length: 49.24 +/- 19.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 287      |
|    time_elapsed    | 2622     |
|    total_timesteps | 587776   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=588000, episode_reward=-409.99 +/- 67.89
Episode length: 47.50 +/- 13.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.5        |
|    mean_reward          | -410        |
| time/                   |             |
|    total_timesteps      | 588000      |
| train/                  |             |
|    approx_kl            | 0.008004948 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.061      |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.001       |
|    loss                 | 810         |
|    n_updates            | 1636        |
|    policy_gradient_loss | -0.000667   |
|    value_loss           | 1.8e+03     |
-----------------------------------------
Eval num_timesteps=588500, episode_reward=-402.79 +/- 92.94
Episode length: 44.16 +/- 13.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.2     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
Eval num_timesteps=589000, episode_reward=-411.19 +/- 70.60
Episode length: 45.68 +/- 16.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
Eval num_timesteps=589500, episode_reward=-407.59 +/- 74.70
Episode length: 52.66 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 288      |
|    time_elapsed    | 2630     |
|    total_timesteps | 589824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=590000, episode_reward=-416.02 +/- 64.42
Episode length: 53.26 +/- 19.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.3         |
|    mean_reward          | -416         |
| time/                   |              |
|    total_timesteps      | 590000       |
| train/                  |              |
|    approx_kl            | 0.0039941347 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.064       |
|    explained_variance   | 0.527        |
|    learning_rate        | 0.001        |
|    loss                 | 878          |
|    n_updates            | 1637         |
|    policy_gradient_loss | 0.00173      |
|    value_loss           | 1.67e+03     |
------------------------------------------
Eval num_timesteps=590500, episode_reward=-419.59 +/- 62.64
Episode length: 52.14 +/- 20.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
Eval num_timesteps=591000, episode_reward=-416.59 +/- 67.15
Episode length: 47.12 +/- 14.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=591500, episode_reward=-415.07 +/- 71.88
Episode length: 47.46 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 289      |
|    time_elapsed    | 2639     |
|    total_timesteps | 591872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=592000, episode_reward=-402.27 +/- 80.44
Episode length: 52.56 +/- 16.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.6        |
|    mean_reward          | -402        |
| time/                   |             |
|    total_timesteps      | 592000      |
| train/                  |             |
|    approx_kl            | 0.008120094 |
|    clip_fraction        | 0.00937     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0505     |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.001       |
|    loss                 | 732         |
|    n_updates            | 1638        |
|    policy_gradient_loss | -0.000432   |
|    value_loss           | 1.51e+03    |
-----------------------------------------
Eval num_timesteps=592500, episode_reward=-425.01 +/- 64.70
Episode length: 50.74 +/- 18.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
Eval num_timesteps=593000, episode_reward=-410.00 +/- 77.78
Episode length: 51.76 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
Eval num_timesteps=593500, episode_reward=-416.62 +/- 81.79
Episode length: 53.42 +/- 21.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 290      |
|    time_elapsed    | 2648     |
|    total_timesteps | 593920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=594000, episode_reward=-404.06 +/- 97.92
Episode length: 46.82 +/- 16.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.8        |
|    mean_reward          | -404        |
| time/                   |             |
|    total_timesteps      | 594000      |
| train/                  |             |
|    approx_kl            | 0.003347391 |
|    clip_fraction        | 0.00911     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0246     |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.001       |
|    loss                 | 1.12e+03    |
|    n_updates            | 1639        |
|    policy_gradient_loss | -6.77e-05   |
|    value_loss           | 2.1e+03     |
-----------------------------------------
Eval num_timesteps=594500, episode_reward=-411.83 +/- 66.18
Episode length: 53.50 +/- 18.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
Eval num_timesteps=595000, episode_reward=-419.03 +/- 71.56
Episode length: 49.34 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=595500, episode_reward=-400.40 +/- 90.59
Episode length: 50.86 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 291      |
|    time_elapsed    | 2656     |
|    total_timesteps | 595968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=596000, episode_reward=-415.39 +/- 70.54
Episode length: 44.82 +/- 13.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44.8         |
|    mean_reward          | -415         |
| time/                   |              |
|    total_timesteps      | 596000       |
| train/                  |              |
|    approx_kl            | 0.0026938329 |
|    clip_fraction        | 0.00368      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.012       |
|    explained_variance   | 0.52         |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+03     |
|    n_updates            | 1640         |
|    policy_gradient_loss | 0.000429     |
|    value_loss           | 1.91e+03     |
------------------------------------------
Eval num_timesteps=596500, episode_reward=-420.19 +/- 80.94
Episode length: 49.60 +/- 14.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
Eval num_timesteps=597000, episode_reward=-423.81 +/- 67.59
Episode length: 52.64 +/- 16.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=597500, episode_reward=-396.19 +/- 75.58
Episode length: 49.12 +/- 12.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=-416.63 +/- 72.10
Episode length: 49.24 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 292      |
|    time_elapsed    | 2667     |
|    total_timesteps | 598016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.26
Eval num_timesteps=598500, episode_reward=-409.97 +/- 97.42
Episode length: 49.24 +/- 15.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -410        |
| time/                   |             |
|    total_timesteps      | 598500      |
| train/                  |             |
|    approx_kl            | 0.038054783 |
|    clip_fraction        | 0.00446     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0319     |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.001       |
|    loss                 | 1.09e+03    |
|    n_updates            | 1641        |
|    policy_gradient_loss | 0.0138      |
|    value_loss           | 1.93e+03    |
-----------------------------------------
Eval num_timesteps=599000, episode_reward=-418.39 +/- 69.24
Episode length: 49.70 +/- 16.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
Eval num_timesteps=599500, episode_reward=-426.19 +/- 55.54
Episode length: 50.76 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 599500   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-426.19 +/- 63.13
Episode length: 49.82 +/- 14.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 293      |
|    time_elapsed    | 2675     |
|    total_timesteps | 600064   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-stop-3-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Early stopping at step 0 due to reaching max kl: 0.02
