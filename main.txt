/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-12.74 +/- 32.34
Episode length: 50.48 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -12.7    |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-20.73 +/- 20.46
Episode length: 49.72 +/- 19.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -20.7    |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-22.53 +/- 13.92
Episode length: 49.02 +/- 16.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -22.5    |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-22.39 +/- 14.64
Episode length: 48.74 +/- 15.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -22.4    |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 64.4     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=78.07 +/- 3.95
Episode length: 63.20 +/- 22.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 63.2         |
|    mean_reward          | 78.1         |
| time/                   |              |
|    total_timesteps      | 2500         |
| train/                  |              |
|    approx_kl            | 0.0109749315 |
|    clip_fraction        | 0.0666       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -3.58e-05    |
|    learning_rate        | 0.001        |
|    loss                 | 113          |
|    n_updates            | 8            |
|    policy_gradient_loss | -0.00641     |
|    value_loss           | 266          |
------------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=78.05 +/- 3.47
Episode length: 63.38 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.4     |
|    mean_reward     | 78       |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=78.25 +/- 3.75
Episode length: 62.60 +/- 20.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.6     |
|    mean_reward     | 78.2     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=77.61 +/- 3.93
Episode length: 66.30 +/- 22.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.3     |
|    mean_reward     | 77.6     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 76.4     |
| time/              |          |
|    fps             | 206      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=4500, episode_reward=-11.59 +/- 0.00
Episode length: 48.60 +/- 15.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.6        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.009295985 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.000971    |
|    learning_rate        | 0.001       |
|    loss                 | 23.1        |
|    n_updates            | 9           |
|    policy_gradient_loss | -0.00798    |
|    value_loss           | 79.7        |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-11.59 +/- 0.01
Episode length: 49.56 +/- 14.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-11.59 +/- 0.01
Episode length: 49.00 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-11.60 +/- 0.00
Episode length: 55.94 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 74.3     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 3        |
|    time_elapsed    | 28       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=-11.59 +/- 0.01
Episode length: 48.56 +/- 19.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.6        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.009638995 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.00281     |
|    learning_rate        | 0.001       |
|    loss                 | 175         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00216    |
|    value_loss           | 218         |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=-11.59 +/- 0.00
Episode length: 48.46 +/- 14.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-11.59 +/- 0.00
Episode length: 50.36 +/- 15.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-11.59 +/- 0.01
Episode length: 46.50 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 77.4     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 4        |
|    time_elapsed    | 37       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=8500, episode_reward=-11.60 +/- 0.00
Episode length: 56.26 +/- 18.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.3        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.012732661 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.00332     |
|    learning_rate        | 0.001       |
|    loss                 | 144         |
|    n_updates            | 11          |
|    policy_gradient_loss | 0.0014      |
|    value_loss           | 180         |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=-11.60 +/- 0.00
Episode length: 50.24 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-11.59 +/- 0.00
Episode length: 49.72 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-11.59 +/- 0.00
Episode length: 49.14 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 71.3     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 5        |
|    time_elapsed    | 46       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=10500, episode_reward=-11.59 +/- 0.00
Episode length: 46.62 +/- 13.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.6        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.010811271 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.00965     |
|    learning_rate        | 0.001       |
|    loss                 | 134         |
|    n_updates            | 12          |
|    policy_gradient_loss | 0.00741     |
|    value_loss           | 213         |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=-11.59 +/- 0.00
Episode length: 47.44 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-11.59 +/- 0.00
Episode length: 51.06 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-11.60 +/- 0.00
Episode length: 48.72 +/- 14.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 70.7     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 6        |
|    time_elapsed    | 54       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=12500, episode_reward=-11.59 +/- 0.00
Episode length: 51.66 +/- 17.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.7        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.017618285 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.0104      |
|    learning_rate        | 0.001       |
|    loss                 | 79.5        |
|    n_updates            | 13          |
|    policy_gradient_loss | 0.00283     |
|    value_loss           | 213         |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=-11.59 +/- 0.00
Episode length: 45.20 +/- 13.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-11.59 +/- 0.01
Episode length: 49.58 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-11.59 +/- 0.00
Episode length: 48.78 +/- 11.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 63.2     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 7        |
|    time_elapsed    | 62       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=14500, episode_reward=-11.60 +/- 0.00
Episode length: 54.82 +/- 16.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54.8         |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 14500        |
| train/                  |              |
|    approx_kl            | 0.0076037287 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.00913      |
|    learning_rate        | 0.001        |
|    loss                 | 117          |
|    n_updates            | 14           |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 173          |
------------------------------------------
Eval num_timesteps=15000, episode_reward=-11.59 +/- 0.00
Episode length: 49.38 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-11.59 +/- 0.01
Episode length: 49.60 +/- 17.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-11.59 +/- 0.00
Episode length: 49.40 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 66.9     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 8        |
|    time_elapsed    | 71       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=16500, episode_reward=-11.59 +/- 0.00
Episode length: 51.52 +/- 16.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.5        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.014095386 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.0131      |
|    learning_rate        | 0.001       |
|    loss                 | 91.1        |
|    n_updates            | 15          |
|    policy_gradient_loss | -2.2e-05    |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=-11.59 +/- 0.00
Episode length: 45.62 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-11.59 +/- 0.00
Episode length: 54.14 +/- 17.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-11.59 +/- 0.00
Episode length: 51.76 +/- 19.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 68.5     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 9        |
|    time_elapsed    | 79       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=18500, episode_reward=-11.59 +/- 0.00
Episode length: 48.02 +/- 18.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48           |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 18500        |
| train/                  |              |
|    approx_kl            | 0.0072748833 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.00781      |
|    learning_rate        | 0.001        |
|    loss                 | 40           |
|    n_updates            | 16           |
|    policy_gradient_loss | 0.00481      |
|    value_loss           | 156          |
------------------------------------------
Eval num_timesteps=19000, episode_reward=-11.60 +/- 0.00
Episode length: 51.28 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-11.59 +/- 0.01
Episode length: 48.94 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-11.60 +/- 0.00
Episode length: 46.30 +/- 14.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 69.5     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 10       |
|    time_elapsed    | 88       |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=-11.59 +/- 0.01
Episode length: 52.92 +/- 18.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.9        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.007846776 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.0125      |
|    learning_rate        | 0.001       |
|    loss                 | 123         |
|    n_updates            | 17          |
|    policy_gradient_loss | 0.00118     |
|    value_loss           | 185         |
-----------------------------------------
Eval num_timesteps=21000, episode_reward=-11.59 +/- 0.00
Episode length: 50.18 +/- 13.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-11.60 +/- 0.00
Episode length: 47.92 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-11.59 +/- 0.00
Episode length: 52.98 +/- 18.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-11.59 +/- 0.00
Episode length: 49.14 +/- 14.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 68.8     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 11       |
|    time_elapsed    | 98       |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=23000, episode_reward=-9.61 +/- 13.86
Episode length: 51.12 +/- 16.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -9.61       |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.007834939 |
|    clip_fraction        | 0.0891      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.0238      |
|    learning_rate        | 0.001       |
|    loss                 | 109         |
|    n_updates            | 18          |
|    policy_gradient_loss | 0.000366    |
|    value_loss           | 215         |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=-11.59 +/- 0.00
Episode length: 49.10 +/- 15.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-11.60 +/- 0.00
Episode length: 51.88 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-11.59 +/- 0.00
Episode length: 53.20 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 68.8     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 12       |
|    time_elapsed    | 106      |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=25000, episode_reward=-17.51 +/- 27.34
Episode length: 52.74 +/- 17.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.7        |
|    mean_reward          | -17.5       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009536096 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.0426      |
|    learning_rate        | 0.001       |
|    loss                 | 140         |
|    n_updates            | 19          |
|    policy_gradient_loss | -0.000549   |
|    value_loss           | 293         |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=-17.65 +/- 27.04
Episode length: 53.40 +/- 19.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -17.7    |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-22.20 +/- 15.14
Episode length: 48.62 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -22.2    |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-16.65 +/- 26.81
Episode length: 50.60 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -16.7    |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 72.2     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 13       |
|    time_elapsed    | 115      |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=27000, episode_reward=-16.50 +/- 27.67
Episode length: 49.26 +/- 15.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | -16.5       |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.009052901 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.0558      |
|    learning_rate        | 0.001       |
|    loss                 | 109         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000292   |
|    value_loss           | 188         |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=-23.11 +/- 13.74
Episode length: 51.38 +/- 19.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -23.1    |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-13.59 +/- 29.41
Episode length: 45.88 +/- 15.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -13.6    |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-18.99 +/- 23.43
Episode length: 50.72 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -19      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 67.9     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 14       |
|    time_elapsed    | 123      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=29000, episode_reward=-16.27 +/- 26.34
Episode length: 48.56 +/- 15.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.6        |
|    mean_reward          | -16.3       |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.007872413 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.0583      |
|    learning_rate        | 0.001       |
|    loss                 | 100         |
|    n_updates            | 21          |
|    policy_gradient_loss | -0.000706   |
|    value_loss           | 238         |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=-17.71 +/- 23.20
Episode length: 46.44 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -17.7    |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-24.65 +/- 5.11
Episode length: 49.92 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -24.7    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-13.59 +/- 32.01
Episode length: 53.14 +/- 20.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -13.6    |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 75.4     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 15       |
|    time_elapsed    | 132      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=31000, episode_reward=-21.41 +/- 20.64
Episode length: 52.42 +/- 16.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.4         |
|    mean_reward          | -21.4        |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0072835153 |
|    clip_fraction        | 0.0644       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.0986       |
|    learning_rate        | 0.001        |
|    loss                 | 102          |
|    n_updates            | 23           |
|    policy_gradient_loss | 0.00557      |
|    value_loss           | 155          |
------------------------------------------
Eval num_timesteps=31500, episode_reward=-18.75 +/- 23.06
Episode length: 49.98 +/- 16.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -18.8    |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-20.92 +/- 20.81
Episode length: 50.78 +/- 14.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -20.9    |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-14.17 +/- 29.42
Episode length: 48.12 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -14.2    |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 74.6     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 16       |
|    time_elapsed    | 140      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=33000, episode_reward=-22.64 +/- 15.03
Episode length: 49.64 +/- 16.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.6        |
|    mean_reward          | -22.6       |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.013661154 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.0576      |
|    learning_rate        | 0.001       |
|    loss                 | 124         |
|    n_updates            | 24          |
|    policy_gradient_loss | -0.00632    |
|    value_loss           | 222         |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=-18.17 +/- 24.21
Episode length: 48.26 +/- 15.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -18.2    |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-15.71 +/- 26.70
Episode length: 46.56 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -15.7    |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-16.59 +/- 25.99
Episode length: 49.78 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -16.6    |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 71.4     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 17       |
|    time_elapsed    | 149      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=35000, episode_reward=-20.55 +/- 20.67
Episode length: 49.24 +/- 15.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -20.6       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.009628992 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.114       |
|    learning_rate        | 0.001       |
|    loss                 | 87.3        |
|    n_updates            | 25          |
|    policy_gradient_loss | 0.00254     |
|    value_loss           | 168         |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=-15.09 +/- 29.72
Episode length: 51.56 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -15.1    |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-22.71 +/- 13.90
Episode length: 50.94 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -22.7    |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-20.91 +/- 20.25
Episode length: 51.30 +/- 21.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -20.9    |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 72.4     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 18       |
|    time_elapsed    | 157      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=37000, episode_reward=-20.97 +/- 19.89
Episode length: 51.26 +/- 18.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.3        |
|    mean_reward          | -21         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.008743309 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.001       |
|    loss                 | 117         |
|    n_updates            | 26          |
|    policy_gradient_loss | 0.00258     |
|    value_loss           | 207         |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=-14.13 +/- 30.33
Episode length: 47.86 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -14.1    |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-18.55 +/- 23.83
Episode length: 49.46 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -18.6    |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-20.05 +/- 20.54
Episode length: 47.66 +/- 13.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -20.1    |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 71       |
| time/              |          |
|    fps             | 233      |
|    iterations      | 19       |
|    time_elapsed    | 166      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=39000, episode_reward=-17.93 +/- 22.92
Episode length: 47.30 +/- 17.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.3         |
|    mean_reward          | -17.9        |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0075877775 |
|    clip_fraction        | 0.0859       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.963       |
|    explained_variance   | 0.189        |
|    learning_rate        | 0.001        |
|    loss                 | 114          |
|    n_updates            | 27           |
|    policy_gradient_loss | -0.00308     |
|    value_loss           | 148          |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-15.35 +/- 29.13
Episode length: 52.86 +/- 20.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -15.4    |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-18.44 +/- 27.16
Episode length: 56.66 +/- 20.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.7     |
|    mean_reward     | -18.4    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-21.03 +/- 20.21
Episode length: 51.18 +/- 15.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -21      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 67.3     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 20       |
|    time_elapsed    | 175      |
|    total_timesteps | 40960    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=41000, episode_reward=-21.17 +/- 15.44
Episode length: 44.72 +/- 15.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.7        |
|    mean_reward          | -21.2       |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.008389094 |
|    clip_fraction        | 0.0967      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.209       |
|    learning_rate        | 0.001       |
|    loss                 | 129         |
|    n_updates            | 28          |
|    policy_gradient_loss | -0.000663   |
|    value_loss           | 209         |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=-13.39 +/- 32.94
Episode length: 52.68 +/- 20.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -13.4    |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-18.67 +/- 25.07
Episode length: 49.80 +/- 16.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-23.83 +/- 4.40
Episode length: 46.86 +/- 16.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-8.88 +/- 37.11
Episode length: 50.68 +/- 19.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -8.88    |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 68.5     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 21       |
|    time_elapsed    | 185      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=43500, episode_reward=-12.01 +/- 32.33
Episode length: 47.28 +/- 14.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.3         |
|    mean_reward          | -12          |
| time/                   |              |
|    total_timesteps      | 43500        |
| train/                  |              |
|    approx_kl            | 0.0055579836 |
|    clip_fraction        | 0.0737       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.2          |
|    learning_rate        | 0.001        |
|    loss                 | 120          |
|    n_updates            | 29           |
|    policy_gradient_loss | 0.0031       |
|    value_loss           | 216          |
------------------------------------------
Eval num_timesteps=44000, episode_reward=-21.16 +/- 20.87
Episode length: 51.38 +/- 15.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -21.2    |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-19.29 +/- 23.80
Episode length: 51.62 +/- 14.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -19.3    |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-17.95 +/- 23.56
Episode length: 47.10 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -18      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 70.3     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 22       |
|    time_elapsed    | 193      |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=45500, episode_reward=-23.73 +/- 4.50
Episode length: 46.42 +/- 15.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.4         |
|    mean_reward          | -23.7        |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 0.0047368295 |
|    clip_fraction        | 0.0459       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.001        |
|    loss                 | 108          |
|    n_updates            | 30           |
|    policy_gradient_loss | 0.00142      |
|    value_loss           | 195          |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-23.83 +/- 3.42
Episode length: 46.76 +/- 12.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-16.79 +/- 28.06
Episode length: 50.40 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -16.8    |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-23.10 +/- 15.61
Episode length: 51.38 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -23.1    |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 71.7     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 23       |
|    time_elapsed    | 202      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=47500, episode_reward=11.89 +/- 45.20
Episode length: 67.54 +/- 51.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 67.5       |
|    mean_reward          | 11.9       |
| time/                   |            |
|    total_timesteps      | 47500      |
| train/                  |            |
|    approx_kl            | 0.00806528 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.209      |
|    learning_rate        | 0.001      |
|    loss                 | 39         |
|    n_updates            | 31         |
|    policy_gradient_loss | 0.00446    |
|    value_loss           | 112        |
----------------------------------------
Eval num_timesteps=48000, episode_reward=15.25 +/- 46.43
Episode length: 51.86 +/- 24.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=4.61 +/- 46.48
Episode length: 60.46 +/- 45.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.5     |
|    mean_reward     | 4.61     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=21.41 +/- 49.85
Episode length: 62.38 +/- 34.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.4     |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 69.1     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 24       |
|    time_elapsed    | 212      |
|    total_timesteps | 49152    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=49500, episode_reward=65.30 +/- 6.19
Episode length: 199.08 +/- 74.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 65.3        |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.008686365 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.148       |
|    learning_rate        | 0.001       |
|    loss                 | 92.9        |
|    n_updates            | 32          |
|    policy_gradient_loss | -0.00461    |
|    value_loss           | 172         |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=65.22 +/- 5.36
Episode length: 202.60 +/- 79.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | 65.2     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=65.12 +/- 5.88
Episode length: 206.46 +/- 73.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 65.1     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=67.80 +/- 7.35
Episode length: 185.04 +/- 53.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 67.8     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 70.9     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 25       |
|    time_elapsed    | 241      |
|    total_timesteps | 51200    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=51500, episode_reward=-11.59 +/- 0.01
Episode length: 49.12 +/- 17.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.1        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.007338499 |
|    clip_fraction        | 0.0984      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.131       |
|    learning_rate        | 0.001       |
|    loss                 | 103         |
|    n_updates            | 33          |
|    policy_gradient_loss | 0.00636     |
|    value_loss           | 181         |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=-11.59 +/- 0.01
Episode length: 49.04 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-11.60 +/- 0.00
Episode length: 48.46 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-11.59 +/- 0.01
Episode length: 52.50 +/- 20.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 73.8     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 26       |
|    time_elapsed    | 249      |
|    total_timesteps | 53248    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=53500, episode_reward=-11.59 +/- 0.00
Episode length: 49.80 +/- 14.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.8        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.009027289 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.094       |
|    learning_rate        | 0.001       |
|    loss                 | 43.8        |
|    n_updates            | 34          |
|    policy_gradient_loss | 0.00363     |
|    value_loss           | 170         |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=-11.59 +/- 0.00
Episode length: 46.08 +/- 14.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-11.59 +/- 0.00
Episode length: 49.78 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-11.59 +/- 0.01
Episode length: 48.66 +/- 14.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 77.2     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 27       |
|    time_elapsed    | 258      |
|    total_timesteps | 55296    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=55500, episode_reward=-11.60 +/- 0.00
Episode length: 50.94 +/- 15.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.9        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 55500       |
| train/                  |             |
|    approx_kl            | 0.007471732 |
|    clip_fraction        | 0.0977      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.0956      |
|    learning_rate        | 0.001       |
|    loss                 | 88.3        |
|    n_updates            | 35          |
|    policy_gradient_loss | 0.00289     |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=-11.59 +/- 0.00
Episode length: 51.26 +/- 19.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-11.59 +/- 0.00
Episode length: 51.78 +/- 15.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-11.59 +/- 0.00
Episode length: 51.52 +/- 16.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 79.7     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 28       |
|    time_elapsed    | 266      |
|    total_timesteps | 57344    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=57500, episode_reward=59.44 +/- 44.48
Episode length: 130.08 +/- 80.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 130         |
|    mean_reward          | 59.4        |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.007685694 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.16        |
|    learning_rate        | 0.001       |
|    loss                 | 89.1        |
|    n_updates            | 36          |
|    policy_gradient_loss | 0.00597     |
|    value_loss           | 134         |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=57.46 +/- 45.34
Episode length: 125.72 +/- 86.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 126      |
|    mean_reward     | 57.5     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=47.50 +/- 48.50
Episode length: 117.42 +/- 88.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 47.5     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=55.30 +/- 46.11
Episode length: 107.38 +/- 66.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 55.3     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 82       |
| time/              |          |
|    fps             | 208      |
|    iterations      | 29       |
|    time_elapsed    | 285      |
|    total_timesteps | 59392    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=59500, episode_reward=-12.85 +/- 31.23
Episode length: 50.66 +/- 16.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.7        |
|    mean_reward          | -12.9       |
| time/                   |             |
|    total_timesteps      | 59500       |
| train/                  |             |
|    approx_kl            | 0.008181377 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.172       |
|    learning_rate        | 0.001       |
|    loss                 | 115         |
|    n_updates            | 37          |
|    policy_gradient_loss | 0.000963    |
|    value_loss           | 190         |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=-18.91 +/- 24.10
Episode length: 50.64 +/- 14.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -18.9    |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-18.73 +/- 26.04
Episode length: 57.08 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.1     |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-18.55 +/- 24.07
Episode length: 49.82 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -18.6    |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 84.8     |
| time/              |          |
|    fps             | 208      |
|    iterations      | 30       |
|    time_elapsed    | 294      |
|    total_timesteps | 61440    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=61500, episode_reward=-19.22 +/- 24.54
Episode length: 52.02 +/- 17.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | -19.2       |
| time/                   |             |
|    total_timesteps      | 61500       |
| train/                  |             |
|    approx_kl            | 0.007227943 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.205       |
|    learning_rate        | 0.001       |
|    loss                 | 40.6        |
|    n_updates            | 38          |
|    policy_gradient_loss | 0.00212     |
|    value_loss           | 142         |
-----------------------------------------
Eval num_timesteps=62000, episode_reward=-18.26 +/- 23.94
Episode length: 48.02 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -18.3    |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-21.29 +/- 18.56
Episode length: 52.28 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -21.3    |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-19.52 +/- 24.21
Episode length: 52.96 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -19.5    |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 89.9     |
| time/              |          |
|    fps             | 209      |
|    iterations      | 31       |
|    time_elapsed    | 302      |
|    total_timesteps | 63488    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=63500, episode_reward=-14.38 +/- 30.55
Episode length: 48.60 +/- 16.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.6         |
|    mean_reward          | -14.4        |
| time/                   |              |
|    total_timesteps      | 63500        |
| train/                  |              |
|    approx_kl            | 0.0072267908 |
|    clip_fraction        | 0.0801       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.203        |
|    learning_rate        | 0.001        |
|    loss                 | 134          |
|    n_updates            | 39           |
|    policy_gradient_loss | -0.0107      |
|    value_loss           | 215          |
------------------------------------------
Eval num_timesteps=64000, episode_reward=-18.95 +/- 23.29
Episode length: 51.16 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -19      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-20.15 +/- 20.06
Episode length: 48.32 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -20.2    |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-18.67 +/- 23.71
Episode length: 49.78 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-22.93 +/- 14.38
Episode length: 50.82 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -22.9    |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 93.3     |
| time/              |          |
|    fps             | 209      |
|    iterations      | 32       |
|    time_elapsed    | 313      |
|    total_timesteps | 65536    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=66000, episode_reward=-18.53 +/- 24.61
Episode length: 50.10 +/- 19.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.1         |
|    mean_reward          | -18.5        |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0077845915 |
|    clip_fraction        | 0.0794       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.124        |
|    learning_rate        | 0.001        |
|    loss                 | 99.3         |
|    n_updates            | 40           |
|    policy_gradient_loss | 0.00302      |
|    value_loss           | 227          |
------------------------------------------
Eval num_timesteps=66500, episode_reward=-18.97 +/- 24.32
Episode length: 50.84 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -19      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-24.59 +/- 5.41
Episode length: 50.62 +/- 21.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -24.6    |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-21.69 +/- 14.48
Episode length: 46.44 +/- 14.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -21.7    |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 91.1     |
| time/              |          |
|    fps             | 210      |
|    iterations      | 33       |
|    time_elapsed    | 321      |
|    total_timesteps | 67584    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=68000, episode_reward=-20.88 +/- 20.32
Episode length: 50.52 +/- 16.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.5        |
|    mean_reward          | -20.9       |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.014699836 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.167       |
|    learning_rate        | 0.001       |
|    loss                 | 96.6        |
|    n_updates            | 41          |
|    policy_gradient_loss | 0.00823     |
|    value_loss           | 199         |
-----------------------------------------
Eval num_timesteps=68500, episode_reward=-21.13 +/- 20.40
Episode length: 51.72 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -21.1    |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-17.43 +/- 23.67
Episode length: 45.46 +/- 14.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -17.4    |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-25.01 +/- 5.57
Episode length: 51.32 +/- 20.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -25      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 92       |
| time/              |          |
|    fps             | 210      |
|    iterations      | 34       |
|    time_elapsed    | 330      |
|    total_timesteps | 69632    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=70000, episode_reward=-18.49 +/- 24.85
Episode length: 49.30 +/- 19.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | -18.5       |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.019826917 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.001       |
|    loss                 | 113         |
|    n_updates            | 42          |
|    policy_gradient_loss | 0.00765     |
|    value_loss           | 186         |
-----------------------------------------
Eval num_timesteps=70500, episode_reward=-14.85 +/- 30.74
Episode length: 51.50 +/- 22.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -14.9    |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-16.79 +/- 33.80
Episode length: 50.56 +/- 18.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -16.8    |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-18.69 +/- 24.09
Episode length: 50.00 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 94.5     |
| time/              |          |
|    fps             | 211      |
|    iterations      | 35       |
|    time_elapsed    | 338      |
|    total_timesteps | 71680    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=72000, episode_reward=-14.15 +/- 29.26
Episode length: 48.34 +/- 18.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.3        |
|    mean_reward          | -14.2       |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.009255004 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.25        |
|    learning_rate        | 0.001       |
|    loss                 | 148         |
|    n_updates            | 43          |
|    policy_gradient_loss | 0.0141      |
|    value_loss           | 254         |
-----------------------------------------
Eval num_timesteps=72500, episode_reward=-21.03 +/- 20.21
Episode length: 52.42 +/- 22.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -21      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-9.47 +/- 33.55
Episode length: 45.38 +/- 16.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -9.47    |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-18.75 +/- 22.52
Episode length: 50.20 +/- 14.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -18.8    |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 65.3     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 36       |
|    time_elapsed    | 347      |
|    total_timesteps | 73728    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=74000, episode_reward=-14.64 +/- 29.81
Episode length: 49.68 +/- 17.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.7        |
|    mean_reward          | -14.6       |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.009122331 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.0331      |
|    learning_rate        | 0.001       |
|    loss                 | 162         |
|    n_updates            | 44          |
|    policy_gradient_loss | -0.00647    |
|    value_loss           | 364         |
-----------------------------------------
Eval num_timesteps=74500, episode_reward=-17.95 +/- 24.27
Episode length: 47.30 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -18      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-16.13 +/- 26.99
Episode length: 47.70 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -16.1    |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-16.76 +/- 27.40
Episode length: 50.14 +/- 12.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -16.8    |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 68       |
| time/              |          |
|    fps             | 213      |
|    iterations      | 37       |
|    time_elapsed    | 355      |
|    total_timesteps | 75776    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=76000, episode_reward=-16.87 +/- 27.63
Episode length: 50.40 +/- 13.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.4        |
|    mean_reward          | -16.9       |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.009732708 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.001       |
|    loss                 | 147         |
|    n_updates            | 45          |
|    policy_gradient_loss | 0.00206     |
|    value_loss           | 164         |
-----------------------------------------
Eval num_timesteps=76500, episode_reward=-14.62 +/- 30.05
Episode length: 50.18 +/- 17.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -14.6    |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-21.59 +/- 18.78
Episode length: 53.36 +/- 16.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -21.6    |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-22.03 +/- 15.40
Episode length: 47.28 +/- 19.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -22      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 67.5     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 38       |
|    time_elapsed    | 364      |
|    total_timesteps | 77824    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=78000, episode_reward=-25.41 +/- 4.43
Episode length: 52.64 +/- 16.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.6         |
|    mean_reward          | -25.4        |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0131390365 |
|    clip_fraction        | 0.201        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.248        |
|    learning_rate        | 0.001        |
|    loss                 | 71.9         |
|    n_updates            | 46           |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 178          |
------------------------------------------
Eval num_timesteps=78500, episode_reward=-14.79 +/- 28.79
Episode length: 50.52 +/- 14.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -14.8    |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-16.41 +/- 26.76
Episode length: 48.92 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -16.4    |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-17.00 +/- 27.47
Episode length: 51.18 +/- 16.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -17      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 64.5     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 39       |
|    time_elapsed    | 372      |
|    total_timesteps | 79872    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=80000, episode_reward=-14.11 +/- 29.56
Episode length: 47.74 +/- 15.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.7        |
|    mean_reward          | -14.1       |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.010608029 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.227       |
|    learning_rate        | 0.001       |
|    loss                 | 92.7        |
|    n_updates            | 47          |
|    policy_gradient_loss | 0.0229      |
|    value_loss           | 194         |
-----------------------------------------
Eval num_timesteps=80500, episode_reward=-19.03 +/- 22.21
Episode length: 51.62 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -19      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-23.78 +/- 15.33
Episode length: 54.54 +/- 19.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-7.29 +/- 43.36
Episode length: 52.50 +/- 19.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -7.29    |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 68.6     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 40       |
|    time_elapsed    | 381      |
|    total_timesteps | 81920    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=82000, episode_reward=-16.09 +/- 29.69
Episode length: 55.18 +/- 18.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.2        |
|    mean_reward          | -16.1       |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.005918892 |
|    clip_fraction        | 0.0398      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.153       |
|    learning_rate        | 0.001       |
|    loss                 | 32.4        |
|    n_updates            | 48          |
|    policy_gradient_loss | 0.000917    |
|    value_loss           | 185         |
-----------------------------------------
Eval num_timesteps=82500, episode_reward=-20.27 +/- 20.35
Episode length: 48.46 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -20.3    |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-20.73 +/- 20.29
Episode length: 50.24 +/- 14.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -20.7    |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-16.65 +/- 27.01
Episode length: 49.88 +/- 14.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -16.7    |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 76.2     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 41       |
|    time_elapsed    | 390      |
|    total_timesteps | 83968    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=84000, episode_reward=50.04 +/- 47.60
Episode length: 125.84 +/- 88.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 50          |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.007747935 |
|    clip_fraction        | 0.0925      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.0965      |
|    learning_rate        | 0.001       |
|    loss                 | 236         |
|    n_updates            | 49          |
|    policy_gradient_loss | 0.0049      |
|    value_loss           | 245         |
-----------------------------------------
Eval num_timesteps=84500, episode_reward=38.28 +/- 44.61
Episode length: 139.18 +/- 98.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 38.3     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=40.04 +/- 39.29
Episode length: 120.74 +/- 79.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 40       |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=49.26 +/- 43.09
Episode length: 146.60 +/- 78.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 147      |
|    mean_reward     | 49.3     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=49.78 +/- 38.58
Episode length: 133.44 +/- 94.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 49.8     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 78.2     |
| time/              |          |
|    fps             | 207      |
|    iterations      | 42       |
|    time_elapsed    | 414      |
|    total_timesteps | 86016    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=86500, episode_reward=-24.51 +/- 4.86
Episode length: 49.78 +/- 16.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.8        |
|    mean_reward          | -24.5       |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.004320196 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.127       |
|    learning_rate        | 0.001       |
|    loss                 | 67.9        |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00345     |
|    value_loss           | 165         |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=-23.31 +/- 13.28
Episode length: 53.08 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -23.3    |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-20.60 +/- 21.88
Episode length: 58.46 +/- 23.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.5     |
|    mean_reward     | -20.6    |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-26.11 +/- 5.25
Episode length: 55.30 +/- 18.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | -26.1    |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 80.2     |
| time/              |          |
|    fps             | 207      |
|    iterations      | 43       |
|    time_elapsed    | 423      |
|    total_timesteps | 88064    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=88500, episode_reward=62.42 +/- 0.14
Episode length: 208.72 +/- 75.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 62.4        |
| time/                   |             |
|    total_timesteps      | 88500       |
| train/                  |             |
|    approx_kl            | 0.008439296 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.141       |
|    learning_rate        | 0.001       |
|    loss                 | 95.8        |
|    n_updates            | 51          |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 136         |
-----------------------------------------
Eval num_timesteps=89000, episode_reward=62.86 +/- 1.51
Episode length: 184.08 +/- 64.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | 62.9     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=62.42 +/- 0.14
Episode length: 207.24 +/- 71.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | 62.4     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=62.46 +/- 0.42
Episode length: 180.22 +/- 53.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 62.5     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 81.2     |
| time/              |          |
|    fps             | 199      |
|    iterations      | 44       |
|    time_elapsed    | 451      |
|    total_timesteps | 90112    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=90500, episode_reward=-24.45 +/- 4.16
Episode length: 49.64 +/- 14.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.6         |
|    mean_reward          | -24.5        |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0068425736 |
|    clip_fraction        | 0.0674       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.001        |
|    loss                 | 53.1         |
|    n_updates            | 52           |
|    policy_gradient_loss | -0.000792    |
|    value_loss           | 121          |
------------------------------------------
Eval num_timesteps=91000, episode_reward=-24.37 +/- 4.65
Episode length: 49.12 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -24.4    |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-24.14 +/- 4.69
Episode length: 48.48 +/- 16.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -24.1    |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-22.25 +/- 12.96
Episode length: 54.38 +/- 44.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | -22.3    |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 80.6     |
| time/              |          |
|    fps             | 200      |
|    iterations      | 45       |
|    time_elapsed    | 460      |
|    total_timesteps | 92160    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=92500, episode_reward=-9.73 +/- 13.88
Episode length: 56.32 +/- 38.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 56.3       |
|    mean_reward          | -9.73      |
| time/                   |            |
|    total_timesteps      | 92500      |
| train/                  |            |
|    approx_kl            | 0.00430568 |
|    clip_fraction        | 0.0437     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.158      |
|    learning_rate        | 0.001      |
|    loss                 | 91.9       |
|    n_updates            | 53         |
|    policy_gradient_loss | 0.00483    |
|    value_loss           | 176        |
----------------------------------------
Eval num_timesteps=93000, episode_reward=-9.77 +/- 13.89
Episode length: 49.96 +/- 31.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -9.77    |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-5.91 +/- 23.50
Episode length: 53.36 +/- 36.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -5.91    |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-5.81 +/- 23.55
Episode length: 58.64 +/- 33.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.6     |
|    mean_reward     | -5.81    |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 88.2     |
| time/              |          |
|    fps             | 200      |
|    iterations      | 46       |
|    time_elapsed    | 469      |
|    total_timesteps | 94208    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=94500, episode_reward=183.99 +/- 36.43
Episode length: 483.08 +/- 114.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 483          |
|    mean_reward          | 184          |
| time/                   |              |
|    total_timesteps      | 94500        |
| train/                  |              |
|    approx_kl            | 0.0056507546 |
|    clip_fraction        | 0.0632       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.126        |
|    learning_rate        | 0.001        |
|    loss                 | 61.5         |
|    n_updates            | 54           |
|    policy_gradient_loss | 0.0026       |
|    value_loss           | 146          |
------------------------------------------
New best mean reward!
Eval num_timesteps=95000, episode_reward=184.11 +/- 36.22
Episode length: 480.98 +/- 121.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 481      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
New best mean reward!
Eval num_timesteps=95500, episode_reward=181.71 +/- 38.88
Episode length: 470.56 +/- 135.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=179.60 +/- 41.40
Episode length: 469.04 +/- 131.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 93.3     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 47       |
|    time_elapsed    | 533      |
|    total_timesteps | 96256    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=96500, episode_reward=153.30 +/- 70.56
Episode length: 382.30 +/- 218.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 382         |
|    mean_reward          | 153         |
| time/                   |             |
|    total_timesteps      | 96500       |
| train/                  |             |
|    approx_kl            | 0.007272183 |
|    clip_fraction        | 0.0512      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.001       |
|    loss                 | 57.5        |
|    n_updates            | 55          |
|    policy_gradient_loss | -0.00215    |
|    value_loss           | 161         |
-----------------------------------------
Eval num_timesteps=97000, episode_reward=142.84 +/- 80.08
Episode length: 365.20 +/- 222.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 365      |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=155.19 +/- 70.06
Episode length: 395.54 +/- 207.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 396      |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=150.57 +/- 64.76
Episode length: 356.92 +/- 224.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 357      |
|    mean_reward     | 151      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | 95.9     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 48       |
|    time_elapsed    | 584      |
|    total_timesteps | 98304    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=98500, episode_reward=-11.60 +/- 0.00
Episode length: 49.76 +/- 18.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.8        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 98500       |
| train/                  |             |
|    approx_kl            | 0.015634334 |
|    clip_fraction        | 0.0942      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.2         |
|    learning_rate        | 0.001       |
|    loss                 | 33.2        |
|    n_updates            | 57          |
|    policy_gradient_loss | 0.0103      |
|    value_loss           | 88.6        |
-----------------------------------------
Eval num_timesteps=99000, episode_reward=-11.59 +/- 0.01
Episode length: 48.74 +/- 15.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-11.59 +/- 0.00
Episode length: 49.78 +/- 14.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-11.59 +/- 0.01
Episode length: 50.96 +/- 18.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 263      |
|    ep_rew_mean     | 100      |
| time/              |          |
|    fps             | 169      |
|    iterations      | 49       |
|    time_elapsed    | 593      |
|    total_timesteps | 100352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=100500, episode_reward=-11.59 +/- 0.00
Episode length: 50.74 +/- 19.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.7         |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 100500       |
| train/                  |              |
|    approx_kl            | 0.0079543125 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.216        |
|    learning_rate        | 0.001        |
|    loss                 | 105          |
|    n_updates            | 58           |
|    policy_gradient_loss | 0.0044       |
|    value_loss           | 153          |
------------------------------------------
Eval num_timesteps=101000, episode_reward=-11.59 +/- 0.00
Episode length: 53.00 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-11.59 +/- 0.00
Episode length: 50.92 +/- 19.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-11.60 +/- 0.00
Episode length: 51.56 +/- 18.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 241      |
|    ep_rew_mean     | 94.5     |
| time/              |          |
|    fps             | 170      |
|    iterations      | 50       |
|    time_elapsed    | 601      |
|    total_timesteps | 102400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=102500, episode_reward=-11.59 +/- 0.00
Episode length: 50.86 +/- 19.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.9        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.009291999 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.001       |
|    loss                 | 127         |
|    n_updates            | 59          |
|    policy_gradient_loss | 0.00887     |
|    value_loss           | 252         |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=-11.60 +/- 0.00
Episode length: 56.00 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-11.59 +/- 0.00
Episode length: 52.78 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-11.59 +/- 0.00
Episode length: 52.24 +/- 16.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 68.8     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 51       |
|    time_elapsed    | 610      |
|    total_timesteps | 104448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=104500, episode_reward=-11.59 +/- 0.00
Episode length: 48.40 +/- 13.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.4        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 104500      |
| train/                  |             |
|    approx_kl            | 0.008271269 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.001       |
|    loss                 | 165         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00579    |
|    value_loss           | 234         |
-----------------------------------------
Eval num_timesteps=105000, episode_reward=-11.59 +/- 0.01
Episode length: 51.82 +/- 17.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-11.59 +/- 0.00
Episode length: 49.34 +/- 14.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-11.60 +/- 0.00
Episode length: 51.66 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 59.5     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 52       |
|    time_elapsed    | 619      |
|    total_timesteps | 106496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=106500, episode_reward=-11.59 +/- 0.01
Episode length: 49.76 +/- 17.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.8        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 106500      |
| train/                  |             |
|    approx_kl            | 0.008198695 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.001       |
|    loss                 | 79          |
|    n_updates            | 61          |
|    policy_gradient_loss | 0.00403     |
|    value_loss           | 168         |
-----------------------------------------
Eval num_timesteps=107000, episode_reward=-11.59 +/- 0.00
Episode length: 50.14 +/- 17.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-11.59 +/- 0.00
Episode length: 48.56 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-11.59 +/- 0.00
Episode length: 50.40 +/- 16.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-11.60 +/- 0.00
Episode length: 50.96 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 59.8     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 53       |
|    time_elapsed    | 629      |
|    total_timesteps | 108544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=109000, episode_reward=-11.59 +/- 0.00
Episode length: 50.58 +/- 19.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.6         |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0064181965 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.266        |
|    learning_rate        | 0.001        |
|    loss                 | 33.6         |
|    n_updates            | 62           |
|    policy_gradient_loss | 0.0056       |
|    value_loss           | 118          |
------------------------------------------
Eval num_timesteps=109500, episode_reward=-11.59 +/- 0.00
Episode length: 49.90 +/- 14.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-11.60 +/- 0.00
Episode length: 53.08 +/- 18.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-11.60 +/- 0.00
Episode length: 52.30 +/- 18.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 57.3     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 54       |
|    time_elapsed    | 638      |
|    total_timesteps | 110592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=111000, episode_reward=-6.01 +/- 22.10
Episode length: 53.98 +/- 23.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54           |
|    mean_reward          | -6.01        |
| time/                   |              |
|    total_timesteps      | 111000       |
| train/                  |              |
|    approx_kl            | 0.0065502403 |
|    clip_fraction        | 0.0848       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.215        |
|    learning_rate        | 0.001        |
|    loss                 | 117          |
|    n_updates            | 63           |
|    policy_gradient_loss | -0.00506     |
|    value_loss           | 147          |
------------------------------------------
Eval num_timesteps=111500, episode_reward=3.21 +/- 33.97
Episode length: 52.58 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | 3.21     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-3.89 +/- 26.12
Episode length: 47.84 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -3.89    |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-7.95 +/- 17.89
Episode length: 47.90 +/- 18.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -7.95    |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 56.4     |
| time/              |          |
|    fps             | 174      |
|    iterations      | 55       |
|    time_elapsed    | 646      |
|    total_timesteps | 112640   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=113000, episode_reward=54.51 +/- 28.78
Episode length: 128.30 +/- 70.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 54.5        |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.014111738 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.146       |
|    learning_rate        | 0.001       |
|    loss                 | 111         |
|    n_updates            | 65          |
|    policy_gradient_loss | 0.000174    |
|    value_loss           | 199         |
-----------------------------------------
Eval num_timesteps=113500, episode_reward=55.86 +/- 32.64
Episode length: 140.62 +/- 80.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 141      |
|    mean_reward     | 55.9     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=48.80 +/- 34.43
Episode length: 131.04 +/- 84.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 48.8     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=59.54 +/- 21.15
Episode length: 131.42 +/- 64.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 59.5     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 58.6     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 56       |
|    time_elapsed    | 666      |
|    total_timesteps | 114688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=115000, episode_reward=45.57 +/- 43.37
Episode length: 55.46 +/- 19.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 55.5         |
|    mean_reward          | 45.6         |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 0.0063854386 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.001        |
|    loss                 | 30           |
|    n_updates            | 66           |
|    policy_gradient_loss | 0.00329      |
|    value_loss           | 123          |
------------------------------------------
Eval num_timesteps=115500, episode_reward=54.99 +/- 57.56
Episode length: 103.02 +/- 142.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 55       |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=36.51 +/- 45.47
Episode length: 53.78 +/- 24.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | 36.5     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=31.33 +/- 45.88
Episode length: 52.82 +/- 24.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | 31.3     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 64.9     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 57       |
|    time_elapsed    | 677      |
|    total_timesteps | 116736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=117000, episode_reward=-9.98 +/- 13.92
Episode length: 53.60 +/- 24.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.6        |
|    mean_reward          | -9.98       |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.009003673 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.142       |
|    learning_rate        | 0.001       |
|    loss                 | 117         |
|    n_updates            | 67          |
|    policy_gradient_loss | 0.0199      |
|    value_loss           | 267         |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=-6.29 +/- 23.26
Episode length: 56.58 +/- 29.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.6     |
|    mean_reward     | -6.29    |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-8.12 +/- 19.40
Episode length: 56.12 +/- 28.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.1     |
|    mean_reward     | -8.12    |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-6.19 +/- 23.40
Episode length: 51.66 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -6.19    |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 68.2     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 58       |
|    time_elapsed    | 686      |
|    total_timesteps | 118784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=119000, episode_reward=143.12 +/- 52.59
Episode length: 395.24 +/- 208.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 395         |
|    mean_reward          | 143         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.008831098 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.001       |
|    loss                 | 62.6        |
|    n_updates            | 68          |
|    policy_gradient_loss | 0.00635     |
|    value_loss           | 178         |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=138.43 +/- 62.94
Episode length: 355.62 +/- 226.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 356      |
|    mean_reward     | 138      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=139.68 +/- 65.84
Episode length: 385.72 +/- 214.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 386      |
|    mean_reward     | 140      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=160.99 +/- 63.90
Episode length: 433.98 +/- 183.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 434      |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 74.8     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 59       |
|    time_elapsed    | 740      |
|    total_timesteps | 120832   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=121000, episode_reward=71.55 +/- 13.80
Episode length: 60.80 +/- 22.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60.8        |
|    mean_reward          | 71.5        |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.008042538 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.22        |
|    learning_rate        | 0.001       |
|    loss                 | 34.5        |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 108         |
-----------------------------------------
Eval num_timesteps=121500, episode_reward=64.37 +/- 26.63
Episode length: 65.38 +/- 22.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.4     |
|    mean_reward     | 64.4     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=63.83 +/- 29.20
Episode length: 60.12 +/- 20.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.1     |
|    mean_reward     | 63.8     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=72.31 +/- 14.03
Episode length: 56.94 +/- 18.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.9     |
|    mean_reward     | 72.3     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 81.2     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 60       |
|    time_elapsed    | 750      |
|    total_timesteps | 122880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=123000, episode_reward=72.45 +/- 5.28
Episode length: 62.68 +/- 22.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 62.7        |
|    mean_reward          | 72.4        |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.008269974 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.237       |
|    learning_rate        | 0.001       |
|    loss                 | 60.3        |
|    n_updates            | 71          |
|    policy_gradient_loss | 0.00308     |
|    value_loss           | 214         |
-----------------------------------------
Eval num_timesteps=123500, episode_reward=72.49 +/- 5.85
Episode length: 62.66 +/- 25.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.7     |
|    mean_reward     | 72.5     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=71.25 +/- 14.43
Episode length: 58.80 +/- 19.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.8     |
|    mean_reward     | 71.2     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=72.59 +/- 4.86
Episode length: 61.36 +/- 20.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.4     |
|    mean_reward     | 72.6     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 86.7     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 61       |
|    time_elapsed    | 760      |
|    total_timesteps | 124928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=125000, episode_reward=70.31 +/- 14.00
Episode length: 65.12 +/- 19.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 65.1        |
|    mean_reward          | 70.3        |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.009474729 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.001       |
|    loss                 | 133         |
|    n_updates            | 72          |
|    policy_gradient_loss | 0.00423     |
|    value_loss           | 192         |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=71.43 +/- 5.11
Episode length: 67.88 +/- 20.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.9     |
|    mean_reward     | 71.4     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=71.65 +/- 14.85
Episode length: 61.02 +/- 24.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61       |
|    mean_reward     | 71.6     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=70.77 +/- 15.41
Episode length: 62.86 +/- 20.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.9     |
|    mean_reward     | 70.8     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 92.6     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 62       |
|    time_elapsed    | 770      |
|    total_timesteps | 126976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=127000, episode_reward=70.51 +/- 15.82
Episode length: 62.44 +/- 21.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 62.4         |
|    mean_reward          | 70.5         |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 0.0075843753 |
|    clip_fraction        | 0.0916       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.377        |
|    learning_rate        | 0.001        |
|    loss                 | 98.4         |
|    n_updates            | 73           |
|    policy_gradient_loss | 0.0101       |
|    value_loss           | 216          |
------------------------------------------
Eval num_timesteps=127500, episode_reward=68.81 +/- 20.21
Episode length: 61.44 +/- 21.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.4     |
|    mean_reward     | 68.8     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=70.65 +/- 6.09
Episode length: 69.36 +/- 23.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.4     |
|    mean_reward     | 70.6     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=72.13 +/- 5.87
Episode length: 64.34 +/- 23.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.3     |
|    mean_reward     | 72.1     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=68.27 +/- 28.09
Episode length: 62.90 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.9     |
|    mean_reward     | 68.3     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 95.6     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 63       |
|    time_elapsed    | 783      |
|    total_timesteps | 129024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=129500, episode_reward=23.94 +/- 53.44
Episode length: 57.80 +/- 28.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 57.8        |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.007618451 |
|    clip_fraction        | 0.0885      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.001       |
|    loss                 | 33.3        |
|    n_updates            | 74          |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 119         |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=15.17 +/- 48.40
Episode length: 50.52 +/- 13.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | 15.2     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-1.01 +/- 41.78
Episode length: 51.42 +/- 18.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -1.01    |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-2.61 +/- 40.53
Episode length: 50.78 +/- 23.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -2.61    |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 100      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 64       |
|    time_elapsed    | 792      |
|    total_timesteps | 131072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=131500, episode_reward=-22.65 +/- 14.63
Episode length: 49.86 +/- 16.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.9        |
|    mean_reward          | -22.6       |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.009698048 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.001       |
|    loss                 | 63.7        |
|    n_updates            | 75          |
|    policy_gradient_loss | 0.00394     |
|    value_loss           | 164         |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=-16.48 +/- 28.02
Episode length: 49.82 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -16.5    |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-13.11 +/- 29.56
Episode length: 44.42 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | -13.1    |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-23.55 +/- 14.47
Episode length: 53.16 +/- 19.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -23.6    |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 94       |
| time/              |          |
|    fps             | 166      |
|    iterations      | 65       |
|    time_elapsed    | 800      |
|    total_timesteps | 133120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=133500, episode_reward=-14.09 +/- 32.31
Episode length: 54.94 +/- 17.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.9        |
|    mean_reward          | -14.1       |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.006571318 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.001       |
|    loss                 | 142         |
|    n_updates            | 76          |
|    policy_gradient_loss | 0.00495     |
|    value_loss           | 194         |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=-17.85 +/- 26.92
Episode length: 54.66 +/- 18.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -17.9    |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-15.83 +/- 30.89
Episode length: 54.32 +/- 18.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -15.8    |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-20.70 +/- 21.00
Episode length: 49.90 +/- 18.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -20.7    |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 81       |
| time/              |          |
|    fps             | 166      |
|    iterations      | 66       |
|    time_elapsed    | 809      |
|    total_timesteps | 135168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=135500, episode_reward=25.60 +/- 53.02
Episode length: 56.82 +/- 21.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.8        |
|    mean_reward          | 25.6        |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.010449016 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.001       |
|    loss                 | 170         |
|    n_updates            | 77          |
|    policy_gradient_loss | 0.00838     |
|    value_loss           | 264         |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=28.63 +/- 57.42
Episode length: 54.24 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=25.88 +/- 50.00
Episode length: 54.00 +/- 20.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | 25.9     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=16.21 +/- 49.32
Episode length: 54.96 +/- 27.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | 16.2     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 74.8     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 67       |
|    time_elapsed    | 819      |
|    total_timesteps | 137216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=137500, episode_reward=11.99 +/- 47.90
Episode length: 55.02 +/- 18.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55          |
|    mean_reward          | 12          |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.009661513 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.001       |
|    loss                 | 138         |
|    n_updates            | 78          |
|    policy_gradient_loss | 0.00262     |
|    value_loss           | 247         |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=28.59 +/- 53.79
Episode length: 51.88 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=11.81 +/- 48.13
Episode length: 51.98 +/- 16.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | 11.8     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=14.63 +/- 49.21
Episode length: 53.12 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 72.2     |
| time/              |          |
|    fps             | 168      |
|    iterations      | 68       |
|    time_elapsed    | 828      |
|    total_timesteps | 139264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=139500, episode_reward=126.71 +/- 63.00
Episode length: 272.30 +/- 234.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 127          |
| time/                   |              |
|    total_timesteps      | 139500       |
| train/                  |              |
|    approx_kl            | 0.0057823635 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.471        |
|    learning_rate        | 0.001        |
|    loss                 | 118          |
|    n_updates            | 79           |
|    policy_gradient_loss | 0.012        |
|    value_loss           | 184          |
------------------------------------------
Eval num_timesteps=140000, episode_reward=124.30 +/- 58.84
Episode length: 266.52 +/- 229.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=119.47 +/- 61.02
Episode length: 249.96 +/- 225.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 119      |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=114.53 +/- 57.48
Episode length: 220.74 +/- 219.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 115      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 70.4     |
| time/              |          |
|    fps             | 163      |
|    iterations      | 69       |
|    time_elapsed    | 863      |
|    total_timesteps | 141312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=141500, episode_reward=116.68 +/- 52.15
Episode length: 263.58 +/- 147.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 264         |
|    mean_reward          | 117         |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.006509525 |
|    clip_fraction        | 0.0654      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.001       |
|    loss                 | 124         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00448     |
|    value_loss           | 175         |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=126.32 +/- 61.78
Episode length: 265.98 +/- 137.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 126      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=120.06 +/- 55.28
Episode length: 242.42 +/- 137.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 120      |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=121.26 +/- 52.30
Episode length: 286.00 +/- 153.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 121      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 73.4     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 70       |
|    time_elapsed    | 900      |
|    total_timesteps | 143360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=143500, episode_reward=71.82 +/- 32.73
Episode length: 172.14 +/- 61.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 172          |
|    mean_reward          | 71.8         |
| time/                   |              |
|    total_timesteps      | 143500       |
| train/                  |              |
|    approx_kl            | 0.0069522704 |
|    clip_fraction        | 0.0781       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.001        |
|    loss                 | 70           |
|    n_updates            | 81           |
|    policy_gradient_loss | 0.00717      |
|    value_loss           | 154          |
------------------------------------------
Eval num_timesteps=144000, episode_reward=72.10 +/- 32.69
Episode length: 187.02 +/- 72.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | 72.1     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=75.34 +/- 37.96
Episode length: 196.08 +/- 71.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 196      |
|    mean_reward     | 75.3     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=75.10 +/- 32.30
Episode length: 209.22 +/- 77.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 75.1     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 77.3     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 71       |
|    time_elapsed    | 927      |
|    total_timesteps | 145408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=145500, episode_reward=0.34 +/- 34.17
Episode length: 65.86 +/- 50.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 65.9         |
|    mean_reward          | 0.345        |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 0.0065403813 |
|    clip_fraction        | 0.0792       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.273        |
|    learning_rate        | 0.001        |
|    loss                 | 28.6         |
|    n_updates            | 82           |
|    policy_gradient_loss | 0.0152       |
|    value_loss           | 131          |
------------------------------------------
Eval num_timesteps=146000, episode_reward=4.60 +/- 37.52
Episode length: 70.44 +/- 48.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.4     |
|    mean_reward     | 4.6      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=10.42 +/- 42.20
Episode length: 70.64 +/- 51.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.6     |
|    mean_reward     | 10.4     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=8.34 +/- 41.04
Episode length: 79.48 +/- 66.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 8.34     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 83.3     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 72       |
|    time_elapsed    | 939      |
|    total_timesteps | 147456   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=147500, episode_reward=54.20 +/- 33.38
Episode length: 143.04 +/- 93.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | 54.2        |
| time/                   |             |
|    total_timesteps      | 147500      |
| train/                  |             |
|    approx_kl            | 0.013838377 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.001       |
|    loss                 | 44          |
|    n_updates            | 84          |
|    policy_gradient_loss | 0.00152     |
|    value_loss           | 146         |
-----------------------------------------
Eval num_timesteps=148000, episode_reward=54.64 +/- 33.35
Episode length: 142.76 +/- 87.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 143      |
|    mean_reward     | 54.6     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=50.18 +/- 37.67
Episode length: 125.02 +/- 76.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 50.2     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=60.08 +/- 29.04
Episode length: 133.96 +/- 72.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 60.1     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=58.40 +/- 32.96
Episode length: 110.06 +/- 62.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 58.4     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 87       |
| time/              |          |
|    fps             | 155      |
|    iterations      | 73       |
|    time_elapsed    | 963      |
|    total_timesteps | 149504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=150000, episode_reward=77.19 +/- 32.63
Episode length: 202.94 +/- 110.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 77.2        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.007426546 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.15        |
|    learning_rate        | 0.001       |
|    loss                 | 90          |
|    n_updates            | 85          |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 138         |
-----------------------------------------
Eval num_timesteps=150500, episode_reward=79.47 +/- 37.70
Episode length: 220.34 +/- 119.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 79.5     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=92.48 +/- 45.80
Episode length: 246.38 +/- 162.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 92.5     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=81.87 +/- 40.60
Episode length: 240.50 +/- 130.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 81.9     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 90.8     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 74       |
|    time_elapsed    | 995      |
|    total_timesteps | 151552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=152000, episode_reward=173.35 +/- 0.74
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 173          |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0084530655 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.001        |
|    loss                 | 106          |
|    n_updates            | 86           |
|    policy_gradient_loss | -0.0121      |
|    value_loss           | 136          |
------------------------------------------
Eval num_timesteps=152500, episode_reward=173.77 +/- 2.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=173.50 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=175.55 +/- 13.89
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 93.5     |
| time/              |          |
|    fps             | 144      |
|    iterations      | 75       |
|    time_elapsed    | 1066     |
|    total_timesteps | 153600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=154000, episode_reward=173.73 +/- 0.57
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 174         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.005822748 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.295       |
|    learning_rate        | 0.001       |
|    loss                 | 33.9        |
|    n_updates            | 87          |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 143         |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=171.57 +/- 15.60
Episode length: 517.48 +/- 52.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=173.89 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=169.32 +/- 21.83
Episode length: 515.10 +/- 48.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | 99.2     |
| time/              |          |
|    fps             | 136      |
|    iterations      | 76       |
|    time_elapsed    | 1136     |
|    total_timesteps | 155648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=156000, episode_reward=131.38 +/- 47.97
Episode length: 307.76 +/- 235.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 308         |
|    mean_reward          | 131         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.009673077 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.001       |
|    loss                 | 41.7        |
|    n_updates            | 88          |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=156500, episode_reward=121.12 +/- 52.70
Episode length: 280.92 +/- 234.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 121      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=111.47 +/- 54.67
Episode length: 244.16 +/- 229.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 111      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=111.68 +/- 53.79
Episode length: 243.14 +/- 231.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 112      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 101      |
| time/              |          |
|    fps             | 134      |
|    iterations      | 77       |
|    time_elapsed    | 1173     |
|    total_timesteps | 157696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=158000, episode_reward=173.99 +/- 0.10
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 174         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.014780998 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.001       |
|    loss                 | 97.8        |
|    n_updates            | 89          |
|    policy_gradient_loss | 0.00697     |
|    value_loss           | 161         |
-----------------------------------------
Eval num_timesteps=158500, episode_reward=171.71 +/- 15.62
Episode length: 517.52 +/- 52.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=171.71 +/- 15.62
Episode length: 522.18 +/- 19.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 522      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=173.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 128      |
|    iterations      | 78       |
|    time_elapsed    | 1244     |
|    total_timesteps | 159744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=160000, episode_reward=29.19 +/- 55.64
Episode length: 69.22 +/- 94.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 69.2        |
|    mean_reward          | 29.2        |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.010061518 |
|    clip_fraction        | 0.0755      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.001       |
|    loss                 | 121         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 155         |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=37.05 +/- 48.34
Episode length: 53.84 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | 37       |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=39.05 +/- 54.71
Episode length: 69.74 +/- 95.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.7     |
|    mean_reward     | 39       |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=25.30 +/- 48.59
Episode length: 52.08 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | 25.3     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | 109      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 79       |
|    time_elapsed    | 1254     |
|    total_timesteps | 161792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=162000, episode_reward=-1.98 +/- 35.61
Episode length: 52.02 +/- 18.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52         |
|    mean_reward          | -1.98      |
| time/                   |            |
|    total_timesteps      | 162000     |
| train/                  |            |
|    approx_kl            | 0.01308591 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.001      |
|    loss                 | 59         |
|    n_updates            | 91         |
|    policy_gradient_loss | -0.001     |
|    value_loss           | 168        |
----------------------------------------
Eval num_timesteps=162500, episode_reward=-5.84 +/- 23.23
Episode length: 46.32 +/- 19.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -5.84    |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-5.93 +/- 23.17
Episode length: 48.48 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -5.93    |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-7.77 +/- 19.34
Episode length: 48.96 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -7.77    |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 80       |
|    time_elapsed    | 1262     |
|    total_timesteps | 163840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=164000, episode_reward=-5.66 +/- 23.51
Episode length: 52.02 +/- 16.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | -5.66       |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.009032798 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.001       |
|    loss                 | 90.5        |
|    n_updates            | 92          |
|    policy_gradient_loss | 0.00565     |
|    value_loss           | 129         |
-----------------------------------------
Eval num_timesteps=164500, episode_reward=-11.61 +/- 0.14
Episode length: 51.02 +/- 14.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-11.65 +/- 0.24
Episode length: 45.58 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -11.7    |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-9.65 +/- 13.87
Episode length: 51.48 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -9.65    |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 286      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 130      |
|    iterations      | 81       |
|    time_elapsed    | 1270     |
|    total_timesteps | 165888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=166000, episode_reward=0.05 +/- 32.02
Episode length: 49.72 +/- 15.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.7         |
|    mean_reward          | 0.0454       |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0068331086 |
|    clip_fraction        | 0.0755       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.498        |
|    learning_rate        | 0.001        |
|    loss                 | 103          |
|    n_updates            | 93           |
|    policy_gradient_loss | 0.00307      |
|    value_loss           | 179          |
------------------------------------------
Eval num_timesteps=166500, episode_reward=8.11 +/- 39.50
Episode length: 50.16 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | 8.11     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=9.78 +/- 40.60
Episode length: 53.02 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | 9.78     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=5.86 +/- 37.64
Episode length: 51.94 +/- 23.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | 5.86     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 121      |
| time/              |          |
|    fps             | 131      |
|    iterations      | 82       |
|    time_elapsed    | 1279     |
|    total_timesteps | 167936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=168000, episode_reward=82.47 +/- 14.22
Episode length: 64.06 +/- 22.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 64.1        |
|    mean_reward          | 82.5        |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.007862444 |
|    clip_fraction        | 0.0456      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.001       |
|    loss                 | 128         |
|    n_updates            | 94          |
|    policy_gradient_loss | 0.0146      |
|    value_loss           | 169         |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=81.75 +/- 2.35
Episode length: 57.42 +/- 18.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.4     |
|    mean_reward     | 81.7     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=80.55 +/- 2.84
Episode length: 64.28 +/- 20.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.3     |
|    mean_reward     | 80.5     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=82.71 +/- 14.54
Episode length: 70.68 +/- 68.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.7     |
|    mean_reward     | 82.7     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 131      |
|    iterations      | 83       |
|    time_elapsed    | 1289     |
|    total_timesteps | 169984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=170000, episode_reward=72.89 +/- 13.55
Episode length: 68.36 +/- 27.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.4        |
|    mean_reward          | 72.9        |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.007684974 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.37        |
|    learning_rate        | 0.001       |
|    loss                 | 175         |
|    n_updates            | 95          |
|    policy_gradient_loss | 0.00181     |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=77.11 +/- 5.19
Episode length: 58.84 +/- 21.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.8     |
|    mean_reward     | 77.1     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=75.09 +/- 5.35
Episode length: 65.40 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.4     |
|    mean_reward     | 75.1     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=75.53 +/- 5.42
Episode length: 65.18 +/- 22.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.2     |
|    mean_reward     | 75.5     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=77.15 +/- 4.92
Episode length: 60.10 +/- 20.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.1     |
|    mean_reward     | 77.1     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 132      |
|    iterations      | 84       |
|    time_elapsed    | 1302     |
|    total_timesteps | 172032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=172500, episode_reward=175.07 +/- 32.92
Episode length: 482.78 +/- 126.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 483          |
|    mean_reward          | 175          |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0086975675 |
|    clip_fraction        | 0.0801       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.285        |
|    learning_rate        | 0.001        |
|    loss                 | 92.8         |
|    n_updates            | 96           |
|    policy_gradient_loss | 0.0122       |
|    value_loss           | 157          |
------------------------------------------
Eval num_timesteps=173000, episode_reward=183.51 +/- 23.82
Episode length: 509.66 +/- 75.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=167.75 +/- 42.71
Episode length: 460.90 +/- 138.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 461      |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=175.90 +/- 34.86
Episode length: 488.56 +/- 111.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 489      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 127      |
|    iterations      | 85       |
|    time_elapsed    | 1368     |
|    total_timesteps | 174080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=174500, episode_reward=83.63 +/- 68.14
Episode length: 201.62 +/- 212.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | 83.6         |
| time/                   |              |
|    total_timesteps      | 174500       |
| train/                  |              |
|    approx_kl            | 0.0053937645 |
|    clip_fraction        | 0.0729       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.272        |
|    learning_rate        | 0.001        |
|    loss                 | 83.6         |
|    n_updates            | 97           |
|    policy_gradient_loss | -0.00843     |
|    value_loss           | 83           |
------------------------------------------
Eval num_timesteps=175000, episode_reward=86.44 +/- 65.41
Episode length: 179.36 +/- 205.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 86.4     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=90.04 +/- 68.31
Episode length: 212.30 +/- 216.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 90       |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=81.11 +/- 73.64
Episode length: 205.36 +/- 219.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 81.1     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 126      |
|    iterations      | 86       |
|    time_elapsed    | 1397     |
|    total_timesteps | 176128   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=176500, episode_reward=189.68 +/- 7.89
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.015036229 |
|    clip_fraction        | 0.0793      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.44        |
|    learning_rate        | 0.001       |
|    loss                 | 132         |
|    n_updates            | 99          |
|    policy_gradient_loss | 0.00563     |
|    value_loss           | 133         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=177000, episode_reward=190.87 +/- 7.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
New best mean reward!
Eval num_timesteps=177500, episode_reward=188.85 +/- 8.02
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=188.60 +/- 7.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 284      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 121      |
|    iterations      | 87       |
|    time_elapsed    | 1469     |
|    total_timesteps | 178176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=178500, episode_reward=108.46 +/- 85.28
Episode length: 304.96 +/- 211.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 305         |
|    mean_reward          | 108         |
| time/                   |             |
|    total_timesteps      | 178500      |
| train/                  |             |
|    approx_kl            | 0.008405862 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.001       |
|    loss                 | 19          |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.00962     |
|    value_loss           | 69.7        |
-----------------------------------------
Eval num_timesteps=179000, episode_reward=112.55 +/- 84.15
Episode length: 321.78 +/- 210.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 322      |
|    mean_reward     | 113      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=112.56 +/- 88.35
Episode length: 278.84 +/- 207.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 113      |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=112.84 +/- 84.47
Episode length: 311.56 +/- 222.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 312      |
|    mean_reward     | 113      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 279      |
|    ep_rew_mean     | 109      |
| time/              |          |
|    fps             | 119      |
|    iterations      | 88       |
|    time_elapsed    | 1511     |
|    total_timesteps | 180224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=180500, episode_reward=53.64 +/- 26.96
Episode length: 189.66 +/- 73.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 190         |
|    mean_reward          | 53.6        |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.006453258 |
|    clip_fraction        | 0.0737      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.001       |
|    loss                 | 18.8        |
|    n_updates            | 101         |
|    policy_gradient_loss | 0.00808     |
|    value_loss           | 134         |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=57.56 +/- 27.89
Episode length: 199.32 +/- 90.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | 57.6     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=59.06 +/- 18.13
Episode length: 174.90 +/- 72.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 59.1     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=52.18 +/- 28.68
Episode length: 173.14 +/- 75.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 52.2     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 273      |
|    ep_rew_mean     | 107      |
| time/              |          |
|    fps             | 118      |
|    iterations      | 89       |
|    time_elapsed    | 1538     |
|    total_timesteps | 182272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=182500, episode_reward=174.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 174         |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.007225958 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.001       |
|    loss                 | 68.5        |
|    n_updates            | 102         |
|    policy_gradient_loss | 0.0124      |
|    value_loss           | 131         |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=173.94 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=173.94 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=173.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    fps             | 114      |
|    iterations      | 90       |
|    time_elapsed    | 1609     |
|    total_timesteps | 184320   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=184500, episode_reward=75.15 +/- 13.74
Episode length: 58.78 +/- 22.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 58.8       |
|    mean_reward          | 75.1       |
| time/                   |            |
|    total_timesteps      | 184500     |
| train/                  |            |
|    approx_kl            | 0.01838645 |
|    clip_fraction        | 0.0938     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.393      |
|    learning_rate        | 0.001      |
|    loss                 | 127        |
|    n_updates            | 104        |
|    policy_gradient_loss | 0.00151    |
|    value_loss           | 166        |
----------------------------------------
Eval num_timesteps=185000, episode_reward=74.15 +/- 14.06
Episode length: 61.42 +/- 19.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.4     |
|    mean_reward     | 74.1     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=72.01 +/- 6.08
Episode length: 62.80 +/- 23.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.8     |
|    mean_reward     | 72       |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=73.27 +/- 13.78
Episode length: 64.88 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.9     |
|    mean_reward     | 73.3     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 94.9     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 91       |
|    time_elapsed    | 1620     |
|    total_timesteps | 186368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=186500, episode_reward=-25.37 +/- 3.81
Episode length: 52.26 +/- 13.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.3        |
|    mean_reward          | -25.4       |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.008775053 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.468       |
|    learning_rate        | 0.001       |
|    loss                 | 124         |
|    n_updates            | 105         |
|    policy_gradient_loss | 0.00111     |
|    value_loss           | 295         |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=-22.93 +/- 13.78
Episode length: 50.64 +/- 15.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -22.9    |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-18.93 +/- 23.37
Episode length: 51.36 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -18.9    |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-20.63 +/- 19.62
Episode length: 49.70 +/- 17.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -20.6    |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 97.9     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 92       |
|    time_elapsed    | 1629     |
|    total_timesteps | 188416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=188500, episode_reward=-15.83 +/- 27.66
Episode length: 47.08 +/- 16.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.1         |
|    mean_reward          | -15.8        |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 0.0067179943 |
|    clip_fraction        | 0.0742       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.387        |
|    learning_rate        | 0.001        |
|    loss                 | 137          |
|    n_updates            | 106          |
|    policy_gradient_loss | 0.00664      |
|    value_loss           | 321          |
------------------------------------------
Eval num_timesteps=189000, episode_reward=-18.81 +/- 22.75
Episode length: 50.40 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -18.8    |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-24.45 +/- 4.79
Episode length: 49.38 +/- 18.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -24.5    |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-23.89 +/- 3.91
Episode length: 47.14 +/- 13.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -23.9    |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 92.6     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 93       |
|    time_elapsed    | 1638     |
|    total_timesteps | 190464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=190500, episode_reward=-14.60 +/- 30.81
Episode length: 49.34 +/- 17.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.3         |
|    mean_reward          | -14.6        |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 0.0071005537 |
|    clip_fraction        | 0.0781       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.001        |
|    loss                 | 119          |
|    n_updates            | 107          |
|    policy_gradient_loss | 0.0038       |
|    value_loss           | 268          |
------------------------------------------
Eval num_timesteps=191000, episode_reward=-19.54 +/- 22.44
Episode length: 53.66 +/- 19.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -19.5    |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-15.08 +/- 29.55
Episode length: 51.42 +/- 18.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -15.1    |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-12.65 +/- 32.43
Episode length: 49.72 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -12.7    |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-15.30 +/- 34.34
Episode length: 53.20 +/- 22.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -15.3    |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 86.9     |
| time/              |          |
|    fps             | 116      |
|    iterations      | 94       |
|    time_elapsed    | 1649     |
|    total_timesteps | 192512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=193000, episode_reward=-20.41 +/- 20.41
Episode length: 49.20 +/- 18.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -20.4       |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.009334046 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.386       |
|    learning_rate        | 0.001       |
|    loss                 | 203         |
|    n_updates            | 108         |
|    policy_gradient_loss | 0.00257     |
|    value_loss           | 393         |
-----------------------------------------
Eval num_timesteps=193500, episode_reward=-20.23 +/- 20.60
Episode length: 48.34 +/- 15.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -20.2    |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-21.50 +/- 20.48
Episode length: 52.90 +/- 18.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -21.5    |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-19.40 +/- 22.92
Episode length: 52.42 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -19.4    |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.6     |
|    ep_rew_mean     | 82.6     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 95       |
|    time_elapsed    | 1657     |
|    total_timesteps | 194560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=195000, episode_reward=-21.17 +/- 20.03
Episode length: 51.90 +/- 17.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.9        |
|    mean_reward          | -21.2       |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.003917395 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.001       |
|    loss                 | 93.4        |
|    n_updates            | 109         |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 183         |
-----------------------------------------
Eval num_timesteps=195500, episode_reward=-24.10 +/- 15.86
Episode length: 54.86 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.9     |
|    mean_reward     | -24.1    |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-20.72 +/- 20.63
Episode length: 50.06 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -20.7    |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-16.83 +/- 26.89
Episode length: 50.94 +/- 18.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -16.8    |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.3     |
|    ep_rew_mean     | 77.7     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 96       |
|    time_elapsed    | 1666     |
|    total_timesteps | 196608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=197000, episode_reward=-18.77 +/- 23.44
Episode length: 50.34 +/- 18.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.3        |
|    mean_reward          | -18.8       |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.009710539 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.001       |
|    loss                 | 164         |
|    n_updates            | 110         |
|    policy_gradient_loss | -2.27e-05   |
|    value_loss           | 334         |
-----------------------------------------
Eval num_timesteps=197500, episode_reward=-25.66 +/- 5.01
Episode length: 53.44 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -25.7    |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-23.14 +/- 13.44
Episode length: 52.06 +/- 19.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -23.1    |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-17.93 +/- 22.99
Episode length: 47.32 +/- 17.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -17.9    |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.1     |
|    ep_rew_mean     | 75       |
| time/              |          |
|    fps             | 118      |
|    iterations      | 97       |
|    time_elapsed    | 1675     |
|    total_timesteps | 198656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=199000, episode_reward=73.83 +/- 14.30
Episode length: 65.08 +/- 25.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 65.1        |
|    mean_reward          | 73.8        |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.007633069 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.001       |
|    loss                 | 135         |
|    n_updates            | 111         |
|    policy_gradient_loss | 0.00128     |
|    value_loss           | 217         |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=79.07 +/- 23.43
Episode length: 60.18 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.2     |
|    mean_reward     | 79.1     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=73.51 +/- 5.24
Episode length: 58.72 +/- 21.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.7     |
|    mean_reward     | 73.5     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=78.19 +/- 23.18
Episode length: 63.58 +/- 25.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.6     |
|    mean_reward     | 78.2     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92       |
|    ep_rew_mean     | 75.6     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 98       |
|    time_elapsed    | 1685     |
|    total_timesteps | 200704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=201000, episode_reward=71.75 +/- 5.01
Episode length: 63.72 +/- 19.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 63.7         |
|    mean_reward          | 71.7         |
| time/                   |              |
|    total_timesteps      | 201000       |
| train/                  |              |
|    approx_kl            | 0.0070736916 |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.475        |
|    learning_rate        | 0.001        |
|    loss                 | 35.2         |
|    n_updates            | 112          |
|    policy_gradient_loss | 0.00444      |
|    value_loss           | 141          |
------------------------------------------
Eval num_timesteps=201500, episode_reward=70.88 +/- 5.78
Episode length: 67.98 +/- 23.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68       |
|    mean_reward     | 70.9     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=72.07 +/- 5.27
Episode length: 64.06 +/- 22.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.1     |
|    mean_reward     | 72.1     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=71.05 +/- 5.23
Episode length: 67.42 +/- 22.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.4     |
|    mean_reward     | 71       |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | 75.8     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 99       |
|    time_elapsed    | 1696     |
|    total_timesteps | 202752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=203000, episode_reward=72.11 +/- 6.05
Episode length: 64.04 +/- 24.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 64          |
|    mean_reward          | 72.1        |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.008420236 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.001       |
|    loss                 | 97.3        |
|    n_updates            | 113         |
|    policy_gradient_loss | 0.00291     |
|    value_loss           | 167         |
-----------------------------------------
Eval num_timesteps=203500, episode_reward=72.51 +/- 5.90
Episode length: 62.38 +/- 25.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.4     |
|    mean_reward     | 72.5     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=72.21 +/- 5.92
Episode length: 64.60 +/- 26.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.6     |
|    mean_reward     | 72.2     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=73.35 +/- 6.29
Episode length: 59.12 +/- 24.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 73.3     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 83.1     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 100      |
|    time_elapsed    | 1706     |
|    total_timesteps | 204800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=205000, episode_reward=143.45 +/- 44.50
Episode length: 364.72 +/- 223.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 143          |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0119529795 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.542        |
|    learning_rate        | 0.001        |
|    loss                 | 92.3         |
|    n_updates            | 114          |
|    policy_gradient_loss | 0.00687      |
|    value_loss           | 155          |
------------------------------------------
Eval num_timesteps=205500, episode_reward=162.67 +/- 40.27
Episode length: 394.56 +/- 209.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 395      |
|    mean_reward     | 163      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=165.25 +/- 58.23
Episode length: 392.46 +/- 212.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 392      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=157.47 +/- 49.41
Episode length: 382.84 +/- 217.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 86.3     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 101      |
|    time_elapsed    | 1759     |
|    total_timesteps | 206848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=207000, episode_reward=174.95 +/- 21.23
Episode length: 505.88 +/- 93.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.011216047 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.001       |
|    loss                 | 101         |
|    n_updates            | 115         |
|    policy_gradient_loss | 0.00272     |
|    value_loss           | 116         |
-----------------------------------------
Eval num_timesteps=207500, episode_reward=176.68 +/- 34.49
Episode length: 504.94 +/- 98.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=181.44 +/- 15.19
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=173.81 +/- 19.45
Episode length: 505.04 +/- 97.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 90       |
| time/              |          |
|    fps             | 114      |
|    iterations      | 102      |
|    time_elapsed    | 1828     |
|    total_timesteps | 208896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=209000, episode_reward=183.58 +/- 30.18
Episode length: 495.56 +/- 116.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 496        |
|    mean_reward          | 184        |
| time/                   |            |
|    total_timesteps      | 209000     |
| train/                  |            |
|    approx_kl            | 0.00782746 |
|    clip_fraction        | 0.0993     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.527      |
|    learning_rate        | 0.001      |
|    loss                 | 67         |
|    n_updates            | 116        |
|    policy_gradient_loss | 0.0146     |
|    value_loss           | 131        |
----------------------------------------
Eval num_timesteps=209500, episode_reward=181.96 +/- 32.31
Episode length: 485.66 +/- 133.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=183.22 +/- 16.29
Episode length: 515.12 +/- 69.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=186.83 +/- 26.72
Episode length: 515.38 +/- 67.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 96.8     |
| time/              |          |
|    fps             | 111      |
|    iterations      | 103      |
|    time_elapsed    | 1896     |
|    total_timesteps | 210944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=211000, episode_reward=81.27 +/- 28.59
Episode length: 67.84 +/- 21.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 67.8        |
|    mean_reward          | 81.3        |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.011682258 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.416       |
|    learning_rate        | 0.001       |
|    loss                 | 69.3        |
|    n_updates            | 117         |
|    policy_gradient_loss | 0.00784     |
|    value_loss           | 114         |
-----------------------------------------
Eval num_timesteps=211500, episode_reward=83.71 +/- 29.88
Episode length: 67.36 +/- 68.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.4     |
|    mean_reward     | 83.7     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=85.23 +/- 32.62
Episode length: 60.14 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.1     |
|    mean_reward     | 85.2     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=78.23 +/- 25.05
Episode length: 72.88 +/- 67.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | 78.2     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 101      |
| time/              |          |
|    fps             | 111      |
|    iterations      | 104      |
|    time_elapsed    | 1907     |
|    total_timesteps | 212992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=213000, episode_reward=175.80 +/- 36.41
Episode length: 495.94 +/- 115.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 496         |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.007028466 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.001       |
|    loss                 | 52.7        |
|    n_updates            | 118         |
|    policy_gradient_loss | 0.00361     |
|    value_loss           | 98.3        |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=171.24 +/- 30.23
Episode length: 495.60 +/- 116.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=180.81 +/- 33.76
Episode length: 505.84 +/- 93.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=159.72 +/- 31.90
Episode length: 456.08 +/- 170.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 456      |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=163.71 +/- 27.48
Episode length: 466.30 +/- 159.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 466      |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    fps             | 108      |
|    iterations      | 105      |
|    time_elapsed    | 1989     |
|    total_timesteps | 215040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=215500, episode_reward=117.27 +/- 49.14
Episode length: 207.08 +/- 218.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | 117         |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.007252274 |
|    clip_fraction        | 0.0607      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.001       |
|    loss                 | 107         |
|    n_updates            | 119         |
|    policy_gradient_loss | 0.000551    |
|    value_loss           | 159         |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=123.39 +/- 49.34
Episode length: 254.10 +/- 231.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 123      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=137.21 +/- 58.72
Episode length: 263.08 +/- 232.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 137      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=119.71 +/- 56.66
Episode length: 196.24 +/- 215.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 196      |
|    mean_reward     | 120      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 107      |
|    iterations      | 106      |
|    time_elapsed    | 2021     |
|    total_timesteps | 217088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=217500, episode_reward=67.07 +/- 18.49
Episode length: 69.82 +/- 26.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 69.8         |
|    mean_reward          | 67.1         |
| time/                   |              |
|    total_timesteps      | 217500       |
| train/                  |              |
|    approx_kl            | 0.0044310284 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.368        |
|    learning_rate        | 0.001        |
|    loss                 | 35.9         |
|    n_updates            | 120          |
|    policy_gradient_loss | 0.00303      |
|    value_loss           | 121          |
------------------------------------------
Eval num_timesteps=218000, episode_reward=68.23 +/- 19.17
Episode length: 63.50 +/- 22.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.5     |
|    mean_reward     | 68.2     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=56.57 +/- 34.36
Episode length: 62.50 +/- 23.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.5     |
|    mean_reward     | 56.6     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=71.15 +/- 13.70
Episode length: 59.22 +/- 19.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.2     |
|    mean_reward     | 71.1     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 107      |
|    iterations      | 107      |
|    time_elapsed    | 2032     |
|    total_timesteps | 219136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=219500, episode_reward=47.61 +/- 56.94
Episode length: 59.06 +/- 21.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 59.1       |
|    mean_reward          | 47.6       |
| time/                   |            |
|    total_timesteps      | 219500     |
| train/                  |            |
|    approx_kl            | 0.00511115 |
|    clip_fraction        | 0.0286     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.23       |
|    learning_rate        | 0.001      |
|    loss                 | 139        |
|    n_updates            | 121        |
|    policy_gradient_loss | 0.00223    |
|    value_loss           | 132        |
----------------------------------------
Eval num_timesteps=220000, episode_reward=47.55 +/- 44.94
Episode length: 68.72 +/- 70.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.7     |
|    mean_reward     | 47.5     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=58.57 +/- 51.34
Episode length: 56.68 +/- 27.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.7     |
|    mean_reward     | 58.6     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=51.09 +/- 55.83
Episode length: 53.02 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | 51.1     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 108      |
|    iterations      | 108      |
|    time_elapsed    | 2041     |
|    total_timesteps | 221184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=221500, episode_reward=110.29 +/- 54.53
Episode length: 204.96 +/- 219.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 205          |
|    mean_reward          | 110          |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 0.0061411113 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.001        |
|    loss                 | 97.3         |
|    n_updates            | 122          |
|    policy_gradient_loss | -0.00294     |
|    value_loss           | 159          |
------------------------------------------
Eval num_timesteps=222000, episode_reward=108.85 +/- 64.10
Episode length: 166.68 +/- 201.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 109      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=95.07 +/- 53.45
Episode length: 136.22 +/- 170.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 136      |
|    mean_reward     | 95.1     |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=106.07 +/- 56.85
Episode length: 205.82 +/- 219.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 106      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 107      |
|    iterations      | 109      |
|    time_elapsed    | 2067     |
|    total_timesteps | 223232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=223500, episode_reward=155.99 +/- 58.94
Episode length: 437.96 +/- 185.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 438         |
|    mean_reward          | 156         |
| time/                   |             |
|    total_timesteps      | 223500      |
| train/                  |             |
|    approx_kl            | 0.005990511 |
|    clip_fraction        | 0.0616      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.001       |
|    loss                 | 87.4        |
|    n_updates            | 123         |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 168         |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=159.05 +/- 49.58
Episode length: 458.12 +/- 165.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 458      |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=164.51 +/- 35.56
Episode length: 467.28 +/- 156.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=153.23 +/- 52.42
Episode length: 428.32 +/- 193.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 428      |
|    mean_reward     | 153      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 237      |
|    ep_rew_mean     | 119      |
| time/              |          |
|    fps             | 105      |
|    iterations      | 110      |
|    time_elapsed    | 2128     |
|    total_timesteps | 225280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=225500, episode_reward=164.87 +/- 26.37
Episode length: 487.94 +/- 126.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 488         |
|    mean_reward          | 165         |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.004780565 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.001       |
|    loss                 | 47.8        |
|    n_updates            | 124         |
|    policy_gradient_loss | 0.00883     |
|    value_loss           | 178         |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=165.03 +/- 25.11
Episode length: 486.00 +/- 132.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=157.29 +/- 49.23
Episode length: 439.78 +/- 182.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=167.12 +/- 41.10
Episode length: 468.54 +/- 152.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    fps             | 103      |
|    iterations      | 111      |
|    time_elapsed    | 2193     |
|    total_timesteps | 227328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=227500, episode_reward=-2.47 +/- 39.86
Episode length: 49.64 +/- 13.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.6         |
|    mean_reward          | -2.47        |
| time/                   |              |
|    total_timesteps      | 227500       |
| train/                  |              |
|    approx_kl            | 0.0059604663 |
|    clip_fraction        | 0.0673       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.411        |
|    learning_rate        | 0.001        |
|    loss                 | 14.7         |
|    n_updates            | 125          |
|    policy_gradient_loss | 0.00462      |
|    value_loss           | 74           |
------------------------------------------
Eval num_timesteps=228000, episode_reward=-6.12 +/- 38.12
Episode length: 48.42 +/- 13.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -6.12    |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-5.76 +/- 43.41
Episode length: 54.76 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.8     |
|    mean_reward     | -5.76    |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-16.10 +/- 29.76
Episode length: 55.36 +/- 14.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.4     |
|    mean_reward     | -16.1    |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 265      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 104      |
|    iterations      | 112      |
|    time_elapsed    | 2201     |
|    total_timesteps | 229376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=229500, episode_reward=49.45 +/- 74.56
Episode length: 137.42 +/- 182.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 137          |
|    mean_reward          | 49.4         |
| time/                   |              |
|    total_timesteps      | 229500       |
| train/                  |              |
|    approx_kl            | 0.0052535906 |
|    clip_fraction        | 0.0358       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.438        |
|    learning_rate        | 0.001        |
|    loss                 | 120          |
|    n_updates            | 126          |
|    policy_gradient_loss | 0.00191      |
|    value_loss           | 148          |
------------------------------------------
Eval num_timesteps=230000, episode_reward=32.89 +/- 77.69
Episode length: 113.92 +/- 166.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 32.9     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=42.28 +/- 80.96
Episode length: 134.40 +/- 183.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 42.3     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=34.33 +/- 68.55
Episode length: 105.78 +/- 155.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 34.3     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 104      |
|    iterations      | 113      |
|    time_elapsed    | 2220     |
|    total_timesteps | 231424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=231500, episode_reward=-17.47 +/- 32.33
Episode length: 56.10 +/- 69.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.1        |
|    mean_reward          | -17.5       |
| time/                   |             |
|    total_timesteps      | 231500      |
| train/                  |             |
|    approx_kl            | 0.005808377 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.001       |
|    loss                 | 87.7        |
|    n_updates            | 127         |
|    policy_gradient_loss | -0.000841   |
|    value_loss           | 127         |
-----------------------------------------
Eval num_timesteps=232000, episode_reward=1.95 +/- 66.89
Episode length: 87.90 +/- 129.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.9     |
|    mean_reward     | 1.95     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-10.73 +/- 33.77
Episode length: 50.46 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -10.7    |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-12.63 +/- 38.35
Episode length: 60.36 +/- 68.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.4     |
|    mean_reward     | -12.6    |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 104      |
|    iterations      | 114      |
|    time_elapsed    | 2230     |
|    total_timesteps | 233472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=233500, episode_reward=-14.97 +/- 29.47
Episode length: 50.98 +/- 18.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51          |
|    mean_reward          | -15         |
| time/                   |             |
|    total_timesteps      | 233500      |
| train/                  |             |
|    approx_kl            | 0.005458012 |
|    clip_fraction        | 0.0379      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.001       |
|    loss                 | 12.7        |
|    n_updates            | 128         |
|    policy_gradient_loss | 0.00346     |
|    value_loss           | 119         |
-----------------------------------------
Eval num_timesteps=234000, episode_reward=-18.71 +/- 23.50
Episode length: 49.64 +/- 13.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-23.11 +/- 13.16
Episode length: 51.38 +/- 18.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -23.1    |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-16.08 +/- 26.63
Episode length: 47.90 +/- 17.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -16.1    |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-18.23 +/- 23.34
Episode length: 48.40 +/- 14.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -18.2    |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 124      |
| time/              |          |
|    fps             | 105      |
|    iterations      | 115      |
|    time_elapsed    | 2240     |
|    total_timesteps | 235520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=236000, episode_reward=162.97 +/- 55.58
Episode length: 408.96 +/- 206.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 409          |
|    mean_reward          | 163          |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0058382205 |
|    clip_fraction        | 0.0402       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.475        |
|    learning_rate        | 0.001        |
|    loss                 | 85.9         |
|    n_updates            | 129          |
|    policy_gradient_loss | -0.004       |
|    value_loss           | 115          |
------------------------------------------
Eval num_timesteps=236500, episode_reward=171.38 +/- 53.88
Episode length: 438.82 +/- 184.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 439      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=170.79 +/- 53.41
Episode length: 438.54 +/- 184.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 439      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=147.11 +/- 66.97
Episode length: 363.88 +/- 224.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 364      |
|    mean_reward     | 147      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 270      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    fps             | 103      |
|    iterations      | 116      |
|    time_elapsed    | 2297     |
|    total_timesteps | 237568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=238000, episode_reward=-16.19 +/- 26.34
Episode length: 48.06 +/- 16.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.1        |
|    mean_reward          | -16.2       |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.006192686 |
|    clip_fraction        | 0.0488      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.001       |
|    loss                 | 25.8        |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.000911    |
|    value_loss           | 102         |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=-22.18 +/- 15.38
Episode length: 48.16 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -22.2    |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-15.91 +/- 29.04
Episode length: 54.48 +/- 19.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -15.9    |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-23.36 +/- 13.48
Episode length: 52.06 +/- 18.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -23.4    |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 261      |
|    ep_rew_mean     | 119      |
| time/              |          |
|    fps             | 103      |
|    iterations      | 117      |
|    time_elapsed    | 2305     |
|    total_timesteps | 239616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=240000, episode_reward=102.19 +/- 67.34
Episode length: 160.12 +/- 194.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 160         |
|    mean_reward          | 102         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.007227747 |
|    clip_fraction        | 0.0604      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.395       |
|    learning_rate        | 0.001       |
|    loss                 | 33.9        |
|    n_updates            | 131         |
|    policy_gradient_loss | 0.00738     |
|    value_loss           | 171         |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=100.00 +/- 66.73
Episode length: 178.14 +/- 206.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 100      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=102.12 +/- 60.85
Episode length: 178.42 +/- 206.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 102      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=104.06 +/- 62.29
Episode length: 169.06 +/- 200.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | 104      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 256      |
|    ep_rew_mean     | 119      |
| time/              |          |
|    fps             | 103      |
|    iterations      | 118      |
|    time_elapsed    | 2330     |
|    total_timesteps | 241664   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=242000, episode_reward=-6.03 +/- 37.06
Episode length: 48.72 +/- 19.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -6.03       |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.008856949 |
|    clip_fraction        | 0.0513      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.001       |
|    loss                 | 122         |
|    n_updates            | 133         |
|    policy_gradient_loss | 0.00495     |
|    value_loss           | 180         |
-----------------------------------------
Eval num_timesteps=242500, episode_reward=-9.45 +/- 32.58
Episode length: 45.72 +/- 19.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -9.45    |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-1.19 +/- 40.60
Episode length: 45.34 +/- 14.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -1.19    |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=10.91 +/- 45.56
Episode length: 52.02 +/- 15.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | 10.9     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 261      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 104      |
|    iterations      | 119      |
|    time_elapsed    | 2338     |
|    total_timesteps | 243712   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=244000, episode_reward=94.81 +/- 50.66
Episode length: 158.34 +/- 195.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 94.8        |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.009726375 |
|    clip_fraction        | 0.058       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.377       |
|    learning_rate        | 0.001       |
|    loss                 | 17          |
|    n_updates            | 135         |
|    policy_gradient_loss | -0.00414    |
|    value_loss           | 90.6        |
-----------------------------------------
Eval num_timesteps=244500, episode_reward=80.81 +/- 52.42
Episode length: 112.28 +/- 153.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 80.8     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=97.73 +/- 51.52
Episode length: 153.84 +/- 197.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 154      |
|    mean_reward     | 97.7     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=84.19 +/- 52.91
Episode length: 124.04 +/- 163.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 84.2     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 256      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 104      |
|    iterations      | 120      |
|    time_elapsed    | 2359     |
|    total_timesteps | 245760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=246000, episode_reward=135.21 +/- 51.44
Episode length: 294.22 +/- 231.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 294         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.006203364 |
|    clip_fraction        | 0.0459      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.542       |
|    learning_rate        | 0.001       |
|    loss                 | 51          |
|    n_updates            | 136         |
|    policy_gradient_loss | 0.00597     |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=246500, episode_reward=121.56 +/- 54.92
Episode length: 219.12 +/- 229.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 122      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=128.04 +/- 62.64
Episode length: 224.68 +/- 225.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 128      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=128.77 +/- 56.65
Episode length: 256.64 +/- 228.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 129      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 265      |
|    ep_rew_mean     | 121      |
| time/              |          |
|    fps             | 103      |
|    iterations      | 121      |
|    time_elapsed    | 2394     |
|    total_timesteps | 247808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=248000, episode_reward=138.33 +/- 70.99
Episode length: 342.12 +/- 233.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 342         |
|    mean_reward          | 138         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.008944942 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.001       |
|    loss                 | 47.3        |
|    n_updates            | 137         |
|    policy_gradient_loss | 0.00367     |
|    value_loss           | 122         |
-----------------------------------------
Eval num_timesteps=248500, episode_reward=129.26 +/- 58.31
Episode length: 266.62 +/- 238.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 129      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=139.68 +/- 53.62
Episode length: 304.08 +/- 239.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | 140      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=141.83 +/- 50.72
Episode length: 315.84 +/- 236.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | 142      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 123      |
| time/              |          |
|    fps             | 102      |
|    iterations      | 122      |
|    time_elapsed    | 2436     |
|    total_timesteps | 249856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=250000, episode_reward=193.00 +/- 1.85
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.009510763 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.001       |
|    loss                 | 39.1        |
|    n_updates            | 138         |
|    policy_gradient_loss | 0.0162      |
|    value_loss           | 60.8        |
-----------------------------------------
New best mean reward!
Eval num_timesteps=250500, episode_reward=193.01 +/- 2.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
New best mean reward!
Eval num_timesteps=251000, episode_reward=191.38 +/- 15.64
Episode length: 515.06 +/- 69.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=194.81 +/- 14.07
Episode length: 515.40 +/- 67.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 100      |
|    iterations      | 123      |
|    time_elapsed    | 2506     |
|    total_timesteps | 251904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=252000, episode_reward=188.73 +/- 21.84
Episode length: 505.24 +/- 96.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | 189          |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0058714645 |
|    clip_fraction        | 0.0504       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.557        |
|    learning_rate        | 0.001        |
|    loss                 | 35.9         |
|    n_updates            | 139          |
|    policy_gradient_loss | 0.00844      |
|    value_loss           | 108          |
------------------------------------------
Eval num_timesteps=252500, episode_reward=190.64 +/- 15.72
Episode length: 515.28 +/- 68.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=193.48 +/- 3.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=191.14 +/- 16.35
Episode length: 515.86 +/- 63.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 98       |
|    iterations      | 124      |
|    time_elapsed    | 2576     |
|    total_timesteps | 253952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=254000, episode_reward=186.37 +/- 25.95
Episode length: 495.78 +/- 115.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 186          |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 0.0064004115 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.649        |
|    learning_rate        | 0.001        |
|    loss                 | 20.4         |
|    n_updates            | 140          |
|    policy_gradient_loss | 0.0116       |
|    value_loss           | 76.8         |
------------------------------------------
Eval num_timesteps=254500, episode_reward=188.37 +/- 21.66
Episode length: 505.74 +/- 94.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=188.72 +/- 21.98
Episode length: 507.34 +/- 86.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=191.10 +/- 15.60
Episode length: 515.32 +/- 67.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=186.58 +/- 26.11
Episode length: 495.60 +/- 116.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 96       |
|    iterations      | 125      |
|    time_elapsed    | 2660     |
|    total_timesteps | 256000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=256500, episode_reward=195.18 +/- 1.45
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 195        |
| time/                   |            |
|    total_timesteps      | 256500     |
| train/                  |            |
|    approx_kl            | 0.00616728 |
|    clip_fraction        | 0.0404     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.721      |
|    learning_rate        | 0.001      |
|    loss                 | 21         |
|    n_updates            | 141        |
|    policy_gradient_loss | 0.00737    |
|    value_loss           | 74.2       |
----------------------------------------
New best mean reward!
Eval num_timesteps=257000, episode_reward=191.30 +/- 21.66
Episode length: 506.86 +/- 89.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=191.31 +/- 21.66
Episode length: 505.56 +/- 95.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=186.44 +/- 29.97
Episode length: 487.96 +/- 125.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 94       |
|    iterations      | 126      |
|    time_elapsed    | 2728     |
|    total_timesteps | 258048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=258500, episode_reward=184.98 +/- 42.26
Episode length: 486.42 +/- 130.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 486          |
|    mean_reward          | 185          |
| time/                   |              |
|    total_timesteps      | 258500       |
| train/                  |              |
|    approx_kl            | 0.0062949415 |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.726        |
|    learning_rate        | 0.001        |
|    loss                 | 22.3         |
|    n_updates            | 142          |
|    policy_gradient_loss | 0.00461      |
|    value_loss           | 55.1         |
------------------------------------------
Eval num_timesteps=259000, episode_reward=192.01 +/- 15.58
Episode length: 515.32 +/- 67.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=190.08 +/- 21.82
Episode length: 506.46 +/- 90.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=191.49 +/- 26.30
Episode length: 506.88 +/- 88.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 93       |
|    iterations      | 127      |
|    time_elapsed    | 2796     |
|    total_timesteps | 260096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=260500, episode_reward=13.38 +/- 75.18
Episode length: 115.26 +/- 165.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 115         |
|    mean_reward          | 13.4        |
| time/                   |             |
|    total_timesteps      | 260500      |
| train/                  |             |
|    approx_kl            | 0.006133877 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.001       |
|    loss                 | 1.55        |
|    n_updates            | 143         |
|    policy_gradient_loss | 0.0103      |
|    value_loss           | 90.8        |
-----------------------------------------
Eval num_timesteps=261000, episode_reward=11.13 +/- 73.81
Episode length: 110.60 +/- 168.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=25.35 +/- 78.18
Episode length: 126.94 +/- 174.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 25.3     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=46.29 +/- 88.33
Episode length: 165.04 +/- 202.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 46.3     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 327      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 93       |
|    iterations      | 128      |
|    time_elapsed    | 2815     |
|    total_timesteps | 262144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=262500, episode_reward=-22.09 +/- 20.17
Episode length: 55.62 +/- 21.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 55.6         |
|    mean_reward          | -22.1        |
| time/                   |              |
|    total_timesteps      | 262500       |
| train/                  |              |
|    approx_kl            | 0.0073224306 |
|    clip_fraction        | 0.0594       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.649        |
|    learning_rate        | 0.001        |
|    loss                 | 113          |
|    n_updates            | 144          |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 98.5         |
------------------------------------------
Eval num_timesteps=263000, episode_reward=-24.57 +/- 4.51
Episode length: 49.48 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -24.6    |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-10.43 +/- 38.40
Episode length: 49.00 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -10.4    |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-12.48 +/- 44.31
Episode length: 60.70 +/- 70.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.7     |
|    mean_reward     | -12.5    |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 327      |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 93       |
|    iterations      | 129      |
|    time_elapsed    | 2824     |
|    total_timesteps | 264192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=264500, episode_reward=125.57 +/- 77.90
Episode length: 267.68 +/- 237.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 268          |
|    mean_reward          | 126          |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 0.0044351444 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.43         |
|    learning_rate        | 0.001        |
|    loss                 | 82.6         |
|    n_updates            | 145          |
|    policy_gradient_loss | -0.00283     |
|    value_loss           | 285          |
------------------------------------------
Eval num_timesteps=265000, episode_reward=123.07 +/- 77.42
Episode length: 249.38 +/- 234.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 123      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=133.74 +/- 76.71
Episode length: 309.14 +/- 234.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 134      |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=124.79 +/- 74.40
Episode length: 267.22 +/- 238.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 125      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 327      |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 93       |
|    iterations      | 130      |
|    time_elapsed    | 2862     |
|    total_timesteps | 266240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=266500, episode_reward=158.78 +/- 62.66
Episode length: 365.04 +/- 223.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 365          |
|    mean_reward          | 159          |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0060008965 |
|    clip_fraction        | 0.0488       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 8.89         |
|    n_updates            | 146          |
|    policy_gradient_loss | 0.00242      |
|    value_loss           | 76.1         |
------------------------------------------
Eval num_timesteps=267000, episode_reward=149.89 +/- 51.24
Episode length: 354.84 +/- 227.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 355      |
|    mean_reward     | 150      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=145.30 +/- 53.46
Episode length: 347.04 +/- 227.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 347      |
|    mean_reward     | 145      |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=157.14 +/- 50.57
Episode length: 337.38 +/- 230.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 336      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 92       |
|    iterations      | 131      |
|    time_elapsed    | 2910     |
|    total_timesteps | 268288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=268500, episode_reward=159.01 +/- 52.24
Episode length: 383.48 +/- 216.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 383         |
|    mean_reward          | 159         |
| time/                   |             |
|    total_timesteps      | 268500      |
| train/                  |             |
|    approx_kl            | 0.011149917 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.001       |
|    loss                 | 35.4        |
|    n_updates            | 147         |
|    policy_gradient_loss | 0.0118      |
|    value_loss           | 75.1        |
-----------------------------------------
Eval num_timesteps=269000, episode_reward=172.55 +/- 45.02
Episode length: 448.24 +/- 175.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 448      |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=180.08 +/- 43.08
Episode length: 451.14 +/- 169.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 451      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=171.34 +/- 43.34
Episode length: 439.62 +/- 182.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 346      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 91       |
|    iterations      | 132      |
|    time_elapsed    | 2969     |
|    total_timesteps | 270336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=270500, episode_reward=89.09 +/- 34.79
Episode length: 81.46 +/- 93.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 81.5         |
|    mean_reward          | 89.1         |
| time/                   |              |
|    total_timesteps      | 270500       |
| train/                  |              |
|    approx_kl            | 0.0099989325 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.572        |
|    learning_rate        | 0.001        |
|    loss                 | 52.8         |
|    n_updates            | 148          |
|    policy_gradient_loss | 0.0152       |
|    value_loss           | 104          |
------------------------------------------
Eval num_timesteps=271000, episode_reward=90.97 +/- 32.11
Episode length: 100.66 +/- 126.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 91       |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=82.41 +/- 14.33
Episode length: 59.82 +/- 22.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.8     |
|    mean_reward     | 82.4     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=86.85 +/- 24.56
Episode length: 71.90 +/- 67.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.9     |
|    mean_reward     | 86.8     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 350      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 91       |
|    iterations      | 133      |
|    time_elapsed    | 2982     |
|    total_timesteps | 272384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=272500, episode_reward=82.35 +/- 2.48
Episode length: 58.30 +/- 18.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 58.3         |
|    mean_reward          | 82.3         |
| time/                   |              |
|    total_timesteps      | 272500       |
| train/                  |              |
|    approx_kl            | 0.0075091235 |
|    clip_fraction        | 0.0859       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.498        |
|    learning_rate        | 0.001        |
|    loss                 | 47.6         |
|    n_updates            | 149          |
|    policy_gradient_loss | -0.000845    |
|    value_loss           | 95.8         |
------------------------------------------
Eval num_timesteps=273000, episode_reward=83.73 +/- 14.48
Episode length: 61.78 +/- 19.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.8     |
|    mean_reward     | 83.7     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=83.83 +/- 14.36
Episode length: 61.52 +/- 21.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.5     |
|    mean_reward     | 83.8     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=80.95 +/- 3.32
Episode length: 68.94 +/- 24.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.9     |
|    mean_reward     | 80.9     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 91       |
|    iterations      | 134      |
|    time_elapsed    | 2992     |
|    total_timesteps | 274432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=274500, episode_reward=85.75 +/- 0.82
Episode length: 62.88 +/- 21.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 62.9        |
|    mean_reward          | 85.7        |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.008864637 |
|    clip_fraction        | 0.0651      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.504       |
|    learning_rate        | 0.001       |
|    loss                 | 62.9        |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 127         |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=85.31 +/- 1.50
Episode length: 59.76 +/- 26.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.8     |
|    mean_reward     | 85.3     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=87.71 +/- 13.84
Episode length: 62.30 +/- 24.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.3     |
|    mean_reward     | 87.7     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=85.52 +/- 1.13
Episode length: 58.96 +/- 20.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59       |
|    mean_reward     | 85.5     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 92       |
|    iterations      | 135      |
|    time_elapsed    | 3002     |
|    total_timesteps | 276480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=276500, episode_reward=82.45 +/- 19.40
Episode length: 61.72 +/- 22.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 61.7        |
|    mean_reward          | 82.4        |
| time/                   |             |
|    total_timesteps      | 276500      |
| train/                  |             |
|    approx_kl            | 0.009622363 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.001       |
|    loss                 | 157         |
|    n_updates            | 151         |
|    policy_gradient_loss | 0.0065      |
|    value_loss           | 287         |
-----------------------------------------
Eval num_timesteps=277000, episode_reward=82.47 +/- 19.41
Episode length: 61.26 +/- 22.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.3     |
|    mean_reward     | 82.5     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=84.41 +/- 13.86
Episode length: 58.00 +/- 20.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58       |
|    mean_reward     | 84.4     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=84.43 +/- 13.86
Episode length: 68.26 +/- 24.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.3     |
|    mean_reward     | 84.4     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=84.45 +/- 13.87
Episode length: 64.40 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.4     |
|    mean_reward     | 84.4     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 92       |
|    iterations      | 136      |
|    time_elapsed    | 3015     |
|    total_timesteps | 278528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=279000, episode_reward=91.45 +/- 21.33
Episode length: 79.02 +/- 92.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79          |
|    mean_reward          | 91.4        |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.008034664 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.001       |
|    loss                 | 98.2        |
|    n_updates            | 152         |
|    policy_gradient_loss | -0.00154    |
|    value_loss           | 175         |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=89.25 +/- 15.32
Episode length: 70.18 +/- 68.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | 89.2     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=91.31 +/- 21.36
Episode length: 86.32 +/- 92.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.3     |
|    mean_reward     | 91.3     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=91.49 +/- 21.42
Episode length: 77.28 +/- 93.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 91.5     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 92       |
|    iterations      | 137      |
|    time_elapsed    | 3027     |
|    total_timesteps | 280576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=281000, episode_reward=181.39 +/- 35.61
Episode length: 467.42 +/- 155.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 467          |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0059092864 |
|    clip_fraction        | 0.0534       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.442        |
|    learning_rate        | 0.001        |
|    loss                 | 63.4         |
|    n_updates            | 153          |
|    policy_gradient_loss | 0.00383      |
|    value_loss           | 149          |
------------------------------------------
Eval num_timesteps=281500, episode_reward=178.83 +/- 38.43
Episode length: 458.42 +/- 165.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 458      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=183.30 +/- 33.03
Episode length: 477.08 +/- 143.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=176.77 +/- 40.34
Episode length: 447.44 +/- 177.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 447      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 91       |
|    iterations      | 138      |
|    time_elapsed    | 3090     |
|    total_timesteps | 282624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=283000, episode_reward=192.53 +/- 15.55
Episode length: 515.12 +/- 69.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 283000      |
| train/                  |             |
|    approx_kl            | 0.007638597 |
|    clip_fraction        | 0.0844      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.001       |
|    loss                 | 47          |
|    n_updates            | 154         |
|    policy_gradient_loss | 0.0039      |
|    value_loss           | 64.4        |
-----------------------------------------
Eval num_timesteps=283500, episode_reward=194.30 +/- 20.60
Episode length: 514.98 +/- 70.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=195.03 +/- 1.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=186.29 +/- 30.01
Episode length: 484.88 +/- 136.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 485      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 90       |
|    iterations      | 139      |
|    time_elapsed    | 3159     |
|    total_timesteps | 284672   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=285000, episode_reward=188.92 +/- 30.40
Episode length: 496.14 +/- 114.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 496         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.015413256 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.001       |
|    loss                 | 29.8        |
|    n_updates            | 156         |
|    policy_gradient_loss | 0.00363     |
|    value_loss           | 90          |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=184.98 +/- 30.66
Episode length: 486.68 +/- 130.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=191.10 +/- 26.50
Episode length: 506.06 +/- 92.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=189.24 +/- 21.91
Episode length: 505.54 +/- 95.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 88       |
|    iterations      | 140      |
|    time_elapsed    | 3227     |
|    total_timesteps | 286720   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=287000, episode_reward=194.88 +/- 1.85
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.006837483 |
|    clip_fraction        | 0.0366      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.001       |
|    loss                 | 7.8         |
|    n_updates            | 158         |
|    policy_gradient_loss | 0.00088     |
|    value_loss           | 59.7        |
-----------------------------------------
Eval num_timesteps=287500, episode_reward=192.48 +/- 26.07
Episode length: 505.32 +/- 96.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=192.56 +/- 15.39
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=193.05 +/- 15.61
Episode length: 516.42 +/- 60.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 314      |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 87       |
|    iterations      | 141      |
|    time_elapsed    | 3296     |
|    total_timesteps | 288768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=289000, episode_reward=197.01 +/- 1.08
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 289000       |
| train/                  |              |
|    approx_kl            | 0.0099664675 |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.593        |
|    learning_rate        | 0.001        |
|    loss                 | 5.57         |
|    n_updates            | 159          |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 61.3         |
------------------------------------------
New best mean reward!
Eval num_timesteps=289500, episode_reward=190.08 +/- 26.38
Episode length: 496.60 +/- 112.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=190.23 +/- 26.26
Episode length: 495.92 +/- 115.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=194.58 +/- 15.49
Episode length: 515.30 +/- 67.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 319      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 86       |
|    iterations      | 142      |
|    time_elapsed    | 3365     |
|    total_timesteps | 290816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=291000, episode_reward=194.16 +/- 15.73
Episode length: 515.14 +/- 69.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 291000      |
| train/                  |             |
|    approx_kl            | 0.008201514 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.001       |
|    loss                 | 33.1        |
|    n_updates            | 160         |
|    policy_gradient_loss | 0.00376     |
|    value_loss           | 76.3        |
-----------------------------------------
Eval num_timesteps=291500, episode_reward=189.63 +/- 26.36
Episode length: 494.92 +/- 119.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=189.88 +/- 26.42
Episode length: 497.68 +/- 108.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=187.26 +/- 30.22
Episode length: 488.92 +/- 122.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 489      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 128      |
| time/              |          |
|    fps             | 85       |
|    iterations      | 143      |
|    time_elapsed    | 3432     |
|    total_timesteps | 292864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=293000, episode_reward=195.30 +/- 1.16
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 195          |
| time/                   |              |
|    total_timesteps      | 293000       |
| train/                  |              |
|    approx_kl            | 0.0045208344 |
|    clip_fraction        | 0.0413       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 0.7          |
|    learning_rate        | 0.001        |
|    loss                 | 121          |
|    n_updates            | 161          |
|    policy_gradient_loss | 0.00278      |
|    value_loss           | 81.7         |
------------------------------------------
Eval num_timesteps=293500, episode_reward=193.29 +/- 15.76
Episode length: 516.08 +/- 62.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=188.75 +/- 26.14
Episode length: 495.88 +/- 115.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=190.87 +/- 21.47
Episode length: 505.68 +/- 94.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 84       |
|    iterations      | 144      |
|    time_elapsed    | 3501     |
|    total_timesteps | 294912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=295000, episode_reward=194.67 +/- 1.35
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.004719422 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.001       |
|    loss                 | 32.5        |
|    n_updates            | 162         |
|    policy_gradient_loss | 0.015       |
|    value_loss           | 64.4        |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=194.58 +/- 0.88
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=194.74 +/- 1.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=194.67 +/- 1.23
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 83       |
|    iterations      | 145      |
|    time_elapsed    | 3573     |
|    total_timesteps | 296960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=297000, episode_reward=188.16 +/- 26.15
Episode length: 495.70 +/- 115.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 188          |
| time/                   |              |
|    total_timesteps      | 297000       |
| train/                  |              |
|    approx_kl            | 0.0054777735 |
|    clip_fraction        | 0.0658       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.732        |
|    learning_rate        | 0.001        |
|    loss                 | 40.6         |
|    n_updates            | 163          |
|    policy_gradient_loss | 0.0102       |
|    value_loss           | 85.6         |
------------------------------------------
Eval num_timesteps=297500, episode_reward=194.74 +/- 1.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=194.38 +/- 1.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=190.34 +/- 21.46
Episode length: 506.78 +/- 89.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=194.75 +/- 1.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 328      |
|    ep_rew_mean     | 135      |
| time/              |          |
|    fps             | 81       |
|    iterations      | 146      |
|    time_elapsed    | 3659     |
|    total_timesteps | 299008   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=299500, episode_reward=180.19 +/- 43.65
Episode length: 469.46 +/- 150.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 469         |
|    mean_reward          | 180         |
| time/                   |             |
|    total_timesteps      | 299500      |
| train/                  |             |
|    approx_kl            | 0.005087831 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.001       |
|    loss                 | 35.8        |
|    n_updates            | 164         |
|    policy_gradient_loss | 0.00138     |
|    value_loss           | 61.7        |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=186.26 +/- 30.77
Episode length: 487.58 +/- 126.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=186.42 +/- 30.22
Episode length: 490.28 +/- 117.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=188.47 +/- 26.63
Episode length: 497.52 +/- 109.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 319      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 80       |
|    iterations      | 147      |
|    time_elapsed    | 3724     |
|    total_timesteps | 301056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=301500, episode_reward=185.75 +/- 35.92
Episode length: 495.52 +/- 116.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 186          |
| time/                   |              |
|    total_timesteps      | 301500       |
| train/                  |              |
|    approx_kl            | 0.0064878836 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.8          |
|    learning_rate        | 0.001        |
|    loss                 | 32.4         |
|    n_updates            | 165          |
|    policy_gradient_loss | 0.000861     |
|    value_loss           | 61.5         |
------------------------------------------
Eval num_timesteps=302000, episode_reward=172.78 +/- 60.95
Episode length: 467.90 +/- 154.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 468      |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=188.20 +/- 32.92
Episode length: 505.24 +/- 96.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=185.35 +/- 41.42
Episode length: 485.32 +/- 134.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 485      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 319      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 148      |
|    time_elapsed    | 3791     |
|    total_timesteps | 303104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=303500, episode_reward=185.52 +/- 26.81
Episode length: 495.50 +/- 116.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 496         |
|    mean_reward          | 186         |
| time/                   |             |
|    total_timesteps      | 303500      |
| train/                  |             |
|    approx_kl            | 0.008224949 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.001       |
|    loss                 | 37.7        |
|    n_updates            | 166         |
|    policy_gradient_loss | 0.0104      |
|    value_loss           | 98.3        |
-----------------------------------------
Eval num_timesteps=304000, episode_reward=188.11 +/- 22.19
Episode length: 505.00 +/- 97.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=183.38 +/- 36.88
Episode length: 496.10 +/- 114.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=191.40 +/- 15.91
Episode length: 515.04 +/- 69.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 323      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 79       |
|    iterations      | 149      |
|    time_elapsed    | 3861     |
|    total_timesteps | 305152   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=305500, episode_reward=169.22 +/- 19.27
Episode length: 506.26 +/- 91.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 305500      |
| train/                  |             |
|    approx_kl            | 0.007140088 |
|    clip_fraction        | 0.0518      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.001       |
|    loss                 | 16.2        |
|    n_updates            | 168         |
|    policy_gradient_loss | 0.0037      |
|    value_loss           | 74.3        |
-----------------------------------------
Eval num_timesteps=306000, episode_reward=171.20 +/- 12.56
Episode length: 514.98 +/- 70.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=172.96 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=170.88 +/- 14.23
Episode length: 515.84 +/- 64.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 337      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 78       |
|    iterations      | 150      |
|    time_elapsed    | 3932     |
|    total_timesteps | 307200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=307500, episode_reward=177.09 +/- 14.63
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 307500      |
| train/                  |             |
|    approx_kl            | 0.007302842 |
|    clip_fraction        | 0.0435      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.001       |
|    loss                 | 8.3         |
|    n_updates            | 169         |
|    policy_gradient_loss | 0.00868     |
|    value_loss           | 63.4        |
-----------------------------------------
Eval num_timesteps=308000, episode_reward=173.23 +/- 13.87
Episode length: 515.06 +/- 69.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=174.94 +/- 4.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=175.18 +/- 4.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 351      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 77       |
|    iterations      | 151      |
|    time_elapsed    | 4003     |
|    total_timesteps | 309248   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=309500, episode_reward=195.07 +/- 5.27
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 309500      |
| train/                  |             |
|    approx_kl            | 0.008690506 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.001       |
|    loss                 | 21.9        |
|    n_updates            | 170         |
|    policy_gradient_loss | 0.00418     |
|    value_loss           | 57          |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=192.17 +/- 16.58
Episode length: 514.94 +/- 70.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=194.66 +/- 5.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=192.77 +/- 7.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 360      |
|    ep_rew_mean     | 141      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 152      |
|    time_elapsed    | 4073     |
|    total_timesteps | 311296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=311500, episode_reward=196.95 +/- 0.89
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 311500       |
| train/                  |              |
|    approx_kl            | 0.0037491496 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.001        |
|    loss                 | 2.28         |
|    n_updates            | 171          |
|    policy_gradient_loss | 0.000497     |
|    value_loss           | 24.6         |
------------------------------------------
Eval num_timesteps=312000, episode_reward=196.67 +/- 0.97
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=196.75 +/- 0.88
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=196.99 +/- 1.02
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 379      |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 75       |
|    iterations      | 153      |
|    time_elapsed    | 4144     |
|    total_timesteps | 313344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=313500, episode_reward=197.37 +/- 0.78
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 313500      |
| train/                  |             |
|    approx_kl            | 0.003195466 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.001       |
|    loss                 | 53.5        |
|    n_updates            | 172         |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 73          |
-----------------------------------------
New best mean reward!
Eval num_timesteps=314000, episode_reward=197.27 +/- 1.02
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=197.35 +/- 0.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=197.23 +/- 0.93
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 394      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 74       |
|    iterations      | 154      |
|    time_elapsed    | 4214     |
|    total_timesteps | 315392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=315500, episode_reward=197.63 +/- 0.67
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 315500      |
| train/                  |             |
|    approx_kl            | 0.006627713 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.001       |
|    loss                 | 62          |
|    n_updates            | 173         |
|    policy_gradient_loss | -0.000769   |
|    value_loss           | 66.7        |
-----------------------------------------
New best mean reward!
Eval num_timesteps=316000, episode_reward=197.44 +/- 0.98
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=197.47 +/- 0.95
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=197.37 +/- 1.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    fps             | 74       |
|    iterations      | 155      |
|    time_elapsed    | 4286     |
|    total_timesteps | 317440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=317500, episode_reward=197.52 +/- 0.87
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 317500      |
| train/                  |             |
|    approx_kl            | 0.005887117 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.001       |
|    loss                 | 47          |
|    n_updates            | 174         |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 76.8        |
-----------------------------------------
Eval num_timesteps=318000, episode_reward=197.27 +/- 1.07
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=197.45 +/- 0.89
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=197.47 +/- 0.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 426      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    fps             | 73       |
|    iterations      | 156      |
|    time_elapsed    | 4358     |
|    total_timesteps | 319488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=319500, episode_reward=197.20 +/- 0.86
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 319500       |
| train/                  |              |
|    approx_kl            | 0.0065522967 |
|    clip_fraction        | 0.051        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.777        |
|    learning_rate        | 0.001        |
|    loss                 | 39.9         |
|    n_updates            | 175          |
|    policy_gradient_loss | 0.00457      |
|    value_loss           | 52.1         |
------------------------------------------
Eval num_timesteps=320000, episode_reward=197.31 +/- 0.89
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=197.57 +/- 0.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=197.29 +/- 1.06
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=197.31 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 441      |
|    ep_rew_mean     | 157      |
| time/              |          |
|    fps             | 72       |
|    iterations      | 157      |
|    time_elapsed    | 4447     |
|    total_timesteps | 321536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=322000, episode_reward=196.66 +/- 1.60
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 322000       |
| train/                  |              |
|    approx_kl            | 0.0051495195 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.722        |
|    learning_rate        | 0.001        |
|    loss                 | 58.4         |
|    n_updates            | 176          |
|    policy_gradient_loss | 0.0193       |
|    value_loss           | 75.9         |
------------------------------------------
Eval num_timesteps=322500, episode_reward=196.78 +/- 1.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=192.68 +/- 29.52
Episode length: 514.84 +/- 71.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=194.44 +/- 15.82
Episode length: 515.06 +/- 69.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 455      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 71       |
|    iterations      | 158      |
|    time_elapsed    | 4518     |
|    total_timesteps | 323584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=324000, episode_reward=196.16 +/- 1.30
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.004678843 |
|    clip_fraction        | 0.029       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.001       |
|    loss                 | 72.4        |
|    n_updates            | 177         |
|    policy_gradient_loss | 0.00852     |
|    value_loss           | 56.6        |
-----------------------------------------
Eval num_timesteps=324500, episode_reward=196.07 +/- 1.06
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=196.24 +/- 1.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=196.27 +/- 1.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 70       |
|    iterations      | 159      |
|    time_elapsed    | 4590     |
|    total_timesteps | 325632   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=326000, episode_reward=195.76 +/- 14.03
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 326000       |
| train/                  |              |
|    approx_kl            | 0.0075471196 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.774        |
|    learning_rate        | 0.001        |
|    loss                 | 88.5         |
|    n_updates            | 178          |
|    policy_gradient_loss | -0.00222     |
|    value_loss           | 63.5         |
------------------------------------------
Eval num_timesteps=326500, episode_reward=192.68 +/- 2.05
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=197.31 +/- 19.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=192.93 +/- 1.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 70       |
|    iterations      | 160      |
|    time_elapsed    | 4662     |
|    total_timesteps | 327680   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=328000, episode_reward=134.74 +/- 84.26
Episode length: 359.84 +/- 230.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 360         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.004300868 |
|    clip_fraction        | 0.0174      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.001       |
|    loss                 | 53.8        |
|    n_updates            | 179         |
|    policy_gradient_loss | 0.00431     |
|    value_loss           | 125         |
-----------------------------------------
Eval num_timesteps=328500, episode_reward=116.15 +/- 100.05
Episode length: 322.34 +/- 238.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 322      |
|    mean_reward     | 116      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=112.77 +/- 97.05
Episode length: 331.40 +/- 237.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 331      |
|    mean_reward     | 113      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=113.85 +/- 96.33
Episode length: 321.98 +/- 238.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 322      |
|    mean_reward     | 114      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 70       |
|    iterations      | 161      |
|    time_elapsed    | 4709     |
|    total_timesteps | 329728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=330000, episode_reward=195.93 +/- 1.53
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.004067964 |
|    clip_fraction        | 0.0215      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.001       |
|    loss                 | 96.5        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 122         |
-----------------------------------------
Eval num_timesteps=330500, episode_reward=195.52 +/- 1.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=195.67 +/- 1.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=194.99 +/- 1.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 69       |
|    iterations      | 162      |
|    time_elapsed    | 4780     |
|    total_timesteps | 331776   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=332000, episode_reward=195.66 +/- 1.43
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0080471365 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.75         |
|    learning_rate        | 0.001        |
|    loss                 | 31.3         |
|    n_updates            | 181          |
|    policy_gradient_loss | 0.0033       |
|    value_loss           | 52.5         |
------------------------------------------
Eval num_timesteps=332500, episode_reward=195.71 +/- 1.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=195.90 +/- 1.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=196.00 +/- 1.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 68       |
|    iterations      | 163      |
|    time_elapsed    | 4852     |
|    total_timesteps | 333824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=334000, episode_reward=194.11 +/- 20.69
Episode length: 514.94 +/- 70.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 194          |
| time/                   |              |
|    total_timesteps      | 334000       |
| train/                  |              |
|    approx_kl            | 0.0048235483 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.831        |
|    learning_rate        | 0.001        |
|    loss                 | 19.3         |
|    n_updates            | 182          |
|    policy_gradient_loss | 0.00308      |
|    value_loss           | 38.4         |
------------------------------------------
Eval num_timesteps=334500, episode_reward=193.71 +/- 1.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=194.21 +/- 1.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=196.22 +/- 13.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 68       |
|    iterations      | 164      |
|    time_elapsed    | 4923     |
|    total_timesteps | 335872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=336000, episode_reward=196.57 +/- 1.25
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 336000      |
| train/                  |             |
|    approx_kl            | 0.005149621 |
|    clip_fraction        | 0.0318      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.001       |
|    loss                 | 7.95        |
|    n_updates            | 183         |
|    policy_gradient_loss | 0.00206     |
|    value_loss           | 45          |
-----------------------------------------
Eval num_timesteps=336500, episode_reward=196.49 +/- 1.21
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=196.87 +/- 1.01
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=196.58 +/- 1.21
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 165      |
|    time_elapsed    | 4995     |
|    total_timesteps | 337920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=338000, episode_reward=194.50 +/- 6.20
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 195        |
| time/                   |            |
|    total_timesteps      | 338000     |
| train/                  |            |
|    approx_kl            | 0.00589815 |
|    clip_fraction        | 0.00625    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.721      |
|    learning_rate        | 0.001      |
|    loss                 | 36.3       |
|    n_updates            | 184        |
|    policy_gradient_loss | -0.00286   |
|    value_loss           | 64.1       |
----------------------------------------
Eval num_timesteps=338500, episode_reward=195.98 +/- 1.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=193.70 +/- 16.44
Episode length: 515.14 +/- 69.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=194.07 +/- 15.72
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 166      |
|    time_elapsed    | 5067     |
|    total_timesteps | 339968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=340000, episode_reward=146.05 +/- 54.31
Episode length: 377.02 +/- 170.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 377          |
|    mean_reward          | 146          |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0050707846 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.738        |
|    learning_rate        | 0.001        |
|    loss                 | 33.1         |
|    n_updates            | 185          |
|    policy_gradient_loss | 0.00269      |
|    value_loss           | 84.1         |
------------------------------------------
Eval num_timesteps=340500, episode_reward=158.78 +/- 50.93
Episode length: 417.00 +/- 161.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 417      |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=137.83 +/- 58.65
Episode length: 345.14 +/- 185.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 345      |
|    mean_reward     | 138      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=148.41 +/- 54.01
Episode length: 371.10 +/- 186.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 371      |
|    mean_reward     | 148      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=146.02 +/- 54.11
Episode length: 373.62 +/- 176.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 374      |
|    mean_reward     | 146      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 167      |
|    time_elapsed    | 5131     |
|    total_timesteps | 342016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=342500, episode_reward=197.30 +/- 1.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 342500      |
| train/                  |             |
|    approx_kl            | 0.009038852 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.001       |
|    loss                 | 19.3        |
|    n_updates            | 186         |
|    policy_gradient_loss | 0.00186     |
|    value_loss           | 52.6        |
-----------------------------------------
Eval num_timesteps=343000, episode_reward=197.63 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=197.24 +/- 0.85
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=197.68 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 168      |
|    time_elapsed    | 5202     |
|    total_timesteps | 344064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=344500, episode_reward=196.60 +/- 3.34
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 344500      |
| train/                  |             |
|    approx_kl            | 0.006673168 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.001       |
|    loss                 | 42.4        |
|    n_updates            | 187         |
|    policy_gradient_loss | 0.00219     |
|    value_loss           | 60.1        |
-----------------------------------------
Eval num_timesteps=345000, episode_reward=197.14 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=197.07 +/- 0.95
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=197.02 +/- 0.93
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 169      |
|    time_elapsed    | 5273     |
|    total_timesteps | 346112   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=346500, episode_reward=192.66 +/- 15.45
Episode length: 514.90 +/- 70.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 346500      |
| train/                  |             |
|    approx_kl            | 0.013422808 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.001       |
|    loss                 | 19.2        |
|    n_updates            | 189         |
|    policy_gradient_loss | 0.00259     |
|    value_loss           | 38.2        |
-----------------------------------------
Eval num_timesteps=347000, episode_reward=185.46 +/- 41.00
Episode length: 504.86 +/- 98.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=186.22 +/- 41.33
Episode length: 505.14 +/- 97.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=190.66 +/- 29.36
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 170      |
|    time_elapsed    | 5343     |
|    total_timesteps | 348160   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=348500, episode_reward=194.11 +/- 47.72
Episode length: 505.20 +/- 97.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | 194          |
| time/                   |              |
|    total_timesteps      | 348500       |
| train/                  |              |
|    approx_kl            | 0.0039134016 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.839        |
|    learning_rate        | 0.001        |
|    loss                 | 44.5         |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.00311      |
|    value_loss           | 28.7         |
------------------------------------------
Eval num_timesteps=349000, episode_reward=181.35 +/- 39.67
Episode length: 487.46 +/- 127.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=192.04 +/- 35.63
Episode length: 515.12 +/- 69.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=187.31 +/- 36.32
Episode length: 505.76 +/- 94.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 171      |
|    time_elapsed    | 5411     |
|    total_timesteps | 350208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=350500, episode_reward=103.85 +/- 105.78
Episode length: 274.98 +/- 240.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 104         |
| time/                   |             |
|    total_timesteps      | 350500      |
| train/                  |             |
|    approx_kl            | 0.005246486 |
|    clip_fraction        | 0.013       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.001       |
|    loss                 | 21.7        |
|    n_updates            | 191         |
|    policy_gradient_loss | 0.00369     |
|    value_loss           | 104         |
-----------------------------------------
Eval num_timesteps=351000, episode_reward=123.85 +/- 102.36
Episode length: 350.96 +/- 232.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 351      |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=116.93 +/- 97.41
Episode length: 342.66 +/- 233.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 343      |
|    mean_reward     | 117      |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=124.14 +/- 91.97
Episode length: 315.02 +/- 237.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 315      |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 172      |
|    time_elapsed    | 5456     |
|    total_timesteps | 352256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=352500, episode_reward=190.54 +/- 17.46
Episode length: 516.00 +/- 63.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 352500      |
| train/                  |             |
|    approx_kl            | 0.012699109 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.001       |
|    loss                 | 149         |
|    n_updates            | 192         |
|    policy_gradient_loss | 0.00202     |
|    value_loss           | 405         |
-----------------------------------------
Eval num_timesteps=353000, episode_reward=187.89 +/- 29.96
Episode length: 515.14 +/- 69.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=176.56 +/- 56.09
Episode length: 475.74 +/- 147.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=181.41 +/- 43.84
Episode length: 495.32 +/- 117.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 173      |
|    time_elapsed    | 5524     |
|    total_timesteps | 354304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=354500, episode_reward=195.68 +/- 1.57
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 354500      |
| train/                  |             |
|    approx_kl            | 0.004924803 |
|    clip_fraction        | 0.0109      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.001       |
|    loss                 | 105         |
|    n_updates            | 193         |
|    policy_gradient_loss | -0.00491    |
|    value_loss           | 186         |
-----------------------------------------
Eval num_timesteps=355000, episode_reward=193.69 +/- 6.06
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=195.49 +/- 1.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=197.41 +/- 13.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 63       |
|    iterations      | 174      |
|    time_elapsed    | 5596     |
|    total_timesteps | 356352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=356500, episode_reward=195.35 +/- 1.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 356500      |
| train/                  |             |
|    approx_kl            | 0.006225486 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.001       |
|    loss                 | 43.2        |
|    n_updates            | 194         |
|    policy_gradient_loss | -0.00421    |
|    value_loss           | 73.7        |
-----------------------------------------
Eval num_timesteps=357000, episode_reward=195.07 +/- 1.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=197.17 +/- 13.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=195.11 +/- 1.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 63       |
|    iterations      | 175      |
|    time_elapsed    | 5666     |
|    total_timesteps | 358400   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=358500, episode_reward=196.42 +/- 1.83
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 358500      |
| train/                  |             |
|    approx_kl            | 0.015873529 |
|    clip_fraction        | 0.0198      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.001       |
|    loss                 | 28.8        |
|    n_updates            | 196         |
|    policy_gradient_loss | 0.0102      |
|    value_loss           | 50.9        |
-----------------------------------------
Eval num_timesteps=359000, episode_reward=200.22 +/- 18.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
New best mean reward!
Eval num_timesteps=359500, episode_reward=195.87 +/- 2.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=196.20 +/- 1.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 176      |
|    time_elapsed    | 5736     |
|    total_timesteps | 360448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=360500, episode_reward=196.48 +/- 2.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 360500      |
| train/                  |             |
|    approx_kl            | 0.004138126 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.001       |
|    loss                 | 14.8        |
|    n_updates            | 197         |
|    policy_gradient_loss | 0.0168      |
|    value_loss           | 54.8        |
-----------------------------------------
Eval num_timesteps=361000, episode_reward=196.50 +/- 1.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=190.06 +/- 26.84
Episode length: 495.98 +/- 114.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=196.88 +/- 1.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 177      |
|    time_elapsed    | 5805     |
|    total_timesteps | 362496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=362500, episode_reward=197.53 +/- 0.88
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 0.0052276365 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.847        |
|    learning_rate        | 0.001        |
|    loss                 | 12           |
|    n_updates            | 198          |
|    policy_gradient_loss | -0.00347     |
|    value_loss           | 31.9         |
------------------------------------------
Eval num_timesteps=363000, episode_reward=197.60 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=197.59 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=197.71 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=195.56 +/- 15.60
Episode length: 515.42 +/- 67.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 178      |
|    time_elapsed    | 5893     |
|    total_timesteps | 364544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=365000, episode_reward=197.06 +/- 1.26
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.006461609 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.001       |
|    loss                 | 17.4        |
|    n_updates            | 199         |
|    policy_gradient_loss | -0.00201    |
|    value_loss           | 34.8        |
-----------------------------------------
Eval num_timesteps=365500, episode_reward=196.88 +/- 1.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=196.66 +/- 1.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=196.96 +/- 1.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 179      |
|    time_elapsed    | 5963     |
|    total_timesteps | 366592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=367000, episode_reward=195.41 +/- 1.77
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.006151966 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.001       |
|    loss                 | 53.6        |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.0368      |
|    value_loss           | 102         |
-----------------------------------------
Eval num_timesteps=367500, episode_reward=193.56 +/- 15.84
Episode length: 515.30 +/- 67.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=191.38 +/- 21.69
Episode length: 505.02 +/- 97.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=195.18 +/- 1.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 180      |
|    time_elapsed    | 6034     |
|    total_timesteps | 368640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=369000, episode_reward=194.69 +/- 1.57
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 195          |
| time/                   |              |
|    total_timesteps      | 369000       |
| train/                  |              |
|    approx_kl            | 0.0065092132 |
|    clip_fraction        | 0.0554       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.612        |
|    learning_rate        | 0.001        |
|    loss                 | 29           |
|    n_updates            | 201          |
|    policy_gradient_loss | 0.00609      |
|    value_loss           | 108          |
------------------------------------------
Eval num_timesteps=369500, episode_reward=194.78 +/- 1.90
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=195.14 +/- 1.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=194.76 +/- 1.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 181      |
|    time_elapsed    | 6106     |
|    total_timesteps | 370688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=371000, episode_reward=196.71 +/- 1.32
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.005050049 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.001       |
|    loss                 | 82.1        |
|    n_updates            | 202         |
|    policy_gradient_loss | 0.0138      |
|    value_loss           | 90.7        |
-----------------------------------------
Eval num_timesteps=371500, episode_reward=196.97 +/- 1.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=196.65 +/- 1.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=196.79 +/- 1.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 182      |
|    time_elapsed    | 6177     |
|    total_timesteps | 372736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=373000, episode_reward=194.75 +/- 15.51
Episode length: 515.18 +/- 68.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 195          |
| time/                   |              |
|    total_timesteps      | 373000       |
| train/                  |              |
|    approx_kl            | 0.0026487596 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.784        |
|    learning_rate        | 0.001        |
|    loss                 | 41.2         |
|    n_updates            | 203          |
|    policy_gradient_loss | 0.0069       |
|    value_loss           | 50.8         |
------------------------------------------
Eval num_timesteps=373500, episode_reward=197.22 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=196.95 +/- 0.98
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=188.16 +/- 29.87
Episode length: 486.34 +/- 131.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 183      |
|    time_elapsed    | 6247     |
|    total_timesteps | 374784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=375000, episode_reward=196.80 +/- 0.86
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0066832285 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.755        |
|    learning_rate        | 0.001        |
|    loss                 | 34.6         |
|    n_updates            | 204          |
|    policy_gradient_loss | 0.00843      |
|    value_loss           | 38.2         |
------------------------------------------
Eval num_timesteps=375500, episode_reward=196.79 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=196.58 +/- 0.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=196.79 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 184      |
|    time_elapsed    | 6318     |
|    total_timesteps | 376832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=377000, episode_reward=197.06 +/- 0.81
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 377000       |
| train/                  |              |
|    approx_kl            | 0.0030362334 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.825        |
|    learning_rate        | 0.001        |
|    loss                 | 44.7         |
|    n_updates            | 205          |
|    policy_gradient_loss | 0.0148       |
|    value_loss           | 36.5         |
------------------------------------------
Eval num_timesteps=377500, episode_reward=197.17 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=197.28 +/- 0.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=197.01 +/- 0.94
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 185      |
|    time_elapsed    | 6388     |
|    total_timesteps | 378880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=379000, episode_reward=196.36 +/- 1.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.010156643 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.875       |
|    learning_rate        | 0.001       |
|    loss                 | 18.6        |
|    n_updates            | 206         |
|    policy_gradient_loss | 0.0126      |
|    value_loss           | 37.9        |
-----------------------------------------
Eval num_timesteps=379500, episode_reward=193.81 +/- 15.53
Episode length: 514.90 +/- 70.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=196.45 +/- 1.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=196.16 +/- 1.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 186      |
|    time_elapsed    | 6460     |
|    total_timesteps | 380928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=381000, episode_reward=196.62 +/- 13.75
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.006132575 |
|    clip_fraction        | 0.0645      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.001       |
|    loss                 | 26.8        |
|    n_updates            | 207         |
|    policy_gradient_loss | 0.0111      |
|    value_loss           | 56.9        |
-----------------------------------------
Eval num_timesteps=381500, episode_reward=194.63 +/- 1.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=194.83 +/- 1.19
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=194.59 +/- 1.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 187      |
|    time_elapsed    | 6531     |
|    total_timesteps | 382976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=383000, episode_reward=196.92 +/- 0.89
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 383000       |
| train/                  |              |
|    approx_kl            | 0.0055216798 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.843        |
|    learning_rate        | 0.001        |
|    loss                 | 54.2         |
|    n_updates            | 208          |
|    policy_gradient_loss | 0.0182       |
|    value_loss           | 53.5         |
------------------------------------------
Eval num_timesteps=383500, episode_reward=196.91 +/- 0.95
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=197.02 +/- 0.85
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=196.73 +/- 0.95
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=197.10 +/- 1.01
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 188      |
|    time_elapsed    | 6620     |
|    total_timesteps | 385024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=385500, episode_reward=122.24 +/- 50.78
Episode length: 206.18 +/- 219.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | 122         |
| time/                   |             |
|    total_timesteps      | 385500      |
| train/                  |             |
|    approx_kl            | 0.009168657 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.001       |
|    loss                 | 25          |
|    n_updates            | 209         |
|    policy_gradient_loss | 0.00552     |
|    value_loss           | 48          |
-----------------------------------------
Eval num_timesteps=386000, episode_reward=133.07 +/- 53.70
Episode length: 255.52 +/- 229.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 133      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=122.40 +/- 51.01
Episode length: 211.02 +/- 216.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 122      |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=126.68 +/- 52.41
Episode length: 231.90 +/- 221.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 127      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 189      |
|    time_elapsed    | 6652     |
|    total_timesteps | 387072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=387500, episode_reward=176.59 +/- 41.84
Episode length: 439.96 +/- 181.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 177          |
| time/                   |              |
|    total_timesteps      | 387500       |
| train/                  |              |
|    approx_kl            | 0.0067588193 |
|    clip_fraction        | 0.0523       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.85         |
|    learning_rate        | 0.001        |
|    loss                 | 33.5         |
|    n_updates            | 210          |
|    policy_gradient_loss | 0.00569      |
|    value_loss           | 38.1         |
------------------------------------------
Eval num_timesteps=388000, episode_reward=183.13 +/- 35.35
Episode length: 468.52 +/- 153.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=172.12 +/- 45.09
Episode length: 417.82 +/- 201.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 418      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=181.10 +/- 37.80
Episode length: 457.38 +/- 167.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 190      |
|    time_elapsed    | 6713     |
|    total_timesteps | 389120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=389500, episode_reward=176.50 +/- 41.95
Episode length: 439.68 +/- 182.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 440          |
|    mean_reward          | 176          |
| time/                   |              |
|    total_timesteps      | 389500       |
| train/                  |              |
|    approx_kl            | 0.0031359757 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.841        |
|    learning_rate        | 0.001        |
|    loss                 | 21.7         |
|    n_updates            | 211          |
|    policy_gradient_loss | 0.00896      |
|    value_loss           | 42.2         |
------------------------------------------
Eval num_timesteps=390000, episode_reward=176.73 +/- 46.51
Episode length: 449.26 +/- 173.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 449      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=178.85 +/- 39.97
Episode length: 448.46 +/- 175.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 448      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=172.30 +/- 45.14
Episode length: 420.54 +/- 196.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 421      |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 191      |
|    time_elapsed    | 6773     |
|    total_timesteps | 391168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=391500, episode_reward=-7.64 +/- 19.40
Episode length: 48.20 +/- 15.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.2         |
|    mean_reward          | -7.64        |
| time/                   |              |
|    total_timesteps      | 391500       |
| train/                  |              |
|    approx_kl            | 0.0067685246 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.756        |
|    learning_rate        | 0.001        |
|    loss                 | 5.88         |
|    n_updates            | 212          |
|    policy_gradient_loss | 0.0151       |
|    value_loss           | 64.7         |
------------------------------------------
Eval num_timesteps=392000, episode_reward=8.60 +/- 49.99
Episode length: 70.48 +/- 94.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.5     |
|    mean_reward     | 8.6      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=-1.49 +/- 36.76
Episode length: 58.08 +/- 68.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | -1.49    |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=9.01 +/- 58.56
Episode length: 86.08 +/- 129.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.1     |
|    mean_reward     | 9.01     |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 192      |
|    time_elapsed    | 6783     |
|    total_timesteps | 393216   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=393500, episode_reward=98.32 +/- 32.75
Episode length: 114.32 +/- 138.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 98.3        |
| time/                   |             |
|    total_timesteps      | 393500      |
| train/                  |             |
|    approx_kl            | 0.008907498 |
|    clip_fraction        | 0.0255      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.001       |
|    loss                 | 65.6        |
|    n_updates            | 214         |
|    policy_gradient_loss | -5.55e-05   |
|    value_loss           | 50          |
-----------------------------------------
Eval num_timesteps=394000, episode_reward=104.94 +/- 40.18
Episode length: 134.28 +/- 171.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 105      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=100.56 +/- 35.61
Episode length: 122.08 +/- 150.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 101      |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=98.39 +/- 32.96
Episode length: 108.14 +/- 140.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 98.4     |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 193      |
|    time_elapsed    | 6801     |
|    total_timesteps | 395264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=395500, episode_reward=6.23 +/- 38.03
Episode length: 47.08 +/- 17.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.1        |
|    mean_reward          | 6.23        |
| time/                   |             |
|    total_timesteps      | 395500      |
| train/                  |             |
|    approx_kl            | 0.004259115 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.001       |
|    loss                 | 31.5        |
|    n_updates            | 215         |
|    policy_gradient_loss | 0.000782    |
|    value_loss           | 62.8        |
-----------------------------------------
Eval num_timesteps=396000, episode_reward=2.27 +/- 34.35
Episode length: 49.76 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | 2.27     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=16.13 +/- 44.45
Episode length: 50.92 +/- 18.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | 16.1     |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=20.09 +/- 46.18
Episode length: 50.82 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | 20.1     |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 194      |
|    time_elapsed    | 6810     |
|    total_timesteps | 397312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=397500, episode_reward=-11.59 +/- 0.00
Episode length: 48.28 +/- 18.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.3         |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 397500       |
| train/                  |              |
|    approx_kl            | 0.0061804214 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0.698        |
|    learning_rate        | 0.001        |
|    loss                 | 74.9         |
|    n_updates            | 216          |
|    policy_gradient_loss | 0.00491      |
|    value_loss           | 105          |
------------------------------------------
Eval num_timesteps=398000, episode_reward=-11.59 +/- 0.01
Episode length: 49.58 +/- 14.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=-11.60 +/- 0.00
Episode length: 48.40 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=-11.59 +/- 0.00
Episode length: 42.84 +/- 12.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 402      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 195      |
|    time_elapsed    | 6818     |
|    total_timesteps | 399360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=399500, episode_reward=-11.60 +/- 0.00
Episode length: 50.98 +/- 20.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51           |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 399500       |
| train/                  |              |
|    approx_kl            | 0.0074948254 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.762       |
|    explained_variance   | -0.102       |
|    learning_rate        | 0.001        |
|    loss                 | 400          |
|    n_updates            | 217          |
|    policy_gradient_loss | 0.00403      |
|    value_loss           | 856          |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-11.59 +/- 0.01
Episode length: 50.62 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=-11.59 +/- 0.00
Episode length: 49.26 +/- 15.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=-11.59 +/- 0.00
Episode length: 55.66 +/- 21.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 91.9     |
| time/              |          |
|    fps             | 58       |
|    iterations      | 196      |
|    time_elapsed    | 6827     |
|    total_timesteps | 401408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=401500, episode_reward=-11.59 +/- 0.00
Episode length: 52.28 +/- 16.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.3        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 401500      |
| train/                  |             |
|    approx_kl            | 0.011075037 |
|    clip_fraction        | 0.0734      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.845      |
|    explained_variance   | -0.268      |
|    learning_rate        | 0.001       |
|    loss                 | 274         |
|    n_updates            | 218         |
|    policy_gradient_loss | 0.00782     |
|    value_loss           | 702         |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=-11.59 +/- 0.00
Episode length: 51.86 +/- 16.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=-11.60 +/- 0.00
Episode length: 52.48 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=-11.59 +/- 0.00
Episode length: 46.84 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | 75.7     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 197      |
|    time_elapsed    | 6836     |
|    total_timesteps | 403456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=403500, episode_reward=-11.60 +/- 0.00
Episode length: 55.04 +/- 20.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55          |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 403500      |
| train/                  |             |
|    approx_kl            | 0.013236484 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.227       |
|    learning_rate        | 0.001       |
|    loss                 | 122         |
|    n_updates            | 219         |
|    policy_gradient_loss | -0.00413    |
|    value_loss           | 297         |
-----------------------------------------
Eval num_timesteps=404000, episode_reward=-11.59 +/- 0.00
Episode length: 50.24 +/- 13.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=-11.60 +/- 0.00
Episode length: 51.10 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=-11.59 +/- 0.00
Episode length: 50.78 +/- 19.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=-11.59 +/- 0.00
Episode length: 51.66 +/- 14.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 74.6     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 198      |
|    time_elapsed    | 6847     |
|    total_timesteps | 405504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=406000, episode_reward=-11.59 +/- 0.01
Episode length: 49.76 +/- 17.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.8         |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 406000       |
| train/                  |              |
|    approx_kl            | 0.0054956996 |
|    clip_fraction        | 0.0399       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.396        |
|    learning_rate        | 0.001        |
|    loss                 | 68.9         |
|    n_updates            | 220          |
|    policy_gradient_loss | 0.00454      |
|    value_loss           | 176          |
------------------------------------------
Eval num_timesteps=406500, episode_reward=-11.59 +/- 0.00
Episode length: 49.46 +/- 14.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=-11.59 +/- 0.00
Episode length: 54.16 +/- 22.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=-11.59 +/- 0.00
Episode length: 55.16 +/- 25.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 74.6     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 199      |
|    time_elapsed    | 6856     |
|    total_timesteps | 407552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=408000, episode_reward=-11.60 +/- 0.00
Episode length: 52.12 +/- 21.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.1        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.037498884 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.001       |
|    loss                 | 24.6        |
|    n_updates            | 222         |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 72.9        |
-----------------------------------------
Eval num_timesteps=408500, episode_reward=-11.59 +/- 0.00
Episode length: 55.12 +/- 17.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=-11.59 +/- 0.01
Episode length: 44.62 +/- 13.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=-11.60 +/- 0.00
Episode length: 57.64 +/- 18.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 55.2     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 200      |
|    time_elapsed    | 6864     |
|    total_timesteps | 409600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=410000, episode_reward=-11.59 +/- 0.00
Episode length: 51.38 +/- 17.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.4        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.006174527 |
|    clip_fraction        | 0.0361      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.941      |
|    explained_variance   | 0.205       |
|    learning_rate        | 0.001       |
|    loss                 | 175         |
|    n_updates            | 223         |
|    policy_gradient_loss | 0.0021      |
|    value_loss           | 304         |
-----------------------------------------
Eval num_timesteps=410500, episode_reward=-11.59 +/- 0.00
Episode length: 53.50 +/- 21.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=-11.59 +/- 0.00
Episode length: 44.96 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=-11.59 +/- 0.00
Episode length: 48.02 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 45.9     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 201      |
|    time_elapsed    | 6873     |
|    total_timesteps | 411648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=412000, episode_reward=-11.60 +/- 0.00
Episode length: 48.86 +/- 14.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.9         |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0053820647 |
|    clip_fraction        | 0.0303       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.466        |
|    learning_rate        | 0.001        |
|    loss                 | 90           |
|    n_updates            | 224          |
|    policy_gradient_loss | -0.00121     |
|    value_loss           | 154          |
------------------------------------------
Eval num_timesteps=412500, episode_reward=-11.60 +/- 0.00
Episode length: 48.30 +/- 11.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=-11.59 +/- 0.00
Episode length: 49.68 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=-11.59 +/- 0.00
Episode length: 50.10 +/- 14.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 46       |
| time/              |          |
|    fps             | 60       |
|    iterations      | 202      |
|    time_elapsed    | 6881     |
|    total_timesteps | 413696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=414000, episode_reward=-11.59 +/- 0.00
Episode length: 53.00 +/- 17.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53          |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.007887425 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.001       |
|    loss                 | 116         |
|    n_updates            | 225         |
|    policy_gradient_loss | 0.0158      |
|    value_loss           | 175         |
-----------------------------------------
Eval num_timesteps=414500, episode_reward=-9.62 +/- 13.86
Episode length: 57.62 +/- 17.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.6     |
|    mean_reward     | -9.62    |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=-11.59 +/- 0.00
Episode length: 48.64 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=-11.59 +/- 0.00
Episode length: 52.62 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 52.3     |
| time/              |          |
|    fps             | 60       |
|    iterations      | 203      |
|    time_elapsed    | 6890     |
|    total_timesteps | 415744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=416000, episode_reward=64.66 +/- 92.43
Episode length: 195.68 +/- 215.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 196          |
|    mean_reward          | 64.7         |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0047404426 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.43         |
|    learning_rate        | 0.001        |
|    loss                 | 88.1         |
|    n_updates            | 226          |
|    policy_gradient_loss | 0.00357      |
|    value_loss           | 110          |
------------------------------------------
Eval num_timesteps=416500, episode_reward=75.60 +/- 100.62
Episode length: 239.78 +/- 233.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 75.6     |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=87.85 +/- 97.69
Episode length: 248.34 +/- 235.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 87.9     |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=68.99 +/- 93.76
Episode length: 200.66 +/- 223.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 69       |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 60.7     |
| time/              |          |
|    fps             | 60       |
|    iterations      | 204      |
|    time_elapsed    | 6921     |
|    total_timesteps | 417792   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=418000, episode_reward=189.58 +/- 33.21
Episode length: 505.06 +/- 97.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 505         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 418000      |
| train/                  |             |
|    approx_kl            | 0.019750105 |
|    clip_fraction        | 0.0551      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.001       |
|    loss                 | 19.4        |
|    n_updates            | 228         |
|    policy_gradient_loss | 0.0111      |
|    value_loss           | 113         |
-----------------------------------------
Eval num_timesteps=418500, episode_reward=187.83 +/- 41.88
Episode length: 505.52 +/- 95.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=196.30 +/- 1.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=192.23 +/- 29.89
Episode length: 515.20 +/- 68.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | 66.3     |
| time/              |          |
|    fps             | 60       |
|    iterations      | 205      |
|    time_elapsed    | 6991     |
|    total_timesteps | 419840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=420000, episode_reward=196.46 +/- 1.34
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.004514457 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.001       |
|    loss                 | 40.1        |
|    n_updates            | 229         |
|    policy_gradient_loss | 0.0102      |
|    value_loss           | 70.3        |
-----------------------------------------
Eval num_timesteps=420500, episode_reward=196.77 +/- 1.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=196.65 +/- 1.19
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=196.78 +/- 1.19
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 72.8     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 206      |
|    time_elapsed    | 7063     |
|    total_timesteps | 421888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=422000, episode_reward=194.67 +/- 1.10
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.004623407 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.001       |
|    loss                 | 19.1        |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.00812     |
|    value_loss           | 94.4        |
-----------------------------------------
Eval num_timesteps=422500, episode_reward=192.69 +/- 15.23
Episode length: 514.76 +/- 71.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=194.87 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=194.76 +/- 1.03
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 79.1     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 207      |
|    time_elapsed    | 7134     |
|    total_timesteps | 423936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=424000, episode_reward=192.08 +/- 15.72
Episode length: 515.52 +/- 66.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | 192          |
| time/                   |              |
|    total_timesteps      | 424000       |
| train/                  |              |
|    approx_kl            | 0.0035358146 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.448        |
|    learning_rate        | 0.001        |
|    loss                 | 70.1         |
|    n_updates            | 231          |
|    policy_gradient_loss | 0.00449      |
|    value_loss           | 120          |
------------------------------------------
Eval num_timesteps=424500, episode_reward=194.67 +/- 1.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=192.07 +/- 15.30
Episode length: 515.34 +/- 67.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=194.25 +/- 1.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 258      |
|    ep_rew_mean     | 85.5     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 208      |
|    time_elapsed    | 7205     |
|    total_timesteps | 425984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=426000, episode_reward=193.09 +/- 2.12
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 426000       |
| train/                  |              |
|    approx_kl            | 0.0047294614 |
|    clip_fraction        | 0.0375       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.638        |
|    learning_rate        | 0.001        |
|    loss                 | 34.4         |
|    n_updates            | 232          |
|    policy_gradient_loss | -0.000874    |
|    value_loss           | 60.7         |
------------------------------------------
Eval num_timesteps=426500, episode_reward=185.92 +/- 27.04
Episode length: 497.12 +/- 110.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=194.07 +/- 14.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=192.96 +/- 2.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=185.54 +/- 26.04
Episode length: 495.76 +/- 115.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 273      |
|    ep_rew_mean     | 92.8     |
| time/              |          |
|    fps             | 58       |
|    iterations      | 209      |
|    time_elapsed    | 7293     |
|    total_timesteps | 428032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=428500, episode_reward=192.06 +/- 6.04
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 192         |
| time/                   |             |
|    total_timesteps      | 428500      |
| train/                  |             |
|    approx_kl            | 0.006654799 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.001       |
|    loss                 | 58.2        |
|    n_updates            | 233         |
|    policy_gradient_loss | 0.00689     |
|    value_loss           | 108         |
-----------------------------------------
Eval num_timesteps=429000, episode_reward=192.54 +/- 5.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=191.53 +/- 5.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=191.54 +/- 6.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 99.2     |
| time/              |          |
|    fps             | 58       |
|    iterations      | 210      |
|    time_elapsed    | 7364     |
|    total_timesteps | 430080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=430500, episode_reward=195.59 +/- 1.31
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 430500       |
| train/                  |              |
|    approx_kl            | 0.0076729846 |
|    clip_fraction        | 0.0671       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.689        |
|    learning_rate        | 0.001        |
|    loss                 | 14.3         |
|    n_updates            | 234          |
|    policy_gradient_loss | 0.000998     |
|    value_loss           | 60.4         |
------------------------------------------
Eval num_timesteps=431000, episode_reward=195.23 +/- 1.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=195.66 +/- 1.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=195.77 +/- 1.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 211      |
|    time_elapsed    | 7436     |
|    total_timesteps | 432128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=432500, episode_reward=197.10 +/- 1.26
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 432500       |
| train/                  |              |
|    approx_kl            | 0.0036121015 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.617        |
|    learning_rate        | 0.001        |
|    loss                 | 17.4         |
|    n_updates            | 235          |
|    policy_gradient_loss | 0.00404      |
|    value_loss           | 81.2         |
------------------------------------------
Eval num_timesteps=433000, episode_reward=197.32 +/- 1.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=196.85 +/- 1.23
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=197.03 +/- 1.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 328      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 212      |
|    time_elapsed    | 7507     |
|    total_timesteps | 434176   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=434500, episode_reward=197.66 +/- 0.73
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 434500       |
| train/                  |              |
|    approx_kl            | 0.0076988847 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.707        |
|    learning_rate        | 0.001        |
|    loss                 | 30           |
|    n_updates            | 237          |
|    policy_gradient_loss | -0.00462     |
|    value_loss           | 66.7         |
------------------------------------------
Eval num_timesteps=435000, episode_reward=197.39 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=197.57 +/- 0.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=197.49 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 345      |
|    ep_rew_mean     | 119      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 213      |
|    time_elapsed    | 7581     |
|    total_timesteps | 436224   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.03
Eval num_timesteps=436500, episode_reward=197.65 +/- 0.56
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 436500     |
| train/                  |            |
|    approx_kl            | 0.03029909 |
|    clip_fraction        | 0.0748     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.001      |
|    loss                 | 17.3       |
|    n_updates            | 242        |
|    policy_gradient_loss | -0.00113   |
|    value_loss           | 39         |
----------------------------------------
Eval num_timesteps=437000, episode_reward=197.44 +/- 0.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=197.55 +/- 0.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=197.57 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | 123      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 214      |
|    time_elapsed    | 7653     |
|    total_timesteps | 438272   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.03
Eval num_timesteps=438500, episode_reward=197.63 +/- 0.74
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 438500      |
| train/                  |             |
|    approx_kl            | 0.006982041 |
|    clip_fraction        | 0.0438      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.001       |
|    loss                 | 20.5        |
|    n_updates            | 251         |
|    policy_gradient_loss | -0.00471    |
|    value_loss           | 29.6        |
-----------------------------------------
Eval num_timesteps=439000, episode_reward=197.66 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=197.58 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=197.54 +/- 0.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 374      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 215      |
|    time_elapsed    | 7725     |
|    total_timesteps | 440320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=440500, episode_reward=197.44 +/- 0.76
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 440500      |
| train/                  |             |
|    approx_kl            | 0.003961613 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.001       |
|    loss                 | 15          |
|    n_updates            | 252         |
|    policy_gradient_loss | -0.00261    |
|    value_loss           | 34.2        |
-----------------------------------------
Eval num_timesteps=441000, episode_reward=197.65 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=197.34 +/- 0.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=197.49 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 383      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 216      |
|    time_elapsed    | 7795     |
|    total_timesteps | 442368   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
Eval num_timesteps=442500, episode_reward=197.69 +/- 0.50
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 442500      |
| train/                  |             |
|    approx_kl            | 0.016029362 |
|    clip_fraction        | 0.0661      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.001       |
|    loss                 | 13.8        |
|    n_updates            | 256         |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 35.5        |
-----------------------------------------
Eval num_timesteps=443000, episode_reward=197.48 +/- 0.85
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=197.31 +/- 0.92
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=197.30 +/- 0.94
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 387      |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 217      |
|    time_elapsed    | 7866     |
|    total_timesteps | 444416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=444500, episode_reward=197.79 +/- 0.51
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 444500       |
| train/                  |              |
|    approx_kl            | 0.0034486959 |
|    clip_fraction        | 0.00541      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.775        |
|    learning_rate        | 0.001        |
|    loss                 | 57.3         |
|    n_updates            | 257          |
|    policy_gradient_loss | 0.00649      |
|    value_loss           | 48.3         |
------------------------------------------
Eval num_timesteps=445000, episode_reward=197.71 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=197.65 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=197.71 +/- 0.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 387      |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 218      |
|    time_elapsed    | 7937     |
|    total_timesteps | 446464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=446500, episode_reward=191.04 +/- 29.90
Episode length: 495.30 +/- 117.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 446500      |
| train/                  |             |
|    approx_kl            | 0.010510254 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.001       |
|    loss                 | 56.1        |
|    n_updates            | 258         |
|    policy_gradient_loss | 0.0102      |
|    value_loss           | 41.4        |
-----------------------------------------
Eval num_timesteps=447000, episode_reward=186.28 +/- 36.51
Episode length: 476.38 +/- 145.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=188.54 +/- 25.94
Episode length: 495.52 +/- 116.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=190.99 +/- 21.86
Episode length: 505.16 +/- 97.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=186.73 +/- 29.93
Episode length: 485.44 +/- 134.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 485      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 219      |
|    time_elapsed    | 8020     |
|    total_timesteps | 448512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=449000, episode_reward=195.52 +/- 1.27
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 449000      |
| train/                  |             |
|    approx_kl            | 0.021077644 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.001       |
|    loss                 | 20          |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00115    |
|    value_loss           | 103         |
-----------------------------------------
Eval num_timesteps=449500, episode_reward=195.68 +/- 1.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=193.10 +/- 15.43
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=195.42 +/- 1.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 413      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 220      |
|    time_elapsed    | 8091     |
|    total_timesteps | 450560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=451000, episode_reward=196.48 +/- 0.93
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 196        |
| time/                   |            |
|    total_timesteps      | 451000     |
| train/                  |            |
|    approx_kl            | 0.00587852 |
|    clip_fraction        | 0.0307     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.001      |
|    loss                 | 27.4       |
|    n_updates            | 261        |
|    policy_gradient_loss | 0.00898    |
|    value_loss           | 66.4       |
----------------------------------------
Eval num_timesteps=451500, episode_reward=196.45 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=196.54 +/- 0.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=196.47 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 428      |
|    ep_rew_mean     | 149      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 221      |
|    time_elapsed    | 8163     |
|    total_timesteps | 452608   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=453000, episode_reward=196.25 +/- 1.21
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 453000       |
| train/                  |              |
|    approx_kl            | 0.0073507065 |
|    clip_fraction        | 0.0422       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.798        |
|    learning_rate        | 0.001        |
|    loss                 | 8.73         |
|    n_updates            | 263          |
|    policy_gradient_loss | 0.00371      |
|    value_loss           | 40.6         |
------------------------------------------
Eval num_timesteps=453500, episode_reward=196.03 +/- 1.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=196.24 +/- 1.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=196.11 +/- 1.02
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 437      |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 222      |
|    time_elapsed    | 8234     |
|    total_timesteps | 454656   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=455000, episode_reward=194.64 +/- 15.64
Episode length: 517.18 +/- 54.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 517         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 455000      |
| train/                  |             |
|    approx_kl            | 0.007844391 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.001       |
|    loss                 | 58.7        |
|    n_updates            | 265         |
|    policy_gradient_loss | 0.00202     |
|    value_loss           | 61.1        |
-----------------------------------------
Eval num_timesteps=455500, episode_reward=197.04 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=197.10 +/- 0.90
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=194.68 +/- 15.50
Episode length: 518.18 +/- 47.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 447      |
|    ep_rew_mean     | 155      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 223      |
|    time_elapsed    | 8306     |
|    total_timesteps | 456704   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=457000, episode_reward=191.06 +/- 29.42
Episode length: 514.92 +/- 70.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 457000       |
| train/                  |              |
|    approx_kl            | 0.0068602175 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.732        |
|    learning_rate        | 0.001        |
|    loss                 | 30           |
|    n_updates            | 267          |
|    policy_gradient_loss | 0.0094       |
|    value_loss           | 64           |
------------------------------------------
Eval num_timesteps=457500, episode_reward=195.36 +/- 1.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=189.28 +/- 33.25
Episode length: 505.06 +/- 97.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=194.96 +/- 1.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 224      |
|    time_elapsed    | 8376     |
|    total_timesteps | 458752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=459000, episode_reward=193.00 +/- 15.42
Episode length: 514.94 +/- 70.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.0040175137 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.721        |
|    learning_rate        | 0.001        |
|    loss                 | 57.8         |
|    n_updates            | 268          |
|    policy_gradient_loss | 0.0042       |
|    value_loss           | 56.1         |
------------------------------------------
Eval num_timesteps=459500, episode_reward=196.97 +/- 13.98
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=195.02 +/- 1.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=195.22 +/- 0.92
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 225      |
|    time_elapsed    | 8447     |
|    total_timesteps | 460800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=461000, episode_reward=195.67 +/- 1.27
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 461000      |
| train/                  |             |
|    approx_kl            | 0.003839985 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.001       |
|    loss                 | 36.3        |
|    n_updates            | 269         |
|    policy_gradient_loss | -0.00208    |
|    value_loss           | 59.5        |
-----------------------------------------
Eval num_timesteps=461500, episode_reward=195.35 +/- 1.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=195.58 +/- 1.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=195.40 +/- 1.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 226      |
|    time_elapsed    | 8519     |
|    total_timesteps | 462848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=463000, episode_reward=196.26 +/- 1.59
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | 196       |
| time/                   |           |
|    total_timesteps      | 463000    |
| train/                  |           |
|    approx_kl            | 0.0047427 |
|    clip_fraction        | 0.0337    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.32     |
|    explained_variance   | 0.833     |
|    learning_rate        | 0.001     |
|    loss                 | 23.6      |
|    n_updates            | 270       |
|    policy_gradient_loss | 0.00726   |
|    value_loss           | 41.3      |
---------------------------------------
Eval num_timesteps=463500, episode_reward=196.54 +/- 1.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=196.44 +/- 1.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=196.63 +/- 1.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 227      |
|    time_elapsed    | 8590     |
|    total_timesteps | 464896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=465000, episode_reward=196.96 +/- 1.03
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.005891676 |
|    clip_fraction        | 0.0526      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.001       |
|    loss                 | 36          |
|    n_updates            | 271         |
|    policy_gradient_loss | -0.0035     |
|    value_loss           | 52.6        |
-----------------------------------------
Eval num_timesteps=465500, episode_reward=196.81 +/- 0.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=196.85 +/- 0.89
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=197.00 +/- 0.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 228      |
|    time_elapsed    | 8661     |
|    total_timesteps | 466944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=467000, episode_reward=197.43 +/- 0.95
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0049924348 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.819        |
|    learning_rate        | 0.001        |
|    loss                 | 29.3         |
|    n_updates            | 272          |
|    policy_gradient_loss | -0.00277     |
|    value_loss           | 41           |
------------------------------------------
Eval num_timesteps=467500, episode_reward=197.48 +/- 0.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=197.43 +/- 0.92
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=197.52 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 229      |
|    time_elapsed    | 8731     |
|    total_timesteps | 468992   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=469000, episode_reward=194.80 +/- 16.26
Episode length: 516.00 +/- 63.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 469000      |
| train/                  |             |
|    approx_kl            | 0.006503613 |
|    clip_fraction        | 0.0559      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.001       |
|    loss                 | 10.7        |
|    n_updates            | 275         |
|    policy_gradient_loss | -0.000584   |
|    value_loss           | 41.2        |
-----------------------------------------
Eval num_timesteps=469500, episode_reward=197.10 +/- 1.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=194.83 +/- 15.83
Episode length: 515.24 +/- 68.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=197.04 +/- 1.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=196.98 +/- 2.43
Episode length: 519.08 +/- 41.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 230      |
|    time_elapsed    | 8817     |
|    total_timesteps | 471040   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=471500, episode_reward=197.22 +/- 0.71
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 471500     |
| train/                  |            |
|    approx_kl            | 0.00842034 |
|    clip_fraction        | 0.0256     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.001      |
|    loss                 | 7.53       |
|    n_updates            | 278        |
|    policy_gradient_loss | -0.00176   |
|    value_loss           | 16.7       |
----------------------------------------
Eval num_timesteps=472000, episode_reward=197.24 +/- 0.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=197.33 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=197.10 +/- 0.85
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 231      |
|    time_elapsed    | 8889     |
|    total_timesteps | 473088   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=473500, episode_reward=192.09 +/- 21.71
Episode length: 508.42 +/- 81.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 508        |
|    mean_reward          | 192        |
| time/                   |            |
|    total_timesteps      | 473500     |
| train/                  |            |
|    approx_kl            | 0.01200134 |
|    clip_fraction        | 0.0547     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.001      |
|    loss                 | 30.6       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.00226   |
|    value_loss           | 23.7       |
----------------------------------------
Eval num_timesteps=474000, episode_reward=179.00 +/- 40.26
Episode length: 468.18 +/- 133.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 468      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=185.11 +/- 32.59
Episode length: 482.40 +/- 127.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 482      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=187.65 +/- 29.58
Episode length: 494.42 +/- 103.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 232      |
|    time_elapsed    | 8954     |
|    total_timesteps | 475136   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=475500, episode_reward=197.53 +/- 0.70
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 475500      |
| train/                  |             |
|    approx_kl            | 0.020091232 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.001       |
|    loss                 | 18.9        |
|    n_updates            | 282         |
|    policy_gradient_loss | 0.00398     |
|    value_loss           | 53.7        |
-----------------------------------------
Eval num_timesteps=476000, episode_reward=197.45 +/- 0.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=197.52 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=197.40 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 233      |
|    time_elapsed    | 9026     |
|    total_timesteps | 477184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=477500, episode_reward=196.74 +/- 1.35
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 477500      |
| train/                  |             |
|    approx_kl            | 0.009204096 |
|    clip_fraction        | 0.0474      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.001       |
|    loss                 | 15.8        |
|    n_updates            | 283         |
|    policy_gradient_loss | 0.00468     |
|    value_loss           | 46.9        |
-----------------------------------------
Eval num_timesteps=478000, episode_reward=196.61 +/- 1.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=196.50 +/- 1.23
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=196.62 +/- 1.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 234      |
|    time_elapsed    | 9098     |
|    total_timesteps | 479232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=479500, episode_reward=196.45 +/- 1.31
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 479500       |
| train/                  |              |
|    approx_kl            | 0.0036892288 |
|    clip_fraction        | 0.00521      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.788        |
|    learning_rate        | 0.001        |
|    loss                 | 21.4         |
|    n_updates            | 284          |
|    policy_gradient_loss | 0.0141       |
|    value_loss           | 41.7         |
------------------------------------------
Eval num_timesteps=480000, episode_reward=196.28 +/- 1.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=196.09 +/- 1.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=196.27 +/- 1.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 235      |
|    time_elapsed    | 9170     |
|    total_timesteps | 481280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=481500, episode_reward=197.04 +/- 1.34
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 481500       |
| train/                  |              |
|    approx_kl            | 0.0036109413 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.827        |
|    learning_rate        | 0.001        |
|    loss                 | 40.2         |
|    n_updates            | 285          |
|    policy_gradient_loss | 0.0105       |
|    value_loss           | 42.9         |
------------------------------------------
Eval num_timesteps=482000, episode_reward=197.36 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=197.11 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=196.88 +/- 1.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 236      |
|    time_elapsed    | 9241     |
|    total_timesteps | 483328   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=483500, episode_reward=197.43 +/- 1.08
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 483500      |
| train/                  |             |
|    approx_kl            | 0.017469125 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.001       |
|    loss                 | 8.78        |
|    n_updates            | 287         |
|    policy_gradient_loss | 0.00924     |
|    value_loss           | 37.9        |
-----------------------------------------
Eval num_timesteps=484000, episode_reward=197.42 +/- 1.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=197.56 +/- 0.93
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=197.38 +/- 1.05
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 237      |
|    time_elapsed    | 9313     |
|    total_timesteps | 485376   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=485500, episode_reward=191.00 +/- 21.30
Episode length: 506.16 +/- 92.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 485500      |
| train/                  |             |
|    approx_kl            | 0.009420932 |
|    clip_fraction        | 0.0249      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.001       |
|    loss                 | 8.33        |
|    n_updates            | 289         |
|    policy_gradient_loss | 0.0058      |
|    value_loss           | 28.4        |
-----------------------------------------
Eval num_timesteps=486000, episode_reward=186.66 +/- 29.44
Episode length: 488.22 +/- 124.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=188.51 +/- 25.85
Episode length: 496.50 +/- 112.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=186.78 +/- 29.79
Episode length: 486.78 +/- 129.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 51       |
|    iterations      | 238      |
|    time_elapsed    | 9382     |
|    total_timesteps | 487424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=487500, episode_reward=196.08 +/- 21.17
Episode length: 515.00 +/- 70.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 487500      |
| train/                  |             |
|    approx_kl            | 0.005221153 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.001       |
|    loss                 | 47.5        |
|    n_updates            | 290         |
|    policy_gradient_loss | 0.00903     |
|    value_loss           | 74.7        |
-----------------------------------------
Eval num_timesteps=488000, episode_reward=196.04 +/- 1.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=196.00 +/- 1.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=196.11 +/- 1.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 51       |
|    iterations      | 239      |
|    time_elapsed    | 9453     |
|    total_timesteps | 489472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=489500, episode_reward=197.86 +/- 0.40
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 489500       |
| train/                  |              |
|    approx_kl            | 0.0067470395 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.816        |
|    learning_rate        | 0.001        |
|    loss                 | 35.4         |
|    n_updates            | 291          |
|    policy_gradient_loss | -0.000379    |
|    value_loss           | 63.1         |
------------------------------------------
Eval num_timesteps=490000, episode_reward=197.82 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=197.86 +/- 0.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=197.82 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=197.78 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 51       |
|    iterations      | 240      |
|    time_elapsed    | 9542     |
|    total_timesteps | 491520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=492000, episode_reward=197.80 +/- 0.46
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 492000       |
| train/                  |              |
|    approx_kl            | 0.0049163825 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.001        |
|    loss                 | 9.24         |
|    n_updates            | 292          |
|    policy_gradient_loss | -5.37e-05    |
|    value_loss           | 20.5         |
------------------------------------------
Eval num_timesteps=492500, episode_reward=197.82 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=197.83 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=197.84 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 51       |
|    iterations      | 241      |
|    time_elapsed    | 9614     |
|    total_timesteps | 493568   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=494000, episode_reward=194.41 +/- 15.67
Episode length: 516.02 +/- 62.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 494000      |
| train/                  |             |
|    approx_kl            | 0.010490499 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.001       |
|    loss                 | 17.3        |
|    n_updates            | 294         |
|    policy_gradient_loss | 0.00143     |
|    value_loss           | 30.4        |
-----------------------------------------
Eval num_timesteps=494500, episode_reward=196.78 +/- 1.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=197.01 +/- 1.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=196.69 +/- 1.85
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 51       |
|    iterations      | 242      |
|    time_elapsed    | 9686     |
|    total_timesteps | 495616   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=496000, episode_reward=197.51 +/- 0.96
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 496000      |
| train/                  |             |
|    approx_kl            | 0.009638605 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.001       |
|    loss                 | 26          |
|    n_updates            | 296         |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 50.9        |
-----------------------------------------
Eval num_timesteps=496500, episode_reward=196.68 +/- 3.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=197.00 +/- 3.04
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=196.93 +/- 3.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 51       |
|    iterations      | 243      |
|    time_elapsed    | 9758     |
|    total_timesteps | 497664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=498000, episode_reward=197.48 +/- 0.81
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 498000       |
| train/                  |              |
|    approx_kl            | 0.0038712178 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.001        |
|    loss                 | 19.8         |
|    n_updates            | 297          |
|    policy_gradient_loss | -0.00111     |
|    value_loss           | 23.2         |
------------------------------------------
Eval num_timesteps=498500, episode_reward=197.46 +/- 0.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=197.22 +/- 0.91
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=197.58 +/- 0.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 50       |
|    iterations      | 244      |
|    time_elapsed    | 9829     |
|    total_timesteps | 499712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=500000, episode_reward=193.48 +/- 15.51
Episode length: 521.62 +/- 23.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 522         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.005876922 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.001       |
|    loss                 | 13.2        |
|    n_updates            | 298         |
|    policy_gradient_loss | 0.00604     |
|    value_loss           | 44.2        |
-----------------------------------------
Eval num_timesteps=500500, episode_reward=186.37 +/- 29.96
Episode length: 498.68 +/- 91.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 499      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=186.60 +/- 29.72
Episode length: 497.08 +/- 94.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=193.60 +/- 15.67
Episode length: 517.96 +/- 49.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 50       |
|    iterations      | 245      |
|    time_elapsed    | 9899     |
|    total_timesteps | 501760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=502000, episode_reward=195.98 +/- 1.55
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 502000      |
| train/                  |             |
|    approx_kl            | 0.006735302 |
|    clip_fraction        | 0.0243      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.001       |
|    loss                 | 34.5        |
|    n_updates            | 299         |
|    policy_gradient_loss | 0.0181      |
|    value_loss           | 106         |
-----------------------------------------
Eval num_timesteps=502500, episode_reward=195.63 +/- 1.95
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
Eval num_timesteps=503000, episode_reward=195.69 +/- 1.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=503500, episode_reward=195.89 +/- 1.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 50       |
|    iterations      | 246      |
|    time_elapsed    | 9971     |
|    total_timesteps | 503808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=504000, episode_reward=197.88 +/- 0.51
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 504000       |
| train/                  |              |
|    approx_kl            | 0.0077418922 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.67         |
|    learning_rate        | 0.001        |
|    loss                 | 123          |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 117          |
------------------------------------------
Eval num_timesteps=504500, episode_reward=197.75 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
Eval num_timesteps=505000, episode_reward=197.80 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=505500, episode_reward=197.81 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 50       |
|    iterations      | 247      |
|    time_elapsed    | 10043    |
|    total_timesteps | 505856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=506000, episode_reward=197.61 +/- 0.56
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 506000       |
| train/                  |              |
|    approx_kl            | 0.0069896546 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.904        |
|    learning_rate        | 0.001        |
|    loss                 | 21.7         |
|    n_updates            | 301          |
|    policy_gradient_loss | -0.00525     |
|    value_loss           | 39.8         |
------------------------------------------
Eval num_timesteps=506500, episode_reward=197.56 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
Eval num_timesteps=507000, episode_reward=197.60 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=507500, episode_reward=197.65 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 50       |
|    iterations      | 248      |
|    time_elapsed    | 10115    |
|    total_timesteps | 507904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=508000, episode_reward=194.49 +/- 15.53
Episode length: 515.32 +/- 67.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 508000      |
| train/                  |             |
|    approx_kl            | 0.004855906 |
|    clip_fraction        | 0.0288      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.001       |
|    loss                 | 21          |
|    n_updates            | 302         |
|    policy_gradient_loss | 0.00465     |
|    value_loss           | 20.9        |
-----------------------------------------
Eval num_timesteps=508500, episode_reward=196.93 +/- 1.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
Eval num_timesteps=509000, episode_reward=194.20 +/- 15.50
Episode length: 515.30 +/- 67.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=509500, episode_reward=192.90 +/- 29.81
Episode length: 514.98 +/- 70.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 50       |
|    iterations      | 249      |
|    time_elapsed    | 10185    |
|    total_timesteps | 509952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=510000, episode_reward=197.80 +/- 0.44
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.007596299 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.001       |
|    loss                 | 27.9        |
|    n_updates            | 303         |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 68          |
-----------------------------------------
Eval num_timesteps=510500, episode_reward=197.84 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
Eval num_timesteps=511000, episode_reward=197.77 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=511500, episode_reward=197.86 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=197.89 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 250      |
|    time_elapsed    | 10274    |
|    total_timesteps | 512000   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=512500, episode_reward=197.56 +/- 0.59
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 512500      |
| train/                  |             |
|    approx_kl            | 0.009270033 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.001       |
|    loss                 | 11.8        |
|    n_updates            | 306         |
|    policy_gradient_loss | -0.00263    |
|    value_loss           | 23.3        |
-----------------------------------------
Eval num_timesteps=513000, episode_reward=197.65 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
Eval num_timesteps=513500, episode_reward=197.53 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 513500   |
---------------------------------
Eval num_timesteps=514000, episode_reward=197.59 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 251      |
|    time_elapsed    | 10346    |
|    total_timesteps | 514048   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=514500, episode_reward=197.75 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | 198       |
| time/                   |           |
|    total_timesteps      | 514500    |
| train/                  |           |
|    approx_kl            | 0.0098486 |
|    clip_fraction        | 0.0147    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.29     |
|    explained_variance   | 0.9       |
|    learning_rate        | 0.001     |
|    loss                 | 0.277     |
|    n_updates            | 311       |
|    policy_gradient_loss | -0.00025  |
|    value_loss           | 26.7      |
---------------------------------------
Eval num_timesteps=515000, episode_reward=197.79 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
Eval num_timesteps=515500, episode_reward=197.55 +/- 0.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 515500   |
---------------------------------
Eval num_timesteps=516000, episode_reward=197.71 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 252      |
|    time_elapsed    | 10417    |
|    total_timesteps | 516096   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=516500, episode_reward=197.62 +/- 0.61
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 516500     |
| train/                  |            |
|    approx_kl            | 0.01193776 |
|    clip_fraction        | 0.0259     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.001      |
|    loss                 | 11         |
|    n_updates            | 313        |
|    policy_gradient_loss | 0.00172    |
|    value_loss           | 35.8       |
----------------------------------------
Eval num_timesteps=517000, episode_reward=197.57 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
Eval num_timesteps=517500, episode_reward=197.51 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 517500   |
---------------------------------
Eval num_timesteps=518000, episode_reward=197.56 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 253      |
|    time_elapsed    | 10487    |
|    total_timesteps | 518144   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.04
Eval num_timesteps=518500, episode_reward=197.43 +/- 0.72
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 518500      |
| train/                  |             |
|    approx_kl            | 0.009015496 |
|    clip_fraction        | 0.0175      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.001       |
|    loss                 | 27.8        |
|    n_updates            | 316         |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 32.6        |
-----------------------------------------
Eval num_timesteps=519000, episode_reward=197.28 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
Eval num_timesteps=519500, episode_reward=197.41 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 519500   |
---------------------------------
Eval num_timesteps=520000, episode_reward=197.24 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 254      |
|    time_elapsed    | 10558    |
|    total_timesteps | 520192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=520500, episode_reward=197.39 +/- 0.67
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 520500       |
| train/                  |              |
|    approx_kl            | 0.0030206356 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.897        |
|    learning_rate        | 0.001        |
|    loss                 | 4.79         |
|    n_updates            | 317          |
|    policy_gradient_loss | 0.000316     |
|    value_loss           | 29.6         |
------------------------------------------
Eval num_timesteps=521000, episode_reward=197.63 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
Eval num_timesteps=521500, episode_reward=197.50 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 521500   |
---------------------------------
Eval num_timesteps=522000, episode_reward=197.35 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 255      |
|    time_elapsed    | 10628    |
|    total_timesteps | 522240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=522500, episode_reward=186.84 +/- 40.85
Episode length: 505.18 +/- 97.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | 187          |
| time/                   |              |
|    total_timesteps      | 522500       |
| train/                  |              |
|    approx_kl            | 0.0028562294 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.925        |
|    learning_rate        | 0.001        |
|    loss                 | 3.79         |
|    n_updates            | 318          |
|    policy_gradient_loss | -0.000793    |
|    value_loss           | 19.5         |
------------------------------------------
Eval num_timesteps=523000, episode_reward=191.00 +/- 21.54
Episode length: 505.22 +/- 96.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
Eval num_timesteps=523500, episode_reward=191.26 +/- 29.16
Episode length: 514.96 +/- 70.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 523500   |
---------------------------------
Eval num_timesteps=524000, episode_reward=191.03 +/- 29.14
Episode length: 514.96 +/- 70.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 256      |
|    time_elapsed    | 10696    |
|    total_timesteps | 524288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=524500, episode_reward=197.73 +/- 0.54
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 524500       |
| train/                  |              |
|    approx_kl            | 0.0045283954 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.83         |
|    learning_rate        | 0.001        |
|    loss                 | 39.5         |
|    n_updates            | 319          |
|    policy_gradient_loss | 0.000332     |
|    value_loss           | 59           |
------------------------------------------
Eval num_timesteps=525000, episode_reward=197.67 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
Eval num_timesteps=525500, episode_reward=197.66 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 525500   |
---------------------------------
Eval num_timesteps=526000, episode_reward=197.57 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 257      |
|    time_elapsed    | 10766    |
|    total_timesteps | 526336   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=526500, episode_reward=197.88 +/- 0.32
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 526500      |
| train/                  |             |
|    approx_kl            | 0.013881465 |
|    clip_fraction        | 0.0676      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.001       |
|    loss                 | 1.41        |
|    n_updates            | 321         |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 9.45        |
-----------------------------------------
Eval num_timesteps=527000, episode_reward=197.78 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
Eval num_timesteps=527500, episode_reward=197.86 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 527500   |
---------------------------------
Eval num_timesteps=528000, episode_reward=197.93 +/- 0.25
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 258      |
|    time_elapsed    | 10837    |
|    total_timesteps | 528384   |
---------------------------------
Eval num_timesteps=528500, episode_reward=197.75 +/- 0.47
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 528500       |
| train/                  |              |
|    approx_kl            | 0.0045973607 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.001        |
|    loss                 | 7.73         |
|    n_updates            | 331          |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 21.6         |
------------------------------------------
Eval num_timesteps=529000, episode_reward=197.78 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
Eval num_timesteps=529500, episode_reward=197.79 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 529500   |
---------------------------------
Eval num_timesteps=530000, episode_reward=197.88 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 259      |
|    time_elapsed    | 10909    |
|    total_timesteps | 530432   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=530500, episode_reward=197.52 +/- 0.58
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 530500       |
| train/                  |              |
|    approx_kl            | 0.0045333817 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 5.11         |
|    n_updates            | 333          |
|    policy_gradient_loss | 0.006        |
|    value_loss           | 17.5         |
------------------------------------------
Eval num_timesteps=531000, episode_reward=197.64 +/- 0.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
Eval num_timesteps=531500, episode_reward=197.61 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 531500   |
---------------------------------
Eval num_timesteps=532000, episode_reward=197.62 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 260      |
|    time_elapsed    | 10981    |
|    total_timesteps | 532480   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=532500, episode_reward=192.15 +/- 21.74
Episode length: 506.10 +/- 92.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 192          |
| time/                   |              |
|    total_timesteps      | 532500       |
| train/                  |              |
|    approx_kl            | 0.0047194636 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 9.2          |
|    n_updates            | 338          |
|    policy_gradient_loss | -0.00209     |
|    value_loss           | 12.9         |
------------------------------------------
Eval num_timesteps=533000, episode_reward=194.52 +/- 15.51
Episode length: 515.40 +/- 67.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=192.22 +/- 21.66
Episode length: 506.74 +/- 89.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
Eval num_timesteps=534000, episode_reward=185.31 +/- 32.75
Episode length: 476.52 +/- 145.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=534500, episode_reward=187.80 +/- 29.86
Episode length: 487.82 +/- 126.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 261      |
|    time_elapsed    | 11065    |
|    total_timesteps | 534528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=535000, episode_reward=192.68 +/- 21.61
Episode length: 505.64 +/- 94.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 0.0034646413 |
|    clip_fraction        | 0.00568      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.001        |
|    loss                 | 24.9         |
|    n_updates            | 339          |
|    policy_gradient_loss | 0.00747      |
|    value_loss           | 42.8         |
------------------------------------------
Eval num_timesteps=535500, episode_reward=194.42 +/- 15.35
Episode length: 515.08 +/- 69.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
Eval num_timesteps=536000, episode_reward=194.55 +/- 15.34
Episode length: 515.70 +/- 65.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=536500, episode_reward=196.83 +/- 1.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 262      |
|    time_elapsed    | 11134    |
|    total_timesteps | 536576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=537000, episode_reward=194.42 +/- 15.62
Episode length: 515.06 +/- 69.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 537000      |
| train/                  |             |
|    approx_kl            | 0.006580053 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.001       |
|    loss                 | 14.1        |
|    n_updates            | 340         |
|    policy_gradient_loss | 0.000991    |
|    value_loss           | 82.4        |
-----------------------------------------
Eval num_timesteps=537500, episode_reward=190.19 +/- 32.91
Episode length: 504.88 +/- 98.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
Eval num_timesteps=538000, episode_reward=196.45 +/- 1.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=538500, episode_reward=192.31 +/- 29.43
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 263      |
|    time_elapsed    | 11205    |
|    total_timesteps | 538624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=539000, episode_reward=194.04 +/- 15.75
Episode length: 515.48 +/- 66.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 539000      |
| train/                  |             |
|    approx_kl            | 0.006291913 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.001       |
|    loss                 | 22.6        |
|    n_updates            | 341         |
|    policy_gradient_loss | 0.0221      |
|    value_loss           | 43.1        |
-----------------------------------------
Eval num_timesteps=539500, episode_reward=191.98 +/- 29.27
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
Eval num_timesteps=540000, episode_reward=192.21 +/- 29.30
Episode length: 515.06 +/- 69.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=540500, episode_reward=196.71 +/- 1.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 264      |
|    time_elapsed    | 11276    |
|    total_timesteps | 540672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=541000, episode_reward=197.43 +/- 0.80
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 541000       |
| train/                  |              |
|    approx_kl            | 0.0052266982 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 55.7         |
|    n_updates            | 342          |
|    policy_gradient_loss | 0.00275      |
|    value_loss           | 73.3         |
------------------------------------------
Eval num_timesteps=541500, episode_reward=197.42 +/- 0.95
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
Eval num_timesteps=542000, episode_reward=197.54 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=542500, episode_reward=197.50 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 265      |
|    time_elapsed    | 11347    |
|    total_timesteps | 542720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=543000, episode_reward=197.56 +/- 0.72
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 543000       |
| train/                  |              |
|    approx_kl            | 0.0034578305 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.761        |
|    learning_rate        | 0.001        |
|    loss                 | 33.5         |
|    n_updates            | 343          |
|    policy_gradient_loss | 0.00186      |
|    value_loss           | 54.3         |
------------------------------------------
Eval num_timesteps=543500, episode_reward=197.64 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
Eval num_timesteps=544000, episode_reward=197.57 +/- 0.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=544500, episode_reward=197.58 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 266      |
|    time_elapsed    | 11418    |
|    total_timesteps | 544768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=545000, episode_reward=197.79 +/- 0.62
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 545000      |
| train/                  |             |
|    approx_kl            | 0.007920309 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.001       |
|    loss                 | 26.2        |
|    n_updates            | 344         |
|    policy_gradient_loss | 0.00754     |
|    value_loss           | 38.6        |
-----------------------------------------
Eval num_timesteps=545500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
Eval num_timesteps=546000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=546500, episode_reward=197.93 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 267      |
|    time_elapsed    | 11489    |
|    total_timesteps | 546816   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=547000, episode_reward=197.91 +/- 0.31
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 547000       |
| train/                  |              |
|    approx_kl            | 0.0119021125 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 8.15         |
|    n_updates            | 346          |
|    policy_gradient_loss | 0.000218     |
|    value_loss           | 13.2         |
------------------------------------------
Eval num_timesteps=547500, episode_reward=197.90 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
Eval num_timesteps=548000, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=548500, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 268      |
|    time_elapsed    | 11560    |
|    total_timesteps | 548864   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=549000, episode_reward=197.95 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 549000      |
| train/                  |             |
|    approx_kl            | 0.006181306 |
|    clip_fraction        | 0.0373      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.001       |
|    loss                 | 9.28        |
|    n_updates            | 348         |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 12.1        |
-----------------------------------------
Eval num_timesteps=549500, episode_reward=197.89 +/- 0.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=197.87 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=550500, episode_reward=197.87 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 269      |
|    time_elapsed    | 11632    |
|    total_timesteps | 550912   |
---------------------------------
Eval num_timesteps=551000, episode_reward=197.90 +/- 0.36
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 551000     |
| train/                  |            |
|    approx_kl            | 0.00604806 |
|    clip_fraction        | 0.0328     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.001      |
|    loss                 | 3.79       |
|    n_updates            | 358        |
|    policy_gradient_loss | -0.00239   |
|    value_loss           | 15         |
----------------------------------------
Eval num_timesteps=551500, episode_reward=197.90 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
Eval num_timesteps=552000, episode_reward=197.94 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=552500, episode_reward=197.88 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 270      |
|    time_elapsed    | 11704    |
|    total_timesteps | 552960   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=553000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 553000      |
| train/                  |             |
|    approx_kl            | 0.010630241 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 4.7         |
|    n_updates            | 360         |
|    policy_gradient_loss | 0.000329    |
|    value_loss           | 8.09        |
-----------------------------------------
Eval num_timesteps=553500, episode_reward=197.92 +/- 0.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
Eval num_timesteps=554000, episode_reward=197.91 +/- 0.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=554500, episode_reward=197.91 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=197.91 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 271      |
|    time_elapsed    | 11793    |
|    total_timesteps | 555008   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=555500, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 555500      |
| train/                  |             |
|    approx_kl            | 0.004439539 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.001       |
|    loss                 | 13.8        |
|    n_updates            | 361         |
|    policy_gradient_loss | 0.00237     |
|    value_loss           | 16.3        |
-----------------------------------------
Eval num_timesteps=556000, episode_reward=197.88 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
Eval num_timesteps=556500, episode_reward=197.82 +/- 0.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 556500   |
---------------------------------
Eval num_timesteps=557000, episode_reward=197.87 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 272      |
|    time_elapsed    | 11865    |
|    total_timesteps | 557056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=557500, episode_reward=197.87 +/- 0.38
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 557500      |
| train/                  |             |
|    approx_kl            | 0.005464323 |
|    clip_fraction        | 0.0449      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 12.5        |
|    n_updates            | 362         |
|    policy_gradient_loss | -0.000245   |
|    value_loss           | 17          |
-----------------------------------------
Eval num_timesteps=558000, episode_reward=197.90 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=558500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 558500   |
---------------------------------
Eval num_timesteps=559000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 273      |
|    time_elapsed    | 11936    |
|    total_timesteps | 559104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=559500, episode_reward=197.90 +/- 0.38
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 559500       |
| train/                  |              |
|    approx_kl            | 0.0033182283 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.001        |
|    loss                 | 2.02         |
|    n_updates            | 363          |
|    policy_gradient_loss | 0.00127      |
|    value_loss           | 7.46         |
------------------------------------------
Eval num_timesteps=560000, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=560500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 560500   |
---------------------------------
Eval num_timesteps=561000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 274      |
|    time_elapsed    | 12006    |
|    total_timesteps | 561152   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=561500, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 561500       |
| train/                  |              |
|    approx_kl            | 0.0077476283 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.001        |
|    loss                 | 14           |
|    n_updates            | 365          |
|    policy_gradient_loss | 0.00135      |
|    value_loss           | 24.6         |
------------------------------------------
Eval num_timesteps=562000, episode_reward=197.92 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
Eval num_timesteps=562500, episode_reward=197.86 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 562500   |
---------------------------------
Eval num_timesteps=563000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 275      |
|    time_elapsed    | 12078    |
|    total_timesteps | 563200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=563500, episode_reward=197.83 +/- 0.46
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 563500       |
| train/                  |              |
|    approx_kl            | 0.0035230578 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.001        |
|    loss                 | 30.2         |
|    n_updates            | 366          |
|    policy_gradient_loss | 0.00409      |
|    value_loss           | 30.7         |
------------------------------------------
Eval num_timesteps=564000, episode_reward=197.87 +/- 0.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=564500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 564500   |
---------------------------------
Eval num_timesteps=565000, episode_reward=197.91 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 276      |
|    time_elapsed    | 12150    |
|    total_timesteps | 565248   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=565500, episode_reward=197.86 +/- 0.42
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 565500      |
| train/                  |             |
|    approx_kl            | 0.002867176 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 12.4        |
|    n_updates            | 367         |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 13.7        |
-----------------------------------------
Eval num_timesteps=566000, episode_reward=197.91 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
Eval num_timesteps=566500, episode_reward=197.91 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 566500   |
---------------------------------
Eval num_timesteps=567000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 177      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 277      |
|    time_elapsed    | 12221    |
|    total_timesteps | 567296   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=567500, episode_reward=197.89 +/- 0.33
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 567500       |
| train/                  |              |
|    approx_kl            | 0.0032258038 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 3.82         |
|    n_updates            | 369          |
|    policy_gradient_loss | -0.000361    |
|    value_loss           | 11.8         |
------------------------------------------
Eval num_timesteps=568000, episode_reward=197.85 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=568500, episode_reward=197.83 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 568500   |
---------------------------------
Eval num_timesteps=569000, episode_reward=197.87 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 278      |
|    time_elapsed    | 12292    |
|    total_timesteps | 569344   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=569500, episode_reward=197.31 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 569500     |
| train/                  |            |
|    approx_kl            | 0.08515987 |
|    clip_fraction        | 0.00368    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.001      |
|    loss                 | 50.7       |
|    n_updates            | 371        |
|    policy_gradient_loss | 0.00349    |
|    value_loss           | 36.5       |
----------------------------------------
Eval num_timesteps=570000, episode_reward=197.51 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
Eval num_timesteps=570500, episode_reward=197.41 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 570500   |
---------------------------------
Eval num_timesteps=571000, episode_reward=196.98 +/- 0.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 279      |
|    time_elapsed    | 12363    |
|    total_timesteps | 571392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=571500, episode_reward=195.33 +/- 2.16
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 571500      |
| train/                  |             |
|    approx_kl            | 0.004654504 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.001       |
|    loss                 | 6.67        |
|    n_updates            | 372         |
|    policy_gradient_loss | 0.00499     |
|    value_loss           | 26.1        |
-----------------------------------------
Eval num_timesteps=572000, episode_reward=195.12 +/- 2.03
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=572500, episode_reward=195.83 +/- 1.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 572500   |
---------------------------------
Eval num_timesteps=573000, episode_reward=187.02 +/- 41.81
Episode length: 505.28 +/- 96.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 280      |
|    time_elapsed    | 12433    |
|    total_timesteps | 573440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=573500, episode_reward=197.60 +/- 0.61
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 573500       |
| train/                  |              |
|    approx_kl            | 0.0051243748 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.763        |
|    learning_rate        | 0.001        |
|    loss                 | 54.7         |
|    n_updates            | 373          |
|    policy_gradient_loss | 0.00339      |
|    value_loss           | 67.1         |
------------------------------------------
Eval num_timesteps=574000, episode_reward=197.65 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
Eval num_timesteps=574500, episode_reward=197.59 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 574500   |
---------------------------------
Eval num_timesteps=575000, episode_reward=197.64 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 281      |
|    time_elapsed    | 12505    |
|    total_timesteps | 575488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=575500, episode_reward=197.53 +/- 0.60
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 575500       |
| train/                  |              |
|    approx_kl            | 0.0030332508 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.001        |
|    loss                 | 21.7         |
|    n_updates            | 374          |
|    policy_gradient_loss | 0.000996     |
|    value_loss           | 22.7         |
------------------------------------------
Eval num_timesteps=576000, episode_reward=197.55 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=197.53 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
Eval num_timesteps=577000, episode_reward=197.52 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=577500, episode_reward=197.50 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 282      |
|    time_elapsed    | 12592    |
|    total_timesteps | 577536   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=578000, episode_reward=197.88 +/- 0.37
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 578000      |
| train/                  |             |
|    approx_kl            | 0.008997406 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.001       |
|    loss                 | 3.82        |
|    n_updates            | 376         |
|    policy_gradient_loss | 0.000343    |
|    value_loss           | 17.3        |
-----------------------------------------
Eval num_timesteps=578500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
Eval num_timesteps=579000, episode_reward=197.90 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=579500, episode_reward=197.91 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 283      |
|    time_elapsed    | 12664    |
|    total_timesteps | 579584   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=580000, episode_reward=197.68 +/- 0.68
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.015313062 |
|    clip_fraction        | 0.0438      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 1.96        |
|    n_updates            | 379         |
|    policy_gradient_loss | 0.000313    |
|    value_loss           | 8.34        |
-----------------------------------------
Eval num_timesteps=580500, episode_reward=197.39 +/- 0.98
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
Eval num_timesteps=581000, episode_reward=197.52 +/- 0.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=581500, episode_reward=197.66 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 284      |
|    time_elapsed    | 12735    |
|    total_timesteps | 581632   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=582000, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 582000      |
| train/                  |             |
|    approx_kl            | 0.006670072 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.001       |
|    loss                 | 82          |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00299    |
|    value_loss           | 36.4        |
-----------------------------------------
Eval num_timesteps=582500, episode_reward=197.94 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
Eval num_timesteps=583000, episode_reward=197.91 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=583500, episode_reward=197.82 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 285      |
|    time_elapsed    | 12805    |
|    total_timesteps | 583680   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=584000, episode_reward=193.15 +/- 29.41
Episode length: 515.02 +/- 69.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | 193          |
| time/                   |              |
|    total_timesteps      | 584000       |
| train/                  |              |
|    approx_kl            | 0.0077133276 |
|    clip_fraction        | 0.00426      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 20.6         |
|    n_updates            | 382          |
|    policy_gradient_loss | 0.00353      |
|    value_loss           | 17.4         |
------------------------------------------
Eval num_timesteps=584500, episode_reward=192.84 +/- 21.65
Episode length: 508.78 +/- 81.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 509      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
Eval num_timesteps=585000, episode_reward=197.44 +/- 0.91
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=585500, episode_reward=197.36 +/- 1.06
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 286      |
|    time_elapsed    | 12876    |
|    total_timesteps | 585728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=586000, episode_reward=179.21 +/- 40.41
Episode length: 463.20 +/- 143.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 463         |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 586000      |
| train/                  |             |
|    approx_kl            | 0.007704956 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.001       |
|    loss                 | 7.92        |
|    n_updates            | 383         |
|    policy_gradient_loss | 0.0204      |
|    value_loss           | 50.6        |
-----------------------------------------
Eval num_timesteps=586500, episode_reward=157.76 +/- 60.14
Episode length: 402.48 +/- 183.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 402      |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
Eval num_timesteps=587000, episode_reward=179.66 +/- 45.09
Episode length: 470.30 +/- 137.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 470      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=587500, episode_reward=153.32 +/- 61.03
Episode length: 389.88 +/- 182.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 390      |
|    mean_reward     | 153      |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 522      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 287      |
|    time_elapsed    | 12934    |
|    total_timesteps | 587776   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=588000, episode_reward=57.68 +/- 45.40
Episode length: 138.92 +/- 89.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 139         |
|    mean_reward          | 57.7        |
| time/                   |             |
|    total_timesteps      | 588000      |
| train/                  |             |
|    approx_kl            | 0.008960974 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.001       |
|    loss                 | 8.79        |
|    n_updates            | 384         |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 49.1        |
-----------------------------------------
Eval num_timesteps=588500, episode_reward=73.50 +/- 34.34
Episode length: 170.66 +/- 80.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 73.5     |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
Eval num_timesteps=589000, episode_reward=69.56 +/- 38.02
Episode length: 170.44 +/- 95.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | 69.6     |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
Eval num_timesteps=589500, episode_reward=81.66 +/- 31.44
Episode length: 180.30 +/- 83.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 81.7     |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 288      |
|    time_elapsed    | 12958    |
|    total_timesteps | 589824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=590000, episode_reward=8.20 +/- 39.60
Episode length: 71.24 +/- 55.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 71.2         |
|    mean_reward          | 8.2          |
| time/                   |              |
|    total_timesteps      | 590000       |
| train/                  |              |
|    approx_kl            | 0.0055672517 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.77        |
|    explained_variance   | 0.0278       |
|    learning_rate        | 0.001        |
|    loss                 | 151          |
|    n_updates            | 385          |
|    policy_gradient_loss | 0.0122       |
|    value_loss           | 505          |
------------------------------------------
Eval num_timesteps=590500, episode_reward=14.15 +/- 43.42
Episode length: 79.76 +/- 76.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.8     |
|    mean_reward     | 14.1     |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
Eval num_timesteps=591000, episode_reward=4.24 +/- 36.29
Episode length: 78.52 +/- 70.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 4.24     |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=591500, episode_reward=12.16 +/- 42.28
Episode length: 81.38 +/- 71.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.4     |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 449      |
|    ep_rew_mean     | 166      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 289      |
|    time_elapsed    | 12970    |
|    total_timesteps | 591872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=592000, episode_reward=-7.63 +/- 19.40
Episode length: 54.76 +/- 36.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54.8         |
|    mean_reward          | -7.63        |
| time/                   |              |
|    total_timesteps      | 592000       |
| train/                  |              |
|    approx_kl            | 0.0077014104 |
|    clip_fraction        | 0.0413       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.757       |
|    explained_variance   | -0.166       |
|    learning_rate        | 0.001        |
|    loss                 | 306          |
|    n_updates            | 386          |
|    policy_gradient_loss | 0.0127       |
|    value_loss           | 415          |
------------------------------------------
Eval num_timesteps=592500, episode_reward=-11.60 +/- 0.00
Episode length: 48.78 +/- 18.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
Eval num_timesteps=593000, episode_reward=-11.60 +/- 0.00
Episode length: 50.28 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
Eval num_timesteps=593500, episode_reward=-9.61 +/- 13.86
Episode length: 55.20 +/- 25.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -9.61    |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 321      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 290      |
|    time_elapsed    | 12979    |
|    total_timesteps | 593920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=594000, episode_reward=-11.59 +/- 0.01
Episode length: 50.88 +/- 17.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.9        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 594000      |
| train/                  |             |
|    approx_kl            | 0.008982703 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.514      |
|    explained_variance   | -0.779      |
|    learning_rate        | 0.001       |
|    loss                 | 186         |
|    n_updates            | 387         |
|    policy_gradient_loss | 0.00787     |
|    value_loss           | 500         |
-----------------------------------------
Eval num_timesteps=594500, episode_reward=-11.59 +/- 0.00
Episode length: 49.16 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
Eval num_timesteps=595000, episode_reward=-11.60 +/- 0.00
Episode length: 52.30 +/- 15.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=595500, episode_reward=-11.59 +/- 0.00
Episode length: 49.28 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 72.4     |
| time/              |          |
|    fps             | 45       |
|    iterations      | 291      |
|    time_elapsed    | 12988    |
|    total_timesteps | 595968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=596000, episode_reward=-11.59 +/- 0.01
Episode length: 50.18 +/- 18.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.2        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 596000      |
| train/                  |             |
|    approx_kl            | 0.015966618 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.685      |
|    explained_variance   | -0.271      |
|    learning_rate        | 0.001       |
|    loss                 | 107         |
|    n_updates            | 388         |
|    policy_gradient_loss | 0.0105      |
|    value_loss           | 311         |
-----------------------------------------
Eval num_timesteps=596500, episode_reward=-11.59 +/- 0.00
Episode length: 48.90 +/- 15.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
Eval num_timesteps=597000, episode_reward=-11.59 +/- 0.01
Episode length: 54.58 +/- 20.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=597500, episode_reward=-11.59 +/- 0.01
Episode length: 51.40 +/- 15.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=-11.59 +/- 0.00
Episode length: 51.34 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 53.8     |
| time/              |          |
|    fps             | 46       |
|    iterations      | 292      |
|    time_elapsed    | 12998    |
|    total_timesteps | 598016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=598500, episode_reward=-11.59 +/- 0.01
Episode length: 50.86 +/- 19.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.9        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 598500      |
| train/                  |             |
|    approx_kl            | 0.009349086 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.981      |
|    explained_variance   | 0.174       |
|    learning_rate        | 0.001       |
|    loss                 | 148         |
|    n_updates            | 389         |
|    policy_gradient_loss | 0.00961     |
|    value_loss           | 329         |
-----------------------------------------
Eval num_timesteps=599000, episode_reward=-11.59 +/- 0.00
Episode length: 48.92 +/- 19.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
Eval num_timesteps=599500, episode_reward=-11.59 +/- 0.00
Episode length: 49.08 +/- 17.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 599500   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-11.59 +/- 0.00
Episode length: 47.40 +/- 19.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.1     |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 46       |
|    iterations      | 293      |
|    time_elapsed    | 13007    |
|    total_timesteps | 600064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=600500, episode_reward=-11.59 +/- 0.00
Episode length: 47.52 +/- 15.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.5        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 600500      |
| train/                  |             |
|    approx_kl            | 0.010519533 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.957      |
|    explained_variance   | -0.0383     |
|    learning_rate        | 0.001       |
|    loss                 | 65.3        |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00316    |
|    value_loss           | 183         |
-----------------------------------------
Eval num_timesteps=601000, episode_reward=-11.60 +/- 0.00
Episode length: 53.40 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 601000   |
---------------------------------
Eval num_timesteps=601500, episode_reward=-11.58 +/- 0.09
Episode length: 51.56 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 601500   |
---------------------------------
Eval num_timesteps=602000, episode_reward=-11.59 +/- 0.00
Episode length: 52.50 +/- 19.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 602000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.6     |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    fps             | 46       |
|    iterations      | 294      |
|    time_elapsed    | 13016    |
|    total_timesteps | 602112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=602500, episode_reward=-11.59 +/- 0.00
Episode length: 48.74 +/- 17.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 602500      |
| train/                  |             |
|    approx_kl            | 0.011431801 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.0776      |
|    learning_rate        | 0.001       |
|    loss                 | 32          |
|    n_updates            | 391         |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 72.8        |
-----------------------------------------
Eval num_timesteps=603000, episode_reward=-11.59 +/- 0.01
Episode length: 50.30 +/- 20.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 603000   |
---------------------------------
Eval num_timesteps=603500, episode_reward=-11.59 +/- 0.00
Episode length: 51.96 +/- 15.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 603500   |
---------------------------------
Eval num_timesteps=604000, episode_reward=-11.59 +/- 0.01
Episode length: 46.94 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 604000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.8     |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 46       |
|    iterations      | 295      |
|    time_elapsed    | 13024    |
|    total_timesteps | 604160   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=604500, episode_reward=-11.60 +/- 0.00
Episode length: 54.08 +/- 16.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.1        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 604500      |
| train/                  |             |
|    approx_kl            | 0.008779818 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.0326      |
|    learning_rate        | 0.001       |
|    loss                 | 63          |
|    n_updates            | 392         |
|    policy_gradient_loss | -0.00559    |
|    value_loss           | 170         |
-----------------------------------------
Eval num_timesteps=605000, episode_reward=-11.59 +/- 0.00
Episode length: 49.24 +/- 14.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 605000   |
---------------------------------
Eval num_timesteps=605500, episode_reward=-11.59 +/- 0.01
Episode length: 45.80 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 605500   |
---------------------------------
Eval num_timesteps=606000, episode_reward=-11.60 +/- 0.00
Episode length: 51.56 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 606000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 27.5     |
| time/              |          |
|    fps             | 46       |
|    iterations      | 296      |
|    time_elapsed    | 13033    |
|    total_timesteps | 606208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=606500, episode_reward=-11.60 +/- 0.00
Episode length: 46.90 +/- 16.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.9         |
|    mean_reward          | -11.6        |
| time/                   |              |
|    total_timesteps      | 606500       |
| train/                  |              |
|    approx_kl            | 0.0071322713 |
|    clip_fraction        | 0.116        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.0416       |
|    learning_rate        | 0.001        |
|    loss                 | 65.6         |
|    n_updates            | 393          |
|    policy_gradient_loss | 0.00428      |
|    value_loss           | 180          |
------------------------------------------
Eval num_timesteps=607000, episode_reward=-9.61 +/- 13.86
Episode length: 48.82 +/- 18.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -9.61    |
| time/              |          |
|    total_timesteps | 607000   |
---------------------------------
Eval num_timesteps=607500, episode_reward=-9.61 +/- 13.86
Episode length: 48.42 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -9.61    |
| time/              |          |
|    total_timesteps | 607500   |
---------------------------------
Eval num_timesteps=608000, episode_reward=-11.60 +/- 0.00
Episode length: 51.60 +/- 15.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 608000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 29.9     |
| time/              |          |
|    fps             | 46       |
|    iterations      | 297      |
|    time_elapsed    | 13041    |
|    total_timesteps | 608256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=608500, episode_reward=49.73 +/- 48.01
Episode length: 61.20 +/- 22.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 61.2         |
|    mean_reward          | 49.7         |
| time/                   |              |
|    total_timesteps      | 608500       |
| train/                  |              |
|    approx_kl            | 0.0049196235 |
|    clip_fraction        | 0.0647       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.0526       |
|    learning_rate        | 0.001        |
|    loss                 | 178          |
|    n_updates            | 394          |
|    policy_gradient_loss | -0.00264     |
|    value_loss           | 236          |
------------------------------------------
Eval num_timesteps=609000, episode_reward=41.85 +/- 49.32
Episode length: 59.10 +/- 21.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 41.8     |
| time/              |          |
|    total_timesteps | 609000   |
---------------------------------
Eval num_timesteps=609500, episode_reward=49.69 +/- 48.02
Episode length: 63.44 +/- 25.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.4     |
|    mean_reward     | 49.7     |
| time/              |          |
|    total_timesteps | 609500   |
---------------------------------
Eval num_timesteps=610000, episode_reward=39.75 +/- 49.33
Episode length: 62.26 +/- 25.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.3     |
|    mean_reward     | 39.7     |
| time/              |          |
|    total_timesteps | 610000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.9     |
|    ep_rew_mean     | 43.2     |
| time/              |          |
|    fps             | 46       |
|    iterations      | 298      |
|    time_elapsed    | 13051    |
|    total_timesteps | 610304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=610500, episode_reward=-7.71 +/- 19.42
Episode length: 50.40 +/- 19.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.4         |
|    mean_reward          | -7.71        |
| time/                   |              |
|    total_timesteps      | 610500       |
| train/                  |              |
|    approx_kl            | 0.0072108405 |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.001        |
|    loss                 | 248          |
|    n_updates            | 395          |
|    policy_gradient_loss | 0.00382      |
|    value_loss           | 361          |
------------------------------------------
Eval num_timesteps=611000, episode_reward=-1.84 +/- 29.68
Episode length: 55.02 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 611000   |
---------------------------------
Eval num_timesteps=611500, episode_reward=-3.88 +/- 26.55
Episode length: 53.82 +/- 19.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -3.88    |
| time/              |          |
|    total_timesteps | 611500   |
---------------------------------
Eval num_timesteps=612000, episode_reward=-3.92 +/- 26.79
Episode length: 48.24 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -3.92    |
| time/              |          |
|    total_timesteps | 612000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.7     |
|    ep_rew_mean     | 54.2     |
| time/              |          |
|    fps             | 46       |
|    iterations      | 299      |
|    time_elapsed    | 13060    |
|    total_timesteps | 612352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=612500, episode_reward=72.57 +/- 5.71
Episode length: 60.36 +/- 21.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60.4        |
|    mean_reward          | 72.6        |
| time/                   |             |
|    total_timesteps      | 612500      |
| train/                  |             |
|    approx_kl            | 0.011661176 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.001       |
|    loss                 | 66.4        |
|    n_updates            | 396         |
|    policy_gradient_loss | 0.0126      |
|    value_loss           | 306         |
-----------------------------------------
Eval num_timesteps=613000, episode_reward=71.95 +/- 5.14
Episode length: 64.06 +/- 23.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.1     |
|    mean_reward     | 71.9     |
| time/              |          |
|    total_timesteps | 613000   |
---------------------------------
Eval num_timesteps=613500, episode_reward=68.43 +/- 19.58
Episode length: 61.32 +/- 20.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.3     |
|    mean_reward     | 68.4     |
| time/              |          |
|    total_timesteps | 613500   |
---------------------------------
Eval num_timesteps=614000, episode_reward=67.19 +/- 24.44
Episode length: 59.90 +/- 24.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.9     |
|    mean_reward     | 67.2     |
| time/              |          |
|    total_timesteps | 614000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.4     |
|    ep_rew_mean     | 57.6     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 300      |
|    time_elapsed    | 13070    |
|    total_timesteps | 614400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=614500, episode_reward=71.85 +/- 4.65
Episode length: 62.34 +/- 16.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 62.3         |
|    mean_reward          | 71.8         |
| time/                   |              |
|    total_timesteps      | 614500       |
| train/                  |              |
|    approx_kl            | 0.0075807082 |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.321        |
|    learning_rate        | 0.001        |
|    loss                 | 109          |
|    n_updates            | 397          |
|    policy_gradient_loss | -0.00826     |
|    value_loss           | 281          |
------------------------------------------
Eval num_timesteps=615000, episode_reward=72.13 +/- 5.70
Episode length: 61.80 +/- 21.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.8     |
|    mean_reward     | 72.1     |
| time/              |          |
|    total_timesteps | 615000   |
---------------------------------
Eval num_timesteps=615500, episode_reward=71.89 +/- 5.22
Episode length: 62.20 +/- 19.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.2     |
|    mean_reward     | 71.9     |
| time/              |          |
|    total_timesteps | 615500   |
---------------------------------
Eval num_timesteps=616000, episode_reward=72.63 +/- 5.70
Episode length: 60.66 +/- 23.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.7     |
|    mean_reward     | 72.6     |
| time/              |          |
|    total_timesteps | 616000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.3     |
|    ep_rew_mean     | 60       |
| time/              |          |
|    fps             | 47       |
|    iterations      | 301      |
|    time_elapsed    | 13080    |
|    total_timesteps | 616448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=616500, episode_reward=71.89 +/- 5.02
Episode length: 63.42 +/- 20.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 63.4        |
|    mean_reward          | 71.9        |
| time/                   |             |
|    total_timesteps      | 616500      |
| train/                  |             |
|    approx_kl            | 0.009325934 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.001       |
|    loss                 | 144         |
|    n_updates            | 398         |
|    policy_gradient_loss | -0.00999    |
|    value_loss           | 383         |
-----------------------------------------
Eval num_timesteps=617000, episode_reward=71.39 +/- 5.61
Episode length: 66.28 +/- 24.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.3     |
|    mean_reward     | 71.4     |
| time/              |          |
|    total_timesteps | 617000   |
---------------------------------
Eval num_timesteps=617500, episode_reward=72.71 +/- 5.68
Episode length: 59.74 +/- 21.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.7     |
|    mean_reward     | 72.7     |
| time/              |          |
|    total_timesteps | 617500   |
---------------------------------
Eval num_timesteps=618000, episode_reward=72.53 +/- 5.77
Episode length: 60.34 +/- 22.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 72.5     |
| time/              |          |
|    total_timesteps | 618000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.9     |
|    ep_rew_mean     | 62       |
| time/              |          |
|    fps             | 47       |
|    iterations      | 302      |
|    time_elapsed    | 13090    |
|    total_timesteps | 618496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=618500, episode_reward=73.11 +/- 5.56
Episode length: 58.46 +/- 20.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 58.5        |
|    mean_reward          | 73.1        |
| time/                   |             |
|    total_timesteps      | 618500      |
| train/                  |             |
|    approx_kl            | 0.006576853 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.419       |
|    learning_rate        | 0.001       |
|    loss                 | 111         |
|    n_updates            | 399         |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 184         |
-----------------------------------------
Eval num_timesteps=619000, episode_reward=73.25 +/- 5.77
Episode length: 57.50 +/- 21.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.5     |
|    mean_reward     | 73.2     |
| time/              |          |
|    total_timesteps | 619000   |
---------------------------------
Eval num_timesteps=619500, episode_reward=72.27 +/- 4.93
Episode length: 61.08 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.1     |
|    mean_reward     | 72.3     |
| time/              |          |
|    total_timesteps | 619500   |
---------------------------------
Eval num_timesteps=620000, episode_reward=71.27 +/- 4.97
Episode length: 65.42 +/- 21.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.4     |
|    mean_reward     | 71.3     |
| time/              |          |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=620500, episode_reward=71.63 +/- 6.12
Episode length: 63.82 +/- 23.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.8     |
|    mean_reward     | 71.6     |
| time/              |          |
|    total_timesteps | 620500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85       |
|    ep_rew_mean     | 65.7     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 303      |
|    time_elapsed    | 13103    |
|    total_timesteps | 620544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=621000, episode_reward=71.15 +/- 5.46
Episode length: 67.22 +/- 24.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 67.2        |
|    mean_reward          | 71.1        |
| time/                   |             |
|    total_timesteps      | 621000      |
| train/                  |             |
|    approx_kl            | 0.018693058 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.001       |
|    loss                 | 44.4        |
|    n_updates            | 400         |
|    policy_gradient_loss | 0.0105      |
|    value_loss           | 91.2        |
-----------------------------------------
Eval num_timesteps=621500, episode_reward=71.55 +/- 5.87
Episode length: 64.70 +/- 22.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.7     |
|    mean_reward     | 71.5     |
| time/              |          |
|    total_timesteps | 621500   |
---------------------------------
Eval num_timesteps=622000, episode_reward=72.77 +/- 4.84
Episode length: 60.34 +/- 20.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 72.8     |
| time/              |          |
|    total_timesteps | 622000   |
---------------------------------
Eval num_timesteps=622500, episode_reward=73.05 +/- 4.20
Episode length: 58.10 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | 73       |
| time/              |          |
|    total_timesteps | 622500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 70.5     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 304      |
|    time_elapsed    | 13113    |
|    total_timesteps | 622592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=623000, episode_reward=84.01 +/- 22.80
Episode length: 83.24 +/- 113.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 83.2      |
|    mean_reward          | 84        |
| time/                   |           |
|    total_timesteps      | 623000    |
| train/                  |           |
|    approx_kl            | 0.0086816 |
|    clip_fraction        | 0.18      |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.29     |
|    explained_variance   | 0.425     |
|    learning_rate        | 0.001     |
|    loss                 | 61.1      |
|    n_updates            | 401       |
|    policy_gradient_loss | 0.0161    |
|    value_loss           | 103       |
---------------------------------------
Eval num_timesteps=623500, episode_reward=85.51 +/- 25.99
Episode length: 97.62 +/- 127.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 85.5     |
| time/              |          |
|    total_timesteps | 623500   |
---------------------------------
Eval num_timesteps=624000, episode_reward=90.49 +/- 30.43
Episode length: 109.94 +/- 154.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 90.5     |
| time/              |          |
|    total_timesteps | 624000   |
---------------------------------
Eval num_timesteps=624500, episode_reward=85.24 +/- 26.00
Episode length: 99.38 +/- 126.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | 85.2     |
| time/              |          |
|    total_timesteps | 624500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 75.2     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 305      |
|    time_elapsed    | 13128    |
|    total_timesteps | 624640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=625000, episode_reward=81.45 +/- 23.51
Episode length: 58.64 +/- 22.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 58.6        |
|    mean_reward          | 81.4        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.017290773 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.503       |
|    learning_rate        | 0.001       |
|    loss                 | 3.98        |
|    n_updates            | 402         |
|    policy_gradient_loss | 0.0017      |
|    value_loss           | 76          |
-----------------------------------------
Eval num_timesteps=625500, episode_reward=85.39 +/- 13.86
Episode length: 63.64 +/- 21.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.6     |
|    mean_reward     | 85.4     |
| time/              |          |
|    total_timesteps | 625500   |
---------------------------------
Eval num_timesteps=626000, episode_reward=87.37 +/- 0.28
Episode length: 63.86 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.9     |
|    mean_reward     | 87.4     |
| time/              |          |
|    total_timesteps | 626000   |
---------------------------------
Eval num_timesteps=626500, episode_reward=85.41 +/- 13.86
Episode length: 58.10 +/- 22.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | 85.4     |
| time/              |          |
|    total_timesteps | 626500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 81.3     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 306      |
|    time_elapsed    | 13138    |
|    total_timesteps | 626688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=627000, episode_reward=-3.67 +/- 26.86
Episode length: 48.72 +/- 19.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -3.67       |
| time/                   |             |
|    total_timesteps      | 627000      |
| train/                  |             |
|    approx_kl            | 0.008268544 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.001       |
|    loss                 | 157         |
|    n_updates            | 403         |
|    policy_gradient_loss | 0.00428     |
|    value_loss           | 253         |
-----------------------------------------
Eval num_timesteps=627500, episode_reward=0.29 +/- 32.17
Episode length: 54.64 +/- 24.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | 0.286    |
| time/              |          |
|    total_timesteps | 627500   |
---------------------------------
Eval num_timesteps=628000, episode_reward=-5.65 +/- 23.51
Episode length: 52.00 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -5.65    |
| time/              |          |
|    total_timesteps | 628000   |
---------------------------------
Eval num_timesteps=628500, episode_reward=-7.63 +/- 19.40
Episode length: 47.24 +/- 16.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -7.63    |
| time/              |          |
|    total_timesteps | 628500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 86.5     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 307      |
|    time_elapsed    | 13146    |
|    total_timesteps | 628736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=629000, episode_reward=-11.59 +/- 0.01
Episode length: 50.14 +/- 18.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.1        |
|    mean_reward          | -11.6       |
| time/                   |             |
|    total_timesteps      | 629000      |
| train/                  |             |
|    approx_kl            | 0.008381281 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.001       |
|    loss                 | 158         |
|    n_updates            | 404         |
|    policy_gradient_loss | -0.000759   |
|    value_loss           | 310         |
-----------------------------------------
Eval num_timesteps=629500, episode_reward=-9.61 +/- 13.86
Episode length: 46.60 +/- 13.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -9.61    |
| time/              |          |
|    total_timesteps | 629500   |
---------------------------------
Eval num_timesteps=630000, episode_reward=-11.59 +/- 0.00
Episode length: 45.52 +/- 12.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 630000   |
---------------------------------
Eval num_timesteps=630500, episode_reward=-11.59 +/- 0.00
Episode length: 49.48 +/- 14.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 630500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 90.9     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 308      |
|    time_elapsed    | 13155    |
|    total_timesteps | 630784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=631000, episode_reward=-9.61 +/- 13.86
Episode length: 49.90 +/- 15.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.9        |
|    mean_reward          | -9.61       |
| time/                   |             |
|    total_timesteps      | 631000      |
| train/                  |             |
|    approx_kl            | 0.008779865 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.001       |
|    loss                 | 45.1        |
|    n_updates            | 405         |
|    policy_gradient_loss | 0.00613     |
|    value_loss           | 163         |
-----------------------------------------
Eval num_timesteps=631500, episode_reward=-7.63 +/- 19.40
Episode length: 52.34 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -7.63    |
| time/              |          |
|    total_timesteps | 631500   |
---------------------------------
Eval num_timesteps=632000, episode_reward=-9.62 +/- 13.86
Episode length: 51.84 +/- 17.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -9.62    |
| time/              |          |
|    total_timesteps | 632000   |
---------------------------------
Eval num_timesteps=632500, episode_reward=-5.65 +/- 23.51
Episode length: 54.34 +/- 21.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -5.65    |
| time/              |          |
|    total_timesteps | 632500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 95.2     |
| time/              |          |
|    fps             | 48       |
|    iterations      | 309      |
|    time_elapsed    | 13164    |
|    total_timesteps | 632832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=633000, episode_reward=73.41 +/- 34.70
Episode length: 57.00 +/- 20.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 57          |
|    mean_reward          | 73.4        |
| time/                   |             |
|    total_timesteps      | 633000      |
| train/                  |             |
|    approx_kl            | 0.008877417 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.001       |
|    loss                 | 64.1        |
|    n_updates            | 406         |
|    policy_gradient_loss | 0.00568     |
|    value_loss           | 139         |
-----------------------------------------
Eval num_timesteps=633500, episode_reward=73.41 +/- 34.70
Episode length: 57.46 +/- 19.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.5     |
|    mean_reward     | 73.4     |
| time/              |          |
|    total_timesteps | 633500   |
---------------------------------
Eval num_timesteps=634000, episode_reward=83.41 +/- 19.60
Episode length: 60.10 +/- 18.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.1     |
|    mean_reward     | 83.4     |
| time/              |          |
|    total_timesteps | 634000   |
---------------------------------
Eval num_timesteps=634500, episode_reward=75.41 +/- 32.50
Episode length: 60.34 +/- 22.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 75.4     |
| time/              |          |
|    total_timesteps | 634500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 100      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 310      |
|    time_elapsed    | 13173    |
|    total_timesteps | 634880   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=635000, episode_reward=87.41 +/- 0.01
Episode length: 66.06 +/- 21.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 66.1        |
|    mean_reward          | 87.4        |
| time/                   |             |
|    total_timesteps      | 635000      |
| train/                  |             |
|    approx_kl            | 0.008179979 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.001       |
|    loss                 | 65.5        |
|    n_updates            | 408         |
|    policy_gradient_loss | 0.00213     |
|    value_loss           | 130         |
-----------------------------------------
Eval num_timesteps=635500, episode_reward=87.41 +/- 0.00
Episode length: 61.94 +/- 21.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.9     |
|    mean_reward     | 87.4     |
| time/              |          |
|    total_timesteps | 635500   |
---------------------------------
Eval num_timesteps=636000, episode_reward=87.41 +/- 0.00
Episode length: 60.06 +/- 20.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.1     |
|    mean_reward     | 87.4     |
| time/              |          |
|    total_timesteps | 636000   |
---------------------------------
Eval num_timesteps=636500, episode_reward=87.41 +/- 0.00
Episode length: 62.74 +/- 24.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.7     |
|    mean_reward     | 87.4     |
| time/              |          |
|    total_timesteps | 636500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 104      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 311      |
|    time_elapsed    | 13183    |
|    total_timesteps | 636928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=637000, episode_reward=87.37 +/- 0.20
Episode length: 64.66 +/- 25.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 64.7         |
|    mean_reward          | 87.4         |
| time/                   |              |
|    total_timesteps      | 637000       |
| train/                  |              |
|    approx_kl            | 0.0064631086 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 0.463        |
|    learning_rate        | 0.001        |
|    loss                 | 71.4         |
|    n_updates            | 409          |
|    policy_gradient_loss | 0.0096       |
|    value_loss           | 102          |
------------------------------------------
Eval num_timesteps=637500, episode_reward=87.41 +/- 0.01
Episode length: 62.56 +/- 23.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.6     |
|    mean_reward     | 87.4     |
| time/              |          |
|    total_timesteps | 637500   |
---------------------------------
Eval num_timesteps=638000, episode_reward=87.41 +/- 0.01
Episode length: 59.66 +/- 23.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.7     |
|    mean_reward     | 87.4     |
| time/              |          |
|    total_timesteps | 638000   |
---------------------------------
Eval num_timesteps=638500, episode_reward=87.41 +/- 0.00
Episode length: 64.40 +/- 26.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.4     |
|    mean_reward     | 87.4     |
| time/              |          |
|    total_timesteps | 638500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 312      |
|    time_elapsed    | 13194    |
|    total_timesteps | 638976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=639000, episode_reward=196.63 +/- 1.23
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 639000      |
| train/                  |             |
|    approx_kl            | 0.007121976 |
|    clip_fraction        | 0.0714      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.001       |
|    loss                 | 22.5        |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00214    |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=639500, episode_reward=194.36 +/- 15.48
Episode length: 515.18 +/- 68.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 639500   |
---------------------------------
Eval num_timesteps=640000, episode_reward=196.38 +/- 1.33
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=640500, episode_reward=194.05 +/- 15.58
Episode length: 514.98 +/- 70.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 640500   |
---------------------------------
Eval num_timesteps=641000, episode_reward=196.39 +/- 1.21
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 641000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 102      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 313      |
|    time_elapsed    | 13282    |
|    total_timesteps | 641024   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=641500, episode_reward=194.54 +/- 15.35
Episode length: 514.94 +/- 70.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 641500      |
| train/                  |             |
|    approx_kl            | 0.009588446 |
|    clip_fraction        | 0.0258      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.001       |
|    loss                 | 14.2        |
|    n_updates            | 412         |
|    policy_gradient_loss | 0.00147     |
|    value_loss           | 87.5        |
-----------------------------------------
Eval num_timesteps=642000, episode_reward=196.86 +/- 1.12
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 642000   |
---------------------------------
Eval num_timesteps=642500, episode_reward=194.50 +/- 15.48
Episode length: 515.44 +/- 66.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 642500   |
---------------------------------
Eval num_timesteps=643000, episode_reward=196.77 +/- 1.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 643000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 102      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 314      |
|    time_elapsed    | 13352    |
|    total_timesteps | 643072   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=643500, episode_reward=148.21 +/- 54.27
Episode length: 322.90 +/- 228.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | 148         |
| time/                   |             |
|    total_timesteps      | 643500      |
| train/                  |             |
|    approx_kl            | 0.007927736 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.468       |
|    learning_rate        | 0.001       |
|    loss                 | 70.7        |
|    n_updates            | 414         |
|    policy_gradient_loss | -0.00107    |
|    value_loss           | 99.2        |
-----------------------------------------
Eval num_timesteps=644000, episode_reward=150.58 +/- 53.97
Episode length: 333.40 +/- 225.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 333      |
|    mean_reward     | 151      |
| time/              |          |
|    total_timesteps | 644000   |
---------------------------------
Eval num_timesteps=644500, episode_reward=157.20 +/- 52.73
Episode length: 356.06 +/- 225.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 356      |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 644500   |
---------------------------------
Eval num_timesteps=645000, episode_reward=146.20 +/- 54.51
Episode length: 308.80 +/- 234.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 146      |
| time/              |          |
|    total_timesteps | 645000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 315      |
|    time_elapsed    | 13398    |
|    total_timesteps | 645120   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=645500, episode_reward=113.66 +/- 46.80
Episode length: 176.32 +/- 197.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 176         |
|    mean_reward          | 114         |
| time/                   |             |
|    total_timesteps      | 645500      |
| train/                  |             |
|    approx_kl            | 0.008747581 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.524       |
|    learning_rate        | 0.001       |
|    loss                 | 98.7        |
|    n_updates            | 416         |
|    policy_gradient_loss | 0.00878     |
|    value_loss           | 78.8        |
-----------------------------------------
Eval num_timesteps=646000, episode_reward=109.19 +/- 43.77
Episode length: 149.80 +/- 188.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 109      |
| time/              |          |
|    total_timesteps | 646000   |
---------------------------------
Eval num_timesteps=646500, episode_reward=104.92 +/- 40.13
Episode length: 142.36 +/- 168.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 105      |
| time/              |          |
|    total_timesteps | 646500   |
---------------------------------
Eval num_timesteps=647000, episode_reward=102.65 +/- 38.01
Episode length: 131.22 +/- 159.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 103      |
| time/              |          |
|    total_timesteps | 647000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 106      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 316      |
|    time_elapsed    | 13419    |
|    total_timesteps | 647168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=647500, episode_reward=190.85 +/- 26.14
Episode length: 496.08 +/- 114.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 647500       |
| train/                  |              |
|    approx_kl            | 0.0067073475 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.742        |
|    learning_rate        | 0.001        |
|    loss                 | 31.2         |
|    n_updates            | 417          |
|    policy_gradient_loss | 0.00758      |
|    value_loss           | 62.5         |
------------------------------------------
Eval num_timesteps=648000, episode_reward=188.55 +/- 29.83
Episode length: 486.64 +/- 130.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 648000   |
---------------------------------
Eval num_timesteps=648500, episode_reward=192.94 +/- 21.55
Episode length: 505.34 +/- 96.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 648500   |
---------------------------------
Eval num_timesteps=649000, episode_reward=192.88 +/- 21.55
Episode length: 505.76 +/- 94.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 649000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 317      |
|    time_elapsed    | 13487    |
|    total_timesteps | 649216   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=649500, episode_reward=197.56 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 649500     |
| train/                  |            |
|    approx_kl            | 0.01128858 |
|    clip_fraction        | 0.0787     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.001      |
|    loss                 | 15.3       |
|    n_updates            | 419        |
|    policy_gradient_loss | 0.0103     |
|    value_loss           | 46         |
----------------------------------------
Eval num_timesteps=650000, episode_reward=197.46 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 650000   |
---------------------------------
Eval num_timesteps=650500, episode_reward=197.35 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 650500   |
---------------------------------
Eval num_timesteps=651000, episode_reward=197.39 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 651000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 318      |
|    time_elapsed    | 13559    |
|    total_timesteps | 651264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=651500, episode_reward=195.10 +/- 15.55
Episode length: 515.90 +/- 63.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 651500      |
| train/                  |             |
|    approx_kl            | 0.003556597 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.001       |
|    loss                 | 45          |
|    n_updates            | 420         |
|    policy_gradient_loss | 0.000463    |
|    value_loss           | 52          |
-----------------------------------------
Eval num_timesteps=652000, episode_reward=195.01 +/- 15.54
Episode length: 515.42 +/- 67.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 652000   |
---------------------------------
Eval num_timesteps=652500, episode_reward=197.31 +/- 0.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 652500   |
---------------------------------
Eval num_timesteps=653000, episode_reward=197.37 +/- 0.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 653000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 254      |
|    ep_rew_mean     | 119      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 319      |
|    time_elapsed    | 13630    |
|    total_timesteps | 653312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=653500, episode_reward=197.54 +/- 0.73
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 653500      |
| train/                  |             |
|    approx_kl            | 0.008237771 |
|    clip_fraction        | 0.0576      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.001       |
|    loss                 | 33.2        |
|    n_updates            | 421         |
|    policy_gradient_loss | 0.00735     |
|    value_loss           | 48.9        |
-----------------------------------------
Eval num_timesteps=654000, episode_reward=197.60 +/- 0.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 654000   |
---------------------------------
Eval num_timesteps=654500, episode_reward=197.53 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 654500   |
---------------------------------
Eval num_timesteps=655000, episode_reward=197.55 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 655000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 320      |
|    time_elapsed    | 13700    |
|    total_timesteps | 655360   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=655500, episode_reward=197.58 +/- 0.65
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 655500      |
| train/                  |             |
|    approx_kl            | 0.011097712 |
|    clip_fraction        | 0.0638      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.001       |
|    loss                 | 19          |
|    n_updates            | 423         |
|    policy_gradient_loss | 0.00591     |
|    value_loss           | 24          |
-----------------------------------------
Eval num_timesteps=656000, episode_reward=197.65 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 656000   |
---------------------------------
Eval num_timesteps=656500, episode_reward=197.69 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 656500   |
---------------------------------
Eval num_timesteps=657000, episode_reward=197.70 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 657000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 286      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 321      |
|    time_elapsed    | 13772    |
|    total_timesteps | 657408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=657500, episode_reward=197.55 +/- 0.78
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 657500      |
| train/                  |             |
|    approx_kl            | 0.006267277 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.001       |
|    loss                 | 64.6        |
|    n_updates            | 424         |
|    policy_gradient_loss | 0.00165     |
|    value_loss           | 80.2        |
-----------------------------------------
Eval num_timesteps=658000, episode_reward=197.78 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 658000   |
---------------------------------
Eval num_timesteps=658500, episode_reward=197.61 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 658500   |
---------------------------------
Eval num_timesteps=659000, episode_reward=197.82 +/- 0.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 659000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 322      |
|    time_elapsed    | 13843    |
|    total_timesteps | 659456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=659500, episode_reward=197.58 +/- 0.70
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 659500      |
| train/                  |             |
|    approx_kl            | 0.004856651 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.001       |
|    loss                 | 21          |
|    n_updates            | 425         |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 35.6        |
-----------------------------------------
Eval num_timesteps=660000, episode_reward=197.74 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=660500, episode_reward=197.62 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 660500   |
---------------------------------
Eval num_timesteps=661000, episode_reward=197.64 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 661000   |
---------------------------------
Eval num_timesteps=661500, episode_reward=197.69 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 661500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 324      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 323      |
|    time_elapsed    | 13932    |
|    total_timesteps | 661504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=662000, episode_reward=197.63 +/- 0.69
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 662000       |
| train/                  |              |
|    approx_kl            | 0.0032029862 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.911        |
|    learning_rate        | 0.001        |
|    loss                 | 23.1         |
|    n_updates            | 426          |
|    policy_gradient_loss | -0.000184    |
|    value_loss           | 24.5         |
------------------------------------------
Eval num_timesteps=662500, episode_reward=197.78 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 662500   |
---------------------------------
Eval num_timesteps=663000, episode_reward=197.74 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 663000   |
---------------------------------
Eval num_timesteps=663500, episode_reward=197.84 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 663500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 324      |
|    time_elapsed    | 14003    |
|    total_timesteps | 663552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=664000, episode_reward=197.29 +/- 1.40
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 664000       |
| train/                  |              |
|    approx_kl            | 0.0062200245 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.901        |
|    learning_rate        | 0.001        |
|    loss                 | 0.626        |
|    n_updates            | 427          |
|    policy_gradient_loss | 0.0151       |
|    value_loss           | 23.4         |
------------------------------------------
Eval num_timesteps=664500, episode_reward=193.40 +/- 29.49
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 664500   |
---------------------------------
Eval num_timesteps=665000, episode_reward=193.12 +/- 29.99
Episode length: 515.04 +/- 69.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 665000   |
---------------------------------
Eval num_timesteps=665500, episode_reward=192.60 +/- 22.21
Episode length: 505.38 +/- 96.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 665500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 348      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 325      |
|    time_elapsed    | 14074    |
|    total_timesteps | 665600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=666000, episode_reward=196.90 +/- 0.82
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 666000      |
| train/                  |             |
|    approx_kl            | 0.010095755 |
|    clip_fraction        | 0.0484      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.001       |
|    loss                 | 7.75        |
|    n_updates            | 428         |
|    policy_gradient_loss | 0.00939     |
|    value_loss           | 26.7        |
-----------------------------------------
Eval num_timesteps=666500, episode_reward=197.07 +/- 0.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 666500   |
---------------------------------
Eval num_timesteps=667000, episode_reward=197.10 +/- 0.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 667000   |
---------------------------------
Eval num_timesteps=667500, episode_reward=196.90 +/- 0.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 667500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 361      |
|    ep_rew_mean     | 143      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 326      |
|    time_elapsed    | 14145    |
|    total_timesteps | 667648   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=668000, episode_reward=197.49 +/- 0.67
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 668000      |
| train/                  |             |
|    approx_kl            | 0.011112367 |
|    clip_fraction        | 0.0218      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.869       |
|    learning_rate        | 0.001       |
|    loss                 | 15.8        |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00167    |
|    value_loss           | 33          |
-----------------------------------------
Eval num_timesteps=668500, episode_reward=197.48 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 668500   |
---------------------------------
Eval num_timesteps=669000, episode_reward=197.64 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 669000   |
---------------------------------
Eval num_timesteps=669500, episode_reward=197.52 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 669500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 371      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 327      |
|    time_elapsed    | 14217    |
|    total_timesteps | 669696   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=670000, episode_reward=197.68 +/- 0.59
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.021429913 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.001       |
|    loss                 | 8.04        |
|    n_updates            | 433         |
|    policy_gradient_loss | 0.00185     |
|    value_loss           | 38          |
-----------------------------------------
Eval num_timesteps=670500, episode_reward=197.64 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 670500   |
---------------------------------
Eval num_timesteps=671000, episode_reward=197.50 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 671000   |
---------------------------------
Eval num_timesteps=671500, episode_reward=197.56 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 671500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 376      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 328      |
|    time_elapsed    | 14289    |
|    total_timesteps | 671744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=672000, episode_reward=197.80 +/- 0.46
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 672000      |
| train/                  |             |
|    approx_kl            | 0.005652338 |
|    clip_fraction        | 0.0521      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.001       |
|    loss                 | 17.5        |
|    n_updates            | 434         |
|    policy_gradient_loss | 0.000473    |
|    value_loss           | 21.7        |
-----------------------------------------
Eval num_timesteps=672500, episode_reward=197.55 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 672500   |
---------------------------------
Eval num_timesteps=673000, episode_reward=197.67 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 673000   |
---------------------------------
Eval num_timesteps=673500, episode_reward=197.52 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 673500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 385      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 329      |
|    time_elapsed    | 14360    |
|    total_timesteps | 673792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=674000, episode_reward=197.49 +/- 0.69
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 674000       |
| train/                  |              |
|    approx_kl            | 0.0024715401 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.001        |
|    loss                 | 42.8         |
|    n_updates            | 435          |
|    policy_gradient_loss | 0.00292      |
|    value_loss           | 38.6         |
------------------------------------------
Eval num_timesteps=674500, episode_reward=197.65 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 674500   |
---------------------------------
Eval num_timesteps=675000, episode_reward=197.51 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 675000   |
---------------------------------
Eval num_timesteps=675500, episode_reward=197.41 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 675500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 395      |
|    ep_rew_mean     | 150      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 330      |
|    time_elapsed    | 14431    |
|    total_timesteps | 675840   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=676000, episode_reward=191.88 +/- 22.31
Episode length: 509.02 +/- 78.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 509        |
|    mean_reward          | 192        |
| time/                   |            |
|    total_timesteps      | 676000     |
| train/                  |            |
|    approx_kl            | 0.05825871 |
|    clip_fraction        | 0.0273     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.001      |
|    loss                 | 68.4       |
|    n_updates            | 437        |
|    policy_gradient_loss | 0.000904   |
|    value_loss           | 35.1       |
----------------------------------------
Eval num_timesteps=676500, episode_reward=196.52 +/- 1.98
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 676500   |
---------------------------------
Eval num_timesteps=677000, episode_reward=190.02 +/- 26.38
Episode length: 501.38 +/- 93.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 501      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 677000   |
---------------------------------
Eval num_timesteps=677500, episode_reward=196.41 +/- 1.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 677500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 408      |
|    ep_rew_mean     | 153      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 331      |
|    time_elapsed    | 14501    |
|    total_timesteps | 677888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=678000, episode_reward=197.56 +/- 0.98
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 678000      |
| train/                  |             |
|    approx_kl            | 0.007090998 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.001       |
|    loss                 | 24          |
|    n_updates            | 438         |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 49.2        |
-----------------------------------------
Eval num_timesteps=678500, episode_reward=197.46 +/- 1.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 678500   |
---------------------------------
Eval num_timesteps=679000, episode_reward=197.39 +/- 1.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 679000   |
---------------------------------
Eval num_timesteps=679500, episode_reward=197.50 +/- 1.01
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 679500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 422      |
|    ep_rew_mean     | 156      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 332      |
|    time_elapsed    | 14572    |
|    total_timesteps | 679936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=680000, episode_reward=197.70 +/- 0.73
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.003766118 |
|    clip_fraction        | 0.0201      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.001       |
|    loss                 | 24.3        |
|    n_updates            | 439         |
|    policy_gradient_loss | -0.000694   |
|    value_loss           | 93.5        |
-----------------------------------------
Eval num_timesteps=680500, episode_reward=197.84 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 680500   |
---------------------------------
Eval num_timesteps=681000, episode_reward=197.77 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 681000   |
---------------------------------
Eval num_timesteps=681500, episode_reward=197.62 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 681500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 333      |
|    time_elapsed    | 14643    |
|    total_timesteps | 681984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=682000, episode_reward=197.66 +/- 0.76
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 682000      |
| train/                  |             |
|    approx_kl            | 0.004405994 |
|    clip_fraction        | 0.00521     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.001       |
|    loss                 | 28.7        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00198    |
|    value_loss           | 35.7        |
-----------------------------------------
Eval num_timesteps=682500, episode_reward=197.66 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 682500   |
---------------------------------
Eval num_timesteps=683000, episode_reward=197.46 +/- 1.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 683000   |
---------------------------------
Eval num_timesteps=683500, episode_reward=197.70 +/- 0.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 683500   |
---------------------------------
Eval num_timesteps=684000, episode_reward=197.93 +/- 0.25
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 455      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 334      |
|    time_elapsed    | 14730    |
|    total_timesteps | 684032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=684500, episode_reward=197.71 +/- 0.64
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 684500       |
| train/                  |              |
|    approx_kl            | 0.0035780855 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.2         |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.001        |
|    loss                 | 23           |
|    n_updates            | 441          |
|    policy_gradient_loss | 0.00211      |
|    value_loss           | 21           |
------------------------------------------
Eval num_timesteps=685000, episode_reward=197.84 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 685000   |
---------------------------------
Eval num_timesteps=685500, episode_reward=197.81 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 685500   |
---------------------------------
Eval num_timesteps=686000, episode_reward=197.79 +/- 0.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 686000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 335      |
|    time_elapsed    | 14801    |
|    total_timesteps | 686080   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=686500, episode_reward=197.39 +/- 0.68
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 686500      |
| train/                  |             |
|    approx_kl            | 0.010525024 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 1.04        |
|    n_updates            | 443         |
|    policy_gradient_loss | 0.00423     |
|    value_loss           | 9.18        |
-----------------------------------------
Eval num_timesteps=687000, episode_reward=197.51 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 687000   |
---------------------------------
Eval num_timesteps=687500, episode_reward=197.36 +/- 0.90
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 687500   |
---------------------------------
Eval num_timesteps=688000, episode_reward=197.37 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 688000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 336      |
|    time_elapsed    | 14874    |
|    total_timesteps | 688128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=688500, episode_reward=196.92 +/- 1.22
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 688500       |
| train/                  |              |
|    approx_kl            | 0.0038968218 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.2         |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.001        |
|    loss                 | 17.1         |
|    n_updates            | 444          |
|    policy_gradient_loss | 0.00142      |
|    value_loss           | 15.2         |
------------------------------------------
Eval num_timesteps=689000, episode_reward=196.93 +/- 1.19
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 689000   |
---------------------------------
Eval num_timesteps=689500, episode_reward=197.16 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 689500   |
---------------------------------
Eval num_timesteps=690000, episode_reward=197.16 +/- 0.93
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 690000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 337      |
|    time_elapsed    | 14944    |
|    total_timesteps | 690176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=690500, episode_reward=194.95 +/- 1.42
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 195          |
| time/                   |              |
|    total_timesteps      | 690500       |
| train/                  |              |
|    approx_kl            | 0.0033265366 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.851        |
|    learning_rate        | 0.001        |
|    loss                 | 26.6         |
|    n_updates            | 445          |
|    policy_gradient_loss | -1.26e-05    |
|    value_loss           | 62.6         |
------------------------------------------
Eval num_timesteps=691000, episode_reward=194.16 +/- 1.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 691000   |
---------------------------------
Eval num_timesteps=691500, episode_reward=194.56 +/- 1.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 691500   |
---------------------------------
Eval num_timesteps=692000, episode_reward=194.92 +/- 1.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 692000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 338      |
|    time_elapsed    | 15015    |
|    total_timesteps | 692224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=692500, episode_reward=38.93 +/- 84.44
Episode length: 129.98 +/- 174.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 130         |
|    mean_reward          | 38.9        |
| time/                   |             |
|    total_timesteps      | 692500      |
| train/                  |             |
|    approx_kl            | 0.002948766 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.001       |
|    loss                 | 31.9        |
|    n_updates            | 446         |
|    policy_gradient_loss | 0.00465     |
|    value_loss           | 41.1        |
-----------------------------------------
Eval num_timesteps=693000, episode_reward=13.63 +/- 67.06
Episode length: 96.40 +/- 144.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 693000   |
---------------------------------
Eval num_timesteps=693500, episode_reward=34.19 +/- 84.21
Episode length: 118.74 +/- 164.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 34.2     |
| time/              |          |
|    total_timesteps | 693500   |
---------------------------------
Eval num_timesteps=694000, episode_reward=40.90 +/- 85.51
Episode length: 129.38 +/- 174.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 129      |
|    mean_reward     | 40.9     |
| time/              |          |
|    total_timesteps | 694000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 339      |
|    time_elapsed    | 15032    |
|    total_timesteps | 694272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=694500, episode_reward=122.83 +/- 95.32
Episode length: 325.16 +/- 229.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 325          |
|    mean_reward          | 123          |
| time/                   |              |
|    total_timesteps      | 694500       |
| train/                  |              |
|    approx_kl            | 0.0059646154 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.968       |
|    explained_variance   | 0.216        |
|    learning_rate        | 0.001        |
|    loss                 | 876          |
|    n_updates            | 447          |
|    policy_gradient_loss | 0.00206      |
|    value_loss           | 1.23e+03     |
------------------------------------------
Eval num_timesteps=695000, episode_reward=136.69 +/- 83.66
Episode length: 362.54 +/- 226.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 363      |
|    mean_reward     | 137      |
| time/              |          |
|    total_timesteps | 695000   |
---------------------------------
Eval num_timesteps=695500, episode_reward=125.87 +/- 90.47
Episode length: 346.62 +/- 228.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 347      |
|    mean_reward     | 126      |
| time/              |          |
|    total_timesteps | 695500   |
---------------------------------
Eval num_timesteps=696000, episode_reward=147.46 +/- 82.57
Episode length: 386.96 +/- 212.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 387      |
|    mean_reward     | 147      |
| time/              |          |
|    total_timesteps | 696000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 463      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 340      |
|    time_elapsed    | 15082    |
|    total_timesteps | 696320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=696500, episode_reward=140.67 +/- 54.76
Episode length: 351.96 +/- 180.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 352         |
|    mean_reward          | 141         |
| time/                   |             |
|    total_timesteps      | 696500      |
| train/                  |             |
|    approx_kl            | 0.012765127 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.456       |
|    learning_rate        | 0.001       |
|    loss                 | 106         |
|    n_updates            | 448         |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 218         |
-----------------------------------------
Eval num_timesteps=697000, episode_reward=131.70 +/- 54.13
Episode length: 337.34 +/- 170.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | 132      |
| time/              |          |
|    total_timesteps | 697000   |
---------------------------------
Eval num_timesteps=697500, episode_reward=133.98 +/- 54.69
Episode length: 352.58 +/- 165.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 353      |
|    mean_reward     | 134      |
| time/              |          |
|    total_timesteps | 697500   |
---------------------------------
Eval num_timesteps=698000, episode_reward=133.83 +/- 54.44
Episode length: 338.60 +/- 172.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 339      |
|    mean_reward     | 134      |
| time/              |          |
|    total_timesteps | 698000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 341      |
|    time_elapsed    | 15128    |
|    total_timesteps | 698368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=698500, episode_reward=197.54 +/- 0.89
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 698500      |
| train/                  |             |
|    approx_kl            | 0.008472225 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.001       |
|    loss                 | 28.9        |
|    n_updates            | 449         |
|    policy_gradient_loss | 0.000713    |
|    value_loss           | 39.9        |
-----------------------------------------
Eval num_timesteps=699000, episode_reward=197.66 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 699000   |
---------------------------------
Eval num_timesteps=699500, episode_reward=197.76 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 699500   |
---------------------------------
Eval num_timesteps=700000, episode_reward=197.63 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 342      |
|    time_elapsed    | 15200    |
|    total_timesteps | 700416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=700500, episode_reward=197.76 +/- 0.54
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 700500     |
| train/                  |            |
|    approx_kl            | 0.00338438 |
|    clip_fraction        | 0.0167     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.001      |
|    loss                 | 21         |
|    n_updates            | 450        |
|    policy_gradient_loss | 0.00154    |
|    value_loss           | 65.7       |
----------------------------------------
Eval num_timesteps=701000, episode_reward=197.78 +/- 0.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 701000   |
---------------------------------
Eval num_timesteps=701500, episode_reward=197.73 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 701500   |
---------------------------------
Eval num_timesteps=702000, episode_reward=197.83 +/- 0.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 702000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 343      |
|    time_elapsed    | 15271    |
|    total_timesteps | 702464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=702500, episode_reward=197.50 +/- 0.79
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 702500       |
| train/                  |              |
|    approx_kl            | 0.0051447554 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.001        |
|    loss                 | 20.4         |
|    n_updates            | 451          |
|    policy_gradient_loss | 0.0104       |
|    value_loss           | 46.9         |
------------------------------------------
Eval num_timesteps=703000, episode_reward=197.48 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 703000   |
---------------------------------
Eval num_timesteps=703500, episode_reward=197.62 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 703500   |
---------------------------------
Eval num_timesteps=704000, episode_reward=197.64 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 704000   |
---------------------------------
Eval num_timesteps=704500, episode_reward=197.53 +/- 0.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 704500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 344      |
|    time_elapsed    | 15360    |
|    total_timesteps | 704512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=705000, episode_reward=197.22 +/- 0.92
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 705000       |
| train/                  |              |
|    approx_kl            | 0.0036058284 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.918        |
|    learning_rate        | 0.001        |
|    loss                 | 2.85         |
|    n_updates            | 452          |
|    policy_gradient_loss | 1.19e-05     |
|    value_loss           | 30.1         |
------------------------------------------
Eval num_timesteps=705500, episode_reward=197.37 +/- 0.89
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 705500   |
---------------------------------
Eval num_timesteps=706000, episode_reward=197.38 +/- 0.85
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 706000   |
---------------------------------
Eval num_timesteps=706500, episode_reward=196.99 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 706500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 345      |
|    time_elapsed    | 15432    |
|    total_timesteps | 706560   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=707000, episode_reward=197.75 +/- 0.42
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 707000      |
| train/                  |             |
|    approx_kl            | 0.012324993 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.001       |
|    loss                 | 7.63        |
|    n_updates            | 454         |
|    policy_gradient_loss | -0.00452    |
|    value_loss           | 24.5        |
-----------------------------------------
Eval num_timesteps=707500, episode_reward=197.70 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 707500   |
---------------------------------
Eval num_timesteps=708000, episode_reward=197.80 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 708000   |
---------------------------------
Eval num_timesteps=708500, episode_reward=197.68 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 708500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 346      |
|    time_elapsed    | 15503    |
|    total_timesteps | 708608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=709000, episode_reward=197.62 +/- 0.62
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 709000      |
| train/                  |             |
|    approx_kl            | 0.004052404 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 24.7        |
|    n_updates            | 455         |
|    policy_gradient_loss | -0.000692   |
|    value_loss           | 28.9        |
-----------------------------------------
Eval num_timesteps=709500, episode_reward=197.42 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 709500   |
---------------------------------
Eval num_timesteps=710000, episode_reward=197.55 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 710000   |
---------------------------------
Eval num_timesteps=710500, episode_reward=197.76 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 710500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 347      |
|    time_elapsed    | 15574    |
|    total_timesteps | 710656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=711000, episode_reward=197.34 +/- 1.01
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 711000      |
| train/                  |             |
|    approx_kl            | 0.009876707 |
|    clip_fraction        | 0.00497     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.001       |
|    loss                 | 12.2        |
|    n_updates            | 456         |
|    policy_gradient_loss | 0.0766      |
|    value_loss           | 23.4        |
-----------------------------------------
Eval num_timesteps=711500, episode_reward=197.41 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 711500   |
---------------------------------
Eval num_timesteps=712000, episode_reward=197.14 +/- 1.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 712000   |
---------------------------------
Eval num_timesteps=712500, episode_reward=197.24 +/- 1.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 712500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 348      |
|    time_elapsed    | 15645    |
|    total_timesteps | 712704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=713000, episode_reward=197.86 +/- 0.39
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 713000       |
| train/                  |              |
|    approx_kl            | 0.0044696303 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.861        |
|    learning_rate        | 0.001        |
|    loss                 | 8.26         |
|    n_updates            | 457          |
|    policy_gradient_loss | 0.000404     |
|    value_loss           | 44.8         |
------------------------------------------
Eval num_timesteps=713500, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 713500   |
---------------------------------
Eval num_timesteps=714000, episode_reward=197.78 +/- 0.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 714000   |
---------------------------------
Eval num_timesteps=714500, episode_reward=197.76 +/- 0.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 714500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 349      |
|    time_elapsed    | 15715    |
|    total_timesteps | 714752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=715000, episode_reward=195.21 +/- 15.58
Episode length: 517.28 +/- 54.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 517          |
|    mean_reward          | 195          |
| time/                   |              |
|    total_timesteps      | 715000       |
| train/                  |              |
|    approx_kl            | 0.0021129309 |
|    clip_fraction        | 0.00174      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.001        |
|    loss                 | 12.1         |
|    n_updates            | 458          |
|    policy_gradient_loss | 0.0066       |
|    value_loss           | 17.3         |
------------------------------------------
Eval num_timesteps=715500, episode_reward=195.42 +/- 15.88
Episode length: 517.38 +/- 53.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 715500   |
---------------------------------
Eval num_timesteps=716000, episode_reward=197.59 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 716000   |
---------------------------------
Eval num_timesteps=716500, episode_reward=197.63 +/- 0.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 716500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 350      |
|    time_elapsed    | 15786    |
|    total_timesteps | 716800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=717000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 717000       |
| train/                  |              |
|    approx_kl            | 0.0047061765 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.001        |
|    loss                 | 18.8         |
|    n_updates            | 459          |
|    policy_gradient_loss | 0.00534      |
|    value_loss           | 18.9         |
------------------------------------------
Eval num_timesteps=717500, episode_reward=197.80 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 717500   |
---------------------------------
Eval num_timesteps=718000, episode_reward=197.85 +/- 0.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 718000   |
---------------------------------
Eval num_timesteps=718500, episode_reward=197.85 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 718500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 351      |
|    time_elapsed    | 15857    |
|    total_timesteps | 718848   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=719000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 719000     |
| train/                  |            |
|    approx_kl            | 0.01201979 |
|    clip_fraction        | 0.0648     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.876      |
|    learning_rate        | 0.001      |
|    loss                 | 15.5       |
|    n_updates            | 464        |
|    policy_gradient_loss | -0.00138   |
|    value_loss           | 24.6       |
----------------------------------------
Eval num_timesteps=719500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 719500   |
---------------------------------
Eval num_timesteps=720000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=720500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 720500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 352      |
|    time_elapsed    | 15929    |
|    total_timesteps | 720896   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=721000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 721000      |
| train/                  |             |
|    approx_kl            | 0.008619341 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 8.01        |
|    n_updates            | 469         |
|    policy_gradient_loss | -0.00295    |
|    value_loss           | 7.55        |
-----------------------------------------
Eval num_timesteps=721500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 721500   |
---------------------------------
Eval num_timesteps=722000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 722000   |
---------------------------------
Eval num_timesteps=722500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 722500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 353      |
|    time_elapsed    | 16000    |
|    total_timesteps | 722944   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=723000, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 723000      |
| train/                  |             |
|    approx_kl            | 0.016930373 |
|    clip_fraction        | 0.0204      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.001       |
|    loss                 | 6.99        |
|    n_updates            | 471         |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 6.83        |
-----------------------------------------
Eval num_timesteps=723500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 723500   |
---------------------------------
Eval num_timesteps=724000, episode_reward=197.94 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 724000   |
---------------------------------
Eval num_timesteps=724500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 724500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 354      |
|    time_elapsed    | 16071    |
|    total_timesteps | 724992   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=725000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.007977613 |
|    clip_fraction        | 0.00642     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 2.67        |
|    n_updates            | 473         |
|    policy_gradient_loss | 0.00142     |
|    value_loss           | 7.63        |
-----------------------------------------
Eval num_timesteps=725500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 725500   |
---------------------------------
Eval num_timesteps=726000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 726000   |
---------------------------------
Eval num_timesteps=726500, episode_reward=197.89 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 726500   |
---------------------------------
Eval num_timesteps=727000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 727000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 355      |
|    time_elapsed    | 16160    |
|    total_timesteps | 727040   |
---------------------------------
Eval num_timesteps=727500, episode_reward=197.79 +/- 0.51
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 727500      |
| train/                  |             |
|    approx_kl            | 0.004753668 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.001       |
|    loss                 | 1.87        |
|    n_updates            | 483         |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 7.72        |
-----------------------------------------
Eval num_timesteps=728000, episode_reward=197.90 +/- 0.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 728000   |
---------------------------------
Eval num_timesteps=728500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 728500   |
---------------------------------
Eval num_timesteps=729000, episode_reward=197.89 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 729000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 356      |
|    time_elapsed    | 16232    |
|    total_timesteps | 729088   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=729500, episode_reward=197.87 +/- 0.41
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 729500      |
| train/                  |             |
|    approx_kl            | 0.015701858 |
|    clip_fraction        | 0.0216      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.001       |
|    loss                 | 5.56        |
|    n_updates            | 485         |
|    policy_gradient_loss | 0.000654    |
|    value_loss           | 22          |
-----------------------------------------
Eval num_timesteps=730000, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 730000   |
---------------------------------
Eval num_timesteps=730500, episode_reward=197.84 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 730500   |
---------------------------------
Eval num_timesteps=731000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 731000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 357      |
|    time_elapsed    | 16304    |
|    total_timesteps | 731136   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=731500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 731500      |
| train/                  |             |
|    approx_kl            | 0.004925618 |
|    clip_fraction        | 0.017       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.001       |
|    loss                 | 1.69        |
|    n_updates            | 487         |
|    policy_gradient_loss | -0.00087    |
|    value_loss           | 16.2        |
-----------------------------------------
Eval num_timesteps=732000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 732000   |
---------------------------------
Eval num_timesteps=732500, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 732500   |
---------------------------------
Eval num_timesteps=733000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 733000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 358      |
|    time_elapsed    | 16376    |
|    total_timesteps | 733184   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=733500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 733500     |
| train/                  |            |
|    approx_kl            | 0.00280112 |
|    clip_fraction        | 0.0173     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.001      |
|    loss                 | 5.7        |
|    n_updates            | 489        |
|    policy_gradient_loss | -0.00162   |
|    value_loss           | 14.7       |
----------------------------------------
Eval num_timesteps=734000, episode_reward=197.89 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 734000   |
---------------------------------
Eval num_timesteps=734500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 734500   |
---------------------------------
Eval num_timesteps=735000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 735000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 359      |
|    time_elapsed    | 16447    |
|    total_timesteps | 735232   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=735500, episode_reward=197.90 +/- 0.38
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 735500      |
| train/                  |             |
|    approx_kl            | 0.005822041 |
|    clip_fraction        | 0.0222      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 0.36        |
|    n_updates            | 491         |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 9.05        |
-----------------------------------------
Eval num_timesteps=736000, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 736000   |
---------------------------------
Eval num_timesteps=736500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 736500   |
---------------------------------
Eval num_timesteps=737000, episode_reward=197.86 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 737000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 360      |
|    time_elapsed    | 16518    |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=737500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 737500       |
| train/                  |              |
|    approx_kl            | 0.0036385302 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 2.32         |
|    n_updates            | 501          |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 13.1         |
------------------------------------------
Eval num_timesteps=738000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 738000   |
---------------------------------
Eval num_timesteps=738500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 738500   |
---------------------------------
Eval num_timesteps=739000, episode_reward=197.88 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 739000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 361      |
|    time_elapsed    | 16589    |
|    total_timesteps | 739328   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.11
Eval num_timesteps=739500, episode_reward=197.65 +/- 0.87
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 739500      |
| train/                  |             |
|    approx_kl            | 0.034161672 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 12.3        |
|    n_updates            | 504         |
|    policy_gradient_loss | -0.000697   |
|    value_loss           | 6.73        |
-----------------------------------------
Eval num_timesteps=740000, episode_reward=197.67 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=740500, episode_reward=197.46 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 740500   |
---------------------------------
Eval num_timesteps=741000, episode_reward=197.68 +/- 0.85
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 741000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 362      |
|    time_elapsed    | 16663    |
|    total_timesteps | 741376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=741500, episode_reward=197.85 +/- 0.55
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 741500      |
| train/                  |             |
|    approx_kl            | 0.006233141 |
|    clip_fraction        | 0.00326     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 2.49        |
|    n_updates            | 505         |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 8.38        |
-----------------------------------------
Eval num_timesteps=742000, episode_reward=197.77 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 742000   |
---------------------------------
Eval num_timesteps=742500, episode_reward=197.88 +/- 0.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 742500   |
---------------------------------
Eval num_timesteps=743000, episode_reward=197.90 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 743000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 363      |
|    time_elapsed    | 16735    |
|    total_timesteps | 743424   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=743500, episode_reward=197.89 +/- 0.36
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 743500     |
| train/                  |            |
|    approx_kl            | 0.00722487 |
|    clip_fraction        | 0.00806    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.001      |
|    loss                 | 6.94       |
|    n_updates            | 507        |
|    policy_gradient_loss | 0.000743   |
|    value_loss           | 9.84       |
----------------------------------------
Eval num_timesteps=744000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 744000   |
---------------------------------
Eval num_timesteps=744500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 744500   |
---------------------------------
Eval num_timesteps=745000, episode_reward=197.92 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 745000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 364      |
|    time_elapsed    | 16806    |
|    total_timesteps | 745472   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=745500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 745500     |
| train/                  |            |
|    approx_kl            | 0.00982378 |
|    clip_fraction        | 0.0712     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.913      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0817    |
|    n_updates            | 509        |
|    policy_gradient_loss | -0.000158  |
|    value_loss           | 21.2       |
----------------------------------------
Eval num_timesteps=746000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 746000   |
---------------------------------
Eval num_timesteps=746500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 746500   |
---------------------------------
Eval num_timesteps=747000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=747500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 747500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 365      |
|    time_elapsed    | 16895    |
|    total_timesteps | 747520   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=748000, episode_reward=197.59 +/- 0.95
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 748000      |
| train/                  |             |
|    approx_kl            | 0.018302457 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 8.81        |
|    n_updates            | 511         |
|    policy_gradient_loss | -0.000759   |
|    value_loss           | 8.63        |
-----------------------------------------
Eval num_timesteps=748500, episode_reward=197.69 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 748500   |
---------------------------------
Eval num_timesteps=749000, episode_reward=197.55 +/- 0.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 749000   |
---------------------------------
Eval num_timesteps=749500, episode_reward=197.38 +/- 1.01
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 749500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 366      |
|    time_elapsed    | 16966    |
|    total_timesteps | 749568   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=750000, episode_reward=197.45 +/- 0.85
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.006459014 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.001       |
|    loss                 | 10.3        |
|    n_updates            | 513         |
|    policy_gradient_loss | -0.00258    |
|    value_loss           | 34.5        |
-----------------------------------------
Eval num_timesteps=750500, episode_reward=197.59 +/- 0.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 750500   |
---------------------------------
Eval num_timesteps=751000, episode_reward=197.37 +/- 0.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 751000   |
---------------------------------
Eval num_timesteps=751500, episode_reward=197.43 +/- 0.88
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 751500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 367      |
|    time_elapsed    | 17037    |
|    total_timesteps | 751616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=752000, episode_reward=196.82 +/- 1.47
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 752000      |
| train/                  |             |
|    approx_kl            | 0.004084901 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.001       |
|    loss                 | 44.7        |
|    n_updates            | 514         |
|    policy_gradient_loss | 0.00733     |
|    value_loss           | 38.5        |
-----------------------------------------
Eval num_timesteps=752500, episode_reward=197.46 +/- 0.90
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 752500   |
---------------------------------
Eval num_timesteps=753000, episode_reward=197.45 +/- 0.88
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 753000   |
---------------------------------
Eval num_timesteps=753500, episode_reward=197.17 +/- 1.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 753500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 368      |
|    time_elapsed    | 17109    |
|    total_timesteps | 753664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=754000, episode_reward=197.43 +/- 1.04
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 754000      |
| train/                  |             |
|    approx_kl            | 0.004207349 |
|    clip_fraction        | 0.00684     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.001       |
|    loss                 | 3.44        |
|    n_updates            | 515         |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 13          |
-----------------------------------------
Eval num_timesteps=754500, episode_reward=197.05 +/- 1.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 754500   |
---------------------------------
Eval num_timesteps=755000, episode_reward=197.41 +/- 1.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 755000   |
---------------------------------
Eval num_timesteps=755500, episode_reward=197.14 +/- 1.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 755500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 369      |
|    time_elapsed    | 17181    |
|    total_timesteps | 755712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=756000, episode_reward=197.83 +/- 0.47
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 756000      |
| train/                  |             |
|    approx_kl            | 0.003249731 |
|    clip_fraction        | 0.00586     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.001       |
|    loss                 | 39.9        |
|    n_updates            | 516         |
|    policy_gradient_loss | 0.00187     |
|    value_loss           | 60.3        |
-----------------------------------------
Eval num_timesteps=756500, episode_reward=197.80 +/- 0.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 756500   |
---------------------------------
Eval num_timesteps=757000, episode_reward=197.85 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 757000   |
---------------------------------
Eval num_timesteps=757500, episode_reward=197.85 +/- 0.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 757500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 370      |
|    time_elapsed    | 17253    |
|    total_timesteps | 757760   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=758000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 758000      |
| train/                  |             |
|    approx_kl            | 0.008645344 |
|    clip_fraction        | 0.00841     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 11.1        |
|    n_updates            | 518         |
|    policy_gradient_loss | 0.00561     |
|    value_loss           | 15          |
-----------------------------------------
Eval num_timesteps=758500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 758500   |
---------------------------------
Eval num_timesteps=759000, episode_reward=197.83 +/- 0.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 759000   |
---------------------------------
Eval num_timesteps=759500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 759500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 371      |
|    time_elapsed    | 17324    |
|    total_timesteps | 759808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=760000, episode_reward=187.15 +/- 29.89
Episode length: 487.80 +/- 126.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 488         |
|    mean_reward          | 187         |
| time/                   |             |
|    total_timesteps      | 760000      |
| train/                  |             |
|    approx_kl            | 0.002353236 |
|    clip_fraction        | 0.00446     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.001       |
|    loss                 | 1.83        |
|    n_updates            | 519         |
|    policy_gradient_loss | -0.000268   |
|    value_loss           | 18.4        |
-----------------------------------------
Eval num_timesteps=760500, episode_reward=191.83 +/- 21.47
Episode length: 507.60 +/- 85.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 760500   |
---------------------------------
Eval num_timesteps=761000, episode_reward=187.50 +/- 29.69
Episode length: 487.06 +/- 128.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 761000   |
---------------------------------
Eval num_timesteps=761500, episode_reward=183.06 +/- 35.60
Episode length: 468.42 +/- 153.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 468      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 761500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 372      |
|    time_elapsed    | 17391    |
|    total_timesteps | 761856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=762000, episode_reward=196.65 +/- 0.98
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 762000      |
| train/                  |             |
|    approx_kl            | 0.008970742 |
|    clip_fraction        | 0.00962     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.001       |
|    loss                 | 50.5        |
|    n_updates            | 520         |
|    policy_gradient_loss | 0.00489     |
|    value_loss           | 154         |
-----------------------------------------
Eval num_timesteps=762500, episode_reward=196.54 +/- 1.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 762500   |
---------------------------------
Eval num_timesteps=763000, episode_reward=196.82 +/- 1.12
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 763000   |
---------------------------------
Eval num_timesteps=763500, episode_reward=196.78 +/- 1.18
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 763500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 373      |
|    time_elapsed    | 17462    |
|    total_timesteps | 763904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=764000, episode_reward=196.80 +/- 0.77
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 764000       |
| train/                  |              |
|    approx_kl            | 0.0035063003 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.727        |
|    learning_rate        | 0.001        |
|    loss                 | 42.8         |
|    n_updates            | 521          |
|    policy_gradient_loss | -0.00279     |
|    value_loss           | 55.7         |
------------------------------------------
Eval num_timesteps=764500, episode_reward=196.94 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 764500   |
---------------------------------
Eval num_timesteps=765000, episode_reward=196.84 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 765000   |
---------------------------------
Eval num_timesteps=765500, episode_reward=196.91 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 765500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 374      |
|    time_elapsed    | 17533    |
|    total_timesteps | 765952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=766000, episode_reward=196.64 +/- 0.83
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 766000       |
| train/                  |              |
|    approx_kl            | 0.0053902236 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.761        |
|    learning_rate        | 0.001        |
|    loss                 | 27.6         |
|    n_updates            | 522          |
|    policy_gradient_loss | 0.000421     |
|    value_loss           | 53.8         |
------------------------------------------
Eval num_timesteps=766500, episode_reward=196.58 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 766500   |
---------------------------------
Eval num_timesteps=767000, episode_reward=196.47 +/- 1.16
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 767000   |
---------------------------------
Eval num_timesteps=767500, episode_reward=196.36 +/- 0.93
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 767500   |
---------------------------------
Eval num_timesteps=768000, episode_reward=196.61 +/- 1.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 375      |
|    time_elapsed    | 17621    |
|    total_timesteps | 768000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=768500, episode_reward=196.29 +/- 1.03
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 768500      |
| train/                  |             |
|    approx_kl            | 0.007326519 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.001       |
|    loss                 | 19.5        |
|    n_updates            | 523         |
|    policy_gradient_loss | 0.00555     |
|    value_loss           | 27.5        |
-----------------------------------------
Eval num_timesteps=769000, episode_reward=196.19 +/- 1.16
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 769000   |
---------------------------------
Eval num_timesteps=769500, episode_reward=195.97 +/- 1.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 769500   |
---------------------------------
Eval num_timesteps=770000, episode_reward=196.46 +/- 1.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 770000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 376      |
|    time_elapsed    | 17693    |
|    total_timesteps | 770048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=770500, episode_reward=196.93 +/- 0.93
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 770500      |
| train/                  |             |
|    approx_kl            | 0.005437338 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.001       |
|    loss                 | 14          |
|    n_updates            | 524         |
|    policy_gradient_loss | 0.00398     |
|    value_loss           | 32.7        |
-----------------------------------------
Eval num_timesteps=771000, episode_reward=197.03 +/- 0.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 771000   |
---------------------------------
Eval num_timesteps=771500, episode_reward=196.80 +/- 0.94
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 771500   |
---------------------------------
Eval num_timesteps=772000, episode_reward=196.92 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 772000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 377      |
|    time_elapsed    | 17764    |
|    total_timesteps | 772096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=772500, episode_reward=197.18 +/- 0.73
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 772500       |
| train/                  |              |
|    approx_kl            | 0.0027346974 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.881        |
|    learning_rate        | 0.001        |
|    loss                 | 2.06         |
|    n_updates            | 525          |
|    policy_gradient_loss | -3.37e-05    |
|    value_loss           | 28           |
------------------------------------------
Eval num_timesteps=773000, episode_reward=197.32 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 773000   |
---------------------------------
Eval num_timesteps=773500, episode_reward=197.29 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 773500   |
---------------------------------
Eval num_timesteps=774000, episode_reward=197.31 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 774000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 378      |
|    time_elapsed    | 17835    |
|    total_timesteps | 774144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=774500, episode_reward=197.33 +/- 0.79
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 774500       |
| train/                  |              |
|    approx_kl            | 0.0039122715 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.858        |
|    learning_rate        | 0.001        |
|    loss                 | 21.4         |
|    n_updates            | 526          |
|    policy_gradient_loss | 0.00592      |
|    value_loss           | 27.9         |
------------------------------------------
Eval num_timesteps=775000, episode_reward=197.35 +/- 0.90
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 775000   |
---------------------------------
Eval num_timesteps=775500, episode_reward=197.46 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 775500   |
---------------------------------
Eval num_timesteps=776000, episode_reward=197.38 +/- 0.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 776000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 379      |
|    time_elapsed    | 17907    |
|    total_timesteps | 776192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=776500, episode_reward=197.61 +/- 0.70
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 776500      |
| train/                  |             |
|    approx_kl            | 0.008036157 |
|    clip_fraction        | 0.0201      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.001       |
|    loss                 | 77.5        |
|    n_updates            | 527         |
|    policy_gradient_loss | 0.00395     |
|    value_loss           | 44.8        |
-----------------------------------------
Eval num_timesteps=777000, episode_reward=190.74 +/- 26.20
Episode length: 495.70 +/- 115.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 777000   |
---------------------------------
Eval num_timesteps=777500, episode_reward=193.23 +/- 21.61
Episode length: 506.38 +/- 91.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 777500   |
---------------------------------
Eval num_timesteps=778000, episode_reward=195.35 +/- 15.44
Episode length: 515.50 +/- 66.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 778000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 380      |
|    time_elapsed    | 17978    |
|    total_timesteps | 778240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=778500, episode_reward=197.77 +/- 0.53
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 778500      |
| train/                  |             |
|    approx_kl            | 0.006449998 |
|    clip_fraction        | 0.00846     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.001       |
|    loss                 | 10.6        |
|    n_updates            | 528         |
|    policy_gradient_loss | 0.00406     |
|    value_loss           | 28.3        |
-----------------------------------------
Eval num_timesteps=779000, episode_reward=197.76 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 779000   |
---------------------------------
Eval num_timesteps=779500, episode_reward=197.72 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 779500   |
---------------------------------
Eval num_timesteps=780000, episode_reward=197.84 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 780000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 381      |
|    time_elapsed    | 18049    |
|    total_timesteps | 780288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=780500, episode_reward=197.32 +/- 0.73
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 780500      |
| train/                  |             |
|    approx_kl            | 0.009405252 |
|    clip_fraction        | 0.0618      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.91        |
|    learning_rate        | 0.001       |
|    loss                 | 8.55        |
|    n_updates            | 529         |
|    policy_gradient_loss | 0.0063      |
|    value_loss           | 24.1        |
-----------------------------------------
Eval num_timesteps=781000, episode_reward=197.21 +/- 0.89
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 781000   |
---------------------------------
Eval num_timesteps=781500, episode_reward=197.17 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 781500   |
---------------------------------
Eval num_timesteps=782000, episode_reward=197.22 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 782000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 382      |
|    time_elapsed    | 18122    |
|    total_timesteps | 782336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=782500, episode_reward=197.60 +/- 0.63
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 782500       |
| train/                  |              |
|    approx_kl            | 0.0027619277 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.001        |
|    loss                 | 35.6         |
|    n_updates            | 530          |
|    policy_gradient_loss | 0.00197      |
|    value_loss           | 19.2         |
------------------------------------------
Eval num_timesteps=783000, episode_reward=197.58 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 783000   |
---------------------------------
Eval num_timesteps=783500, episode_reward=197.41 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 783500   |
---------------------------------
Eval num_timesteps=784000, episode_reward=197.65 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 784000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 383      |
|    time_elapsed    | 18193    |
|    total_timesteps | 784384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=784500, episode_reward=197.65 +/- 0.89
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 784500      |
| train/                  |             |
|    approx_kl            | 0.009809748 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.001       |
|    loss                 | 16.1        |
|    n_updates            | 531         |
|    policy_gradient_loss | -0.0037     |
|    value_loss           | 20.5        |
-----------------------------------------
Eval num_timesteps=785000, episode_reward=197.69 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 785000   |
---------------------------------
Eval num_timesteps=785500, episode_reward=197.58 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 785500   |
---------------------------------
Eval num_timesteps=786000, episode_reward=197.68 +/- 0.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 786000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 384      |
|    time_elapsed    | 18265    |
|    total_timesteps | 786432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=786500, episode_reward=197.07 +/- 1.13
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 786500       |
| train/                  |              |
|    approx_kl            | 0.0076439506 |
|    clip_fraction        | 0.00716      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.869        |
|    learning_rate        | 0.001        |
|    loss                 | 6.33         |
|    n_updates            | 532          |
|    policy_gradient_loss | 0.00382      |
|    value_loss           | 34.6         |
------------------------------------------
Eval num_timesteps=787000, episode_reward=197.12 +/- 0.93
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 787000   |
---------------------------------
Eval num_timesteps=787500, episode_reward=197.04 +/- 1.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 787500   |
---------------------------------
Eval num_timesteps=788000, episode_reward=197.04 +/- 1.05
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 788000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 385      |
|    time_elapsed    | 18337    |
|    total_timesteps | 788480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=788500, episode_reward=196.82 +/- 0.97
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 788500       |
| train/                  |              |
|    approx_kl            | 0.0048434087 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.001        |
|    loss                 | 26.4         |
|    n_updates            | 533          |
|    policy_gradient_loss | 0.0114       |
|    value_loss           | 80.9         |
------------------------------------------
Eval num_timesteps=789000, episode_reward=196.44 +/- 1.04
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 789000   |
---------------------------------
Eval num_timesteps=789500, episode_reward=196.63 +/- 1.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 789500   |
---------------------------------
Eval num_timesteps=790000, episode_reward=196.60 +/- 1.15
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 790000   |
---------------------------------
Eval num_timesteps=790500, episode_reward=196.69 +/- 1.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 790500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 386      |
|    time_elapsed    | 18426    |
|    total_timesteps | 790528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=791000, episode_reward=197.55 +/- 0.95
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 791000      |
| train/                  |             |
|    approx_kl            | 0.005222961 |
|    clip_fraction        | 0.0213      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.001       |
|    loss                 | 9.32        |
|    n_updates            | 534         |
|    policy_gradient_loss | 0.00632     |
|    value_loss           | 22.3        |
-----------------------------------------
Eval num_timesteps=791500, episode_reward=197.62 +/- 0.70
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 791500   |
---------------------------------
Eval num_timesteps=792000, episode_reward=197.70 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 792000   |
---------------------------------
Eval num_timesteps=792500, episode_reward=197.76 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 792500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 387      |
|    time_elapsed    | 18496    |
|    total_timesteps | 792576   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=793000, episode_reward=195.29 +/- 15.43
Episode length: 515.60 +/- 65.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 793000      |
| train/                  |             |
|    approx_kl            | 0.015598161 |
|    clip_fraction        | 0.0161      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.001       |
|    loss                 | 6.07        |
|    n_updates            | 536         |
|    policy_gradient_loss | -0.000146   |
|    value_loss           | 14.2        |
-----------------------------------------
Eval num_timesteps=793500, episode_reward=190.76 +/- 26.21
Episode length: 495.88 +/- 115.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 793500   |
---------------------------------
Eval num_timesteps=794000, episode_reward=197.45 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 794000   |
---------------------------------
Eval num_timesteps=794500, episode_reward=190.72 +/- 26.20
Episode length: 495.70 +/- 115.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 794500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 388      |
|    time_elapsed    | 18565    |
|    total_timesteps | 794624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=795000, episode_reward=197.36 +/- 0.76
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 795000       |
| train/                  |              |
|    approx_kl            | 0.0056597046 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.846        |
|    learning_rate        | 0.001        |
|    loss                 | 33.2         |
|    n_updates            | 537          |
|    policy_gradient_loss | 0.0022       |
|    value_loss           | 36.7         |
------------------------------------------
Eval num_timesteps=795500, episode_reward=197.17 +/- 0.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 795500   |
---------------------------------
Eval num_timesteps=796000, episode_reward=197.16 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 796000   |
---------------------------------
Eval num_timesteps=796500, episode_reward=197.11 +/- 0.85
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 796500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 389      |
|    time_elapsed    | 18636    |
|    total_timesteps | 796672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=797000, episode_reward=195.20 +/- 15.42
Episode length: 515.14 +/- 69.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 797000      |
| train/                  |             |
|    approx_kl            | 0.003677577 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.001       |
|    loss                 | 50.9        |
|    n_updates            | 538         |
|    policy_gradient_loss | 0.00394     |
|    value_loss           | 52.3        |
-----------------------------------------
Eval num_timesteps=797500, episode_reward=193.06 +/- 21.58
Episode length: 505.72 +/- 94.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 797500   |
---------------------------------
Eval num_timesteps=798000, episode_reward=190.64 +/- 26.35
Episode length: 495.94 +/- 115.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 798000   |
---------------------------------
Eval num_timesteps=798500, episode_reward=193.08 +/- 21.79
Episode length: 507.02 +/- 88.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 798500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 390      |
|    time_elapsed    | 18705    |
|    total_timesteps | 798720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=799000, episode_reward=197.26 +/- 0.89
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 799000      |
| train/                  |             |
|    approx_kl            | 0.005014216 |
|    clip_fraction        | 0.0458      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.001       |
|    loss                 | 9.8         |
|    n_updates            | 539         |
|    policy_gradient_loss | 0.000574    |
|    value_loss           | 37.9        |
-----------------------------------------
Eval num_timesteps=799500, episode_reward=197.53 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 799500   |
---------------------------------
Eval num_timesteps=800000, episode_reward=197.56 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=800500, episode_reward=197.41 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 800500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 391      |
|    time_elapsed    | 18779    |
|    total_timesteps | 800768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=801000, episode_reward=197.52 +/- 0.77
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 801000       |
| train/                  |              |
|    approx_kl            | 0.0045504556 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.2         |
|    explained_variance   | 0.903        |
|    learning_rate        | 0.001        |
|    loss                 | 17.3         |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00131     |
|    value_loss           | 34.1         |
------------------------------------------
Eval num_timesteps=801500, episode_reward=197.61 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 801500   |
---------------------------------
Eval num_timesteps=802000, episode_reward=197.59 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 802000   |
---------------------------------
Eval num_timesteps=802500, episode_reward=197.53 +/- 0.83
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 802500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 392      |
|    time_elapsed    | 18849    |
|    total_timesteps | 802816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=803000, episode_reward=197.73 +/- 0.66
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 803000       |
| train/                  |              |
|    approx_kl            | 0.0036622612 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 28.8         |
|    n_updates            | 541          |
|    policy_gradient_loss | 0.00667      |
|    value_loss           | 13.8         |
------------------------------------------
Eval num_timesteps=803500, episode_reward=197.43 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 803500   |
---------------------------------
Eval num_timesteps=804000, episode_reward=197.46 +/- 0.97
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 804000   |
---------------------------------
Eval num_timesteps=804500, episode_reward=197.60 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 804500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 393      |
|    time_elapsed    | 18920    |
|    total_timesteps | 804864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=805000, episode_reward=197.58 +/- 0.74
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 805000       |
| train/                  |              |
|    approx_kl            | 0.0042503523 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.84         |
|    learning_rate        | 0.001        |
|    loss                 | 16.2         |
|    n_updates            | 542          |
|    policy_gradient_loss | 0.00937      |
|    value_loss           | 44.7         |
------------------------------------------
Eval num_timesteps=805500, episode_reward=197.19 +/- 1.16
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 805500   |
---------------------------------
Eval num_timesteps=806000, episode_reward=197.38 +/- 1.16
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 806000   |
---------------------------------
Eval num_timesteps=806500, episode_reward=197.51 +/- 1.05
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 806500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 394      |
|    time_elapsed    | 18992    |
|    total_timesteps | 806912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=807000, episode_reward=197.21 +/- 0.88
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 807000     |
| train/                  |            |
|    approx_kl            | 0.00551751 |
|    clip_fraction        | 0.0102     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.001      |
|    loss                 | 2.59       |
|    n_updates            | 543        |
|    policy_gradient_loss | 0.00302    |
|    value_loss           | 22.3       |
----------------------------------------
Eval num_timesteps=807500, episode_reward=197.35 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 807500   |
---------------------------------
Eval num_timesteps=808000, episode_reward=197.31 +/- 0.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 808000   |
---------------------------------
Eval num_timesteps=808500, episode_reward=197.51 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 808500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 395      |
|    time_elapsed    | 19063    |
|    total_timesteps | 808960   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=809000, episode_reward=197.46 +/- 0.71
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 809000      |
| train/                  |             |
|    approx_kl            | 0.011233267 |
|    clip_fraction        | 0.0191      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.001       |
|    loss                 | 17.1        |
|    n_updates            | 545         |
|    policy_gradient_loss | 0.00513     |
|    value_loss           | 26.6        |
-----------------------------------------
Eval num_timesteps=809500, episode_reward=197.37 +/- 0.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 809500   |
---------------------------------
Eval num_timesteps=810000, episode_reward=197.60 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 810000   |
---------------------------------
Eval num_timesteps=810500, episode_reward=197.33 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 810500   |
---------------------------------
Eval num_timesteps=811000, episode_reward=197.46 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 811000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 396      |
|    time_elapsed    | 19150    |
|    total_timesteps | 811008   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=811500, episode_reward=197.63 +/- 0.72
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 811500      |
| train/                  |             |
|    approx_kl            | 0.009869275 |
|    clip_fraction        | 0.0272      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.001       |
|    loss                 | 10.7        |
|    n_updates            | 547         |
|    policy_gradient_loss | -0.00154    |
|    value_loss           | 12.2        |
-----------------------------------------
Eval num_timesteps=812000, episode_reward=197.79 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 812000   |
---------------------------------
Eval num_timesteps=812500, episode_reward=197.71 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 812500   |
---------------------------------
Eval num_timesteps=813000, episode_reward=197.63 +/- 0.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 813000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 397      |
|    time_elapsed    | 19222    |
|    total_timesteps | 813056   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=813500, episode_reward=197.49 +/- 0.77
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 813500      |
| train/                  |             |
|    approx_kl            | 0.010207568 |
|    clip_fraction        | 0.0149      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.001       |
|    loss                 | 12.8        |
|    n_updates            | 551         |
|    policy_gradient_loss | 0.000112    |
|    value_loss           | 25.8        |
-----------------------------------------
Eval num_timesteps=814000, episode_reward=197.43 +/- 0.91
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 814000   |
---------------------------------
Eval num_timesteps=814500, episode_reward=197.69 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 814500   |
---------------------------------
Eval num_timesteps=815000, episode_reward=197.57 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 815000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 398      |
|    time_elapsed    | 19293    |
|    total_timesteps | 815104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=815500, episode_reward=197.48 +/- 0.73
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 815500       |
| train/                  |              |
|    approx_kl            | 0.0044563627 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 5.22         |
|    n_updates            | 552          |
|    policy_gradient_loss | 0.0127       |
|    value_loss           | 9.48         |
------------------------------------------
Eval num_timesteps=816000, episode_reward=197.72 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 816000   |
---------------------------------
Eval num_timesteps=816500, episode_reward=197.69 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 816500   |
---------------------------------
Eval num_timesteps=817000, episode_reward=197.57 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 817000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 399      |
|    time_elapsed    | 19364    |
|    total_timesteps | 817152   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=817500, episode_reward=197.78 +/- 0.57
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 817500     |
| train/                  |            |
|    approx_kl            | 0.00826938 |
|    clip_fraction        | 0.0126     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.954      |
|    learning_rate        | 0.001      |
|    loss                 | 10.2       |
|    n_updates            | 554        |
|    policy_gradient_loss | 0.00116    |
|    value_loss           | 14.1       |
----------------------------------------
Eval num_timesteps=818000, episode_reward=197.77 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 818000   |
---------------------------------
Eval num_timesteps=818500, episode_reward=197.62 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 818500   |
---------------------------------
Eval num_timesteps=819000, episode_reward=197.87 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 819000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 400      |
|    time_elapsed    | 19434    |
|    total_timesteps | 819200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=819500, episode_reward=197.53 +/- 0.77
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 819500      |
| train/                  |             |
|    approx_kl            | 0.004779878 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.001       |
|    loss                 | 18.6        |
|    n_updates            | 555         |
|    policy_gradient_loss | 0.000471    |
|    value_loss           | 16.6        |
-----------------------------------------
Eval num_timesteps=820000, episode_reward=197.85 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=820500, episode_reward=197.74 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 820500   |
---------------------------------
Eval num_timesteps=821000, episode_reward=197.76 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 821000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 401      |
|    time_elapsed    | 19506    |
|    total_timesteps | 821248   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=821500, episode_reward=197.10 +/- 1.18
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 821500     |
| train/                  |            |
|    approx_kl            | 0.02126167 |
|    clip_fraction        | 0.0026     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.001      |
|    loss                 | 3.43       |
|    n_updates            | 557        |
|    policy_gradient_loss | -0.00117   |
|    value_loss           | 29         |
----------------------------------------
Eval num_timesteps=822000, episode_reward=197.20 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 822000   |
---------------------------------
Eval num_timesteps=822500, episode_reward=197.06 +/- 1.07
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 822500   |
---------------------------------
Eval num_timesteps=823000, episode_reward=197.21 +/- 1.04
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 823000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 402      |
|    time_elapsed    | 19577    |
|    total_timesteps | 823296   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=823500, episode_reward=197.09 +/- 1.25
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 823500      |
| train/                  |             |
|    approx_kl            | 0.015525651 |
|    clip_fraction        | 0.00893     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.001       |
|    loss                 | 13.7        |
|    n_updates            | 559         |
|    policy_gradient_loss | 0.00141     |
|    value_loss           | 40.8        |
-----------------------------------------
Eval num_timesteps=824000, episode_reward=197.43 +/- 1.06
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 824000   |
---------------------------------
Eval num_timesteps=824500, episode_reward=197.40 +/- 1.02
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 824500   |
---------------------------------
Eval num_timesteps=825000, episode_reward=197.39 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 825000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 403      |
|    time_elapsed    | 19648    |
|    total_timesteps | 825344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=825500, episode_reward=197.73 +/- 0.46
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 825500       |
| train/                  |              |
|    approx_kl            | 0.0061556012 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.711        |
|    learning_rate        | 0.001        |
|    loss                 | 44.5         |
|    n_updates            | 560          |
|    policy_gradient_loss | 0.00457      |
|    value_loss           | 77           |
------------------------------------------
Eval num_timesteps=826000, episode_reward=197.55 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 826000   |
---------------------------------
Eval num_timesteps=826500, episode_reward=197.63 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 826500   |
---------------------------------
Eval num_timesteps=827000, episode_reward=197.68 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 827000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 404      |
|    time_elapsed    | 19718    |
|    total_timesteps | 827392   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=827500, episode_reward=197.26 +/- 0.74
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 827500      |
| train/                  |             |
|    approx_kl            | 0.009487028 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.001       |
|    loss                 | 5.01        |
|    n_updates            | 563         |
|    policy_gradient_loss | 0.00219     |
|    value_loss           | 50.4        |
-----------------------------------------
Eval num_timesteps=828000, episode_reward=197.21 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 828000   |
---------------------------------
Eval num_timesteps=828500, episode_reward=197.35 +/- 0.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 828500   |
---------------------------------
Eval num_timesteps=829000, episode_reward=197.41 +/- 0.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 829000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 405      |
|    time_elapsed    | 19789    |
|    total_timesteps | 829440   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=829500, episode_reward=197.86 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | 198       |
| time/                   |           |
|    total_timesteps      | 829500    |
| train/                  |           |
|    approx_kl            | 0.0075899 |
|    clip_fraction        | 0.0571    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.25     |
|    explained_variance   | 0.919     |
|    learning_rate        | 0.001     |
|    loss                 | 3.73      |
|    n_updates            | 565       |
|    policy_gradient_loss | 0.0027    |
|    value_loss           | 22.4      |
---------------------------------------
Eval num_timesteps=830000, episode_reward=197.72 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 830000   |
---------------------------------
Eval num_timesteps=830500, episode_reward=197.81 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 830500   |
---------------------------------
Eval num_timesteps=831000, episode_reward=197.67 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 831000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 406      |
|    time_elapsed    | 19860    |
|    total_timesteps | 831488   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=831500, episode_reward=197.70 +/- 0.57
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 831500      |
| train/                  |             |
|    approx_kl            | 0.010362955 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.001       |
|    loss                 | 13.5        |
|    n_updates            | 568         |
|    policy_gradient_loss | -0.00216    |
|    value_loss           | 23.6        |
-----------------------------------------
Eval num_timesteps=832000, episode_reward=197.70 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 832000   |
---------------------------------
Eval num_timesteps=832500, episode_reward=197.56 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 832500   |
---------------------------------
Eval num_timesteps=833000, episode_reward=197.47 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 833000   |
---------------------------------
Eval num_timesteps=833500, episode_reward=197.57 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 833500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 407      |
|    time_elapsed    | 19949    |
|    total_timesteps | 833536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=834000, episode_reward=197.72 +/- 0.62
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 834000      |
| train/                  |             |
|    approx_kl            | 0.006706036 |
|    clip_fraction        | 0.0696      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 8.35        |
|    n_updates            | 569         |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 8.3         |
-----------------------------------------
Eval num_timesteps=834500, episode_reward=197.77 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 834500   |
---------------------------------
Eval num_timesteps=835000, episode_reward=197.71 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 835000   |
---------------------------------
Eval num_timesteps=835500, episode_reward=197.66 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 835500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 408      |
|    time_elapsed    | 20020    |
|    total_timesteps | 835584   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=836000, episode_reward=197.76 +/- 0.61
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 836000      |
| train/                  |             |
|    approx_kl            | 0.011337786 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 5.99        |
|    n_updates            | 572         |
|    policy_gradient_loss | 0.00206     |
|    value_loss           | 9.95        |
-----------------------------------------
Eval num_timesteps=836500, episode_reward=197.82 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 836500   |
---------------------------------
Eval num_timesteps=837000, episode_reward=197.82 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 837000   |
---------------------------------
Eval num_timesteps=837500, episode_reward=197.83 +/- 0.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 837500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 409      |
|    time_elapsed    | 20091    |
|    total_timesteps | 837632   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=838000, episode_reward=197.79 +/- 0.49
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 838000      |
| train/                  |             |
|    approx_kl            | 0.006328903 |
|    clip_fraction        | 0.00618     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.001       |
|    loss                 | 2.09        |
|    n_updates            | 574         |
|    policy_gradient_loss | -0.00033    |
|    value_loss           | 23.7        |
-----------------------------------------
Eval num_timesteps=838500, episode_reward=197.65 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 838500   |
---------------------------------
Eval num_timesteps=839000, episode_reward=197.77 +/- 0.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 839000   |
---------------------------------
Eval num_timesteps=839500, episode_reward=197.71 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 839500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 410      |
|    time_elapsed    | 20163    |
|    total_timesteps | 839680   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=840000, episode_reward=197.79 +/- 0.53
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.004983393 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.001       |
|    loss                 | 5.64        |
|    n_updates            | 577         |
|    policy_gradient_loss | -0.00184    |
|    value_loss           | 19.1        |
-----------------------------------------
Eval num_timesteps=840500, episode_reward=197.63 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 840500   |
---------------------------------
Eval num_timesteps=841000, episode_reward=197.67 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 841000   |
---------------------------------
Eval num_timesteps=841500, episode_reward=197.81 +/- 0.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 841500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 411      |
|    time_elapsed    | 20235    |
|    total_timesteps | 841728   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=842000, episode_reward=197.57 +/- 0.57
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 842000      |
| train/                  |             |
|    approx_kl            | 0.010976896 |
|    clip_fraction        | 0.00901     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.001       |
|    loss                 | 6.35        |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00138    |
|    value_loss           | 9.11        |
-----------------------------------------
Eval num_timesteps=842500, episode_reward=197.54 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 842500   |
---------------------------------
Eval num_timesteps=843000, episode_reward=197.59 +/- 0.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 843000   |
---------------------------------
Eval num_timesteps=843500, episode_reward=197.66 +/- 0.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 843500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 181      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 412      |
|    time_elapsed    | 20306    |
|    total_timesteps | 843776   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=844000, episode_reward=197.68 +/- 0.64
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | 198       |
| time/                   |           |
|    total_timesteps      | 844000    |
| train/                  |           |
|    approx_kl            | 0.0092355 |
|    clip_fraction        | 0.017     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.16     |
|    explained_variance   | 0.946     |
|    learning_rate        | 0.001     |
|    loss                 | 4.59      |
|    n_updates            | 582       |
|    policy_gradient_loss | -0.00294  |
|    value_loss           | 15.6      |
---------------------------------------
Eval num_timesteps=844500, episode_reward=197.84 +/- 0.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 844500   |
---------------------------------
Eval num_timesteps=845000, episode_reward=197.69 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 845000   |
---------------------------------
Eval num_timesteps=845500, episode_reward=197.75 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 845500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 413      |
|    time_elapsed    | 20378    |
|    total_timesteps | 845824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=846000, episode_reward=193.13 +/- 29.69
Episode length: 515.02 +/- 69.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 846000      |
| train/                  |             |
|    approx_kl            | 0.002849761 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.001       |
|    loss                 | 3.52        |
|    n_updates            | 583         |
|    policy_gradient_loss | 0.012       |
|    value_loss           | 19.3        |
-----------------------------------------
Eval num_timesteps=846500, episode_reward=197.67 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 846500   |
---------------------------------
Eval num_timesteps=847000, episode_reward=197.50 +/- 0.91
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 847000   |
---------------------------------
Eval num_timesteps=847500, episode_reward=197.47 +/- 0.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 847500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 414      |
|    time_elapsed    | 20449    |
|    total_timesteps | 847872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=848000, episode_reward=197.16 +/- 1.22
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 848000       |
| train/                  |              |
|    approx_kl            | 0.0041746967 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.001        |
|    loss                 | 46           |
|    n_updates            | 584          |
|    policy_gradient_loss | 0.00422      |
|    value_loss           | 34.9         |
------------------------------------------
Eval num_timesteps=848500, episode_reward=197.04 +/- 1.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 848500   |
---------------------------------
Eval num_timesteps=849000, episode_reward=196.95 +/- 1.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 849000   |
---------------------------------
Eval num_timesteps=849500, episode_reward=196.64 +/- 1.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 849500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 415      |
|    time_elapsed    | 20521    |
|    total_timesteps | 849920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.17
Eval num_timesteps=850000, episode_reward=197.90 +/- 0.36
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.014834433 |
|    clip_fraction        | 0.0126      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.001       |
|    loss                 | 39.3        |
|    n_updates            | 585         |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 45.8        |
-----------------------------------------
Eval num_timesteps=850500, episode_reward=197.84 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 850500   |
---------------------------------
Eval num_timesteps=851000, episode_reward=197.91 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 851000   |
---------------------------------
Eval num_timesteps=851500, episode_reward=197.79 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 851500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 416      |
|    time_elapsed    | 20592    |
|    total_timesteps | 851968   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.12
Eval num_timesteps=852000, episode_reward=196.58 +/- 1.53
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 852000      |
| train/                  |             |
|    approx_kl            | 0.023309099 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.001       |
|    loss                 | 1.19        |
|    n_updates            | 587         |
|    policy_gradient_loss | -0.00217    |
|    value_loss           | 18.6        |
-----------------------------------------
Eval num_timesteps=852500, episode_reward=197.22 +/- 1.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 852500   |
---------------------------------
Eval num_timesteps=853000, episode_reward=197.09 +/- 1.12
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 853000   |
---------------------------------
Eval num_timesteps=853500, episode_reward=196.76 +/- 1.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 853500   |
---------------------------------
Eval num_timesteps=854000, episode_reward=196.89 +/- 1.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 417      |
|    time_elapsed    | 20683    |
|    total_timesteps | 854016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=854500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 854500      |
| train/                  |             |
|    approx_kl            | 0.003931362 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.001       |
|    loss                 | 70          |
|    n_updates            | 588         |
|    policy_gradient_loss | -0.000493   |
|    value_loss           | 99.6        |
-----------------------------------------
Eval num_timesteps=855000, episode_reward=197.67 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 855000   |
---------------------------------
Eval num_timesteps=855500, episode_reward=197.78 +/- 0.52
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 855500   |
---------------------------------
Eval num_timesteps=856000, episode_reward=195.53 +/- 15.60
Episode length: 517.64 +/- 51.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 856000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 418      |
|    time_elapsed    | 20754    |
|    total_timesteps | 856064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=856500, episode_reward=197.88 +/- 0.40
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 856500       |
| train/                  |              |
|    approx_kl            | 0.0029545645 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.001        |
|    loss                 | 1.83         |
|    n_updates            | 589          |
|    policy_gradient_loss | 0.00699      |
|    value_loss           | 22           |
------------------------------------------
Eval num_timesteps=857000, episode_reward=197.85 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 857000   |
---------------------------------
Eval num_timesteps=857500, episode_reward=197.80 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 857500   |
---------------------------------
Eval num_timesteps=858000, episode_reward=197.78 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 858000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 419      |
|    time_elapsed    | 20826    |
|    total_timesteps | 858112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=858500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 858500      |
| train/                  |             |
|    approx_kl            | 0.002382271 |
|    clip_fraction        | 0.00434     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0711      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 27.4        |
-----------------------------------------
Eval num_timesteps=859000, episode_reward=197.91 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 859000   |
---------------------------------
Eval num_timesteps=859500, episode_reward=197.88 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 859500   |
---------------------------------
Eval num_timesteps=860000, episode_reward=197.86 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 420      |
|    time_elapsed    | 20897    |
|    total_timesteps | 860160   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=860500, episode_reward=197.90 +/- 0.36
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 860500      |
| train/                  |             |
|    approx_kl            | 0.026723465 |
|    clip_fraction        | 0.00651     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.001       |
|    loss                 | 1.92        |
|    n_updates            | 592         |
|    policy_gradient_loss | -0.000275   |
|    value_loss           | 19.2        |
-----------------------------------------
Eval num_timesteps=861000, episode_reward=197.93 +/- 0.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 861000   |
---------------------------------
Eval num_timesteps=861500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 861500   |
---------------------------------
Eval num_timesteps=862000, episode_reward=197.88 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 862000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 421      |
|    time_elapsed    | 20969    |
|    total_timesteps | 862208   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=862500, episode_reward=196.65 +/- 1.66
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 862500      |
| train/                  |             |
|    approx_kl            | 0.006319381 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.001       |
|    loss                 | 0.383       |
|    n_updates            | 594         |
|    policy_gradient_loss | 0.00289     |
|    value_loss           | 30.9        |
-----------------------------------------
Eval num_timesteps=863000, episode_reward=197.01 +/- 1.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 863000   |
---------------------------------
Eval num_timesteps=863500, episode_reward=194.21 +/- 15.51
Episode length: 516.44 +/- 59.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 863500   |
---------------------------------
Eval num_timesteps=864000, episode_reward=196.92 +/- 1.19
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 864000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 422      |
|    time_elapsed    | 21042    |
|    total_timesteps | 864256   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=864500, episode_reward=188.79 +/- 29.46
Episode length: 515.04 +/- 69.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 864500      |
| train/                  |             |
|    approx_kl            | 0.022609051 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.001       |
|    loss                 | 3.31        |
|    n_updates            | 596         |
|    policy_gradient_loss | 0.00639     |
|    value_loss           | 24.1        |
-----------------------------------------
Eval num_timesteps=865000, episode_reward=178.40 +/- 51.94
Episode length: 485.56 +/- 133.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 865000   |
---------------------------------
Eval num_timesteps=865500, episode_reward=155.39 +/- 79.31
Episode length: 427.74 +/- 194.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 428      |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 865500   |
---------------------------------
Eval num_timesteps=866000, episode_reward=177.26 +/- 56.24
Episode length: 476.40 +/- 145.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 866000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 423      |
|    time_elapsed    | 21107    |
|    total_timesteps | 866304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=866500, episode_reward=197.81 +/- 0.47
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 866500      |
| train/                  |             |
|    approx_kl            | 0.005443087 |
|    clip_fraction        | 0.0026      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.001       |
|    loss                 | 41.5        |
|    n_updates            | 597         |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 46.8        |
-----------------------------------------
Eval num_timesteps=867000, episode_reward=197.81 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 867000   |
---------------------------------
Eval num_timesteps=867500, episode_reward=197.82 +/- 0.42
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 867500   |
---------------------------------
Eval num_timesteps=868000, episode_reward=197.74 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 868000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 424      |
|    time_elapsed    | 21178    |
|    total_timesteps | 868352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=868500, episode_reward=197.69 +/- 0.57
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 868500       |
| train/                  |              |
|    approx_kl            | 0.0020970954 |
|    clip_fraction        | 0.0026       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.15        |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.001        |
|    loss                 | 0.0479       |
|    n_updates            | 598          |
|    policy_gradient_loss | -0.00534     |
|    value_loss           | 28.3         |
------------------------------------------
Eval num_timesteps=869000, episode_reward=197.58 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 869000   |
---------------------------------
Eval num_timesteps=869500, episode_reward=197.76 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 869500   |
---------------------------------
Eval num_timesteps=870000, episode_reward=197.54 +/- 0.90
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 870000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 425      |
|    time_elapsed    | 21250    |
|    total_timesteps | 870400   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=870500, episode_reward=197.61 +/- 0.69
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 870500      |
| train/                  |             |
|    approx_kl            | 0.015191264 |
|    clip_fraction        | 0.00888     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.001       |
|    loss                 | 7.11        |
|    n_updates            | 600         |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 17.2        |
-----------------------------------------
Eval num_timesteps=871000, episode_reward=197.75 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 871000   |
---------------------------------
Eval num_timesteps=871500, episode_reward=197.72 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 871500   |
---------------------------------
Eval num_timesteps=872000, episode_reward=197.53 +/- 0.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 872000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 426      |
|    time_elapsed    | 21322    |
|    total_timesteps | 872448   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=872500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 872500      |
| train/                  |             |
|    approx_kl            | 0.015422003 |
|    clip_fraction        | 0.00985     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 10.5        |
|    n_updates            | 602         |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 17.4        |
-----------------------------------------
Eval num_timesteps=873000, episode_reward=197.94 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 873000   |
---------------------------------
Eval num_timesteps=873500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 873500   |
---------------------------------
Eval num_timesteps=874000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 874000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 427      |
|    time_elapsed    | 21394    |
|    total_timesteps | 874496   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=874500, episode_reward=197.65 +/- 0.75
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 874500      |
| train/                  |             |
|    approx_kl            | 0.017662752 |
|    clip_fraction        | 0.0105      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.001       |
|    loss                 | 24.4        |
|    n_updates            | 604         |
|    policy_gradient_loss | -0.00105    |
|    value_loss           | 28.9        |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=197.77 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 875000   |
---------------------------------
Eval num_timesteps=875500, episode_reward=197.79 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 875500   |
---------------------------------
Eval num_timesteps=876000, episode_reward=197.81 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 876000   |
---------------------------------
Eval num_timesteps=876500, episode_reward=197.80 +/- 0.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 876500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 428      |
|    time_elapsed    | 21483    |
|    total_timesteps | 876544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=877000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 877000       |
| train/                  |              |
|    approx_kl            | 0.0044640633 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.001        |
|    loss                 | 16.1         |
|    n_updates            | 605          |
|    policy_gradient_loss | -0.00129     |
|    value_loss           | 15.2         |
------------------------------------------
Eval num_timesteps=877500, episode_reward=197.84 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 877500   |
---------------------------------
Eval num_timesteps=878000, episode_reward=197.82 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 878000   |
---------------------------------
Eval num_timesteps=878500, episode_reward=197.86 +/- 0.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 878500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 429      |
|    time_elapsed    | 21555    |
|    total_timesteps | 878592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=879000, episode_reward=197.85 +/- 0.70
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 879000      |
| train/                  |             |
|    approx_kl            | 0.004501531 |
|    clip_fraction        | 0.0135      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.001       |
|    loss                 | 0.438       |
|    n_updates            | 606         |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 22.2        |
-----------------------------------------
Eval num_timesteps=879500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 879500   |
---------------------------------
Eval num_timesteps=880000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=880500, episode_reward=197.89 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 880500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 430      |
|    time_elapsed    | 21627    |
|    total_timesteps | 880640   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=881000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 881000       |
| train/                  |              |
|    approx_kl            | 0.0051009553 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 14.8         |
|    n_updates            | 612          |
|    policy_gradient_loss | -0.000556    |
|    value_loss           | 14.5         |
------------------------------------------
Eval num_timesteps=881500, episode_reward=197.91 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 881500   |
---------------------------------
Eval num_timesteps=882000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 882000   |
---------------------------------
Eval num_timesteps=882500, episode_reward=197.94 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 882500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 431      |
|    time_elapsed    | 21699    |
|    total_timesteps | 882688   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.04
Eval num_timesteps=883000, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 883000       |
| train/                  |              |
|    approx_kl            | 0.0058555827 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.001        |
|    loss                 | 2.16         |
|    n_updates            | 619          |
|    policy_gradient_loss | 0.000385     |
|    value_loss           | 18           |
------------------------------------------
Eval num_timesteps=883500, episode_reward=197.86 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 883500   |
---------------------------------
Eval num_timesteps=884000, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 884000   |
---------------------------------
Eval num_timesteps=884500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 884500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 432      |
|    time_elapsed    | 21771    |
|    total_timesteps | 884736   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
Eval num_timesteps=885000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 885000      |
| train/                  |             |
|    approx_kl            | 0.013959666 |
|    clip_fraction        | 0.00414     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.001       |
|    loss                 | 11.4        |
|    n_updates            | 623         |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 19.3        |
-----------------------------------------
Eval num_timesteps=885500, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 885500   |
---------------------------------
Eval num_timesteps=886000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 886000   |
---------------------------------
Eval num_timesteps=886500, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 886500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 433      |
|    time_elapsed    | 21842    |
|    total_timesteps | 886784   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=887000, episode_reward=197.94 +/- 0.30
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 887000      |
| train/                  |             |
|    approx_kl            | 0.008260763 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.001       |
|    loss                 | 13.2        |
|    n_updates            | 625         |
|    policy_gradient_loss | -0.000341   |
|    value_loss           | 17.3        |
-----------------------------------------
Eval num_timesteps=887500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 887500   |
---------------------------------
Eval num_timesteps=888000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 888000   |
---------------------------------
Eval num_timesteps=888500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 888500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 434      |
|    time_elapsed    | 21915    |
|    total_timesteps | 888832   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=889000, episode_reward=197.90 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 889000     |
| train/                  |            |
|    approx_kl            | 0.00736683 |
|    clip_fraction        | 0.00815    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.001      |
|    loss                 | 7.6        |
|    n_updates            | 627        |
|    policy_gradient_loss | -0.00195   |
|    value_loss           | 4.87       |
----------------------------------------
Eval num_timesteps=889500, episode_reward=197.95 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 889500   |
---------------------------------
Eval num_timesteps=890000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 890000   |
---------------------------------
Eval num_timesteps=890500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 890500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 435      |
|    time_elapsed    | 21985    |
|    total_timesteps | 890880   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=891000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 891000      |
| train/                  |             |
|    approx_kl            | 0.011379295 |
|    clip_fraction        | 0.027       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.001       |
|    loss                 | -0.0355     |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.000564   |
|    value_loss           | 18.6        |
-----------------------------------------
Eval num_timesteps=891500, episode_reward=197.91 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 891500   |
---------------------------------
Eval num_timesteps=892000, episode_reward=197.87 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 892000   |
---------------------------------
Eval num_timesteps=892500, episode_reward=197.93 +/- 0.25
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 892500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 436      |
|    time_elapsed    | 22058    |
|    total_timesteps | 892928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=893000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 893000      |
| train/                  |             |
|    approx_kl            | 0.014927419 |
|    clip_fraction        | 0.0373      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 19.2        |
|    n_updates            | 632         |
|    policy_gradient_loss | -0.000187   |
|    value_loss           | 12.8        |
-----------------------------------------
Eval num_timesteps=893500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 893500   |
---------------------------------
Eval num_timesteps=894000, episode_reward=197.94 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 894000   |
---------------------------------
Eval num_timesteps=894500, episode_reward=197.89 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 894500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 437      |
|    time_elapsed    | 22128    |
|    total_timesteps | 894976   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.03
Eval num_timesteps=895000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 895000       |
| train/                  |              |
|    approx_kl            | 0.0050167674 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 0.0387       |
|    n_updates            | 637          |
|    policy_gradient_loss | 0.000851     |
|    value_loss           | 7.34         |
------------------------------------------
Eval num_timesteps=895500, episode_reward=197.90 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 895500   |
---------------------------------
Eval num_timesteps=896000, episode_reward=197.88 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 896000   |
---------------------------------
Eval num_timesteps=896500, episode_reward=197.93 +/- 0.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 896500   |
---------------------------------
Eval num_timesteps=897000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 897000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 438      |
|    time_elapsed    | 22218    |
|    total_timesteps | 897024   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=897500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 897500       |
| train/                  |              |
|    approx_kl            | 0.0069974307 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.001        |
|    loss                 | 5.73         |
|    n_updates            | 639          |
|    policy_gradient_loss | 0.0023       |
|    value_loss           | 9.57         |
------------------------------------------
Eval num_timesteps=898000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 898000   |
---------------------------------
Eval num_timesteps=898500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 898500   |
---------------------------------
Eval num_timesteps=899000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 899000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 439      |
|    time_elapsed    | 22290    |
|    total_timesteps | 899072   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=899500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 899500      |
| train/                  |             |
|    approx_kl            | 0.006634176 |
|    clip_fraction        | 0.00479     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.001       |
|    loss                 | 6.97        |
|    n_updates            | 643         |
|    policy_gradient_loss | 0.00107     |
|    value_loss           | 17.1        |
-----------------------------------------
Eval num_timesteps=900000, episode_reward=197.84 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=900500, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 900500   |
---------------------------------
Eval num_timesteps=901000, episode_reward=197.92 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 901000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 440      |
|    time_elapsed    | 22363    |
|    total_timesteps | 901120   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=901500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 901500      |
| train/                  |             |
|    approx_kl            | 0.014215894 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 30.8        |
|    n_updates            | 646         |
|    policy_gradient_loss | -0.00404    |
|    value_loss           | 15.8        |
-----------------------------------------
Eval num_timesteps=902000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 902000   |
---------------------------------
Eval num_timesteps=902500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 902500   |
---------------------------------
Eval num_timesteps=903000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 903000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 441      |
|    time_elapsed    | 22433    |
|    total_timesteps | 903168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=903500, episode_reward=197.94 +/- 0.30
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 903500      |
| train/                  |             |
|    approx_kl            | 0.006148629 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.001       |
|    loss                 | 0.264       |
|    n_updates            | 647         |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 13.2        |
-----------------------------------------
Eval num_timesteps=904000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 904000   |
---------------------------------
Eval num_timesteps=904500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 904500   |
---------------------------------
Eval num_timesteps=905000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 905000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 442      |
|    time_elapsed    | 22504    |
|    total_timesteps | 905216   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=905500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 905500      |
| train/                  |             |
|    approx_kl            | 0.015129958 |
|    clip_fraction        | 0.0827      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.001       |
|    loss                 | 30.1        |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.000797   |
|    value_loss           | 28.3        |
-----------------------------------------
Eval num_timesteps=906000, episode_reward=197.91 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 906000   |
---------------------------------
Eval num_timesteps=906500, episode_reward=197.94 +/- 0.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 906500   |
---------------------------------
Eval num_timesteps=907000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 907000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 443      |
|    time_elapsed    | 22575    |
|    total_timesteps | 907264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=907500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 907500       |
| train/                  |              |
|    approx_kl            | 0.0017358079 |
|    clip_fraction        | 0.0012       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.882        |
|    learning_rate        | 0.001        |
|    loss                 | 50.5         |
|    n_updates            | 651          |
|    policy_gradient_loss | 0.000266     |
|    value_loss           | 46.2         |
------------------------------------------
Eval num_timesteps=908000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 908000   |
---------------------------------
Eval num_timesteps=908500, episode_reward=197.93 +/- 0.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 908500   |
---------------------------------
Eval num_timesteps=909000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 909000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 444      |
|    time_elapsed    | 22647    |
|    total_timesteps | 909312   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
Eval num_timesteps=909500, episode_reward=197.89 +/- 0.34
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 909500       |
| train/                  |              |
|    approx_kl            | 0.0076148254 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.001        |
|    loss                 | 9.18         |
|    n_updates            | 661          |
|    policy_gradient_loss | -8.74e-05    |
|    value_loss           | 6.93         |
------------------------------------------
Eval num_timesteps=910000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 910000   |
---------------------------------
Eval num_timesteps=910500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 910500   |
---------------------------------
Eval num_timesteps=911000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 911000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 445      |
|    time_elapsed    | 22719    |
|    total_timesteps | 911360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=911500, episode_reward=197.76 +/- 0.67
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 911500      |
| train/                  |             |
|    approx_kl            | 0.005353635 |
|    clip_fraction        | 0.000868    |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 13.6        |
|    n_updates            | 662         |
|    policy_gradient_loss | -0.00232    |
|    value_loss           | 24.9        |
-----------------------------------------
Eval num_timesteps=912000, episode_reward=197.84 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 912000   |
---------------------------------
Eval num_timesteps=912500, episode_reward=197.79 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 912500   |
---------------------------------
Eval num_timesteps=913000, episode_reward=197.79 +/- 0.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 913000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 446      |
|    time_elapsed    | 22791    |
|    total_timesteps | 913408   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=913500, episode_reward=196.87 +/- 0.87
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 913500      |
| train/                  |             |
|    approx_kl            | 0.019301364 |
|    clip_fraction        | 0.00576     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.001       |
|    loss                 | 5.36        |
|    n_updates            | 664         |
|    policy_gradient_loss | -0.000636   |
|    value_loss           | 42.1        |
-----------------------------------------
Eval num_timesteps=914000, episode_reward=196.96 +/- 0.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 914000   |
---------------------------------
Eval num_timesteps=914500, episode_reward=196.96 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 914500   |
---------------------------------
Eval num_timesteps=915000, episode_reward=196.78 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 915000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 447      |
|    time_elapsed    | 22863    |
|    total_timesteps | 915456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=915500, episode_reward=196.08 +/- 1.01
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 915500      |
| train/                  |             |
|    approx_kl            | 0.004500922 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.001       |
|    loss                 | 32          |
|    n_updates            | 665         |
|    policy_gradient_loss | 0.00537     |
|    value_loss           | 59.7        |
-----------------------------------------
Eval num_timesteps=916000, episode_reward=196.25 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 916000   |
---------------------------------
Eval num_timesteps=916500, episode_reward=196.35 +/- 0.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 916500   |
---------------------------------
Eval num_timesteps=917000, episode_reward=196.41 +/- 0.93
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 917000   |
---------------------------------
Eval num_timesteps=917500, episode_reward=196.46 +/- 0.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 917500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 448      |
|    time_elapsed    | 22953    |
|    total_timesteps | 917504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=918000, episode_reward=196.48 +/- 0.88
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 918000      |
| train/                  |             |
|    approx_kl            | 0.003986898 |
|    clip_fraction        | 0.0145      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.001       |
|    loss                 | 38.8        |
|    n_updates            | 666         |
|    policy_gradient_loss | 0.0156      |
|    value_loss           | 50.2        |
-----------------------------------------
Eval num_timesteps=918500, episode_reward=196.17 +/- 1.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 918500   |
---------------------------------
Eval num_timesteps=919000, episode_reward=196.08 +/- 0.94
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 919000   |
---------------------------------
Eval num_timesteps=919500, episode_reward=196.13 +/- 1.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 919500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 449      |
|    time_elapsed    | 23024    |
|    total_timesteps | 919552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=920000, episode_reward=197.33 +/- 0.91
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.005263658 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.001       |
|    loss                 | 26.2        |
|    n_updates            | 667         |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 35.9        |
-----------------------------------------
Eval num_timesteps=920500, episode_reward=197.34 +/- 0.91
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 920500   |
---------------------------------
Eval num_timesteps=921000, episode_reward=197.30 +/- 0.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 921000   |
---------------------------------
Eval num_timesteps=921500, episode_reward=197.28 +/- 1.06
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 921500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 450      |
|    time_elapsed    | 23095    |
|    total_timesteps | 921600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=922000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 922000      |
| train/                  |             |
|    approx_kl            | 0.004249205 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.001       |
|    loss                 | 8.67        |
|    n_updates            | 668         |
|    policy_gradient_loss | 0.00364     |
|    value_loss           | 27.6        |
-----------------------------------------
Eval num_timesteps=922500, episode_reward=197.85 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 922500   |
---------------------------------
Eval num_timesteps=923000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 923000   |
---------------------------------
Eval num_timesteps=923500, episode_reward=197.95 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 923500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 451      |
|    time_elapsed    | 23167    |
|    total_timesteps | 923648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=924000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 924000      |
| train/                  |             |
|    approx_kl            | 0.005556614 |
|    clip_fraction        | 0.0258      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.001       |
|    loss                 | 15.2        |
|    n_updates            | 669         |
|    policy_gradient_loss | 0.00283     |
|    value_loss           | 48.6        |
-----------------------------------------
Eval num_timesteps=924500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 924500   |
---------------------------------
Eval num_timesteps=925000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 925000   |
---------------------------------
Eval num_timesteps=925500, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 925500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 452      |
|    time_elapsed    | 23237    |
|    total_timesteps | 925696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=926000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 926000      |
| train/                  |             |
|    approx_kl            | 0.008081228 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.001       |
|    loss                 | 11          |
|    n_updates            | 670         |
|    policy_gradient_loss | 0.0027      |
|    value_loss           | 18.3        |
-----------------------------------------
Eval num_timesteps=926500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 926500   |
---------------------------------
Eval num_timesteps=927000, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 927000   |
---------------------------------
Eval num_timesteps=927500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 927500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 453      |
|    time_elapsed    | 23308    |
|    total_timesteps | 927744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=928000, episode_reward=197.66 +/- 0.47
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 928000       |
| train/                  |              |
|    approx_kl            | 0.0052298033 |
|    clip_fraction        | 0.0686       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.001        |
|    loss                 | 1.8          |
|    n_updates            | 671          |
|    policy_gradient_loss | 0.00172      |
|    value_loss           | 18.5         |
------------------------------------------
Eval num_timesteps=928500, episode_reward=197.61 +/- 0.57
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 928500   |
---------------------------------
Eval num_timesteps=929000, episode_reward=197.70 +/- 0.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 929000   |
---------------------------------
Eval num_timesteps=929500, episode_reward=197.67 +/- 0.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 929500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 454      |
|    time_elapsed    | 23379    |
|    total_timesteps | 929792   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=930000, episode_reward=197.10 +/- 3.34
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.011471258 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 35.5        |
|    n_updates            | 674         |
|    policy_gradient_loss | -0.000572   |
|    value_loss           | 24.9        |
-----------------------------------------
Eval num_timesteps=930500, episode_reward=197.43 +/- 0.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 930500   |
---------------------------------
Eval num_timesteps=931000, episode_reward=196.56 +/- 4.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 931000   |
---------------------------------
Eval num_timesteps=931500, episode_reward=197.64 +/- 0.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 931500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 455      |
|    time_elapsed    | 23452    |
|    total_timesteps | 931840   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=932000, episode_reward=197.88 +/- 0.32
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 932000      |
| train/                  |             |
|    approx_kl            | 0.008735997 |
|    clip_fraction        | 0.0399      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.001       |
|    loss                 | 59.6        |
|    n_updates            | 678         |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 30.1        |
-----------------------------------------
Eval num_timesteps=932500, episode_reward=197.92 +/- 0.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 932500   |
---------------------------------
Eval num_timesteps=933000, episode_reward=197.87 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 933000   |
---------------------------------
Eval num_timesteps=933500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 933500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 456      |
|    time_elapsed    | 23523    |
|    total_timesteps | 933888   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=934000, episode_reward=197.86 +/- 0.35
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 934000      |
| train/                  |             |
|    approx_kl            | 0.007062804 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 5.87        |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.0009     |
|    value_loss           | 13.7        |
-----------------------------------------
Eval num_timesteps=934500, episode_reward=197.90 +/- 0.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 934500   |
---------------------------------
Eval num_timesteps=935000, episode_reward=197.89 +/- 0.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 935000   |
---------------------------------
Eval num_timesteps=935500, episode_reward=197.89 +/- 0.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 935500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 457      |
|    time_elapsed    | 23593    |
|    total_timesteps | 935936   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=936000, episode_reward=197.89 +/- 0.34
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 936000       |
| train/                  |              |
|    approx_kl            | 0.0059348107 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.887        |
|    learning_rate        | 0.001        |
|    loss                 | 0.368        |
|    n_updates            | 682          |
|    policy_gradient_loss | 0.00124      |
|    value_loss           | 29.6         |
------------------------------------------
Eval num_timesteps=936500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 936500   |
---------------------------------
Eval num_timesteps=937000, episode_reward=197.88 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 937000   |
---------------------------------
Eval num_timesteps=937500, episode_reward=197.82 +/- 0.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 937500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 458      |
|    time_elapsed    | 23665    |
|    total_timesteps | 937984   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=938000, episode_reward=197.82 +/- 0.38
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 938000      |
| train/                  |             |
|    approx_kl            | 0.011626184 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.001       |
|    loss                 | 0.402       |
|    n_updates            | 685         |
|    policy_gradient_loss | -0.000664   |
|    value_loss           | 14          |
-----------------------------------------
Eval num_timesteps=938500, episode_reward=197.81 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 938500   |
---------------------------------
Eval num_timesteps=939000, episode_reward=197.81 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 939000   |
---------------------------------
Eval num_timesteps=939500, episode_reward=197.83 +/- 0.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 939500   |
---------------------------------
Eval num_timesteps=940000, episode_reward=197.79 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 459      |
|    time_elapsed    | 23753    |
|    total_timesteps | 940032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=940500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 940500     |
| train/                  |            |
|    approx_kl            | 0.00525728 |
|    clip_fraction        | 0.0249     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.001      |
|    loss                 | 0.998      |
|    n_updates            | 686        |
|    policy_gradient_loss | -0.00165   |
|    value_loss           | 11         |
----------------------------------------
Eval num_timesteps=941000, episode_reward=197.90 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 941000   |
---------------------------------
Eval num_timesteps=941500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 941500   |
---------------------------------
Eval num_timesteps=942000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 942000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 460      |
|    time_elapsed    | 23825    |
|    total_timesteps | 942080   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=942500, episode_reward=197.81 +/- 0.44
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 942500      |
| train/                  |             |
|    approx_kl            | 0.006466977 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.001       |
|    loss                 | 1.81        |
|    n_updates            | 689         |
|    policy_gradient_loss | -0.00435    |
|    value_loss           | 11.9        |
-----------------------------------------
Eval num_timesteps=943000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 943000   |
---------------------------------
Eval num_timesteps=943500, episode_reward=197.89 +/- 0.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 943500   |
---------------------------------
Eval num_timesteps=944000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 944000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 461      |
|    time_elapsed    | 23896    |
|    total_timesteps | 944128   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=944500, episode_reward=197.41 +/- 0.93
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 944500      |
| train/                  |             |
|    approx_kl            | 0.027886104 |
|    clip_fraction        | 0.011       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 10.4        |
|    n_updates            | 691         |
|    policy_gradient_loss | -0.00287    |
|    value_loss           | 10.3        |
-----------------------------------------
Eval num_timesteps=945000, episode_reward=197.50 +/- 0.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 945000   |
---------------------------------
Eval num_timesteps=945500, episode_reward=197.45 +/- 0.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 945500   |
---------------------------------
Eval num_timesteps=946000, episode_reward=197.43 +/- 0.89
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 946000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 462      |
|    time_elapsed    | 23967    |
|    total_timesteps | 946176   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=946500, episode_reward=196.53 +/- 1.75
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 946500      |
| train/                  |             |
|    approx_kl            | 0.011335173 |
|    clip_fraction        | 0.016       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.001       |
|    loss                 | 4.18        |
|    n_updates            | 693         |
|    policy_gradient_loss | 0.000641    |
|    value_loss           | 54          |
-----------------------------------------
Eval num_timesteps=947000, episode_reward=189.83 +/- 26.59
Episode length: 495.76 +/- 115.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 947000   |
---------------------------------
Eval num_timesteps=947500, episode_reward=194.80 +/- 15.55
Episode length: 515.10 +/- 69.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 947500   |
---------------------------------
Eval num_timesteps=948000, episode_reward=193.97 +/- 15.61
Episode length: 515.16 +/- 68.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 948000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 463      |
|    time_elapsed    | 24036    |
|    total_timesteps | 948224   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=948500, episode_reward=197.39 +/- 1.47
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 948500      |
| train/                  |             |
|    approx_kl            | 0.014047343 |
|    clip_fraction        | 0.0239      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.001       |
|    loss                 | 5.41        |
|    n_updates            | 695         |
|    policy_gradient_loss | 0.000104    |
|    value_loss           | 37.5        |
-----------------------------------------
Eval num_timesteps=949000, episode_reward=197.55 +/- 1.15
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 949000   |
---------------------------------
Eval num_timesteps=949500, episode_reward=197.11 +/- 1.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 949500   |
---------------------------------
Eval num_timesteps=950000, episode_reward=197.55 +/- 1.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 464      |
|    time_elapsed    | 24108    |
|    total_timesteps | 950272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=950500, episode_reward=195.93 +/- 1.36
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 950500      |
| train/                  |             |
|    approx_kl            | 0.008275609 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.001       |
|    loss                 | 37.4        |
|    n_updates            | 696         |
|    policy_gradient_loss | 0.00582     |
|    value_loss           | 62.9        |
-----------------------------------------
Eval num_timesteps=951000, episode_reward=191.90 +/- 21.68
Episode length: 505.36 +/- 96.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 951000   |
---------------------------------
Eval num_timesteps=951500, episode_reward=194.02 +/- 15.43
Episode length: 515.06 +/- 69.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 951500   |
---------------------------------
Eval num_timesteps=952000, episode_reward=196.10 +/- 1.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 952000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 465      |
|    time_elapsed    | 24178    |
|    total_timesteps | 952320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=952500, episode_reward=197.58 +/- 0.87
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 952500      |
| train/                  |             |
|    approx_kl            | 0.009144599 |
|    clip_fraction        | 0.0181      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.001       |
|    loss                 | 32.4        |
|    n_updates            | 697         |
|    policy_gradient_loss | 0.0183      |
|    value_loss           | 38.7        |
-----------------------------------------
Eval num_timesteps=953000, episode_reward=197.86 +/- 0.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 953000   |
---------------------------------
Eval num_timesteps=953500, episode_reward=197.60 +/- 1.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 953500   |
---------------------------------
Eval num_timesteps=954000, episode_reward=197.70 +/- 0.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 954000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 466      |
|    time_elapsed    | 24249    |
|    total_timesteps | 954368   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=954500, episode_reward=197.77 +/- 0.53
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 954500      |
| train/                  |             |
|    approx_kl            | 0.008752271 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 0.767       |
|    n_updates            | 699         |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 14.3        |
-----------------------------------------
Eval num_timesteps=955000, episode_reward=197.80 +/- 0.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 955000   |
---------------------------------
Eval num_timesteps=955500, episode_reward=197.63 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 955500   |
---------------------------------
Eval num_timesteps=956000, episode_reward=197.68 +/- 0.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 956000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 467      |
|    time_elapsed    | 24319    |
|    total_timesteps | 956416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=956500, episode_reward=197.12 +/- 1.15
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 956500      |
| train/                  |             |
|    approx_kl            | 0.007362111 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.001       |
|    loss                 | 1.27        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00103    |
|    value_loss           | 26.2        |
-----------------------------------------
Eval num_timesteps=957000, episode_reward=197.71 +/- 0.69
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 957000   |
---------------------------------
Eval num_timesteps=957500, episode_reward=197.42 +/- 1.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 957500   |
---------------------------------
Eval num_timesteps=958000, episode_reward=197.27 +/- 1.01
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 958000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 468      |
|    time_elapsed    | 24390    |
|    total_timesteps | 958464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=958500, episode_reward=197.25 +/- 1.31
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 958500       |
| train/                  |              |
|    approx_kl            | 0.0036557764 |
|    clip_fraction        | 0.00911      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.755        |
|    learning_rate        | 0.001        |
|    loss                 | 109          |
|    n_updates            | 701          |
|    policy_gradient_loss | -0.00237     |
|    value_loss           | 79.6         |
------------------------------------------
Eval num_timesteps=959000, episode_reward=195.12 +/- 16.14
Episode length: 515.12 +/- 69.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 959000   |
---------------------------------
Eval num_timesteps=959500, episode_reward=197.54 +/- 0.97
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 959500   |
---------------------------------
Eval num_timesteps=960000, episode_reward=197.43 +/- 1.03
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=960500, episode_reward=197.30 +/- 1.16
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 960500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 469      |
|    time_elapsed    | 24478    |
|    total_timesteps | 960512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=961000, episode_reward=197.49 +/- 0.92
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 961000      |
| train/                  |             |
|    approx_kl            | 0.009799481 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.001       |
|    loss                 | 19.6        |
|    n_updates            | 702         |
|    policy_gradient_loss | 0.000305    |
|    value_loss           | 35.7        |
-----------------------------------------
Eval num_timesteps=961500, episode_reward=197.70 +/- 0.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 961500   |
---------------------------------
Eval num_timesteps=962000, episode_reward=197.65 +/- 0.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 962000   |
---------------------------------
Eval num_timesteps=962500, episode_reward=197.67 +/- 0.66
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 962500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 470      |
|    time_elapsed    | 24549    |
|    total_timesteps | 962560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=963000, episode_reward=197.90 +/- 0.35
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 963000      |
| train/                  |             |
|    approx_kl            | 0.005305298 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.001       |
|    loss                 | 3.34        |
|    n_updates            | 703         |
|    policy_gradient_loss | -0.00147    |
|    value_loss           | 23.2        |
-----------------------------------------
Eval num_timesteps=963500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 963500   |
---------------------------------
Eval num_timesteps=964000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 964000   |
---------------------------------
Eval num_timesteps=964500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 964500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 471      |
|    time_elapsed    | 24620    |
|    total_timesteps | 964608   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=965000, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 965000      |
| train/                  |             |
|    approx_kl            | 0.022724487 |
|    clip_fraction        | 0.00937     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0513      |
|    n_updates            | 708         |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 16.4        |
-----------------------------------------
Eval num_timesteps=965500, episode_reward=197.84 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 965500   |
---------------------------------
Eval num_timesteps=966000, episode_reward=197.78 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 966000   |
---------------------------------
Eval num_timesteps=966500, episode_reward=197.82 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 966500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 472      |
|    time_elapsed    | 24691    |
|    total_timesteps | 966656   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=967000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 967000      |
| train/                  |             |
|    approx_kl            | 0.006743925 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 19.5        |
|    n_updates            | 710         |
|    policy_gradient_loss | 0.00226     |
|    value_loss           | 19.2        |
-----------------------------------------
Eval num_timesteps=967500, episode_reward=197.95 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 967500   |
---------------------------------
Eval num_timesteps=968000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 968000   |
---------------------------------
Eval num_timesteps=968500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 968500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 473      |
|    time_elapsed    | 24765    |
|    total_timesteps | 968704   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=969000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 969000      |
| train/                  |             |
|    approx_kl            | 0.007645373 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 13.4        |
|    n_updates            | 713         |
|    policy_gradient_loss | -0.00147    |
|    value_loss           | 12.1        |
-----------------------------------------
Eval num_timesteps=969500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 969500   |
---------------------------------
Eval num_timesteps=970000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 970000   |
---------------------------------
Eval num_timesteps=970500, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 970500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 474      |
|    time_elapsed    | 24838    |
|    total_timesteps | 970752   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.03
Eval num_timesteps=971000, episode_reward=197.36 +/- 0.65
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 971000      |
| train/                  |             |
|    approx_kl            | 0.012672124 |
|    clip_fraction        | 0.0152      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.001       |
|    loss                 | 7.15        |
|    n_updates            | 718         |
|    policy_gradient_loss | 0.000405    |
|    value_loss           | 7.94        |
-----------------------------------------
Eval num_timesteps=971500, episode_reward=197.26 +/- 0.94
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 971500   |
---------------------------------
Eval num_timesteps=972000, episode_reward=197.31 +/- 0.79
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 972000   |
---------------------------------
Eval num_timesteps=972500, episode_reward=195.05 +/- 15.41
Episode length: 516.90 +/- 56.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 972500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 475      |
|    time_elapsed    | 24909    |
|    total_timesteps | 972800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=973000, episode_reward=197.33 +/- 0.92
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 973000       |
| train/                  |              |
|    approx_kl            | 0.0027056795 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.001        |
|    loss                 | 31.5         |
|    n_updates            | 719          |
|    policy_gradient_loss | 0.011        |
|    value_loss           | 37.2         |
------------------------------------------
Eval num_timesteps=973500, episode_reward=197.44 +/- 0.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 973500   |
---------------------------------
Eval num_timesteps=974000, episode_reward=197.42 +/- 0.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 974000   |
---------------------------------
Eval num_timesteps=974500, episode_reward=197.17 +/- 1.05
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 974500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 476      |
|    time_elapsed    | 24981    |
|    total_timesteps | 974848   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=975000, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.013151411 |
|    clip_fraction        | 0.00293     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.001       |
|    loss                 | 20.1        |
|    n_updates            | 721         |
|    policy_gradient_loss | 0.00141     |
|    value_loss           | 19.4        |
-----------------------------------------
Eval num_timesteps=975500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 975500   |
---------------------------------
Eval num_timesteps=976000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 976000   |
---------------------------------
Eval num_timesteps=976500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 976500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 477      |
|    time_elapsed    | 25052    |
|    total_timesteps | 976896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=977000, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 977000      |
| train/                  |             |
|    approx_kl            | 0.004381812 |
|    clip_fraction        | 0.0724      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.001       |
|    loss                 | 4.2         |
|    n_updates            | 722         |
|    policy_gradient_loss | -0.000707   |
|    value_loss           | 5.21        |
-----------------------------------------
Eval num_timesteps=977500, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 977500   |
---------------------------------
Eval num_timesteps=978000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 978000   |
---------------------------------
Eval num_timesteps=978500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 978500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 478      |
|    time_elapsed    | 25123    |
|    total_timesteps | 978944   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=979000, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 979000      |
| train/                  |             |
|    approx_kl            | 0.005046072 |
|    clip_fraction        | 0.0457      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 0.9         |
|    n_updates            | 724         |
|    policy_gradient_loss | 4.31e-05    |
|    value_loss           | 11.4        |
-----------------------------------------
Eval num_timesteps=979500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 979500   |
---------------------------------
Eval num_timesteps=980000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=980500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 980500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 479      |
|    time_elapsed    | 25194    |
|    total_timesteps | 980992   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=981000, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 981000       |
| train/                  |              |
|    approx_kl            | 0.0103574395 |
|    clip_fraction        | 0.0628       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.001        |
|    loss                 | 10.4         |
|    n_updates            | 726          |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 20.1         |
------------------------------------------
Eval num_timesteps=981500, episode_reward=197.86 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 981500   |
---------------------------------
Eval num_timesteps=982000, episode_reward=197.90 +/- 0.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 982000   |
---------------------------------
Eval num_timesteps=982500, episode_reward=197.92 +/- 0.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 982500   |
---------------------------------
Eval num_timesteps=983000, episode_reward=197.89 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 983000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 480      |
|    time_elapsed    | 25284    |
|    total_timesteps | 983040   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=983500, episode_reward=197.95 +/- 0.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 983500      |
| train/                  |             |
|    approx_kl            | 0.009749905 |
|    clip_fraction        | 0.00164     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.001       |
|    loss                 | 70.4        |
|    n_updates            | 728         |
|    policy_gradient_loss | 0.000415    |
|    value_loss           | 51.3        |
-----------------------------------------
Eval num_timesteps=984000, episode_reward=197.97 +/- 0.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 984000   |
---------------------------------
Eval num_timesteps=984500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 984500   |
---------------------------------
Eval num_timesteps=985000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 985000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 481      |
|    time_elapsed    | 25356    |
|    total_timesteps | 985088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=985500, episode_reward=197.80 +/- 0.46
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 985500       |
| train/                  |              |
|    approx_kl            | 0.0050300327 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.857        |
|    learning_rate        | 0.001        |
|    loss                 | 2.26         |
|    n_updates            | 729          |
|    policy_gradient_loss | 0.00236      |
|    value_loss           | 28           |
------------------------------------------
Eval num_timesteps=986000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 986000   |
---------------------------------
Eval num_timesteps=986500, episode_reward=197.87 +/- 0.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 986500   |
---------------------------------
Eval num_timesteps=987000, episode_reward=197.83 +/- 0.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 987000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 482      |
|    time_elapsed    | 25426    |
|    total_timesteps | 987136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=987500, episode_reward=197.92 +/- 0.34
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 987500       |
| train/                  |              |
|    approx_kl            | 0.0055474173 |
|    clip_fraction        | 0.0615       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.903        |
|    learning_rate        | 0.001        |
|    loss                 | 2.07         |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00176     |
|    value_loss           | 19.9         |
------------------------------------------
Eval num_timesteps=988000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 988000   |
---------------------------------
Eval num_timesteps=988500, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 988500   |
---------------------------------
Eval num_timesteps=989000, episode_reward=197.88 +/- 0.37
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 989000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 483      |
|    time_elapsed    | 25496    |
|    total_timesteps | 989184   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=989500, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 989500      |
| train/                  |             |
|    approx_kl            | 0.011053763 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 0.233       |
|    n_updates            | 732         |
|    policy_gradient_loss | -0.000645   |
|    value_loss           | 16.9        |
-----------------------------------------
Eval num_timesteps=990000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 990000   |
---------------------------------
Eval num_timesteps=990500, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 990500   |
---------------------------------
Eval num_timesteps=991000, episode_reward=197.97 +/- 0.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 991000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 484      |
|    time_elapsed    | 25567    |
|    total_timesteps | 991232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=991500, episode_reward=197.87 +/- 0.43
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 991500       |
| train/                  |              |
|    approx_kl            | 0.0069967005 |
|    clip_fraction        | 0.0421       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.854        |
|    learning_rate        | 0.001        |
|    loss                 | 21.4         |
|    n_updates            | 733          |
|    policy_gradient_loss | 0.000805     |
|    value_loss           | 38.5         |
------------------------------------------
Eval num_timesteps=992000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 992000   |
---------------------------------
Eval num_timesteps=992500, episode_reward=197.89 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 992500   |
---------------------------------
Eval num_timesteps=993000, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 993000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 485      |
|    time_elapsed    | 25637    |
|    total_timesteps | 993280   |
---------------------------------
Eval num_timesteps=993500, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 993500      |
| train/                  |             |
|    approx_kl            | 0.004969595 |
|    clip_fraction        | 0.0358      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 1.39        |
|    n_updates            | 743         |
|    policy_gradient_loss | -0.00338    |
|    value_loss           | 9.94        |
-----------------------------------------
Eval num_timesteps=994000, episode_reward=197.93 +/- 0.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 994000   |
---------------------------------
Eval num_timesteps=994500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 994500   |
---------------------------------
Eval num_timesteps=995000, episode_reward=197.97 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 995000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 486      |
|    time_elapsed    | 25708    |
|    total_timesteps | 995328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=995500, episode_reward=197.81 +/- 0.42
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 995500       |
| train/                  |              |
|    approx_kl            | 0.0047912346 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.001        |
|    loss                 | 0.682        |
|    n_updates            | 744          |
|    policy_gradient_loss | 0.0251       |
|    value_loss           | 12           |
------------------------------------------
Eval num_timesteps=996000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=996500, episode_reward=197.89 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 996500   |
---------------------------------
Eval num_timesteps=997000, episode_reward=197.91 +/- 0.41
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 997000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 487      |
|    time_elapsed    | 25779    |
|    total_timesteps | 997376   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=997500, episode_reward=197.93 +/- 0.28
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 997500     |
| train/                  |            |
|    approx_kl            | 0.01165178 |
|    clip_fraction        | 0.0144     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.001      |
|    loss                 | 1.54       |
|    n_updates            | 749        |
|    policy_gradient_loss | -0.00154   |
|    value_loss           | 14.8       |
----------------------------------------
Eval num_timesteps=998000, episode_reward=197.87 +/- 0.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 998000   |
---------------------------------
Eval num_timesteps=998500, episode_reward=197.95 +/- 0.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 998500   |
---------------------------------
Eval num_timesteps=999000, episode_reward=197.91 +/- 0.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 999000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 488      |
|    time_elapsed    | 25850    |
|    total_timesteps | 999424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=999500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 999500      |
| train/                  |             |
|    approx_kl            | 0.004534781 |
|    clip_fraction        | 0.00833     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.001       |
|    loss                 | 4.69        |
|    n_updates            | 750         |
|    policy_gradient_loss | 0.00537     |
|    value_loss           | 41.6        |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=197.94 +/- 0.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1000500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 1000500  |
---------------------------------
Eval num_timesteps=1001000, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 1001000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 489      |
|    time_elapsed    | 25921    |
|    total_timesteps | 1001472  |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-stop-4-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Early stopping at step 1 due to reaching max kl: 0.03
